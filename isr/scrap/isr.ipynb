{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import httplib2\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import codecs\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "base=\"http://pubsonline.informs.org/toc/isre/\"\n",
    "url = base+'1/1'   \n",
    "body = {}\n",
    "headers = {'Content-type': 'application/x-www-form-urlencoded'}\n",
    "http = httplib2.Http()\n",
    "response, content = http.request(url, 'POST', headers=headers, body=urllib.urlencode(body))\n",
    "headers = {'Cookie': response['set-cookie']}\n",
    "folder='vol_iss/'\n",
    "for v in range(1,28):\n",
    "    for i in range(1,5):\n",
    "        title=folder+str(v)+'_'+str(i)\n",
    "        if not os.path.isfile(title):\n",
    "            print ([v,i])\n",
    "            r=random.uniform(3, 5)\n",
    "            time.sleep(r) # delays for 3 to 5 seconds\n",
    "            url=base+str(v)+'/'+str(i)\n",
    "            response, html = http.request(url, 'GET', headers=headers)\n",
    "            f = open(title, 'w')\n",
    "            f.write(html) \n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dois=[]\n",
    "folder='vol_iss/'\n",
    "for f in os.listdir(folder):\n",
    "    if not '.' in f:\n",
    "        html=open(folder+f)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links=soup.find_all('a',{'class':'ref nowrap'})\n",
    "        dois=dois+[i['href'].replace('/doi/abs/','') for i in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use downloadCitation to get articles in batches of 100s\n",
    "dois=[('doi',i) for i in dois]\n",
    "l=list(chunks(dois,100))\n",
    "len(l)}\n",
    "\n",
    "for i in l:\n",
    "    data=i+[('include', 'abs'),('format','endnote')]\n",
    "    url = 'http://pubsonline.informs.org/action/downloadCitation'\n",
    "    r = requests.post(url, data)\n",
    "    f=codecs.open('isr_raw.txt','a', 'utf-8')\n",
    "    f.write(r.text) \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base='http://pubsonline.informs.org/doi/abs/'\n",
    "folder='htmls/'\n",
    "session = requests.Session()\n",
    "\n",
    "for doi in dois:\n",
    "    title=doi.replace('/','_')+'.html'\n",
    "    if not os.path.isfile(folder+title):\n",
    "        url= base+doi\n",
    "        response = session.get(url)\n",
    "        html=response.text\n",
    "        f = codecs.open(folder+title, \"w\", \"utf-8\")\n",
    "        f.write(html,) \n",
    "        f.close()\n",
    "        print url\n",
    "        r=random.uniform(3, 4)\n",
    "        time.sleep(r) # delays for 3 to 5 seconds"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
