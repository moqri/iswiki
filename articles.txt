 
---start---
'''Effect of Knowledge-Sharing Trajectories on Innovative Outcomes in Temporary Online Crowds'''
{{header}}
{{article
|author= Ann Majchrzak,Arvind Malhotra,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = There is substantial research on the effects of formal control structures (i.e., incentives, identities, organization, norms) on knowledge sharing leading to innovative outcomes in online communities. However, there is little research on how knowledge-sharing trajectories in temporary online crowds create innovative outcomes without these structures. Such research is particularly of interest in the context of temporary online crowds solicited with crowdsourcing in which there is only minimal structure for knowledge sharing. We identify eight types of crowdsourcing with different knowledge-sharing patterns. The focus of this study is on the one type of crowdsourcing-collaborative innovation challenges-in which there is the least restriction on knowledge sharing in the crowd. A content analysis was conducted of all time-stamped posts made in five different collaborative innovation challenges to identify different knowledge-sharing trajectories used. We found that a paradox-framed trajectory was more likely to be followed by innovative outcomes compared to three other knowledge-sharing trajectories. A paradox-framed trajectory is one in which a novel solution emerges when different participants post in the following sequence: (1) contributing a paradox associated with the problem objective, (2) sharing assumptions to validate the paradox, and (3) sharing initial ideas for resolving the paradox in a manner that meets the problem statement. Based on the findings, a theory of paradox-framed trajectories in temporary online crowds is presented along with implications for knowledge creation theories in general and online knowledge-creating communities in particular.
|keyword = innovation,sequences,crowdsourcing,knowledge,online communities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Creating Value in Online Communities: The Sociomaterial Configuring of Strategy, Platform, and Stakeholder Engagement'''
{{header}}
{{article
|author= Michael Barrett,Eivor Oborn,Wanda Orlikowski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = How is value created in an online community (OC) over time? We explored this question through a longitudinal field study of an OC in the healthcare arena. We found that multiple kinds of value were produced and changed over time as different participants engaged with the OC and its evolving technology in various ways. To explain our findings, we theorize OC value as performed through the ongoing sociomaterial configuring of strategies, digital platforms, and stakeholder engagement. We develop a process perspective to explain these dynamics and identify multiple different kinds of value being created by an OC over time: financial, epistemic, ethical, service, reputational, and platform. Our research points to the importance of expanding the notion of OC users to encompass a broader understanding of stakeholders. It further suggests that creating OC value increasingly requires going beyond a dyadic relationship between the OC and the firm to encompassing a more complex relationship involving a wider ecosystem of stakeholders.
|keyword = digital,online communities,health IT,value,computer-mediated communication and collaboration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Intellectual Property Norms in Online Communities: How User-Organized Intellectual Property Regulation Supports Innovation'''
{{header}}
{{article
|author= Julia Bauer,Nikolaus Franke,Philipp Tuertscher,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = In many online communities, users reveal innovative and potentially valuable intellectual property (IP) under conditions that entail the risk of theft and imitation. When there is rivalry and formal IP law is not effective, this could lead to underinvestment or withholding of IP, unless user-organized norms compensate for these shortcomings. This study is the first to explore the characteristics and functioning of such a norms-based IP system in the setting of anonymous, large-scale, and loose-knit online communities. To do so, we use data on the Threadless crowdsourcing community obtained through netnography, a survey, and a field experiment. On this basis, we identify an integrated system of well-established norms that regulate the use of IP within this community. We analyze the system's characteristics and functioning, and we find that the "legal certainty" it provides is conducive to cooperation, cumulative effects, and innovation. We generalize our findings from the case by developing propositions aimed to spark further research. These propositions focus on similarities and differences between norms-based IP systems in online and off-line settings, and the conditions that determine the existence of norms-based IP systems as well as their form and effectiveness in online communities. In this way, we contribute to the literatures on norms-based IP systems and online communities and offer advice for the management of crowdsourcing communities.
|keyword = intellectual property systems,social norms,innovation,online communities,crowdsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coordinating Interdependencies in Online Communities: A Study of an Open Source Software Project'''
{{header}}
{{article
|author= Aron Lindberg,Nicholas Berente,James Gaskin,Kalle Lyytinen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = To manage work interdependencies, online communities draw on a variety of arm's length coordination mechanisms offered by information technology platforms and associated practices. However, "unresolved interdependencies" remain that cannot be addressed by such arm's length mechanisms. These interdependencies reflect, for example, unidentified or emerging knowledge-based dependencies between the community members or unaccounted relationships between ongoing community tasks. At the same time, online communities cannot resort to hierarchical coordination mechanisms such as incentives or command structures to address such interdependencies. So, how do they manage such interdependencies? To address this question, we conduct an exploratory, theory-generating case study involving qualitative and computational analyses of development activities within an open source software community: Rubinius. We analyze the ongoing management of interdependencies within the community and find that unresolved interdependencies are associated with alternatively structured sequences of activities, which we define as routines. In particular, we observe that two distinct classes of interdependencies-development and developer interdependencies-are associated with alternative forms of routine variation. We identify two generalized routine components-direct implementation and knowledge integration, which address these two distinct classes of unresolved interdependencies. In particular, direct implementation deals with development interdependencies within the code that are not already coordinated through modular interfaces, while knowledge integration resolves unaccounted interdependencies between developers. We conclude with implications for research into organizing principles for online communities and note the significance of our findings for the study of coordination in organization studies in general.
|keyword = online communities,open source software,routines,interdependencies,coordination,activity variation,order variation,sequence analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward a Theory of Remixing in Online Innovation Communities'''
{{header}}
{{article
|author= Michael A. Stanko,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Within online innovation communities, remixing (i.e., the community's use of an existing innovation as source material or inspiration to aid in the development of further innovations) is an interesting form of knowledge collaboration. This study investigates an open theoretical question: Why are particular innovations remixed by online innovation communities? Some innovations languish, while others are widely remixed. Community members (even those unknown to the innovation's creator) may remix, taking the source innovation in directions the original innovator may have never imagined. Within online innovation communities, remixing is not bound by some of the constraints to knowledge collaboration found in more traditional environments. To address our research question, we begin with variables constituent to innovation diffusion theory, essentially remixing this long-established theory to predict cumulative remixing in online innovation communities, using arguments grounded in the user innovation and learning literatures. We also consider two forms of communication that are relevant to knowledge sharing in online communities (online community interaction and front page presence). Regression analysis (using data pertaining to 498 3D printable innovations) shows that community interaction is highly influential in determining which innovations are remixed by the community. Conversely, the innovation having a presence on the community's front page does not have a significant effect on remixing. Observability has an inverse-U-shaped relationship with remixing; this suggests the value placed on experiential learning. Fuzzy set qualitative comparative analysis (fsQCA) is used as a supplementary analysis technique (with robustness testing), and the results are largely consistent with regression findings but offer interesting insight into innovation configurations that consistently result in remixing from the community. Within specific configurations, fsQCA results indicate a contingent effect of observability and complexity; that is, under certain conditions, complex innovations are more likely to be remixed by the community.
|keyword = online innovation communities,remixing,user-generated content,innovation diffusion theory,3D printing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Turbulent Stability of Emergent Roles: The Dualistic Nature of Self-Organizing Knowledge Coproduction'''
{{header}}
{{article
|author= Ofer Arazy,Johannes Daxenberger,Hila Lifshitz-Assaf,Oded Nov,Iryna Gurevych,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Increasingly, new forms of organizing for knowledge production are built around self-organizing coproduction community models with ambiguous role definitions. Current theories struggle to explain how high-quality knowledge is developed in these settings and how participants self-organize in the absence of role definitions, traditional organizational controls, or formal coordination mechanisms. In this article, we engage the puzzle by investigating the temporal dynamics underlying emergent roles on individual and organizational levels. Comprised of a multilevel large-scale empirical study of Wikipedia stretching over a decade, our study investigates emergent roles in terms of prototypical activity patterns that organically emerge from individuals' knowledge production actions. Employing a stratified sample of 1,000 Wikipedia articles, we tracked 200,000 distinct participants and 700,000 coproduction activities, and recorded each activity's type. We found that participants' role-taking behavior is turbulent across roles, with substantial flow in and out of coproduction work. Our findings at the organizational level, however, show that work is organized around a highly stable set of emergent roles, despite the absence of traditional stabilizing mechanisms such as predefined work procedures or role expectations. This dualism in emergent work is conceptualized as "turbulent stability." We attribute the stabilizing factor to the artifact-centric production process and present evidence to illustrate the mutual adjustment of role taking according to the artifact's needs and stage. We discuss the importance of the affordances of Wikipedia in enabling such tacit coordination. This study advances our theoretical understanding of the nature of emergent roles and self-organizing knowledge coproduction. We discuss the implications for custodians of online communities as well as for managers of firms engaging in self-organized knowledge collaboration.
|keyword = online production communities,coproduction,Wikipedia,emergent roles,stability,mobility,artifact-centric,boundary infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Folding and Unfolding: Balancing Openness and Transparency in Open Source Communities'''
{{header}}
{{article
|author= Maha Shaikh,Emmanuelle Vaast,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Open source communities rely on the espoused premise of complete openness and transparency of source code and development process. Yet, openness and transparency at times need to be balanced out with moments of less open and transparent work. Through our detailed study of Linux Kernel development, we build a theory that explains that transparency and openness are nuanced and changing qualities that certain developers manage as they use multiple digital technologies and create, in moments of needs, more opaque and closed digital spaces of work. We refer to these spaces as digital folds. Our paper contributes to the extant literature by providing a process theory of how transparency and openness are balanced with opacity and closure in open source communities according to the needs of the development work; by conceptualizing the nature of digital folds and their role in providing quiet spaces of work; and, by articulating how the process of digital folding and unfolding is made far more possible by select elite actors' navigating the line between the pragmatics of coding and the accepted ideology of openness and transparency.
|keyword = open source communities,digital folds,transparency,openness,opacity,closure,software development work,qualitative research,archival data,ideology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Special Section Introduction-Ubiquitous IT and Digital Vulnerabilities'''
{{header}}
{{article
|author= Sam Ransbotham,Robert G. Fichman,Ram Gopal,Alok Gupta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = While information technology benefits society in numerous ways, it unfortunately also has potential to create new vulnerabilities. This special issue intends to stimulate thought and research into understanding and mitigating these vulnerabilities. We identify four mechanisms by which ubiquitous computing makes various entities (people, devices, organizations, societies, etc.) more vulnerable, including: increased visibility, enhanced cloaking, increased interconnectedness, and decreased costs. We use the papers in the special issue to explain these mechanisms, and then outline a research agenda for future work on digital vulnerabilities spanning four areas that are, or could become, significant societal problems with implications at multiple levels of analysis: Online harassment and incivility, technology-driven economic inequality, industrial Internet of Things, and algorithmic ethics and bias.
|keyword = online harassment,economic inequality,Internet of things,algorithmic bias,algorithmic ethics,artificial intelligence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing the Impact of Granular Privacy Controls on Content Sharing and Disclosure on Facebook'''
{{header}}
{{article
|author= Huseyin Cavusoglu,Tuan Q. Phan,Hasan Cavusoglu,Edoardo M. Airoldi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = We examine the role of granular privacy controls on dynamic content-sharing activities and disclosure patterns of Facebook users based on the exogenous policy change in December 2009. Using a unique panel data set, we first conduct regression discontinuity analyses to verify a discontinuous jump in context generation activities and disclosure patterns around the time of the policy change. We next estimate unobserved effects models to assess the short-run and long-run effects of the change. Results show that Facebook users, on average, increase use of wall posts and decrease use of private messages after the introduction of granular privacy controls. Also, users' disclosure patterns change to reflect the increased openness in content sharing. These effects are realized immediately and over time. More importantly, we show that user-specific factors play crucial roles in shaping users' varying reactions to the policy change. While more privacy sensitive users (those who do not reveal their gender and/or those who have exclusive disclosure patterns ex ante) share more content openly and less content secretly than before, less privacy sensitive users (those who reveal their gender and/or those who have inclusive disclosure patterns ex ante) share less content openly and more content secretly after the change. Hence, the policy change effectively diminishes variation among Facebook users in terms of content-generation activities and disclosure patterns. Therefore, characterizing the privacy change as a way to foster openness across all user categories does not reveal the change's true influence. Although an average Facebook user seems to favor increased openness, the policy change has different impacts on various groups of users based on their sensitivity to privacy, and this impact is not necessarily toward increased openness. To our knowledge, this is the first study that relies on observational data to assess the impact of a major privacy change on dynamic content-sharing activities and the resulting disclosure patterns of Facebook users.
|keyword = online social networks,privacy,privacy controls,content sharing,disclosure,openness,secrecy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''More Harm Than Good? How Messages That Interrupt Can Make Us Vulnerable'''
{{header}}
{{article
|author= Jeffrey L. Jenkins,Bonnie Brinton Anderson,Anthony Vance,C. Brock Kirwan,David Eargle,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = System-generated alerts are ubiquitous in personal computing and, with the proliferation of mobile devices, daily activity. While these interruptions provide timely information, research shows they come at a high cost in terms of increased stress and decreased productivity. This is due to dual-task interference (DTI), a cognitive limitation in which even simple tasks cannot be simultaneously performed without significant performance loss. Although previous research has examined how DTI impacts the performance of a primary task (the task that was interrupted), no research has examined the effect of DTI on the interrupting task. This is an important gap because in many contexts, failing to heed an alert-the interruption itself-can introduce critical vulnerabilities. Using security messages as our context, we address this gap by using functional magnetic resonance imaging (fMRI) to explore how (1) DTI occurs in the brain in response to interruptive alerts, (2) DTI influences message security disregard, and (3) the effects of DTI can be mitigated by finessing the timing of the interruption. We show that neural activation is substantially reduced under a condition of high DTI, and the degree of reduction in turn significantly predicts security message disregard. Interestingly, we show that when a message immediately follows a primary task, neural activity in the medial temporal lobe is comparable to when attending to the message is the only task. Further, we apply these findings in an online behavioral experiment in the context of a web-browser warning. We demonstrate a practical way to mitigate the DTI effect by presenting the warning at low-DTI times, and show how mouse cursor tracking and psychometric measures can be used to validate low-DTI times in other contexts. Our findings suggest that although alerts are pervasive in personal computing, they should be bounded in their presentation. The timing of interruptions strongly influences the occurrence of DTI in the brain, which in turn substantially impacts alert disregard. This paper provides a theoretically grounded, cost-effective approach to reduce the effects of DTI for a wide variety of interruptive messages that are important but do not require immediate attention.
|keyword = multitasking,dual-task interference,security message,information security,Amazon Mechanical Turk,laboratory experimentation,fMRI,NeuroIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When Being Hot Is Not Cool: Monitoring Hot Lists for Information Security'''
{{header}}
{{article
|author= Yonghua Ji,Subodha Kumar,Vijay Mookerjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = We study operational and managerial problems arising in the context of security monitoring where sessions, rather than raw individual events, are monitored to prevent attacks. The objective of the monitoring problem is to maximize the benefit of monitoring minus the monitoring cost. The key trade-off in our model is that as more sessions are monitored, the attack costs should decrease. However, the monitoring cost would likely increase with the number of sessions being monitored. A key step in solving the problem is to derive the probability density of a system with n sessions being monitored with a session's age measured as the time elapsed since it last generated a suspicious event. We next optimize the number of sessions monitored by trading off the attack cost saved with the cost of monitoring. A profiling step is added prior to monitoring and a resulting two-dimensional optimization problem is studied. Through numerical simulation, we find that a simple size-based policy is quite robust for a very reasonable range of values and, under typical situations, performs almost as well as the two more sophisticated policies do. Also, we find that adopting a simplified policy without using the option of managing sessions using age threshold can greatly increase the ease of finding an optimal solution, and reduce operational overhead with little performance loss compared with a policy using such an option. The insights gained from the mechanics of profiling and monitoring are leveraged to suggest a socially optimal contract for outsourcing these activities in a reward-based contract. We also study penalty-based contracts. Such contracts (specifically, when the penalty is levied as a percentage of the monthly service fee) do not achieve the social optimum. We show how an appropriate penalty coefficient can be chosen to implement a socially optimal penalty-based contract. In addition, we provide a high-level comparison between reward- and penalty-based contracts. In a penalty-based contract, the setting of the fixed payment can be challenging because it requires additional knowledge of the total expected malicious event rate, which needs to be observed through a period of no monitoring.
|keyword = IT security,monitoring and profiling,outsourcing,optimization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Excessive Dependence on Mobile Social Apps: A Rational Addiction Perspective'''
{{header}}
{{article
|author= Hyeokkoo Eric Kwon,Hyunji So,Sang Pil Han,Wonseok Oh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Drawing on the rational addiction framework, this study explores the digital vulnerabilities driven by dependence on mobile social apps (e.g., social network sites and social games). Rational addicts anticipate the future consequences of their current behaviors and attempt to maximize utility from their intertemporal consumption choices. Conversely, myopic addicts tend toward immediate gratification and fail to fully recognize the future consequences of their current consumption. In lieu of conducting self-report surveys or aggregate-level demand estimation, this research examines addictive behaviors on the basis of consumption quantity at an individual level. To empirically validate rational addiction in the context of social app consumption, we collect and analyze 13-month, individual-level panel data on the weekly app usage of thousands of smartphone users. Results indicate that the average social app user conducts herself in a forward-looking manner and rationally adjusts consumption over time to derive optimal utility. The subgroup analysis, however, indicates that substantial variations in addictiveness and forward-looking propensities exist across demographically diverse groups. For example, addictive behaviors toward social network sites are more myopic in nature among older, less-educated, high-income groups. Additionally, the type of social app moderates the effects of demographic characteristics on the nature of addictive behaviors. We provide implications that policymakers can use to effectively manage mobile addiction problems, with the recommendations focusing on asymmetric social policies (e.g., information-and capacity-enhancing measures).
|keyword = addiction,mobile,rational addiction,myopic addiction,digital vulnerability,IT and health impact,mobile social apps,app consumption,econometrics,panel data,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Fake Reviews on Online Visibility: A Vulnerability Assessment of the Hotel Industry'''
{{header}}
{{article
|author= Theodoros Lappas,Gaurav Sabnis,Georgios Valkanas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Extant research has focused on the detection of fake reviews on online review platforms, motivated by the well-documented impact of customer reviews on the users' purchase decisions. The problem is typically approached from the perspective of protecting the credibility of review platforms, as well as the reputation and revenue of the reviewed firms. However, there is little examination of the vulnerability of individual businesses to fake review attacks. This study focuses on formalizing the visibility of a business to the customer base and on evaluating its vulnerability to fake review attacks. We operationalize visibility as a function of the features that a business can cover and its position in the platform's review-based ranking. Using data from over 2.3 million reviews of 4,709 hotels from 17 cities, we study how visibility can be impacted by different attack strategies. We find that even limited injections of fake reviews can have a significant effect and explore the factors that contribute to this vulnerable state. Specifically, we find that, in certain markets, 50 fake reviews are sufficient for an attacker to surpass any of its competitors in terms of visibility. We also compare the strategy of self-injecting positive reviews with that of injecting competitors with negative reviews and find that each approach can be as much as 40% more effective than the other across different settings. We empirically explore response strategies for an attacked hotel, ranging from the enhancement of its own features to detecting and disputing fake reviews. In general, our measure of visibility and our modeling approach regarding attack and response strategies shed light on how businesses that are targeted by fake reviews can detect and tackle such attacks.
|keyword = customer reviews,fake reviews,knowledge management,vulnerability assessment,decision support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why Do Adults Engage in Cyberbullying on Social Media? An Integration of Online Disinhibition and Deindividuation Effects with the Social Structure and Social Learning Model'''
{{header}}
{{article
|author= Paul Benjamin Lowry,Jun Zhang,Chuang Wang,Mikko Siponen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = The dramatic increase in social media use has challenged traditional social structures and shifted a great deal of interpersonal communication from the physical world to cyberspace. Much of this social media communication has been positive: Anyone around the world who has access to the Internet has the potential to communicate with and attract a massive global audience. Unfortunately, such ubiquitous communication can be also used for negative purposes such as cyberbullying, which is the focus of this paper. Previous research on cyberbullying, consisting of 135 articles, has improved the understanding of why individuals-mostly adolescents-engage in cyberbullying. However, our study addresses two key gaps in this literature: (1) how the information technology (IT) artifact fosters/inhibits cyberbullying and (2) why people are socialized to engage in cyberbullying. To address these gaps, we propose the social media cyberbullying model (SMCBM), which modifies Akers' [Akers RL (2011) Social Learning and Social Structure: A General Theory of Crime and Deviance, 2nd ed. (Transaction Publishers, New Brunswick, NJ)] social structure and social learning model. Because Akers developed his model for crimes in the physical world, we add a rich conceptualization of anonymity composed of five subconstructs as a key social media structural variable in the SMCBM to account for the IT artifact. We tested the SMCBM with 1,003 adults who have engaged in cyberbullying. The empirical findings support the SMCBM. Heavy social media use combined with anonymity facilitates the social learning process of cyberbullying in social media in a way that fosters cyberbullying. Our results indicate new directions for cyberbullying research and implications for anticyberbullying practices.
|keyword = cyberbullying,cyberstalking,cyberharassment,social media,social media cyberbullying model,SMCBM,neutralization,anonymity,disinhibition,deindividuation,differential association,differential reinforcement,definition,imitation,social structure and social learning model,SSSL model,social learning,social learning theory,SLT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ELECTRONIC HEALTH RECORDS: HOW CAN IS RESEARCHERS CONTRIBUTE TO TRANSFORMING HEALTHCARE?'''
{{header}}
{{article
|author= Rajiv Kohli,Sharon Swee-Lin Tan,
|source= MIS QUARTERLY
|year= 2016
|abstract = Electronic health records (EHR) facilitate integration of patient health history for planning safe and proper treatment. Combined with data analytics, aggregate-level EHR enable examination and development of effective medicines and therapies for chronic diseases. Although promising efforts to implement EHRs are underway, social and organizational challenges plague EHR development and widespread use. These challenges are due to lingering issues such as privacy, interoperability, and security among key stakeholders (patients, providers, and purveyors). Based upon stakeholders' needs and the issues, we identify two primary thematic areas-integration and analytics-in which the information systems (IS) discipline can contribute to EHRs. Through the accumulated body of knowledge, IS researchers are well positioned and have the expertise to design, develop, and facilitate the use of EHR in the delivery of healthcare. We identify potential research opportunities in each of the two thematic areas that have the potential to transform the delivery of healthcare. We conclude with a recommendation for IS scholars to collaborate with allied healthcare disciplines in order to advance the use of EHR to improve patient care.
|keyword = Electronic health records,information systems research,healthcare,electronic medical records,integration,analytics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ADVERTISING VERSUS BROKERAGE MODEL FOR ONLINE TRADING PLATFORMS'''
{{header}}
{{article
|author= Jianqing Chen,Ming Fan,Mingzhi Li,
|source= MIS QUARTERLY
|year= 2016
|abstract = The two leading online consumer-to-consumer platforms use very different revenue models: eBay.com in the United States uses a brokerage model in which sellers pay eBay on a transaction basis, whereas Taobao.com in China uses an advertising model in which sellers can use the basic platform service for free and pay Taobao for advertising services to increase their exposure. This paper studies how the chosen revenue model affects the revenue of a platform, buyers' payoffs, sellers' payoffs, and social welfare. We find that when little space can be dedicated to advertising under the advertising model, the brokerage model generates more revenue for the platform than the advertising model. When a significant proportion of space is dedicated to advertising under the advertising model, matching probability on a platform plays a critical role in determining which revenue model can generate more revenue: If the matching probability is high, the brokerage model generates more revenue; otherwise, the advertising model generates more revenue. Buyers are always better off under the advertising model because of larger participation by the sellers in the platform's free service. Sellers are better off under the advertising model in most scenarios. The only exception is when the matching probability is low and the platform dedicates considerable space to advertising. Under these conditions, the sellers with payoffs similar to the marginal advertiser who is indifferent about advertising can be worse off under the advertising model. Finally, the advertising model generates more social welfare than the brokerage model.
|keyword = Online platforms,two-sided markets,business model,revenue model,advertising,commission,customer to customer,Taobao,eBay,analytical modeling,economic modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE COMPENSATORY INTERACTION BETWEEN USER CAPABILITIES AND TECHNOLOGY CAPABILITIES IN INFLUENCING TASK PERFORMANCE: AN EMPIRICAL ASSESSMENT IN TELEMEDICINE CONSULTATIONS'''
{{header}}
{{article
|author= Christina Serrano,Elena Karahanna,
|source= MIS QUARTERLY
|year= 2016
|abstract = Although technology-enabled task performance has been a long-standing outcome of interest in information systems research, existing studies primarily emphasize characteristics of the technology and task, rather than the user, in shaping performance outcomes. Given that both technology and people have inherent limitations, a worthwhile research pursuit is to examine how one might compensate for the limitations of the other in order to achieve successful task performance. We propose a new conceptualization of user abilities, task-specific user capabilities, and examine their compensatory effects with technology capabilities in shaping performance outcomes within the context of e-consultations (i.e., technology-mediated expert consultations). Specifically, we theorize the user capabilities of presentation (information giving) and elicitation (information seeking) as the task-specific user capabilities in this context. Leveraging the theory of compensatory adaptation, we propose that these user capabilities can overcome the limitations of technology and result in successful task performance outcomes. We employ mixed methods (qualitative field study, survey field study, and a lab experiment) to develop and test our model within the context of telemedicine consultations, a form of e-consultation. Convergent findings across the studies suggest that both user capabilities and technology capabilities are important facilitators of task performance and that these capabilities compensate for each other.
|keyword = Task-technology fit,system use outcomes,task performance,user capabilities,technology capabilities,telemedicine,telehealth,health information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TV'S DIRTY LITTLE SECRET: THE NEGATIVE EFFECT OF POPULAR TV ON ONLINE AUCTION SALES'''
{{header}}
{{article
|author= Oliver Hinz,Shawndra Hill,Ju-Young Kim,
|source= MIS QUARTERLY
|year= 2016
|abstract = Timing online auctions to attract a large number of prospective buyers is important for sellers. This study examines whether online auction sellers need to account for exogenous effects like TV viewing when timing and predicting their auction results. An ongoing debate questions whether TV viewers can spread their attention across multiple devices while watching TV, for example, by concurrently shopping online or posting on social media. Recent research has focused on understanding cross-media effects; however, little attention has been given to TV viewership's relationship with a very important economic activity, namely participation in online auctions. We examine this potential cross-media effect by analyzing the four-year sales history of a German online auction platform and addressing potential endogeneity problems with an instrumental variable approach. We use three different instrumental variables that have different advantages and disadvantages but can, in sum, be used for triangulation as they lead to the same result. The analyses reveal a significant negative crossmedia effect between TV consumption and online auction sales, indicating that TV consumption and online auction sales might compete for the scarce attention of consumers and are thus substitutes for each other rather than complements.
|keyword = Cross-media effects,online auctions,attention economy,instrumental variable approach,second screen,electronic commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLORING BIDDER HETEROGENEITY IN MULTICHANNEL SEQUENTIAL B2B AUCTIONS'''
{{header}}
{{article
|author= Yixin Lu,Alok Gupta,Wolfgang Ketter,Eric van Heck,
|source= MIS QUARTERLY
|year= 2016
|abstract = The proliferation of online auctions has attracted significant research interest in understanding real-life bidding behavior. However, most of the empirical work has focused on business-to-consumer (B2C) auctions. A natural question is whether the findings obtained from B2C auctions are applicable to business-to-business (B2B) auctions, which often involve much higher stakes. In this paper, we examine how professional bidders choose their bidding strategies in multichannel, sequential B2B auctions. Using an extensive data set from the world's largest B2B market for cut flowers, we find a stable taxonomy of bidding behavior and identify five distinctive bidding strategies. In addition, we demonstrate that bidders' choice of strategies is associated with their demand, budget constraint, and transaction cost. These findings challenge the conventional view that bidders' bidding strategies will converge as they gain experience. We also analyze the economic impacts of different strategies. Our results provide useful implications for practical design of B2B auctions.
|keyword = Auction design,B2B auctions,bidder taxonomy,sequential auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CAPTURING THE COMPLEXITY OF MALLEABLE IT USE: ADAPTIVE STRUCTURATION THEORY FOR INDIVIDUALS'''
{{header}}
{{article
|author= Kurt W. Schmitz,James T. C. Teng,Kimberly J. Webb,
|source= MIS QUARTERLY
|year= 2016
|abstract = The confluence of widely available malleable technology and the "bring your own device" (BYOD) trend creates a new dynamic for information technology innovation in the workplace. Nontechnical users are empowered to adapt pliable technology in the course of normal usage episodes. We develop a theoretical perspective of adaptation behaviors by extending the adaptive structuration theory (AST) to the level of individuals, and present a topology of adaptation behaviors to capture the rich landscape of this emerging phenomenon. Based on this new theoretical perspective, we propose a research model and perform a survey study targeting young professionals to empirically investigate adaptation of malleable IT by users. Our findings reveal the compounding effects of four distinct adaptation behaviors including the insight that task adaptation mediates the effect of technology adaptation on individual performance. This study contributes by providing a theoretical framework for examining adaptation behaviors, extending AST to the level of individuals, and addressing specific criticisms of AST in the information systems literature.
|keyword = Technology adaptation,task adaptation,exploration,exploitation,structuration episodes,post adoption IT use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SENIOR EXECUTIVES' IT MANAGEMENT RESPONSIBILITIES: SERIOUS IT-RELATED DEFICIENCIES AND CEO/CFO TURNOVER'''
{{header}}
{{article
|author= Adi Masli,Vernon J. Richardson,Marcia Weidenmier Watson,Robert W. Zmud,
|source= MIS QUARTERLY
|year= 2016
|abstract = While the information systems scholarly and practice literatures both stress the importance of senior executive engagement with IT management, the recommendations for doing so remain, at best, limited and general. Examining the influence of serious IT-related deficiencies on CEO/CFO turnover within the post-SOX financial reporting context, specific CEO/CFO IT management responsibilities are identified: CEOs are shown to be held accountable for global IT management responsibilities, and CFOs are shown to be held accountable for demand-side IT management responsibilities. Implications for information systems research, management research, and information systems practice are provided.
|keyword = IT management,executive responsibilities,managerial delegation,executive turnover,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PIRATES IN THE LAB: USING INCENTIVIZED CHOICE EXPERIMENTS TO EXPLORE PREFERENCE FOR (UN) AUTHORIZED CONTENT'''
{{header}}
{{article
|author= Piotr Cwiakowski,Marek Giergiczny,Michal Krawczk,
|source= MIS QUARTERLY
|year= 2016
|abstract = In this paper, we report on a laboratory experiment aimed at investigating factors affecting choice between different versions of a full-length movie. In particular, we are able to pin down the willingness to pay for a legal, rather than pirated, copy and compare it to the impact of such characteristics as picture quality or delay in delivery. We find a modest but highly significant preference for the authorized version. We also find that when the proceeds from legal sales are transferred to a good cause, willingness to pay for the unauthorized copy is reduced. Our method does not seem to suffer from hypothetical bias.
|keyword = Digital piracy,choice experiments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE MAKING OF A GOOD IMPRESSION: INFORMATION HIDING IN AD EXCHANGES'''
{{header}}
{{article
|author= Zhen Sun,Milind Dawande,Ganesh Janakiraman,Vijay Mookerjee,
|source= MIS QUARTERLY
|year= 2016
|abstract = In this paper, we examine information revelation designs and policies in ad exchanges that use a second-price auction mechanism. Two auction designs are studied: one-call and two-call. Under the one-call design, the ad exchange makes one call to all bidders at the beginning of an auction. Under the two-call design, in addition to the call to all bidders at the beginning of the auction, the exchange calls out the winning bidder at the end of the auction; this second call enables the winning bidder to match the right advertiser for the impression. Thus, the two-call design requires a higher level of technical sophistication but offers to the auction site the choice of the timing and the extent of information released to bidders about an impression. While valuations are private to bidders, there are two possibilities as far as the information available to the ad exchange on these bidder valuations is concerned: One, the ad exchange has no reliable knowledge about bidder valuations. For this situation, we develop simple information revelation policies that do not use any knowledge of the valuations and establish their performance guarantees. Two, the ad exchange has distributional knowledge about bidder valuations. For this situation, we develop an informed heuristic that exploits this information. While the heuristic continues to offer the same performance guarantee as that of the simple policies, we show that its performance on a comprehensive test bed is near-optimal. The welfare implications of the information revelation policy of the ad exchange on other stakeholders of the ecosystem are also analyzed.
|keyword = Ad exchanges,information revelation designs,heuristic policies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REVISITING GROUP-BASED TECHNOLOGY ADOPTION AS A DYNAMIC PROCESS: THE ROLE OF CHANGING ATTITUDE-RATIONALE CONFIGURATIONS'''
{{header}}
{{article
|author= Petra Saskia Bayerl,Kristina Lauche,Carolyn Axtell,
|source= MIS QUARTERLY
|year= 2016
|abstract = In this study, we set out to better understand the dynamics behind group-based technology adoption by investigating the underlying mechanisms of changes in collective adoption decisions over time. Using a longitudinal multi-case study of production teams in the British oil and gas industry, we outline how internally or externally triggered modifications to the constellation of adoption rationales and attitudes toward a focal technology between subgroups caused changes to adoption decisions within a team. The constellations further seemed to impact usage patterns including conflicts about ICT use and the stability of adoption. Based on these observations, we suggest that group-based adoption can be differentiated in qualitatively different technology adoption states (TAS), which emerge as the result of disparate attitude-rationale configurations across subgroups in a user collective. With this reconceptualization of collective adoption as technology adoption states, our study extends current group-based models by providing a new, qualitative lens to view the creation and stability of adoption patterns in complex user groups. With this, our study offers a process view on the (dis)continuance of information systems and provides a basis for practical guidelines on how to deal with problematic adoption situations when actors from multiple (sub) groups are involved.
|keyword = Technology adoption,collective adoption,diversity,distribution,group valence,process view,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PARTICIPATION IN OPEN KNOWLEDGE COMMUNITIES AND JOB-HOPPING: EVIDENCE FROM ENTERPRISE SOFTWARE'''
{{header}}
{{article
|author= Peng Huang,Zhongju Zhang,
|source= MIS QUARTERLY
|year= 2016
|abstract = Using longitudinal data of IT professionals' activities in the SAP Community Network, and the career histories of these professionals obtained from LinkedIn, we investigate the relationship between an individual's participation in Internet-enabled open knowledge communities and a major event of his/her career development: job-hopping. We measure individual participation in open knowledge communities by two dimensions of related activities: contribution and learning. We provide empirical evidence that contribution to knowledge communities leads to a higher likelihood of job-hopping, yet a greater amount of learning is associated with a higher probability of retention. We argue that the effect of contribution can be attributed to job market signaling and the effect of learning is primarily driven by enhanced job performance and career advancement within the current organization. A series of robustness tests were conducted to address the self-selection bias and to rule out some possible alternative explanations to these mechanisms. Our work contributes to the existing body of literature on networks of practice and provides supporting evidence that participation in these networks indeed leads to career benefits and status advancements. Additionally, our study takes the first step to fill the gap in the current literature on voluntary employee turnover that has so far ignored the impacts of employee participation in external knowledge communities, thus providing both theoretical and practical insights in the area of organizational research.
|keyword = Knowledge community,network of practice,knowledge management,online forums,job-hopping,career development,enterprise software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Secret Admirers: An Empirical Examination of Information Hiding and Contribution Dynamics in Online Crowdfunding'''
{{header}}
{{article
|author= Gordon Burtch,Anindya Ghose,Sunil Wattal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Individuals' actions in online social contexts are growing increasingly visible and traceable. Many online platforms account for this by providing users with granular control over when and how their identity or actions are made visible to peers. However, little work has sought to understand the effect that a user's decision to conceal information might have on observing peers, who are likely to refer to that information when deciding on their own actions. We leverage a unique impression-level data set from one of the world's largest online crowdfunding platforms, where contributors are given the option to conceal their username or contribution amount from public display, with each transaction. We demonstrate that when campaign contributors elect to conceal information, it has a negative influence on subsequent visitors' likelihood of conversion, as well as on their average contributions, conditional on conversion. Moreover, we argue that social norms are an important driver of information concealment, providing evidence of peer influence in the decision to conceal. We discuss the implications of our results for the provision of online information hiding mechanisms, as well as the design of crowdfunding platforms and electronic markets more generally.
|keyword = crowdfunding,social norms,information hiding,peer influence,anonymity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do Incentive Hierarchies Induce User Effort? Evidence from an Online Knowledge Exchange'''
{{header}}
{{article
|author= Paulo B. Goes,Chenhui Guo,Mingfeng Lin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = To motivate user contributions, user-generated content sites routinely deploy incentive hierarchies, where users achieve increasingly higher statuses in the community after achieving increasingly more difficult goals. Yet the existing empirical literature remains largely unclear whether such hierarchies are indeed effective in inducing user contributions. We gather data from a large online crowd-based knowledge exchange to answer this question, and draw on goal setting and status hierarchy theories to study users' contributions before and after they reach consecutive ranks on a vertical incentive hierarchy. We find evidence that even though these glory-based incentives may motivate users to contribute more before the goals are reached, user contribution levels drop significantly after that. The positive effect on user contribution appears only temporary. Moreover, such impacts are increasingly smaller for higher ranks. Our results highlight some unintended and heretofore undocumented effects of incentive hierarchies, and have important implications for business models that rely on user contributions, such as knowledge exchange and crowdsourcing, as well as the broader phenomenon of gamification in other contexts.
|keyword = online knowledge exchange,motivation,status,incentive hierarchy,goals,effort,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Co-Creation in Information Technology Projects: A Differential Games Approach'''
{{header}}
{{article
|author= Emre M. Demirezen,Subodha Kumar,Bala Shetty,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = We study the relationship between a client and a vendor in value co-creation environments such as information technology projects. We consider that the client gets utility from the project throughout the development period and that the effort levels are not verifiable if not monitored. The output is contingent on the effort levels of each party, and we allow these effort levels to be dynamic. Hence, the client needs to optimally decide the terms of payment structures so as to maximize her net value. We analyze the performance of different payment structures and find the best one for the client in diverse settings. We show that the remaining time of the project and the client's valuation of the project regulate the behavior of the effort levels and some other characteristics in the collaboration. We derive the conditions under which the client chooses not to monitor the vendor's effort and operates in a double moral hazard environment. In addition, we find that the equilibrium effort levels or the values the parties gain from the collaboration do not necessarily increase when the output becomes more sensitive to either party's effort. Based on the results of our model, we present several other managerial insights.
|keyword = value co-creation,collaboration,information technology services,differential game,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Competing Ads on Click Performance in Sponsored Search'''
{{header}}
{{article
|author= Ashish Agarwal,Tridas Mukhopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing lowquality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.
|keyword = sponsored search,hierarchical Bayesian estimation,online advertising,online auctions,search engine marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Implementation of an Information and Communication Technology in a Developing Country: A Multimethod Longitudinal Study in a Bank in India'''
{{header}}
{{article
|author= Viswanath Venkatesh,Hillol Bala,V. Sambamurthy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Developing countries, such as India and China, are the fastest growing economies in the world. The successful implementation of information and communication technologies (ICTs) in these countries is likely to hinge on a set of institutional factors that are shaped by the environmental tension between two competing forces, emergent catalysts, such as new economic policies and reform programs, and traditional challenges, such as infrastructure and traditional value systems. To unearth the temporal dynamics underlying the success and failure of ICT implementations in organizations in developing countries, we conducted a two-year multimethod study of an ICT implementation at a large bank in India. Based on data collected from over 1,000 employees and over 1,000 customers, we found, relative to preimplementation levels for up to two years postimplementation, that we characterized as the shakedown phase (1) operational efficiency did not improve, (2) job satisfaction declined, and (3) customer satisfaction declined. In-depth interviews of approximately 40 members of top management, 160 line employees, and 200 customers indicated that these outcomes could be attributed to the strong influence of a set of institutional factors, such as ICT-induced change, labor economics, Western isomorphism, parallel-manual system, and technology adaptation. The interplay between these institutional factors and the environmental tension posed a formidable challenge for the bank during our study that led to the poor and unintended outcomes.
|keyword = developing countries,ICT implementation,operational efficiency,job satisfaction,customer satisfaction,institutional factors,environmental tension,traditional challenges,emergent catalysts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Should Online Content Providers Be Allowed To Subsidize Content?-An Economic Analysis'''
{{header}}
{{article
|author= Soohyun Cho,Liangfei Qiu,Subhajyoti Bandyopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Internet service providers (ISPs) are experimenting with a business model that allows content providers (CPs) to subsidize Internet access for end consumers. In this study, we develop a game-theoretical model to analyze the effects of this sponsorship of consumer data usage. We find that the ISP's optimal network management choice of data sponsorship crucially depends on market conditions, such as the revenue rates of CPs and the fit cost of consumers. If the fit cost is low, the ISP will either allow both CPs to subsidize consumers' Internet access, or will allow only the more competitive CP to subsidize, depending on the per-consumer revenue generation rates of CPs. If the fit cost is high, it is in the ISPs interest not to allow any subsidization. We also identify conditions under which the ISP's network management choices of data sponsorship deviate from social optimum. These results should be of interest to the telecom industry as it searches additional revenue models, and to online CPs competing for customer loyalty. It should also be of interest to policymakers investigating into this issue.
|keyword = Internet service provider,online content provider,usage subsidization,consumer surplus,social welfare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pricing Data Services: Pricing by Minutes, by Gigs, or by Megabytes per Second?'''
{{header}}
{{article
|author= Ying-Ju Chen,Ke-Wei Huang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Motivated by the pervasive discrepancy among the pricing schemes of data services, this paper investigates the selection of pricing metrics (variables) and the corresponding pricing plans. We construct a stylized model in which a monopoly data services seller faces heterogeneous consumers whose utilities depend on the usage and the connection speed. We examine three options for the seller to conduct the second-degree (indirect) price discrimination: by minutes, by gigabytes (Gigs), and by megabytes per second (Mbps). We show that the after-sales self-selection behaviors have a significant impact on the seller's profitability, and it leads to a first-order influence on the pricing metric selection. We prove that either pricing by Gigs or Mbps can be optimal. Pricing by Gigs can dominate pricing by Mbps even if the consumer's utility is more sensitive in changes in the connection speed. We also find that when incorporating the bandwidth costs or congestion costs, pricing by Mbps becomes more attractive as it allows the seller to directly control the congestion effect. These findings may help practitioners to develop their own pricing plans and pricing metrics selection.
|keyword = service pricing,price discrimination,versioning,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Open Content, Linus' Law, and Neutral Point of View'''
{{header}}
{{article
|author= Shane Greenstein,Feng Zhu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = The diffusion of the Internet and digital technologies has enabled many organizations to use the open-content production model to produce and disseminate knowledge. While several prior studies have shown that the open-content production model can lead to high-quality output in the context of uncontroversial and verifiable information, it is unclear whether this production model will produce any desirable outcome when information is controversial, subjective, and unverifiable. We examine whether the open-content production model helps achieve a neutral point of view (NPOV) using data from Wikipedia's articles on U.S. politics. Our null hypothesis builds on Linus' Law, which argues that with enough eyeballs, all bugs are shallow. Our findings are consistent with a narrow interpretation of Linus' Law, namely, a greater number of contributors to an article makes an article more neutral. No evidence supports a broad interpretation of Linus' Law. Moreover, several empirical facts suggest the law does not shape many articles. The majority of articles receive little attention, and most articles change only mildly from their initial slant. Our study provides the first empirical evidence on the limit of Linus' Law. While many organizations believe that they could improve their knowledge production by leveraging communities, we show that in the case of Wikipedia, there are aspects, such as NPOV, that the community does not always achieve successfully.
|keyword = open innovation,Wikipedia,open content,Linus' Law,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT Outsourcing and the Impact of Advisors on Clients and Vendors'''
{{header}}
{{article
|author= Ravi Bapna,Alok Gupta,Gautam Ray,Shweta Singh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = There is significant information asymmetry in the information technology (IT) outsourcing market. Clients are uncertain about vendors' capabilities and vendors are uncertain about clients' requirements. Prior literature has examined many devices to reduce such information asymmetry, e.g., vendor reputation, client-vendor prior relationship, vendors' Capability Maturity Model (CMM) rating, vendor location, and technological diversity of the vendor. We examine the impact of (to our knowledge) a hitherto unconsidered device, i.e., the use of an advisor. In the context of global sourcing, third-party advisors, with their accumulated knowledge of client requirements and the vendor landscape, can mitigate the information asymmetry between clients and vendors. However, in an extensive data set of IT outsourcing contracts going back two decades we found use of advisors to be rare (less than 5% of contracts go through an advisor). This motivates us to rigorously analyze their impact on clients and vendors as an open empirical question. Using a data set of 753 large IT outsourcing contracts, and through a series of econometric specifications and robustness tests, we establish that the presence of an advisor is associated with higher revenue for vendors and more positive contract outcomes. This analysis presents what is to our knowledge the first concrete evidence that third-party advisors can mitigate the information asymmetry in the IT outsourcing market and lead to better matching that benefits clients as well as vendors.
|keyword = IT outsourcing,advisor,information asymmetry,propensity score matching,coarsened exact matching,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing Promotion Ladders to Mitigate Turnover of IT Professionals'''
{{header}}
{{article
|author= Frank MacCrory,Vidyanand Choudhary,Alain Pinsonneault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Chronic excessive turnover among information technology (IT) professionals has been costly to firms for decades with annual turnover rates as high as 24% even among Computerworld's "100 Best Places to Work in IT." Prior information systems literature has identified two key factors affecting turnover: boundary-spanning roles and low promotability in one's current firm. We draw on tournament theory, which is primarily concerned with inducing effort in employees, to decompose promotability into two distinct constructs: the likelihood of promotion and benefit from promotion, and demonstrate that each has a distinct role in affecting turnover rates. Our key result is that a job ladder motivating IT professionals with large, infrequent promotions will lead to higher turnover than a job ladder with smaller, more frequent promotions. We describe the conditions under which rearranging the job ladder creates economic value for the firm. We also offer an explanation for the observation that jobs characterized by boundary-spanning activities have higher turnover, and show that such jobs are more sensitive to the effect of likelihood of promotion on turnover. We test our hypotheses on a detailed data set covering 5,704 IT professionals over a five-year period. We confirm that likelihood of promotion has the predicted effects on turnover of IT professionals. A one standard deviation increase in likelihood of promotion decreases turnover by over 99%, consistent with our prediction. The empirical analysis also confirms the predicted effects of boundary spanning activities.
|keyword = IT turnover,tournament theory,econometrics,career concerns,management of IT human resources,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the Continuance of Secure Behavior: A Longitudinal Field Study of Mobile Device Authentication'''
{{header}}
{{article
|author= Paul John Steinbart,Mark J. Keith,Jeffry Babb,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = It is not enough to get information technology (IT) users to adopt a secure behavior. They must also continue to behave securely. Positive outcomes of secure behavior may encourage the continuance of that behavior, whereas negative outcomes may lead users to adopt less-secure behaviors. For example, in the context of authentication, login success rates may determine whether users continue to use a strong credential or switch to less secure behaviors (e.g., storing a credential or changing to a weaker, albeit easier to successfully enter, credential). Authentication is a particularly interesting security behavior for information systems researchers to study because it is affected by an IT artifact (the design of the user interface). Laptops and desktop computers use full-size physical keyboards. However, users are increasingly adopting mobile devices, which provide either miniature physical keypads or touchscreens for entering authentication credentials. The difference in interface design affects the ease of correctly entering authentication credentials. Thus, the move to use of mobile devices to access systems provides an opportunity to study the effects of the user interface on authentication behaviors. We extend existing process models of secure behaviors to explain what influences their (dis) continuance. We conduct a longitudinal field experiment to test our predictions and find that the user interface does affect login success rates. In turn, poor performance (login failures) leads to discontinuance of a secure behavior and the adoption of less-secure behaviors. In summary, we find that a process model reveals important insights about how the IT artifact leads people to (dis) continue secure behaviors.
|keyword = continuance of security behavior,security behaviors,authentication,password,passphrase,mobile computing,smartphone,usability,user interface,longitudinal research,field experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sustainability of Healthcare Information Exchanges: A Game-Theoretic Approach'''
{{header}}
{{article
|author= Emre M. Demirezen,Subodha Kumar,Arun Sen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Based on our interactions with the key personnel of three different healthcare information exchange (HIE) providers in Texas, we develop models to study the sustainability of HIEs and participation levels in these networks. We first examine how heterogeneity among healthcare practitioners (HPs) (in terms of their expected benefit from the HIE membership) affects participation of HPs in HIEs. We find that, under certain conditions, low-gain HPs choose not to join HIEs. Hence, we explore several measures that can encourage more participation in these networks and find that it might be beneficial to (i) establish a second HIE in the region, (ii) propose more value to the low-gain HPs, or (iii) offer or incentivize value-added services. We present several other interesting and useful results that are somewhat counterintuitive. For example, increasing the highest benefit the HPs can get from the HIE might decrease the number of HPs that want to join the HIE. Furthermore, since the amount of funds from the government and the other agencies often changes (and will eventually cease), we analyze how the changes in the benefit HPs obtain from the HIE affect (i) participation in the network, (ii) the HIE subscription fee and the fee for value-added service, (iii) the number of HPs that request value-added service, and (iv) the net values of the HIE provider and HPs. We also provide guidelines for policy makers and HIE providers that may help them improve the sustainability of HIEs and increase the participation levels in these networks.
|keyword = healthcare management,HIE networks,network externalities,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does Product Market Competition Drive CVC Investment? Evidence from the US IT Industry'''
{{header}}
{{article
|author= Keongtae Kim,Anandasivam Gopal,Gerard Hoberg,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = We study the effect of product market competition on the propensity to use corporate venture capital (CVC) as a part of an information technology (IT) firm's innovation strategy. Using novel measures of product market competition based on product descriptions from firm 10-K statements and accounting for potential endogeneity, we investigate how product market competition between 1997 and 2007 relates to the magnitude of CVC spending. We first find that firms in competitive markets make higher research and development (R&D) and CVC investments. In addition, we find that increasing product market competition leads to a shift away from internal R&D spending and into CVC. These movements are significantly stronger for technology leaders, i.e., firms with deep patent stocks, in the IT industry. We also find that CVC appears to be an effective way of exploiting external knowledge for technology leaders in the IT-producing industry, but not for technology slow starters. CVC investments lead to significantly more patent applications for technology leaders but no appreciable difference for slow starters. Our results provide new insights for theories of innovation in competitive, dynamic markets, potentially as part of a portfolio that includes internal R&D as well as open innovation models.
|keyword = information technology,product market competition,corporate venture capital,technology leadership,innovation,econometrics,text analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product Line Extension in Consumer Software Markets in the Presence of Free Alternatives'''
{{header}}
{{article
|author= Aaron Baird,Chadwick J. Miller,T. S. Raghu,Rajiv K. Sinha,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Hypercompetitive consumer software markets pit incumbents against free alternatives and pirates. Although the extant literature has studied firm level strategic responses to consumer heterogeneity and piracy, there is a lack of understanding of consumer reactions to digital goods choice sets that include firm product extensions such as the introduction of premium or free alternatives. With context-dependent preferences as the theoretical basis, this study systematically examines the impact of piracy controls and product line extensions on welfare in a consumer software market context (i.e., willingness to pay (WTP) and changes in consumer and producer surplus). In two controlled experiments using double-bound-dichotomous-choice WTP elicitation, we investigate how piracy controls and product line extensions impact two different platforms of the same software (PC Adobe applications and mobile Adobe applications) in terms of propensity to pirate and WTP. We show that introducing a premium or free vertical extension has different impacts on consumers' WTP for the focal product depending on whether it is a low-cost or high-cost market even when controlling for individual differences, such as price fairness perceptions, product feature value, brand perceptions, etc. By contrast, piracy controls reduce piracy rates but have a limited impact on consumer WTP for the focal product in both contexts. By calculating the overall welfare of the market, we show that there is alignment in consumer and producer interests at current and estimated optimal price levels in both high-cost and low-cost markets. However, the introduction of a free product extension leads to a higher surplus in the high-cost market, whereas the introduction of the premium product extension leads to a higher surplus in the low-cost market.
|keyword = product line extensions,versioning,free alternatives,willingness to pay,piracy,context-dependent preferences,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Framing Innovation Opportunities While Staying Committed to an Organizational Epistemic Stance'''
{{header}}
{{article
|author= Anne-Laure Fayard,Emmanouil Gkeredakis,Natalia Levina,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = This paper examines how an organization's culture, and in particular its stance toward the pursuit of knowledge and innovation, matters when confronting new digitally enabled practices for generating novel insights. We draw on an in-depth interpretive study of how two innovation consulting firms encountered crowdsourcing for innovation. Our findings suggest that, although both organizations relied on a similar set of organizational arrangements in their daily consulting work, they enacted different positions vis-a-vis crowdsourcing: one firm further experimented with it, whereas the other rejected it altogether. These different positions emerged as organizational actors examined, framed, and evaluated crowdsourcing as an alternative for generating knowledge. To interpret these findings, we draw on philosophy of science and develop the concept of organizational epistemic stance, defined as an attitude that organizational actors collectively enact in pursuing knowledge. Our analysis suggests that when organizational actors encounter and explore information technology-enabled practices, such as crowdsourcing and big data analytics, they are likely to remain committed to their epistemic stance to frame such practices and judge their potential value for pursuing knowledge. This paper contributes to our understanding of encounters with, and adoption and diffusion of, new organizational practices and offers new ways of thinking about crowdsourcing.
|keyword = knowledge creation,interpretive research,IT innovation adoption,philosophy of science,epistemic stance,crowdsourcing,organizational culture,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitating the Transformational: An Exploration of Control in Cyberinfrastructure Projects and the Discovery of Field Control'''
{{header}}
{{article
|author= Gregory D. Moody,Laurie J. Kirsch,Sandra A. Slaughter,Brian Kimball Dunn,Qin Weng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Emerging from rapid advances in digitization and technological capabilities is a new form of information systems development project: cyber projects. Cyber projects are complex, massive, and ambitious, often involving hundreds of academic, government, and industry professionals, requiring years of development, and costing millions of dollars. In our study, we examine how control is exercised in cyber projects. Based on a longitudinal study over eight years, we develop a process theory of the control of cyber projects. Initially we observe that project control is driven by the field, i.e., all of the individual or collective entities that subscribe to the general purpose of the project. This form of control is later replaced by a more bureaucratic form from government-sponsored entities to ensure that traditional project objectives are met. Once construction begins and the field understands the implications and promise of the project, we observe that control is again exerted by the primary project users in the field, complemented by authority-based control exerted by the government-sponsored entisty in the field.
|keyword = control,project management,cyberinfrastructure projects,strategic action fields,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Characteristics and Economic Consequences of Jump Bids in Combinatorial Auctions'''
{{header}}
{{article
|author= Pallab Sanyal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Jump bidding, which refers to bidding above the minimum necessary, is a robust behavior that has been observed in a variety of ascending auctions in the field as well as the laboratory. However, the phenomenon has yet to be studied in combinatorial auctions, which are a type of multiobject auction that allows bidders to bid on a set of objects. Such auctions have been found to be beneficial when objects exhibit synergy, e.g., are complementary. In this paper, we explore jump bidding behavior in combinatorial auctions as a function of design choices of the mechanism. In particular, we examine the effects of price revelation schemes on the nature and extent of jump bidding. Furthermore, we study the effects of jump bidding on the economic performance of the auctions. To conduct our study, first, we develop hypotheses using auction theories and behavioral theories of how people use reference prices as anchors, and second, we conduct a laboratory experiment to test our hypotheses and examine bidder behavior. We find that the nature of the prices that the auctioneer chooses to offer as feedback to the bidders can considerably influence their jump bidding behavior, leading to significant differences in auction outcomes. We demonstrate that in combinatorial auctions, in addition to the theories of jump bidding proposed in the literature, bounded rationality of the bidders plays a part in the nature and extent of jump bidding. Our study reveals that in the cognitively challenging package-bidding environment, bidders often pursue computationally frugal but suboptimal heuristics. Our results have important policy implications for mechanism designers.
|keyword = jump bidding,combinatorial auctions,price revelation,bidder behavior,bounded rationality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On the Ontological Quality and Logical Quality of Conceptual-Modeling Grammars: The Need for a Dual Perspective'''
{{header}}
{{article
|author= Roger Clarke,Andrew Burton-Jones,Ron Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = A core activity in information systems development involves building a conceptual model of the domain that an information system is intended to support. Such models are created using a conceptual-modeling (CM) grammar. Just as high-quality conceptual models facilitate high-quality systems development, high-quality CM grammars facilitate high-quality CM. This paper provides a new perspective on ways to improve the quality of the semantics of CM grammars. For many years, the leading approach to this topic has relied on ontological theory. We show, however, that the ontological approach captures only half the story; it needs to be coupled with a logical approach. We explain how the ontological and logical qualities of CM grammars interrelate. Furthermore, we outline three contributions of a logical approach to evaluating the quality of CM grammars: a means of seeing some familiar CM problems in simpler ways, illumination of new problems, and proving the benefit of modifying existing CM grammars in particular ways. We demonstrate these benefits in the context of the Entity-Relationship grammar. More generally, our paper opens a new area of research with many opportunities for future research and practice.
|keyword = conceptual modeling,semantics,ontology,logic,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leader Influence on Sustained Participation in Online Collaborative Work Communities: A Simulation-Based Approach'''
{{header}}
{{article
|author= Wonseok Oh,Jae Yun Moon,Jungpil Hahn,Taekyung Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = From the perspective of leader-member exchange theory, we investigate how two forms of leadership style (uniform leader-member exchange (ULMX) and differential leader-member exchange (DLMX)) impact member participation in online collaborative work communities (OCWC). Furthermore, based on computer simulations, we also examine the moderating impact of key contextual factors on the relationship between leadership style and member contributions. Efficacy of leadership style in OCWCs is greatly influenced by environmental conditions. DLMX is more effective in sustaining member commitment under high environmental uncertainty, regardless of network size and structure. ULMX is more effective in decentralized structures and during the early stage of community growth. The simulation-based insights suggest that supervisory behavior does matter to member retention and sustained participation in OCWCs, but its impact is significantly moderated by many contextual factors, such as community size, structure, maturity, and environmental uncertainty. In certain situations ULMX prevails, but in others DLMX is more effective. These two forms of governance in fact complement each other, rather than being mutually exclusive forms of leadership style. To attain a maximal outcome, leaders should flexibly adapt their governance styles between DLMX and ULMX over the life cycle of an OCWC to maximize member retention and performance benefits.
|keyword = online collaborative work communities (OCWC),leadership style,uniform LMX,differentiated LMX,sustained participation,network structure,network size,network maturity,computer simulations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Content and Collaboration: An Affiliation Network Approach to Information Quality in Online Peer Production Communities'''
{{header}}
{{article
|author= Gerald C. Kane,Sam Ransbotham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = The 15-year history of collaboration on Wikipedia offers insight into how peer production communities create knowledge. In this research, we combine disparate content and collaboration approaches through a social network analysis approach known as an affiliation network. It captures both how knowledge is transferred in a peer production network and also the underlying skills possessed by its contributors in a single methodological approach. We test this approach on the Wikipedia articles dedicated to medical information developed in a subcommunity known as a WikiProject. Overall, we find that the position of an article in the affiliation network is associated with the quality of the article. We further investigate information quality through additional qualitative and quantitative approaches including expert coders using medical students, crowdsourcing using Amazon Mechanical Turk, and visualization using network graphs. A review by fourth-year medical students indicates that the Wikipedia quality rating is a reliable measure of information quality. Amazon Mechanical Turk ratings, however, are a less reliable measure of information quality, reflecting observable content characteristics such as article length and the number of references.
|keyword = social media,information quality,network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Internet-Enabled Move to the Market in Logistics'''
{{header}}
{{article
|author= Fengmei Gong,Barrie R. Nault,Mohammad S. Rahman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Logistics outsourcing has increased with the commercialization of the Internet, implying a reduction in the corresponding transaction costs. The Internet-with its universal connectivity and open standards-radically enhanced information technology (IT) capabilities, and we hypothesize this has reduced external transaction costs relatively more than internal governance costs. Using transaction cost theory as a lens, we examine whether the commercialization of the Internet coincided with a move to the market in logistics-one of the most connected industries in the economy. We estimate the relationship between IT and outsourced logistics in a production function based on two data sets from 1987 to 2008. We find that the effects of IT on outsourced logistics have changed in the post-Internet era. After the commercialization of the Internet, an industry's own IT investment and outsourced logistics became complements, whereas they were not before. It suggests that because of the unique characteristics of the Internet as an enabler, IT reduced external transaction costs relatively more than internal governance costs. Consequently, industries favored the market form of the provision of logistics. We also find similar impacts of customers' IT investments on a focal industry's outsourced logistics. Previous studies argued that IT led to the shift from hierarchies to markets, or provided indirect evidence through measures of firm size or integration. Using a production theory model, our study provides systematic empirical evidence to support that the Internet enabled a move to the market in the provision of logistics.
|keyword = information technology,IT,organizational boundaries,hierarchies and markets,logistics outsourcing,IT spillovers,production function framework,input-output tables,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Gamification of Technology-Mediated Training: Not All Competitions Are the Same'''
{{header}}
{{article
|author= Radhika Santhanam,De Liu,Wei-Cheng Milton Shen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Gamification, an application of game design elements to non-gaming contexts, is proposed as a way to add engagement in technology-mediated training programs. Yet there is hardly any information on how to adapt game design elements to improve learning outcomes and promote learner engagement. To address this issue, we focus on a popular game design element, competition, and specifically examine the effects of different competitive structures, i.e., whether a person faces a higher-skilled, lower-skilled or equally-skilled competitor, on learning and engagement. We study a gamified training design for databases, where trainees play a trivia-based mini-game with a competitor after each e-training module. Trainees who faced a lower-skilled competitor reported higher self-efficacy beliefs and better learning outcomes, supporting the effect of peer appraisal, a less examined aspect of social cognitive theory. Yet trainees who faced equally-skilled competitors reported higher levels of engagement, supporting the balance principle of flow theory. Our study findings indicate that no one competitive structure can simultaneously address learning and engagement outcomes. The choice of competitive structures depends on the priority of the outcomes in training. Our findings provide one explanation for the mixed findings on the effect of competitive gamification designs in technology mediated training.
|keyword = gamification,laboratory experiment,social cognitive theory,technology-mediated learning,competition,flow,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE DUALITY OF EMPOWERMENT AND MARGINALIZATION IN MICROTASK CROWDSOURCING: GIVING VOICE TO THE LESS POWERFUL THROUGH VALUE SENSITIVE DESIGN'''
{{header}}
{{article
|author= Xuefei (Nancy) Deng,K. D. Joshi,Robert D. Galliers,
|source= MIS QUARTERLY
|year= 2016
|abstract = Crowdsourcing (CS) of micro tasks is a relatively new, open source work form enabled by information and communication technologies. While anecdotal evidence of its benefits abounds, our understanding of the phenomenon's societal consequences remains limited. Drawing on value sensitive design (VSD), we explore microtask CS as perceived by crowd workers, revealing their values as a means of informing the design of CS platforms. Analyzing detailed narratives of 210 crowd workers participating in Amazon's Mechanical Turk (MTurk), we uncover a set of nine values they share: access, autonomy, fairness, transparency, communication, security, accountability, making an impact, and dignity. We find that these values are implicated in four crowdsourcing structures: compensation, governance, technology, and microtask. Two contrasting perceptions-empowerment and marginalization-coexist, forming a duality of microtask CS. The study contributes to the CS and VSD literatures, heightens awareness of worker marginalization in microtask CS, and offers guidelines for improving CS practice. Specifically, we offer recommendations regarding the ethical use of crowd workers (including for academic research), and call for improving MTurk platform design for greater worker empowerment.
|keyword = Crowdsourcing,societal impacts,crowd worker value,ICT ethics,empowerment,marginalization,value sensitive design,open source,Amazon's Mechanical Turk,microsourcing,gig economy,on-demand workforce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ARE SOCIAL MEDIA EMANCIPATORY OR HEGEMONIC? SOCIETAL EFFECTS OF MASS MEDIA DIGITIZATION IN THE CASE OF THE SOPA DISCOURSE'''
{{header}}
{{article
|author= Shaila M. Miranda,Amber Young,Emre Yetgin,
|source= MIS QUARTERLY
|year= 2016
|abstract = Mass media digitization is an unfolding phenomenon, posing novel societal opportunities and challenges that researchers are beginning to note. We build on and extend MIS research on process digitization and digital versus traditional communication media to study how and to what extent social media-one form of digital mass media-are emancipatory (i.e., permitting wide-spread participation in public discourse and surfacing of diverse perspectives) versus hegemonic (i.e., contributing to ideological control by a few). While a pressing concern to activists and scholars, systematic study of this issue has been elusive, owing partially to the complexity of the emancipation and hegemony concepts. Using a case study approach, we iteratively engaged with data on the discourse surrounding the Stop Online Piracy Act (SOPA) and source literature to identify six facets of interpretive media packages (i.e., competing social constructions of an issue) as measurable constructs pertinent to emancipation and hegemony. These facets included three structural constraints (on authorship, citation, and influence) and three content restrictions (on frames, signatures, and emotion). We investigated propositions regarding effects of social versus traditional media and lean versus rich social media on these interpretive media package facets by comparing the SOPA discourse across two lean traditional and social media (newspapers and Twitter) and two rich traditional and social media (television and YouTube). Our findings paradoxically revealed social media to be emancipatory with regard to structural constraints, but hegemonic with regard to an important content restriction (i.e., frames). Lean social media mitigated structural advantages and exacerbated content problems. These findings suggest that, as with traditional media, some inevitable evils accompany the societal benefits of social media and that mass media is having a detrimental effect on public discourse. We offer practical steps by which private and public institutions may counter this effect, theoretical implications for wider consideration of the six interpretive media package facets proposed here, and encouragement to MIS researchers to increase their efforts to compare different digitized processes so that a more comprehensive theory of the effects of different forms of digitized processes can be developed.
|keyword = Social media,traditional mass media,social construction of meaning,network analysis,case study,content analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGITAL ACTION REPERTOIRES AND TRANSFORMING A SOCIAL MOVEMENT ORGANIZATION'''
{{header}}
{{article
|author= Lisen Selander,Sirkka L. Jarvenpaa,
|source= MIS QUARTERLY
|year= 2016
|abstract = An emerging research agenda focuses on social media's influence on political activism. Specific attention has recently been paid to digital social movement organizing and action repertoire development. The literature acknowledges the changing face of activism at the movement level, but little is known about the relationship between social movement organizations (SMOs) and digital action repertoires. Understanding this relationship is critical because strong adherence to values is at the heart of establishing action repertoires with legitimacy and persistence. In this paper, we rely on a two-year longitudinal study of the Swedish affiliate of Amnesty International. We examine the transformation in engagement and interaction that followed the organization's introduction of new action repertoires. Drawing on resource mobilization theory and the collective action space model, we elaborate how new action repertoires both stabilized and challenged the values of the SMO, as well as gradually broadened the interactions of supporters and deepened their modes of engagement. We offer a value-based model on the antecedents and effects of new action repertoires from the SMO perspective. The empirical findings and the model build new theory on social media and digital activism at the organizational level, complementing the predominant movement level research in the extant literature.
|keyword = Collaboration,organization,societal change,case study,networks and communities,digital activism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMBATING INFANT MORTALITY IN RURAL INDIA: EVIDENCE FROM A FIELD STUDY OF EHEALTH KIOSK IMPLEMENTATIONS'''
{{header}}
{{article
|author= Viswanath Venkatesh,Arun Rai,Tracy Ann Sykes,Ruba Aljafari,
|source= MIS QUARTERLY
|year= 2016
|abstract = The United Nations' Millennium Development Goals listed high infant mortality rates as a major problem in developing countries, especially in rural areas. Given the powerful information dissemination capabilities, information and communication technologies (ICTs) have been suggested as interventions to build infant care awareness and to modify healthcare behaviors. We examine how the use of one ICT intervention-specifically, eHealth kiosks disseminating authenticated and accessible medical information-can alleviate the problem of high infant mortality in rural India. We investigate how mothers' social networks affect their use of eHealth kiosks, seeking professional medical care for their infants and, ultimately, infant mortality. Drawing on the social epidemiology and social networks literatures, we focus on advice and hindrance from both strong and weak ties as the conduit of social influence on mothers' health-related behaviors for the care of their infants. Over a period of 7 years, we studied 4,620 infants across 10 villages where the eHealth kiosks were implemented along with support resources for proxy use. The results revealed that (1) eHealth kiosk use promotes seeking professional medical care and reduces infant mortality, (2) mothers are especially vulnerable to hindrance from both strong and weak ties as they choose to maintain the status quo of traditional infant healthcare practices (e.g., reliance on untrained personnel, superstitions, fatalism) in villages, and (3) advice from both strong and weak ties offers the potential to break down misplaced beliefs about infant healthcare practices and to develop literacy on seeking professional medical care. In contrast, in a comparative group of 10 neighboring villages, the reduction in infant mortality was not as pronounced and the effect of professional medical care in reducing infant mortality was lower. Our findings suggest that an ICT intervention can effectively address one of society's most important problems (i.e., infant mortality) even in parts of the world with limited resources and deep suspicion of technology and change. Overall, we believe such an ICT intervention will complement other investments being made, including the facilitation of use (proxy use) and provision of professional medical facilities to reduce infant mortality.
|keyword = Social networks,strong ties,weak ties,infant mortality,Millennium Development Goals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE INTERNET AND RACIAL HATE CRIME: OFFLINE SPILLOVERS FROM ONLINE ACCESS'''
{{header}}
{{article
|author= Jason Chan,Anindya Ghose,Robert Seamans,
|source= MIS QUARTERLY
|year= 2016
|abstract = This research note reports on an empirical investigation of the effect of the Internet on racial hate crimes in the United States from the period 2001-2008. We find evidence that, on average, broadband availability increases racial hate crimes. We also document that the Internet's impact on these hate crimes is not uniform in that the positive effect is stronger in areas with higher levels of racism, which we identify as those with more segregation and a higher proportion of racially charged search terms, but not significant in areas with lower levels of racism. We analyze in depth whether Internet access will enhance hate group operations but find no support for the idea that this mechanism is driving the result. In contrast, we find that online access is increasing the incidence of racial hate crimes executed by lone wolf perpetrators. Several other mechanisms that could be driving the results are described. Overall, our results shed light on one of the many offline societal challenges from increased online access.
|keyword = Internet,broadband,online-offline interaction,hate crime,hate groups,race,econometrics,panel models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION AND COMMUNICATION TECHNOLOGY AND THE SOCIAL INCLUSION OF REFUGEES'''
{{header}}
{{article
|author= Antonio Diaz Andrade,Bill Doolin,
|source= MIS QUARTERLY
|year= 2016
|abstract = The social inclusion of newly resettled refugees is a significant issue confronting both refugees and their host societies. Information and communication technologies (ICTs) are increasingly viewed as a useful resource in programs that provide settlement services or promote participation in society. This paper moves beyond the conventional discussion on the digital divide to explore what people are actually able to do and achieve with ICTs. We draw on an analysis of the use of ICTs for particular purposes by more than 50 resettled refugees to develop an explanation of the process by which ICT use contributes to their social inclusion. We propose that ICT constitutes a resource from which a set of five valuable capabilities is derived: to participate in an information society, to communicate effectively, to understand a new society, to be socially connected, and to express a cultural identity. In realizing these capabilities through ICT use, refugees exercise their agency and enhance their well-being in ways that assist them to function effectively in a new society and regain control over their disrupted lives.
|keyword = Information and communication technology,social inclusion,refugees,capability approach,well-being,agency,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DOES INFORMATION AND COMMUNICATION TECHNOLOGY LEAD TO THE WELL-BEING OF NATIONS? A COUNTRY-LEVEL EMPIRICAL INVESTIGATION'''
{{header}}
{{article
|author= Kartik K. Ganju,Paul A. Pavlou,Rajiv D. Banker,
|source= MIS QUARTERLY
|year= 2016
|abstract = This paper examines the role of information and communication technology (ICT) in enhancing the well-being of nations. Extending research on the role of ICT in the productivity of nations, we posit that the effects of ICT may not be limited to productivity (e.g., GDP), and we argue that the use of ICT can also improve the wellbeing of a country by helping citizens to develop their social capital and achieve social equality, enabling access to health-related information and health services, providing education to disadvantaged communities, and facilitating commerce. Using a number of empirical specifications, specifically a fixed-effects model and an instrumental variable approach, our results show that the level of ICT use (number of fixed telephones, Internet, mobile phones) in a country predict a country's well-being (despite accounting for GDP and several other control variables that also predict a country's well-being). Furthermore, by using an exploratory method (biclustering) of identifying both country-specific and ICT-specific variables simultaneously, we identify clusters of countries with similar patterns in terms of their use of ICT, and we show that not all countries increase their level of well-being by using ICT in the same manner. Interestingly, we find that less developed countries increase their level of well-being with mobile phones primarily, while more developed countries increase their level of well-being with any ICT system. Contributions and implications for enhancing the wellbeing of nations with ICT are discussed.
|keyword = Country well-being,ICT investments,ICT policy,ICT use,ICT adoption,effects of ICT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EVOLUTION OF AN ICT PLATFORM-ENABLED ECOSYSTEM FOR POVERTY ALLEVIATION: THE CASE OF EKUTIR'''
{{header}}
{{article
|author= Srivardhini K. Jha,Alain Pinsonneault,Laurette Dube,
|source= MIS QUARTERLY
|year= 2016
|abstract = This paper analyzes the pioneering work of eKutir, a social business in India that leverages an information and communication technology (ICT) platform to progressively build a self-sustaining ecosystem to address multiple facets of smallholder farmer poverty. The study reveals that eKutir's ecosystem has evolved through five distinct phases, each expanding the number and type of actors engaged and the breadth of ICT-supported services provided. The evolution displays a distinct pattern where the five elements of the ecosystem progressively evolve and reinforce one another to create a system that is economically sustainable, scalable, and can accelerate transformative change. The study has important implications for the design of emergent ICT platforms, which can enable an ecosystem-based approach to address complex problems.
|keyword = Poverty alleviation,evolution,collaboration,loosely coupled integration,ICT-enabled ecosystem,scalability,ICT platform,Development 2.0,societal transformation,convergent innovation,ICT for sustainability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MULTIAGENT COMPETITIVE GAMING PLATFORM TO ADDRESS SOCIETAL CHALLENGES'''
{{header}}
{{article
|author= Wolfgang Ketter,Markus Peters,John Collins,Alok Gupta,
|source= MIS QUARTERLY
|year= 2016
|abstract = The shift toward sustainable electricity systems is one of the grand challenges of the 21st century. Decentralized production from renewable sources, electric mobility, and related advances are at odds with traditional power systems where central, large-scale generation of electricity follows inelastic consumer demand. Information systems innovations can enable new forms of dynamic electricity trading that leverage real-time consumption information and that use price signals to incentivize sustainable consumption behaviors. However, the best designs for these innovations, and the societal implications of different design choices, are largely unclear. We are addressing these challenges through the Power Trading Agent Competition (Power TAC), a competitive gaming platform on which numerous research groups now jointly devise, benchmark, and improve IS-based solutions to the sustainable electricity challenge. Based on the Power TAC community's results, we give preliminary empirical evidence for the efficacy of competitive gaming platforms, and for the community's contributions toward resolving the sustainable electricity challenge.
|keyword = Competitive benchmarking,design science,energy informatics,energy information systems,competitive intelligent agents,research competitions,smart grids,sustainability,virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MULTIPLEX APPROPRIATION IN COMPLEX SYSTEMS IMPLEMENTATION: THE CASE OF BRAZIL'S CORRESPONDENT BANKING SYSTEM'''
{{header}}
{{article
|author= Paul M. Leonardi,Diane E. Bailey,Eduardo H. Diniz,Dan Sholler,Bonnie Nardi,
|source= MIS QUARTERLY
|year= 2016
|abstract = When user needs do not align with system designers' visions, new technology implementation becomes a complex process as users appropriate the new technology to meet their needs. Prior studies recognize this complexity, but focus on the complex implementation of simple systems in which user groups are well defined and the IT artifact is the primary change. We extend the research lens by examining the implementation of the Brazilian correspondent banking system, a complex system involving multiple actors, system elements, and settings intended to address the social problem of financial exclusion. Our case study comparison of two settings-retail stores and post offices-reveals that actors' appropriations extended beyond the IT artifact to include technical, role, usage, social, and policy appropriations. The intended users (poor clients in remote and underserved areas) barely interacted with the IT artifact or other system elements; instead, they relied upon remote bankers (correspondents) to appropriate the system on their behalf. Because rewards, incentives, and constraints differed by setting, correspondents' appropriations differed by setting. We call the resulting mix of appropriations across multiple elements by multiple actors in multiple settings multiplex appropriation. Complex societal challenges often involve multiple users in multiple settings with varied needs and few technology skills; thus, designing systems to meet user requirements may prove impossible. Instead, allowing multiplex appropriation might foster system success because, rather than forcing a global alignment among system elements or trying to ascertain multiple user needs, it allows for multiple local alignments of system elements that fit local settings.
|keyword = Technology use,appropriation,multiplexity,complexity,complex systems,implementation,ICT4D,developing countries,user-centered design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EMERGENCE OF SELF-ORGANIZING E-COMMERCE ECOSYSTEMS IN REMOTE VILLAGES OF CHINA: A TALE OF DIGITAL EMPOWERMENT FOR RURAL DEVELOPMENT'''
{{header}}
{{article
|author= Carmen Leong,Shan L. Pan,Sue Newell,Lili Cui,
|source= MIS QUARTERLY
|year= 2016
|abstract = The emergence of Alibaba's Taobao (e-commerce) Villages in remote China has challenged the assumption that rural, underserved communities must always be the recipients of aid to stimulate ICT-enabled development. Based on an in-depth case study of two remote villages in China, this research note shows how ICT (e-commerce) can empower a marginalized community, giving rise to a rural e-commerce ecosystem that can aid self-development. We propose the concept of digital empowerment to explicate our findings in the exploration of community-driven development: first, we identify the critical actors of a rural e-commerce ecosystem and how they use ICTs; second, we illustrate how the same ICT can be used for different affordances by the actors in the evolution of a rural e-commerce ecosystem. The paper also presents unintended consequences of rural e-commerce development. We conclude with suggestions on how to make ICT useful for rural development and, in doing this, challenge some of the prevailing theoretical arguments about this process.
|keyword = ICT-enabled rural development,rural e-commerce ecosystem,digital empowerment,social consequences of ICT,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IDENTITY VERIFICATION AND SOCIETAL CHALLENGES: EXPLAINING THE GAP BETWEEN SERVICE PROVISION AND DEVELOPMENT OUTCOMES'''
{{header}}
{{article
|author= Kathy McGrath,
|source= MIS QUARTERLY
|year= 2016
|abstract = The capability to produce a secure, reliable form of identification on request is taken for granted by many citizens, especially those living in countries with advanced economies. This capability provides numerous development benefits for individuals, from accessing government and business services to establishing their right of residence and employment in a region. Furthermore, nationwide use of reliable means of identification can help to combat crime and illegal immigration. Efforts to introduce identity verification services in Nigeria have been presented by policymakers as an intervention that would lead to a wide range of such development outcomes. However, these benefits are proving difficult to realize. The use of identity smart cards aims to improve the current situation in which most Nigerian citizens do not possess reliable means of identifying themselves by, say, an international passport or driving license. Although IS research is well aware that the provision of a service does not of itself deliver development outcomes, the nature and role of ICT-based services in development is not well understood. Therefore, this research contributes in two ways. First, it directly addresses the relationship between ICTs and development policies and outcomes, with which much IS research engages minimally or not at all. Second, it explains citizens' suspicion of the intervention in Nigeria and then uses secondary data from more successful cases to address the question of why some countries achieve desired development outcomes from the provision of identity verification services while others do not.
|keyword = Social mechanism,trust,distrust,suspicion,ambivalence,national identity cards,comparative study,socioeconomic development,financial reform,generative mechanism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ICT, INTERMEDIARIES, AND THE TRANSFORMATION OF GENDERED POWER STRUCTURES'''
{{header}}
{{article
|author= Elisa Oreglia,Janaki Srinivasan,
|source= MIS QUARTERLY
|year= 2016
|abstract = Information and communication technologies (ICTs) are believed to hold much potential to empower women, both socially and economically, in low-income and rural communities. In this paper, we focus on rural women who mediate ICT use as telecenter operators in India and as helpers and enablers for family members in rural China. We explore under what circumstances they may be able to renegotiate existing gendered power structures. We argue that acts of reconciling or confronting the different spaces they inhabit can allow intermediaries to remake their own identities and positions in their community. This process, rather than the potential associated with ICTs, is where spaces for empowerment often lie.
|keyword = Gender,intermediaries,Global South,China,India,ICT,mobile phones,telecenters,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''YOU CAN'T BRIBE A COMPUTER: DEALING WITH THE SOCIETAL CHALLENGE OF CORRUPTION THROUGH ICT'''
{{header}}
{{article
|author= Shirish C. Srivastava,Thompson S. H. Teo,Sarv Devaraj,
|source= MIS QUARTERLY
|year= 2016
|abstract = Despite the influence of information and communication technologies (ICTs) on enhancing transparency and fairness, there is limited theoretical understanding of how ICT affects corruption. Adopting an institutional perspective, we conceptualize the mechanisms through which e-government influences corruption in a nation. Specifically, we theorize the relationship between e-government and corruption at two levels: (1) base corruption observed in national institutions (political, legal, and media institutions), and (2) permeated corruption in the national stakeholder service systems (business and citizen systems). Using panel data from 63 countries over a 4-year period, we test the direct and mediated effects of e-government on corruption in national institutions and stakeholder service systems, respectively. This exploratory study provides preliminary insights into the mechanisms through which corruption manifests in a nation and demonstrates how e-government can be helpful in alleviating it. In addition, the study offers important implications that we believe will be instrumental in stimulating future research on the subject.
|keyword = Corruption,e-government,institutions,ICT impact,base corruption,permeated corruption,stakeholder service systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On the Longitudinal Effects of IT Use on Firm-Level Employment'''
{{header}}
{{article
|author= Hilal Atasoy,Rajiv D. Banker,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = The effect of information technology (IT) on employment is a crucial question in today's economy given the increased digitization of work. To analyze the relationship between IT use and firm-level employment, we examine the longitudinal role of IT use in the firm's total number of employees. Our data set comes from the emerging economy of Turkey, and it represents firms of different sizes and industries. The data capture the firm's use of enterprise applications, such as enterprise resource planning and customer relationship management, and the use of Web applications, such as e-banking and e-government. Our empirical specifications exploit both within-firm and between-firm variations to show the positive effect of IT use on firm-level employment, which varies across IT applications over time. Interestingly, we find that the effects of the use of enterprise applications materialize after two years, whereas the effects of the use of Web applications are realized in the current year. We also examine whether the role of IT use in firm-level employment are moderated by firm size, average wage rate, and industry technology intensity. The long-term effects of the use of enterprise applications on firm-level employment are more pronounced in larger firms, with higher average wages, and in high-technology industries. The results are robust to alternative specifications and tests that address causality and endogeneity concerns. Implications for research, practice, and public policy are discussed.
|keyword = IT use,firm-level employment,longitudinal effects of IT,causality tests,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Rate or Trade? Identifying Winning Ideas in Open Idea Sourcing'''
{{header}}
{{article
|author= Ivo Blohm,Christoph Riedl,Johann Fueller,Jan Marco Leimeister,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Information technology (IT) has created new patterns of digitally-mediated collaboration that allow open sourcing of ideas for new products and services. These novel sociotechnical arrangements afford finely-grained manipulation of how tasks can be represented and have changed the way organizations ideate. In this paper, we investigate differences in behavioral decision-making resulting from IT-based support of open idea evaluation. We report results from a randomized experiment of 120 participants comparing IT-based decision-making support using a rating scale (representing a judgment task) and a preference market (representing a choice task). We find that the rating scale-based task invokes significantly higher perceived ease of use than the preference market-based task and that perceived ease of use mediates the effect of the task representation treatment on the users' decision quality. Furthermore, we find that the understandability of ideas being evaluated, which we assess through the ideas' readability, and the perception of the task's variability moderate the strength of this mediation effect, which becomes stronger with increasing perceived task variability and decreasing understandability of the ideas. We contribute to the literature by explaining how perceptual differences of task representations for open idea evaluation affect the decision quality of users and translate into differences in mechanism accuracy. These results enhance our understanding of how crowdsourcing as a novel mode of value creation may effectively complement traditional work structures.
|keyword = crowdsourcing,computer-mediated communication and collaboration,decision support systems,idea evaluation,rating scales,preference markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Comparing Open and Sealed Bid Auctions: Evidence from Online Labor Markets'''
{{header}}
{{article
|author= Yili Hong,Chong (Alex) Wang,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Online labor markets are Web-based platforms that enable buyers to identify and contract for information technology (IT) services with service providers using buyer-determined (BD) auctions. BD auctions in online labor markets either follow an open or a sealed bid format. We compare open and sealed bid auctions in online labor markets to identify which format is superior in terms of obtaining more bids and a higher buyer surplus. Our theoretical analysis suggests that the relative advantage of open versus sealed bid auctions hinges on the role of reducing service providers' valuation uncertainty (difficulty in assessing the cost to execute a project) and competition uncertainty (difficulty in assessing the intensity of the competition from other service providers), which largely depend on the relative importance of the common value (versus the private value) component of the auctioned IT services, calling for an empirical investigation to compare open and sealed bid auctions. Based on a unique data set of 71,437 open bid auctions and 7,499 sealed bid auctions posted by 21,799 buyers at a leading online labor market, we find that, on average, although sealed bid auctions attract 18.4% more bids, open bid auctions offer buyers $10.87 higher surplus. Furthermore, open bid auctions are 55.3% more likely to result in a buyer's selection of a certain service provider and 22.1% more likely to reach a contract (conditional on the buyer's making a selection) with a provider, and they generate higher buyer satisfaction. In contrast to conventional wisdom that "the more bids the better" and industry practice of treating sealed bid auctions as a premium feature, our results suggest that the buyer surplus gained from the reduction in valuation uncertainty enabled by open bid auctions outweighs the buyer surplus gained from the higher competition uncertainty in sealed bid auctions, which renders open bid auctions a superior auction design in online labor markets.
|keyword = auction format,auction theory,open bids,sealed bids,valuation uncertainty,competition uncertainty,online labor markets,buyer surplus,auction performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Mandatory Standards and Organizational Information Security'''
{{header}}
{{article
|author= Chul Ho Lee,Xianjun Geng,Srinivasan Raghunathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Mandatory security standards that force firms to establish minimum levels of security controls are enforced in many domains, including information security. The information security domain is characterized by multiple intertwined security controls, not all of which can be regulated by standards, but compliance with existing security standards is often used by firms to deflect liability if a security breach occurs. We analyze a stylized setting where a firm has two security controls that are linked in either a serial or a parallel configuration. One control is directly regulated by a security standard, whereas the other one is not. We show that a higher security standard does not necessarily lead to a higher firm security. Furthermore, the conditions under which a higher standard hurts the firm security are sharply different in the two-serial and parallel-configurations. If standard compliance leads to reduced liability for a firm following a breach, such liability reduction in turn weakens the tie between the standard and firm security. Under a setting in which the firm meets the optimal standard set by a policy maker, both firm security and social welfare are higher when the damage to the firm following a breach takes a higher share of the total damage to social welfare, and also when the firm takes a larger share of liability.
|keyword = information security,security regulation,unverifiability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Citizens' Uncertainty in E-Government Services: The Mediating and Moderating Roles of Transparency and Trust'''
{{header}}
{{article
|author= Viswanath Venkatesh,James Y. L. Thong,Frank K. Y. Chan,Paul J. H. Hu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = This paper investigates how citizens' uncertainty in e-government services can be managed. First, we draw from uncertainty reduction theory, and propose that transparency and trust are two key means of reducing citizens' uncertainty in e-government services. Second, we identify two key sets of relevant drivers of e-government service use: (1) information quality characteristics, i. e., accuracy and completeness; and (2) channel characteristics, i. e., convenience and personalization. We propose that the means of uncertainty reduction, information quality characteristics, and channel characteristics are interrelated factors that jointly influence citizens' intentions to use e-government. We tested our model with 4,430 Hong Kong citizens' reactions to two e-government services: government websites and online appointment booking. Our results show that the information quality and channel characteristics predict citizens' intentions to use e-government. Furthermore, transparency and trust mediate as well as moderate the effects of information quality and channel characteristics on intentions. A follow-up survey found that citizens' intentions predict use and ultimately, citizens' satisfaction.
|keyword = e-services,electronic government,uncertainty reduction,transparency,trust,technology adoption,citizen satisfaction,public management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Double-Edged Sword of Backward Compatibility: The Adoption of Multigenerational Platforms in the Presence of Intergenerational Services'''
{{header}}
{{article
|author= Il-Horn Hann,Byungwan Koh,Marius F. Niculescu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = We investigate the impact of the intergenerational nature of services, via backward compatibility, on the adoption of multigenerational platforms. We consider a mobile Internet platform that has evolved over several generations and for which users download complementary services from third-party providers. These services are often intergenerational: newer platform generations are backward compatible with respect to services released under earlier generation platforms. In this paper, we propose a model to identify the main drivers of consumers' choice of platform generation, accounting for (i) the migration from older to newer platform generations, (ii) the indirect network effect on platform adoption due to same-generation services, and (iii) the effect on platform adoption due to the consumption of intergenerational services via backward compatibility. Using data on mobile Internet platform adoption and services consumption for the time period of 2001-2007 from a major wireless carrier in an Asian country, we estimate the three effects noted above. We show that both the migration from older to newer platform generations and the indirect network effects are significant. The surprising finding is that intergenerational services that connect subsequent generations of platforms essentially engender backward compatibility with two opposing effects. Whereas an intergenerational service may accelerate the migration to the subsequent platform generations, it may also, perhaps unintentionally, provide a fresh lease on life for earlier generation platforms due to the continued use of earlier generation services on newer platform generations.
|keyword = platform economics,multigeneration diffusion,backward compatibility,lease on life,network economics,mobile Internet,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When Do Consumers Value Positive vs. Negative Reviews? An Empirical Investigation of Confirmation Bias in Online Word of Mouth'''
{{header}}
{{article
|author= Dezhi Yin,Sabyasachi Mitra,Han Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = In the online word-of-mouth literature, research has consistently shown that negative reviews have a greater impact on product sales than positive reviews. Although this negativity effect is well documented at the product level, there is less consensus on whether negative or positive reviews are perceived to be more helpful by consumers. A limited number of studies document a higher perceived helpfulness for negative reviews under certain conditions, but accumulating empirical evidence suggests the opposite. To reconcile these contradictory findings, we propose that consumers can form initial beliefs about a product on the basis of the product's summary rating statistics (such as the average and dispersion of the product's ratings) and that these initial beliefs play a vital role in their subsequent evaluation of individual reviews. Using a unique panel data set collected from Apple's App Store, we empirically demonstrate confirmation bias-that consumers have a tendency to perceive reviews that confirm (versus disconfirm) their initial beliefs as more helpful, and that this tendency is moderated by their confidence in their initial beliefs. Furthermore, we show that confirmation bias can lead to greater perceived helpfulness for positive reviews (positivity effect) when the average product rating is high, and for negative reviews (negativity effect) when the average product rating is low. Thus, the mixed findings in the literature can be a consequence of confirmation bias. This paper is among the first to incorporate the important role of consumers' initial beliefs and confidence in such beliefs (a fundamental dimension of metacognition) into their evaluation of online reviews, and our findings have significant implications for researchers, retailers, and review websites.
|keyword = positive-negative asymmetry,negativity effect,positivity effect,confirmation bias,confidence in beliefs,metacognition,online word of mouth,product review,review rating,review helpfulness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cloud Computing Spot Pricing Dynamics: Latency and Limits to Arbitrage'''
{{header}}
{{article
|author= Hsing Kenneth Cheng,Zhi Li,Andy Naranjo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = This study examines cloud computing spot pricing dynamics and the influence of latency on those pricing dynamics. Using the Amazon Elastic Compute Cloud U.S. East and West market spot instance pricing and latency intraday data from April 9, 2010, to May 22, 2011, we find considerable time variation in spot instance prices, and prices are often persistently higher in the West. Bivariate vector autoregressive model results show that within-market autoregressive pricing effects are larger than across-market effects. We also document that over 70% of the relative price discovery occurs in the East market. Our regression results further show that East-West latency differentials have a significantly positive effect on East-West pricing differentials. Latency creates a dynamic pricing wedge that widens or narrows conditional on the latency differentials. Using an error correction model, the speed of adjustment from long-run pricing convergence errors causes the short-run price differential to narrow, but the adjustment does not completely offset the price differential.
|keyword = cloud computing,spot pricing,pricing dynamics,latency,arbitrage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology, Customer Satisfaction, and Profit: Theory and Evidence'''
{{header}}
{{article
|author= Sunil Mithas,M. S. Krishnan,Claes Fornell,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = This paper studies the effect of aggregate information technology (IT) investments on customer satisfaction and profits at the firm level. Using data on 109 U.S. firms for the 1994-1996 and 1999-2006 periods, we find that aggregate IT investments have a positive association with customer satisfaction. However, the strength of the relationship varied across the 1994-1996 and 1999-2006 periods. Specifically, IT investments had a more positive influence on customer satisfaction for the 1994-1996 period than for the 1999-2006 period. Conversely, IT investments had a positive effect on profits in the 1999-2006 period, but a negative effect in the 1994-1996 period. These findings extend prior discourse in the information systems literature on the role of customer satisfaction as a mechanism that explains how IT-enabled benefits are "passed on to consumers" [Rai A, Patnayakuni R, Patnayakuni N (1997) Technology investment and business performance. Comm. ACM 40(7):90]. Our additional exploratory analyses showing that IT investments had a stronger effect on perceived quality than on perceived value provide an explanation for some of the observed effects of IT on customer satisfaction and profits. Together, these contributions and implications provide new insights to assess returns on IT investments by focusing on customer satisfaction, an important intangible and leading measure of firm performance, stock returns, and stock risk.
|keyword = information technology,customer satisfaction,intangibles,firm performance,profit,business value of IT,customer relationship management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''In CARSs We Trust: How Context-Aware Recommendations Affect Customers' Trust and Other Business Performance Measures of Recommender Systems'''
{{header}}
{{article
|author= Umberto Panniello,Michele Gorgoglione,Alexander Tuzhilin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Most of the work on context-aware recommender systems has focused on demonstrating that the contextual information leads to more accurate recommendations. Little work has been done, however, on studying how much the contextual information affects the business performance. In this paper, we study how including context in recommendations affects customers' trust, sales, and other crucial business-related performance measures. To do this, we delivered content-based and context-aware recommendations through a live controlled experiment with real customers of a commercial European online publisher. We measured the recommendations' accuracy and diversification, how much customers spent purchasing products during the experiment, the quantity and price of their purchases, and the customers' level of trust. We show that collecting and using contextual information in recommendations affects business-related performance measures, such as company sales, by improving the accuracy and diversification of recommendations, which in turn improves trust and, ultimately, business performance results.
|keyword = business value of IT,case studies,economics of IS,electronic commerce,field experiments,recommender systems,context aware,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Expectation Disconfirmation Theory and Polynomial Modeling to Understand Trust in Technology'''
{{header}}
{{article
|author= Nancy K. Lankton,D. Harrison McKnight,Ryan T. Wright,Jason Bennett Thatcher,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2016
|abstract = Trust in technology is an emerging research domain that examines trust in the technology artifact instead of trust in people. Although previous research finds that trust in technology can predict important outcomes, little research has examined the effect of unmet trust in technology expectations on trusting intentions. Furthermore, both trust and expectation disconfirmation theories suggest that trust disconfirmation effects may be more complex than the linear expectation disconfirmation model depicts. However, this complexity may only exist under certain contextual conditions. The current study contributes to this literature by introducing a nonlinear expectation disconfirmation theory model that extends understanding of trust-in-technology expectations and disconfirmation. Not only does the model include both technology trust expectations and technology trusting intention, it also introduces the concept of expectation maturity as a contextual factor. We collected data from three technology usage contexts that differ in expectation maturity, which we operationalize as length of the introductory period. We find that the situation, in terms of expectation maturity, consistently matters. Using polynomial regression and response surface analyses, we find that in contexts with a longer introductory period (i.e., higher expectation maturity), disconfirmation has a nonlinear relationship with trusting intention. When the introductory period is shorter (i.e., expectation maturity is lower), disconfirmation has a linear relationship with trusting intention. This unique set of empirical findings shows when it is valuable to use nonlinear modeling for understanding technology trust disconfirmation. We conclude with implications for future research.
|keyword = expectation disconfirmation theory,trust,technology trust,IT continuance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN EXPLORATORY STUDY OF THE FORMATION AND IMPACT OF ELECTRONIC SERVICE FAILURES'''
{{header}}
{{article
|author= Chee-Wee Tan,Izak Benbasat,Ronald T. Cenfetelli,
|source= MIS QUARTERLY
|year= 2016
|abstract = E-commerce service failures have been the bane of e-commerce, compelling customers to either abandon transactions entirely or switch to traditional brick-and-mortar establishments. Yet, there is a paucity of studies that investigates how such failures manifest on e-commerce websites and their impact on consumers. This paper, therefore, synthesizes extant literature on e-service and system success to arrive at a novel classification system that delineates e-commerce service failures into information, functional, and system categories, each with its own set of constituent dimensions. Extending expectation disconfirmation theory (EDT), we further distinguish among disconfirmed outcome, process, and cost expectancies as major consequences of e-commerce service failures. A theoretical model of e-commerce service failure classifications and their consequences was constructed together with testable propositions that relate the three failure categories to consumers' disconfirmed expectancies. Finally, we explore the validity of our theoretical model based on descriptive accounts of actual occurrences of e-commerce service failures and their corresponding consequences. Consistent with our theoretical model, information and functional failures were found to be associated with disconfirmed outcome and process expectancies respectively. System failures, on the other hand, do not affect consumers' disconfirmed expectancies, thereby contradicting our predictions. Post hoc analysis on constituent dimensions of information, functional, and system failures yielded additional insights on the preceding observations.
|keyword = E-commerce service failure,expectation disconfirmation theory,information failure,functional failure,system failure,disconfirmed outcome expectancy,disconfirmed process expectancy,disconfirmed cost expectancy,critical incident technique (CIT),qualitative comparative analysis (QCA),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FREE VERSUS FOR-A-FEE: THE IMPACT OF A PAYWALL ON THE PATTERN AND EFFECTIVENESS OF WORD-OF-MOUTH VIA SOCIAL MEDIA'''
{{header}}
{{article
|author= Hyelim Oh,Animesh Animesh,Alain Pinsonneault,
|source= MIS QUARTERLY
|year= 2016
|abstract = Information goods providers such as print newspapers are experimenting with different pricing models for their online content. Despite research on the topic, it is still not clear how information pricing strategy influences word-of-mouth (WOM) via social media, which has become a dominant channel for raising awareness about a newspaper's articles and attracting new visitors to its website. Using The New York Times' paywall rollout as a natural experiment, our study examines how the implementation of paywall by a firm (i.e., a shift from "free" to "for-a-fee") influences the pattern and effectiveness of online WOM in social media. Our results indicate that implementing a paywall (i.e., charging for content that was earlier available for free) has a disproportionate impact on WOM for popular and niche articles, creating a longer tail in the WOM (i.e., content sharing) distribution. Further, we find that the impact of WOM on the NYT's website traffic weakens significantly after the introduction of a paywall. These results show that a paywall has implications for product and promotion strategies. The study offers novel and important implications for the theory and practice of strategic use of social media and paywall.
|keyword = Information goods,information pricing,paywall,social media,long tail,word-of-mouth,website traffic,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTRACT DESIGN CHOICES AND THE BALANCE OF EX ANTE AND EX POST TRANSACTION COSTS IN SOFTWARE DEVELOPMENT OUTSOURCING'''
{{header}}
{{article
|author= Michel Benaroch,Yossi Lichtenstein,Lior Fink,
|source= MIS QUARTERLY
|year= 2016
|abstract = This paper examines multiple contract design choices in the context of transaction and relational attributes and consequent ex ante and ex post transaction costs. It focuses on two understudied themes in the IT outsourcing literature. First, while the literature is predominantly concerned with opportunism and consequent ex post hazard costs that contracts can safeguard against, parties to a contract also economize on ex ante transaction costs by their choice of contract type and contract extensiveness. Second, the literature studies the aggregate extensiveness of contracts rather than of distinct contract functions: safeguarding, coordination, and adaptability. Against this backdrop, our research model portrays a nuanced picture that is anchored in the following theoretical interpretation: transaction and relational attributes have implications on specific ex ante and ex post transaction costs, and these implications can be balanced by respective choices in both contract type and the extensiveness of specific contract functions. These two contract design choices complement and substitute for each other in their ability to economize on specific transaction costs. Our analysis of 210 software development outsourcing contracts finds that explanatory power increases when analyzing the extensiveness of individual contract functions rather than the aggregate contract extensiveness, highlighting subtle competing influences that are otherwise masked by an aggregate measure. Our analysis also shows that a preference for time-and-material contracts counteracts the effect of certain transaction attributes on contract extensiveness, and even cancels it out in the case of transaction uncertainty.
|keyword = Contract functions (safeguarding, coordination, adaptability),contract extensiveness,contract type,ex ante and ex post transaction costs,transaction cost economics,software development outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''VALUING INFORMATION TECHNOLOGY RELATED INTANGIBLE ASSETS'''
{{header}}
{{article
|author= Adam Saunders,Erik Brynjolfsson,
|source= MIS QUARTERLY
|year= 2016
|abstract = In this article, we assess the value of information technology related intangible assets and then use data on business practices and management capabilities to understand how this value is distributed across firms. Using a panel of 127 firms over the period 2003-2006, we replicate and extend the finding from Brynjolfsson, Hitt, and Yang (2002) that $1 of computer hardware is correlated with more than $10 of market value. We account for the "missing $9" by broadening the definition of IT to include capitalized software, and then include all purchased and internally developed software, other internal IT services, IT consulting, and IT-related training (whether or not it is capitalized by the firm). In addition, we use data on IT-related business practices in order to analyze the distribution of IT-related intangibles within the sample. Our results suggest that the "invisible" IT not accounted for on balance sheets is being priced into the market value of firms. We also estimate that there is a 45% to 76% premium in market value for the firms with the highest organizational IT capabilities (based on separate measures of human resource practices, management practices, internal IT use, external IT use, and Internet capabilities), as compared to those with the lowest organizational IT capabilities. Our results thus suggest that contributions of IT to value depend heavily on other factors, and are not a rising tide that lifts all boats.
|keyword = IT value,market value,IT-related intangibles,IT capabilities,intangible assets,R&D value,brand value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPETITIVE BUNDLING IN INFORMATION MARKETS: A SELLER-SIDE ANALYSIS'''
{{header}}
{{article
|author= Srinivasan Raghunathan,Sumit Sarkar,
|source= MIS QUARTERLY
|year= 2016
|abstract = The emerging field of data analytics and the increasing importance of data and information in decision making has created a large market for buying and selling information and information-related services. In this market, for some types of information products, it is common for a consumer to purchase the same type of information product from multiple sources. In other situations, a consumer may buy different types of information products from different sources and synthesize the information. On the seller side, bundling of different types of information products appears to have emerged as a key design strategy to improve profitability. This paper examines bundling decisions of a duopoly in the information market in which each seller offers two (or more) types of information products. A pair of competing information products from the two sellers can be substitutes or complements and consumers may find it profitable to purchase the same type of information from both sellers. We show that bundling by both sellers emerges as the equilibrium outcome when (at least) one competing pair consists of substitutes and (at least) one pair consists of complements. In this case, bundling by both sellers benefits them both by softening the price competition between their offerings. Softening of competition does not occur when all competing pairs in the bundles have only substitutes (complements) even if the degree of substitutability (complementarity) between products within a pair varies across pairs, resulting in an equilibrium in which each information type is sold separately by both sellers.
|keyword = Bundling,pricing,competition,analytical models,information markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHEN DOES REPOSITORY KMS USE LIFT PERFORMANCE? THE ROLE OF ALTERNATIVE KNOWLEDGE SOURCES AND TASK ENVIRONMENTS'''
{{header}}
{{article
|author= Seung Hyun Kim,Tridas Mukhopadhyay,Robert E. Kraut,
|source= MIS QUARTERLY
|year= 2016
|abstract = Despite a general consensus that use of information technology (IT) is an important link between IT investments and performance, the extant literature provides only a limited explanation as to when the use of IT lifts performance. We posit that the impact of knowledge management systems (KMS) usage is contingent on users' alternative sources of knowledge as well as their specific task environments. We investigate under what conditions repository KMS use leads to higher performance outcomes in a retail grocery context. We use a unique longitudinal dataset composed of objective measures of KMS use and sales performance of 273 managers over 146 weeks collected from a retail grocery chain. We obtain two main results. First, we find a diminishing impact of KMS use for managers who also use other sources of codified knowledge, namely physical or computerized alternative knowledge sources, whereas a complementary relationship seems to exist between KMS use and social sources of knowledge. Second, KMS use produces higher benefits for managers whose task environments require a greater volume of information and knowledge, but smaller benefits for those managers whose task environments demand rapidly changing information and knowledge. Our work contributes to both the IT business value and the KM literature by studying the contingent impact of IT usage while broadening the theoretical scope of the situated knowledge performance framework with a critical empirical test based on fine-grained objective and longitudinal data.
|keyword = Business value of IT,knowledge management systems,IT usage,knowledge channels,contingent impact,retail grocery,complementarity,situated knowledge performance framework,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DEAL-SEEKING VERSUS BRAND-SEEKING: SEARCH BEHAVIORS AND PURCHASE PROPENSITIES IN SPONSORED SEARCH PLATFORMS'''
{{header}}
{{article
|author= Il Im,Jongkun Jun,Wonseok Oh,Seok-Oh Jeong,
|source= MIS QUARTERLY
|year= 2016
|abstract = Using a database of 11,001 unique sponsored search keywords, we investigate the relationship between the characteristics of keywords oriented around deal-seeking and brand-seeking and consumer search behaviors and buying propensities. On the basis of the search depth versus search breadth framework, we hypothesize that deal-seeking keywords elicit a search of greater breadth, whereas brand-seeking keywords induce a search of greater depth. We also explore the moderating effect of product type (search or experience goods) on the relationship between keyword characteristics and consumer search behaviors and how high-demand seasons (e.g., scheduled sales) influence consumers' search and purchase behaviors. In addition, we estimate the effectiveness of keywords on the basis of their sales-per-cost performance. The findings indicate that search queries containing deal-seeking keywords are associated with higher click-through rates and conversion rates than are search queries without such keywords. We also find that the positive effect of deal-seeking keywords on click-through rates is more pronounced for experience goods than for search goods. However, we identify a negative interaction between experience goods and brand-seeking keywords. A comparison of deal-seeking and brand-seeking keywords in terms of cost effectiveness reveals that deal-seeking keywords generate approximately three times the sales of those produced by brand-seeking keywords.
|keyword = Sponsored search,deal-seeking,brand-seeking,experience goods,click-through rate,conversion rate,hierarchical generalized linear model,propensity score weighting,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INDIVIDUALS' INTERNET SECURITY PERCEPTIONS AND BEHAVIORS: POLYCONTEXTUAL CONTRASTS BETWEEN THE UNITED STATES AND CHINA'''
{{header}}
{{article
|author= Yan Chen,Fatemeh Mariam Zahedi,
|source= MIS QUARTERLY
|year= 2016
|abstract = Little is known about the context sensitivity of users' online security perceptions and behaviors to national and individual attributes, and there is inadequate research about the spectrum of users' behaviors in dealing with online security threats. In addressing this gap, this paper draws on two complementary theoretical bases: (1) the contextualization of the protection motivation theory (PMT) to online security behavior and (2) a pol-ycontextual lens for the cross-national comparison of users' security behaviors in the United States and China. The conceptualized model is tested based on 718 survey observations collected from the United States and China. The results support our model and show the divergence between the United States, an exemplar of modern Western society, and China, an exemplar of traditional Eastern society, in forming threat perceptions and in seeking help and avoidance as coping behaviors. Our results also uncovered the significant moderating impacts of espoused culture on the way perceptions of security threats and coping appraisals influence security behaviors. Our findings underline the importance of context-sensitive theory building in security research and provide insights into the motivators and moderators of individuals' online security behaviors in the two nations.
|keyword = Individual users,protection motivation theory,coping theory,security behaviors,seeking help,security self-efficacy,security response efficacy,cross-national research,espoused national culture,poly-contextual lens,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW INFORMATION TECHNOLOGY STRATEGY AND INVESTMENTS INFLUENCE FIRM PERFORMANCE: CONJECTURE AND EMPIRICAL EVIDENCE'''
{{header}}
{{article
|author= Sunil Mithas,Roland T. Rust,
|source= MIS QUARTERLY
|year= 2016
|abstract = In this paper, we develop conjectures for understanding how information technology (IT) strategy and IT investments jointly influence profitability and the market value of the firm. We view IT strategy as an expression of the dominant strategic objective that the firm chooses to emphasize, which can be revenue expansion, cost reduction, or a dual emphasis in which both goals are pursued. Using data from more than 300 firms in the United States, we find that at the mean value of IT investments, firms with a dual IT strategic emphasis have a higher market value as measured by Tobin's Q than firms with a revenue or a cost emphasis, but they have similar levels of profitability. Of greater importance, IT strategic emphasis plays a significant role in moderating the relationship between IT investments and firm performance. Dual-emphasis firms have a stronger IT-Tobin's Q relationship than revenue-emphasis firms. Dual-emphasis firms also have a stronger IT-profitability relationship than either revenue-or cost-emphasis firms. Overall, these findings imply that, at low levels of IT investment, the firm may need to choose between revenue expansion and cost reduction, but at higher levels of IT investment, dual-emphasis in IT strategy or IT strategic ambidexterity increasingly pays off.
|keyword = Information technology strategic emphasis,IT ambidexterity,IT strategic ambidexterity,firm performance,profitability,IT investments,revenue growth,cost reduction,dual emphasis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE CREATION OF SOCIAL VALUE: CAN AN ONLINE HEALTH COMMUNITY REDUCE RURAL-URBAN HEALTH DISPARITIES?'''
{{header}}
{{article
|author= Jie Mein Goh,Guodong (Gordon) Gao,Ritu Agarwal,
|source= MIS QUARTERLY
|year= 2016
|abstract = The striking growth of online communities in recent years has sparked significant interest in understanding and quantifying benefits of participation. While research has begun to document the economic outcomes associated with online communities, quantifying the social value created in these collectives has been largely overlooked. This study proposes that online health communities create social value by addressing rural-urban health disparities via improved health capabilities. Using a unique data set from a rare disease community, we provide one of the first empirical studies of social value creation. Our quantitative analysis using exponential random graph models reveals patterns of social support exchanged between users and the variations in these patterns based on users' location. We find that, overall, urban users are net suppliers of social support while rural participants are net recipients, suggesting that technology-mediated online health communities are able to alleviate rural-urban health disparities. This study advances extant understanding of value production in online collectives, and yields implications for policy.
|keyword = Healthcare,online communities,social value,disparities,social network,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product Information Websites: Are They Good for Consumers?'''
{{header}}
{{article
|author= Panos M. Markopoulos,Ravi Aron,Lyle H. Ungar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Product information websites have become ubiquitous in supporting B2C E-Commerce. This study explores their impact on firm profitability, consumer surplus, and social welfare. Using an analytical model, we show that firms take advantage of such infomediaries and reduce their own information investments, increasing their profitability. Surprisingly, we find that the existence of these websites may actually reduce social welfare. Contrary to the common belief that product information websites are good for buyers, we show that they may be hurting consumers, even when they seek to maximize consumer surplus as their principal goal. These findings question the uncritical acceptance of infomediaries as beneficial to markets in general, and buyers in particular, especially when the infomediaries assume roles that substitute the information disclosure investments that sellers freely choose to make.
|keyword = B2C eCommerce,consumer surplus,electronic markets,infomediaries,information dissemination,information economics,product information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Emotion in IT Investment Decision Making with A Real Options Perspective: The Intertwining of Cognition and Regret'''
{{header}}
{{article
|author= Eun Hee Park,Balasubramaniam Ramesh,Lan Cao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Real options thinking helps individuals cope with uncertainties in the information technology (IT) investment decision-making process. Our study examines the intertwining of cognition and a specific emotion, regret, in IT real options decision making using a multisite case study of IT investments. We develop a process model that explains the context in which regret is triggered, the strategies used to regulate regret, and how these strategies influence the valuation, creation, exercise, and revaluation of several types of IT real options. Our research highlights that while some regret regulation strategies may be productive, others are counterproductive. Our findings extend the studies on cognition-based models of IT real options by including emotion, which allows us to explore human behavior on a psychological level and offer a more fundamental understanding of IT real options thinking. Practitioners must recognize the presence of regret that could cloud cognition-based analysis and must be sensitive to its impact.
|keyword = emotion,IT investment decision,IT real options decision,regret,regret regulation strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Not As Smart As We Think: A Study of Collective Intelligence in Virtual Groups'''
{{header}}
{{article
|author= Jordan B. Barlow,Alan R. Dennis,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Organizations increasingly use virtual groups for many types of work, yet little research has examined factors that make groups perform better across multiple different types of tasks. Previous research has proposed that groups, like individuals, have a general factor of collective intelligence, an ability to perform consistently across multiple types of tasks. We studied groups that used computer-mediated communication (CMC) to investigate whether collective intelligence is similar or different when groups work using CMC. A collective intelligence factor did not emerge among groups using CMC, suggesting that collective intelligence manifests itself differently depending on context. This is in contrast to previous findings. Our results surface a need for more research on boundary conditions of the construct of collective intelligence. Our findings also have practical implications: managers should take care when organizing virtual group work because groups that perform well on one type of task will not necessarily be the groups that do well on other tasks
|keyword = collective intelligence,computer-mediated communication,group performance,intelligence,task types,virtual groups,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Warning to Wallpaper: Why the Brain Habituates to Security Warnings and What Can Be Done About It'''
{{header}}
{{article
|author= Bonnie Brinton Anderson,Anthony Vance,C. Brock Kirwan,Jeffrey L. Jenkins,David Eargle,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Warning messages are fundamental to users' security interactions. Unfortunately, they are largely ineffective, as shown by prior research. A key contributor to this failure is habituation: decreased response to a repeated warning. Previous research has only inferred the occurrence of habituation to warnings, or measured it indirectly, such as through the proxy of a related behavior. Therefore, there is a gap in our understanding of how habituation to security warnings develops in the brain. Without direct measures of habituation, we are limited in designing warnings that can mitigate its effects. In this study, we use neurophysiological measures to directly observe habituation as it occurs in the brain and behaviorally. We also design a polymorphic warning artifact that repeatedly changes its appearance in order to resist the effects of habituation. In an experiment using functional magnetic resonance imaging (fMRI; n = 25), we found that our polymorphic warning was significantly more resistant to habituation than were conventional warnings in regions of the brain related to attention. In a second experiment (n = 80), we implemented the four most resistant polymorphic warnings in a realistic setting. Using mouse cursor tracking as a surrogate for attention to unobtrusively measure habituation on participants' personal computers, we found that polymorphic warnings reduced habituation compared to conventional warnings. Together, our findings reveal the substantial influence of neurobiology on users' habituation to security warnings and security behavior in general, and we offer our polymorphic warning design as an effective solution to practice
|keyword = behavioral information systems security,cybersecurity,fMRI,functional magnetic resonance imaging,habituation,mouse cursor tracking,neurobiology,NeuroIS,polymorphic warnings,security warnings,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Empirical Assessment of Alternative Designs for Enhancing Different Types of Trusting Beliefs in Online Recommendation Agents'''
{{header}}
{{article
|author= Weiquan Wang,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = competence, integrity, and benevolence are the three key trusting beliefs that are widely acknowledged in the trust literature. Drawing on users' different dispositional attribution of these trusting beliefs, we investigate the different influence of two sets of experiential reasons on the competence belief versus the benevolence and integrity beliefs in online recommendation agents (RAs). The two sets of experiential reasons encompass interactive reason, including three performance factors (namely, perceived cognitive effort, advice quality, and perceived strategy restrictiveness), and knowledge-based reason (i.e., perceived transparency of an RA). Data were collected through a laboratory experiment to test our hypotheses. Results demonstrate that the three performance factors affect only the competence belief, whereas perceived RA transparency influences all three trusting beliefs. In addition, the effects of perceived transparency on competence are partially mediated by perceived cognitive effort and advice quality. The research contributes to the trust literature by revealing the different antecedents of the three trusting beliefs and provides guidelines for designers to choose specific design elements to improve a particular trusting belief of the user toward an RA.
|keyword = advice quality,cognitive effort,decision support,recommendation agent,restrictiveness,transparency,trust,trusting beliefs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Augmented Virtual Doctor Office: Theory-based Design and Assessment'''
{{header}}
{{article
|author= Fatemeh Mariam Zahedi,Nitin Walia,Hemant Jain,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Motivated by rising demands for medical care and the recent trends in medical care delivery, this work designs, develops, and evaluates the augmented virtual doctor office (AVDO). AVDO is intended to provide group medical visits in cyberspace (Cyber GMV). This research adopts the design science approach to design AVDO based on the extension of media naturalness (MN) theory. AVDO is implemented in an augmented world setting that integrates real visual cues with a virtual-world technology (Second Life((R)) in this case). The assessment of AVDO is carried out in two ways: (1) through a synthesis of the extended MN theory and technology acceptance theories to assess the relationships of design features as perceived by patients with outcomes that include understanding, perceived effectiveness, trust, and behavior intentions, and (2) through the assessment of AVDO's proof of value and proof of use as a supplementary channel for the delivery of medical care. Our work shows how the design features significantly influence outcomes and patients' positive views of the design's value and use. Theoretical and practical contributions of the work are presented.
|keyword = augmented virtual doctor office,augmented worlds,design science,group medical visits,health informatics,Second Life((R)),virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Firm Boundaries, Information Processing Capacity, and Performance in Manufacturing Firms'''
{{header}}
{{article
|author= Jaime Gomez,Idana Salazar,Pilar Vargas,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = We analyze the alignment between information processing needs and capacities. First, we explore the relationship between firms' vertical and horizontal boundaries and information technology (IT) capabilities. The literature postulates that less internalization leads firms to invest more in information technology. However, we argue that the use of taper integration and diversification increases the demand for IT-based resources such as IT infrastructure and IT human resources. Second, we propose that the fit between information processing needs and capacity has a positive effect on performance. Our hypotheses are tested on a panel of Spanish manufacturing firms, and the results provide general support for our arguments. One advantage of the data is that they include not only large firms but also small and medium-size manufacturers. From a theoretical perspective, the study contributes to the literature by providing novel insights on how decisions on the vertical dimension condition investments in IT capabilities. It also adds new evidence on the diversification-IT capabilities relationship and studies the consequences of alignment of corporate strategies and IT capabilities on firm performance. From a managerial perspective, our study suggests that changes in the vertical and horizontal limits should be followed by changes in IT capabilities to improve performance.
|keyword = diversification,information processing capacity,information processing needs,information technology capabilities,IS-organization fit,taper integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Social Interaction Dynamics on Platforms'''
{{header}}
{{article
|author= Ferdinand Thies,Michael Wessel,Alexander Benlian,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Despite the increasing relevance of online social interactions on platforms, there is still little research on the temporal interaction dynamics between electronic word-of-mouth (eWOM, a form of opinion-based social interaction), popularity information (a form of action-based social interaction), and consumer decision making. Drawing on a panel data set of more than 23,300 crowdfunding campaigns from Indiegogo, we investigate the dynamic effects of these social interactions on consumers' funding decisions using the panel vector autoregressive methodology. Our analysis shows that both eWOM and popularity information are critical influencing mechanisms in crowdfunding. However, our overarching finding is that eWOM surrounding crowdfunding campaigns on Indiegogo or Facebook has a significant yet substantially weaker predictive power than popularity information. We also find that whereas popularity information has a more immediate effect on consumers' funding behavior, its effectiveness decays rather quickly, while the impact of eWOM recedes more slowly. This study contributes to the extant literature by (1) providing a more nuanced understanding of the dynamic effects of opinion-based and action-based social interactions, (2) unraveling both within-platform and cross-platform dynamics, and (3) showing that social interactions are perceived as quality indicators on crowdfunding platforms that help consumers reduce risks associated with their investment decisions. These results can help platform providers and complementors to stimulate contribution behavior and increase the prosperity of a platform.
|keyword = crowdfunding,electronic word-of-mouth,eWOM,informational cascades,online platforms,panel vector autoregression,popularity information,platform success,reward-based crowdfunding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Early Predictions of Movie Success: The Who, What, and When of Profitability'''
{{header}}
{{article
|author= Michael T. Lash,Kang Zhao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = We focus on predicting the profitability of a movie to support movie-investment decisions at early stages of film production. By leveraging data from various sources, and using social network analysis and text mining techniques, the proposed system extracts several types of features, including who is in the cast, what a movie is about, when a movie will be released, as well as hybrid features. Experiment results showed that the system outperforms benchmark methods by a large margin. Novel features we proposed made weighty contributions to the prediction. In addition to designing a decision support system with practical utility, we also analyzed key factors of movie profitability. Furthermore, we demonstrated the prescriptive value of our system by illustrating how it can be used to recommend a set of profit-maximizing cast members. This research highlights the power of predictive and prescriptive data analytics in information systems to aid business decisions.
|keyword = decision support,movie investments,movie profitability,predictive analytics,prescriptive analytics,social network analysis,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Influence of Firm's Recovery Endeavors upon Privacy Breach on Online Customer Behavior'''
{{header}}
{{article
|author= Ben C. F. Choi,Sung S. Kim,Zhenhui (Jack) Jiang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = The soaring number of privacy breaches has prompted affected firms to learn how to effectively recover damaged customer relationships. In this study we develop and test a model that explains how online customer behavior is influenced by a firm's recovery endeavors when privacy breaches occur. Drawing on a service recovery perspective, we integrate the notions of justice perceptions and psychological responses into a theoretical framework. The proposed model was tested against data collected from 1,007 online customers based on a hypothetical scenario. Results show that three types of justice perceptions, distributive, procedural, and interactional justice, jointly affect psychological responsesthat is, perceived breach and feelings of violation. In addition, psychological responses were shown to be important in shaping postincident outcomes such as post-word of mouth and post-likelihood of switching. The study gives researchers and practitioners a useful conceptual tool for analyzing the effectiveness of organizational practices in recovering customer relationship after privacy breaches.
|keyword = online customer behavior,privacy,privacy breaches,psychological responses,security,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving: A Dual Pathway to Creativity'''
{{header}}
{{article
|author= Niek Althuizen,Astrid Reichel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = We investigate the effectiveness of three types of IT-enabled cognitive stimulation tools for enhancing creative problem solving: mind mappers, process guides, and stimuli providers. Based on the dual pathway to creativity models, the authors examine the extent to which these tools are capable of stimulating individuals to explore their knowledge base more deeply (i.e., the persistence pathway) and more broadly (i.e., the flexibility pathway) and, hence, help to produce more novel ideas. In a laboratory study with business students, they find that, as compared to unaided individuals, IT-enabled stimuli providers enhance individual creativity more than process guides and mind mappers. As for the underlying creative process, stimuli providers push individuals to explore their knowledge base more deeply and more broadly, leading to more novel but, unexpectedly, also more useful ideas. The reported findings may facilitate the development of creativity support systems and their assignment to individuals and tasks.
|keyword = creativity,creative problem solving,creativity stimulation,dual pathway to creativity,IT-enabled cognitive stimulation,knowledge activation,search for ideas in associative memory (SIAM),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Information Systems to Sense Opportunities for Innovation: Integrating Postadoptive Use Behaviors with the Dynamic Managerial Capability Perspective'''
{{header}}
{{article
|author= Nicholas Roberts,Damon E. Campbell,Leo R. Vijayasarathy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Fast-paced environmental changes require that managers quickly sense opportunities for organizational innovation. Information systems (IS) that support business intelligence and analytics help managers access and analyze data from various sources, thereby providing insight into potential opportunities. Building on the dynamic managerial capability perspective, we investigate the extent to which two managerial IS use behaviors-routine use and innovative use-influence a manager's volume and diversity of ideas for organizational innovation. We also examine the moderating role of three organization-level entrepreneurial orientation characteristics-autonomy, innovativeness, and risk taking. We test our research model with survey data collected from 248 managers. Our results show that routine IS use is not related to volume or diversity of ideas for organizational innovation. However, innovative IS use is positively related to idea volume and idea diversity. Furthermore, organizational autonomy and innovativeness positively moderate the aforementioned innovative use/idea relationships. Our study contributes to the literature by linking postadoptive IS use behaviors to managerial sensing ability, an important dynamic managerial capability. We also further the understanding of how organizational factors such as entrepreneurial orientation play a key role in determining whether, when, and how managers use IS to develop ideas for organizational innovation.
|keyword = business analytics,decision support systems,dynamic managerial capability,idea set,innovative use of IS,IS use,IT business value,sensing ability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sharing Knowledge in Social Q&A Sites: The Unintended Consequences of Extrinsic Motivation'''
{{header}}
{{article
|author= Li Zhao,Brian Detlor,Catherine E. Connelly,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = In order to motivate individuals to share their knowledge in online communities, the use of extrinsic rewards and goals is a typical approach. However, extrinsic motivation may have unintended consequences. Although past studies have examined the direct effect of extrinsic motivation on intrinsic motivation, no research to date has investigated how extrinsic motivation moderates the impact of intrinsic motivation on knowledge sharing, or how the effect of extrinsic motivation on intrinsic motivation is contingent upon whether a member is active or not. Drawing on attribution theory and theory of planned behavior, the study was conducted with data collected from a large social Q&A site consisting of multiple online communities with millions of registered users; the datawere analyzed with moderated regression and structural equation modeling. Results show that the effect of enjoyment in helping others on attitude toward knowledge sharing is undermined by virtual organizational rewards, while the effect of knowledge self-efficacy on attitude toward knowledge sharing is undermined by reciprocity. The results also show that the effect of virtual organizational rewards on enjoyment in helping others is contingent upon whether members are active or not. Specifically, for active members, virtual organizational rewards undermine enjoyment in helping others; for inactive members, however, virtual organizational rewards increase enjoyment in helping others. These findings enrich the research on unintended consequences of extrinsic motivation specifically, and the theory of motivation in general. Additionally, these findings provide practical insights on how and when to use extrinsic rewards/goals to motivate individuals to share knowledge in social Q&A sites.
|keyword = active involvement,crowding out,extrinsic motivation,intrinsic motivation,knowledge sharing,online communities,online community participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Concurrent IT Sourcing: Mechanisms and Contingent Advantages'''
{{header}}
{{article
|author= Amrit Tiwana,Stephen K. Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = A growing trend to simultaneously insource and outsource the same information technology (IT) activities ("concurrent IT sourcing") has not yet received research attention. Although it is widespread and recent empirical studies have detected that in-house IT can complement IT outsourcing, when and how concurrent IT sourcing pays off is not yet understood. This study introduces the notion of concurrent IT sourcing. It then develops two interrelated ideas: concurrent IT sourcing simultaneously enhances in-house and outsourced IT performance: (a) via distinctive mechanisms, but (b) only when vendors' IT capabilities complement the client's. Econometric tests using survey data from 233 firms support these ideas. Our novel contribution is to explain when and how concurrent IT sourcing enhances a client firm's inhouse and outsourced IT performance. The explanatory mechanisms for outsourced IT performance are socialization and modeling of clients' in-house IT practices by vendors; for inhouse IT performance they are knowledge spillovers and ratcheting. For practice, our study shows that when a firm's in-house capabilities complement its IT vendors' capabilities, firms can simultaneously outsource and insource the same IT activities to enhance both in-house and outsourced IT performance.
|keyword = concurrent IT sourcing,insourcing,IT capabilities,IT governance,outsourcing,plural governance,seemingly unrelated regression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Influentials, Imitables, or Susceptibles? Virality and Word-of-Mouth Conversations in Online Social Networks'''
{{header}}
{{article
|author= Anjana Susarla,Jeong-Ha Oh,Yong Tan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Motivated by the rise of social media platforms that achieve a fusion of content and community, we consider the role of word-of-mouth communications (WOM) structured through a network. Using a data set from YouTube, we examine how cascades of WOM interactions enhance the popularity of videos. We first estimate the impact of channel influence and other network parameters in initiating WOM communications. The probit estimation considers the selection effect in videos that are likely to be associated with a greater propensity to trigger WOM. We find that factors related to a channel's ability to be a connector and a translator is most likely to result in the incidence of WOM. We then examine how cascades of WOM conversations have persistent impacts on subsequent video popularity. Empirically, the main issue here is heterogeneity in the epidemic potential of a video. Since the threshold might vary across videos, we use a finite mixture model. We also conduct a simultaneous estimation using latent instrumental variables to address endogeneity from unobservables. Our research has implications for researchers and practitioners by highlighting how WOM travels through networks of influence and susceptibility in disseminating awareness, and holds insights in regard to designing social recommendation systems and identifying trending topics in social media.
|keyword = electronic word of mouth,eWOM,finite mixture model,latent instrumental variables,opinion cascades,peer effects,social media,social recommendations,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Determinants and Impacts of Aesthetics in Users' First Interaction with Websites'''
{{header}}
{{article
|author= Zhenhui (Jack) Jiang,Weiquan Wang,Bernard C. Y. Tan,Jie Yu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Though aesthetics is generally acknowledged as an important aspect of website design, extant information systems (IS) research on web user experience has rarely studied what affects website aesthetics and how aesthetics influences users' perceptions of the website and the organization behind the website. In this paper, we synthesize prior literature from different academic domains and propose users' perceived quality of five design elements (i.e., unity, complexity, intensity, novelty, and interactivity) as determinants of website aesthetics. We further theorize the effects of aesthetics on users' attitudes toward the website and their perception of the corporate image. Two studies were conducted to test the research model. In Study 1, we adopted a card sorting method and the results provide substantial support to the determinants of website aesthetics. In Study 2, we conducted a survey using ten company portal websites that were unknown to survey respondents. Our analysis further confirms the effects of users' perceived quality of the five design elements on the perception of website aesthetics. The findings of Study 2 also show that users' perception of aesthetics has significant impacts on perceived utility and their attitudes toward the website, which further affects the corporate image exhibited via the website. In addition, we find that in users' first interaction with a website, perceived aesthetics has a larger impact on their attitudes toward the website than perceived utility.
|keyword = corporate image,design complexity,design unity,website aesthetics,web design,website intensity,website interactivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Duopoly Pricing Strategy for Information Products with Premium Service: Free Product or Bundling?'''
{{header}}
{{article
|author= Zan Zhang,Guofang Nan,Minqiang Li,Yong Tan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Many software firms, especially mobile app providers, offer perpetually free basic products to users, but premiums are charged for access to the additional features or functionalities. While the free offering helps capture potential customers, it might cannibalize the sales of premium goods or services. This paper adopts a game theoretical approach to examine the impact of free offering on the competition between two firms in the presence of network effects. The firms can either offer a free core product and a paid service or offer them as a bundle. The core product has stand-alone value and can be used separately but the value-added service has no value without the core product. We derive the market equilibria and present conditions under which the free offering strategy outperforms the bundling strategy. We show that when a firm's core product has a sufficient advantage in product quality, it is better for this firm to sell the bundle but for the other to use free strategy. However, if the core products are similar in terms of quality, it is optimal for them to use the same strategies. Whether to offer a free product depends largely on the core products' quality. We also show that the firms may be caught in a prisoner's dilemma when both adopt the free strategy. Finally, we find that the profitability of the firm that offers a free product always increases in network effects intensity and market size, but this is not the case for the firm that sells the bundle. This study contributes to understanding the behavior of feature-limited free offering in a duopoly setting. Our findings also provide insights into the design of free product and the impact of network effects on the firms' offering decisions.
|keyword = free core product,freemium,information products,network effects,premium service,pricing,product bundling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Network Structure on Malware Propagation: A Growth Curve Perspective'''
{{header}}
{{article
|author= Hong Guo,Hsing Kenneth Cheng,Ken Kelley,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Malicious software, commonly termed "malware," continuously presents one of the top security concerns, and causes tremendous worldwide financial losses for organizations. In this paper, we propose a structural risk model to analyze malware propagation dynamics measured by a four-parameter (asymptote, point of inflection, rate, and infection proportion at inflection) growth curve. Using both social network data and technological network infrastructure from a large organization, we estimate the proposed structural risk model based on incident-specific nonlinear growth curves. This paper provides empirical evidence for the explanatory power of the structural characteristics of the underlying networks on malware propagation dynamics. This research provides useful findings for security managers in designing their malware defense strategies. We also simulate three common malware defense strategies (preselected immunization strategies, countermeasure dissemination strategies, and security awareness programs) based on the proposed structural risk model and show that they outperform existing strategies in terms of reducing the size of malware infection.
|keyword = information systems security,malware defense,malware propagation,malware propagation trajectory,network analysis,social networks,technological networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''More Than Meets the Eye: How Oculometric Behaviors Evolve Over the Course of Automated Deception Detection Interactions'''
{{header}}
{{article
|author= Jeffrey G. Proudfoot,Jeffrey L. Jenkins,Judee K. Burgoon,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Eye-tracking technology has exhibited promise for identifying deception in automated screening systems. Prior deception research using eye trackers has focused on the detection and interpretation of brief oculometric variations in response to stimuli (e.g., specific images or interview questions). However, more research is needed to understand how variations in oculometric behaviors evolve over the course of an interaction with a deception detection system. Using latent growth curve modeling, we tested hypotheses explaining how two oculometric behaviors-pupil dilation and eye-gaze fixation patterns-evolve over the course of a system interaction for three groups of participants: deceivers who see relevant stimuli (i.e., stimuli pertinent to their deception), deceivers who do not see relevant stimuli, and truth-tellers. The results indicate that the oculometric indicators of deceivers evolve differently over the course of an interaction, and that these trends are indicative of deception regardless of whether relevant stimuli are shown.
|keyword = automated screening systems,concealed information test (CIT),deception detection,eye tracking,latent growth curve modeling,pupil dilation,oculometrics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Validation of Malicious Insider Characteristics'''
{{header}}
{{article
|author= Nan (Peter) Liang,David P. Biros,Andy Luse,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Malicious insiders continue to pose a great threat to organizations. With their knowledge and access to organizational resources, malicious insiders could launch attacks more easily that result in more damaging impacts compared to outsiders. However, empirical research about malicious insiders is rare due to the unavailability of data. With few exceptions, many studies focus on a small number of cases. In order to identify common characteristics of a large number of malicious insiders, this study employs text mining to analyze 133 real-world cases of offenders from military units, intelligence agencies, and business organizations with data available to the public. Contributions of this study reside in two aspects: first, we use public data from documented malicious insider cases, implying a potentially valuable data source for future studies in this domain; second, we validate malicious insider characteristics identified in previous research, thereby establishing a foundation for more comprehensive research in the future.
|keyword = data classification,insider attacks,insider threat,malicious insider,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer-Mediated Deception: Strategies Revealed by Language-Action Cues in Spontaneous Communication'''
{{header}}
{{article
|author= Shuyuan Mary Ho,Jeffrey T. Hancock,Cheryl Booth,Xiuwen Liu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Computer-mediated deception threatens the security of online users' private and personal information. Previous research confirms that humans are bad lie detectors, while demonstrating that certain observable linguistic features can provide crucial cues to detect deception. We designed and conducted an experiment that creates spontaneous deception scenarios in an interactive online game environment. Logistic regression, and certain classification methodologies were applied to analyzing data collected during fall 2014 through spring 2015. Our findings suggest that certain language-action cues (e.g., cognitive load, affective process, latency, and wordiness) reveal patterns of information behavior manifested by deceivers in spontaneous online communication. Moreover, computational approaches to analyzing these language-action cues can provide significant accuracy in detecting computer-mediated deception.
|keyword = computer-mediated communication,computer-mediated deception,deception detection,deceptive communications,human-computer interaction,interpersonal deception theory,language-action cues,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Detecting Fraudulent Behavior on Crowdfunding Platforms: The Role of Linguistic and Content-Based Cues in Static and Dynamic Contexts'''
{{header}}
{{article
|author= Michael Siering,Jascha-Alexander Koch,Amit V. Deokar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Crowdfunding platforms offer founders the possibility to collect funding for project realization. With the advent of these platforms, the risk of fraud has risen. Fraudulent founders provide inaccurate information or pretend interest toward a project. Within this study, we propose deception detection support mechanisms to address this novel type of Internet fraud. We analyze a sample of fraudulent and nonfraudulent projects published at a leading crowdfunding platform. We examine whether the analysis of dynamic communication during the funding period is valuable for identifying fraudulent behavior-apart from analyzing only the static information related to the project. We investigate whether content-based cues and linguistic cues are valuable for fraud detection. The selection of cues and the subsequent feature engineering is based on theories in areas of communication, psychology, and computational linguistics. Our results should be helpful to the stakeholders of crowdfunding platforms and researchers of fraud detection.
|keyword = crowdfunding,deception detection,fraud detection,text mining,content-based cues,linguistic cues,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What Online Reviewer Behaviors Really Matter? Effects of Verbal and Nonverbal Behaviors on Detection of Fake Online Reviews'''
{{header}}
{{article
|author= Dongsong Zhang,Lina Zhou,Juan Luo Kehoe,Isil Yakut Kilic,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = The value and credibility of online consumer reviews are compromised by significantly increasing yet difficult-to-identify fake reviews. Extant models for automated online fake review detection rely heavily on verbal behaviors of reviewers while largely ignoring their nonverbal behaviors. This research identifies a variety of nonverbal behavioral features of online reviewers and examines their relative importance for the detection of fake reviews in comparison to that of verbal behavioral features. The results of an empirical evaluation using real-world online reviews reveal that incorporating nonverbal features of reviewers can significantly improve the performance of online fake review detection models. Moreover, compared with verbal features, nonverbal features of reviewers are shown to be more important for fake review detection. Furthermore, model pruning based on a sensitivity analysis improves the parsimony of the developed fake review detection model without sacrificing its performance.
|keyword = deception detection,eWoM,electronic word of mouth,feature pruning,fake online reviews,online reviewer behavior,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining Hacker Participation Length in Cybercriminal Internet-Relay-Chat Communities'''
{{header}}
{{article
|author= Victor Benjamin,Bin Zhang,Jr. Jay F. Nunamaker,Hsinchun Chen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = To further cybersecurity, there is interest in studying online cybercriminal communities to learn more about emerging cyber threats. Literature documents the existence of many online Internet Relay Chat (IRC) cybercriminal communities where cybercriminals congregate and share hacking tools, malware, and more. However, many cybercriminal community participants appear unskilled and have fleeting interests, making it difficult to detect potential long-term or key participants. This is a challenge for researchers and practitioners to quickly identify cybercriminals that may provide credible threat intelligence. Thus, we propose a computational approach to analyze cybercriminals IRC communities in order to identify potential long-term and key participants. We use the extended Cox model to scrutinize cybercriminal IRC participation for better understanding of behaviors exhibited by cybercriminals of importance. Results indicate that key cybercriminals may be quickly identifiable by assessing the scale of their interaction and networks with other participants.
|keyword = community participation,cybercrime,cybercriminal community,Internet Relay Chat,participation duration modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Untangling a Web of Lies: Exploring Automated Detection of Deception in Computer-Mediated Communication'''
{{header}}
{{article
|author= Stephan Ludwig,Tom Van Laer,Ko De Ruyter,Mike Friedman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = Safeguarding organizations against opportunism and severe deception in computer-mediated communication (CMC) presents a major challenge to chief information officers and information technology managers. New insights into linguistic cues of deception derive from the speech acts innate to CMC. Applying automated text analysis to archival e-mail exchanges in a CMC system as part of a reward program, we assess the ability of word use (micro level), message development (macro level), and intertextual exchange cues (meta level) to detect severe deception by business partners. We empirically assess the predictive ability of our framework using an ordinal multilevel regression model. Results indicate that deceivers minimize the use of referencing and self-deprecation but include more superfluous descriptions and flattery. Deceitful channel partners also over-structure their arguments and rapidly mimic the linguistic style of the account manager across dyadic e-mail exchanges. Thanks to its diagnostic value, the proposed framework can support firms' decision making and guide compliance monitoring system development.
|keyword = automated text analysis,channel partners,computer-mediated communication,deception detection,deception severity,linguistic cues,speech act theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Versioning: Go Vertical in a Horizontal Market?'''
{{header}}
{{article
|author= Debabrata Dey,Atanu Lahiri,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = The issue of versioning of information goods has resurfaced, in part as a result of the recent popularity of downloadable contents (DLC) among video game manufacturers. The central idea behind the DLC strategy, zero-day DLCs in particular, is that consumers who find the base version of a game to be sufficiently close to their tastes would want more of its capabilities and would pay a premium to upgrade by purchasing a DLC. To better understand the implications of such a product-line strategy, in this work, we combine the literature on versioning with that on consumer learning. In doing so, we uncover an interesting economic phenomenon that, for an experience good, a manufacturer's desire to vertically differentiate could actually stem from its inability to otherwise elicit unobserved heterogeneity in consumers' perceived fit. In other words, we generalize versioning to accommodate both vertical and horizontal heterogeneity.
|keyword = consumer learning,downloadable content,experience good,information good,product sampling,versioning,vertical differentiation,video games,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product Pricing in a Peer-to-Peer Economy'''
{{header}}
{{article
|author= Thomas A. Weber,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = The emergence of a collaborative economy has been driven by advances in information technology that allow consumers to borrow and rent goods among peers on a secondary sharing market. In a dynamic setting, consumers make intertemporal decisions about purchases and their participation in the sharing market. This study introduces an overlapping-generations model to analyze product pricing and consumer choice with and without a sharing market. The model quantifies the impacts of a peer-to-peer economy on the demand for ownership, the product price, and all participants' payoffs, including consumer surplus, profits, and social welfare. Given consumers that are heterogeneous with respect to their consumption needs and valuations, it illustrates which of them are prone to participate in a sharing economy and whether a retailer ( or manufacturer) can benefit from the presence of a secondary exchange. A sharing market tends to increase the price of new products by a "sharing premium," which positively depends on the retailer's commitment ability. The price increment becomes relatively smaller for higher-cost products. Low-cost products and sufficiently impatient consumers together make a peer-to-peer economy unattractive for retailers. For high-cost products, however, the latter stand to gain from the existence of a sharing market. Most important, the introduction of a peer-to-peer economy increases both consumer surplus and social welfare, thus creating an implicit imperative for a social planner to help promote collaborative consumption, for instance, by providing incentives for retailers and manufacturers that tend to offset possible expected negative payoff effects from consumer sharing.
|keyword = collaborative consumption,market equilibrium,monopoly pricing,overlapping generations,peer-to-peer economy,product pricing,sharing economy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Perverse Effects in Defense of Computer Systems: When More Is Less'''
{{header}}
{{article
|author= Josephine Wolff,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2016
|abstract = With computer security spending on the rise, organizations seem to have accepted the notion that buying more-and more expensive-defenses allows them to better protect their computer systems. In the context of complex computer systems, however, defenses can also have the opposite effect, creating new, unforeseen vulnerabilities in the systems they are intended to protect. Advocacy for defense-in-depth and diverse security measures has contributed to this "more is better" mentality for defending computer systems, which fails to consider the complex interaction of different components in these systems, especially with regard to what impact new security controls may have on the operation and functionality of other, preexisting defenses. We give examples of several categories of perverse effects in defending computer systems and draw on the theory of unintended consequences and the duality of technology to analyze the origins of these perverse effects, and to develop a classification scheme for the different types and some methods for avoiding them.
|keyword = complex systems,computer systems,cyber defense,cyber-security,defense-in-depth,information security,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AMBIENT AWARENESS AND KNOWLEDGE ACQUISITION: USING SOCIAL MEDIA TO LEARN "WHO KNOWS WHAT" AND "WHO KNOWS WHOM"'''
{{header}}
{{article
|author= Paul M. Leonardi,
|source= MIS QUARTERLY
|year= 2015
|abstract = The argument proffered in this paper is that use of enterprise social networking technologies can increase the accuracy of people's metaknowledge (knowledge of "who knows what" and "who knows whom") at work. The results of a quasi-natural field experiment in which only one of two matched-sample groups within a large financial services firm was given access to the enterprise social networking technology for six months revealed that by making people's communications with specific partners visible to others in the organization, the technology enabled observers to become aware of the communications occurring amongst their coworkers and to make inferences about what and whom those coworkers knew based on the contents of the messages they sent and to whom they were sent. Consequently only individuals in the group that used the social networking technology for six months improved the accuracy of their metaknowledge (a 31% improvement in knowledge of who knows what and an 88% improvement in knowledge of who knows whom). There were no improvements in the other group over the same time period. Based on these findings, how technologically enabled "ambient awareness"-awareness of ambient communications occurring amongst others in the organization-can be an important antecedent for knowledge acquisition is discussed.
|keyword = Social networking sites,knowledge sharing,organizational learning,accuracy,technology use,communication,collaboration,transparency,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WORK HARDER OR WORK SMARTER? INFORMATION TECHNOLOGY AND RESOURCE ALLOCATION IN HEALTHCARE PROCESSES'''
{{header}}
{{article
|author= Adrian Yeow,Kim Huat Goh,
|source= MIS QUARTERLY
|year= 2015
|abstract = While the impacts of health information technology (HIT) are widely studied, prior research presents mixed findings. In this study, a granular examination of the impact of HIT systems on how resources are allocated to healthcare tasks and processes was undertaken. A longitudinal field study that combined interview, archival, observation, and survey data was conducted. The effects of telemedicine on the input allocative efficiency of the healthcare process through the reallocation of organizational resources was evaluated and an assessment of whether gains in allocative efficiency resulted in improvements in organizational outcomes, such as lower hospitalization rates and lower uncertainty in patient wait time, was conducted. Applying the theory of swift and even flow, our findings suggest that the gains in allocative efficiency for some processes are associated with improved organizational outcomes.
|keyword = Healthcare,IT,telemedicine,stochastic frontier analysis,resource allocations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SOFTWARE PROCESS DIVERSITY: CONCEPTUALIZATION, MEASUREMENT, AND ANALYSIS OF IMPACT ON PROJECT PERFORMANCE'''
{{header}}
{{article
|author= Narayan Ramasubbu,Anandhi Bharadwaj,Giri Kumar Tayi,
|source= MIS QUARTERLY
|year= 2015
|abstract = This article investigates software process diversity, defined as the project condition arising out of the simultaneous use of multiple software development process frameworks within a single project. Software process diversity is conceptualized as the response of a project team to such contingencies as requirements volatility, design and technological novelty, customer involvement, and the level of organizational process compliance enforced on the project. Moreover, we conceptualize that the degree of fit (or match) between a project's software process diversity and the level of process compliance enforced on the project impacts overall project performance. This conceptualization was empirically tested by utilizing data collected from 410 large commercial software projects of a multinational firm. The results show that higher levels of requirements volatility, design and technological novelty, and customer involvement increased software process diversity within a project. However, software process diversity decreased relative to increases in the level of process compliance enforced on the project. A higher degree of fit between the process diversity and process compliance of a project, rather than the effects of those variables independently, was found to be significantly associated with a higher level of project performance, as measured in terms of project productivity and software quality. These results indicate that increasing software process diversity in response to project-level contingencies improves project performance only when there is a concomitant increase in organizational process compliance efforts. The implications of these results for research are discussed and prescriptive guidelines derived to manage the fit between process diversity and process compliance for improving software project performance.
|keyword = Software process diversity,process compliance,plan-based processes,agile processes,software engineering,productivity,quality,fit as matching,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY IMPACTS ON FIRM PERFORMANCE: AN EXTENSION OF KOHLI AND DEVARAJ (2003)'''
{{header}}
{{article
|author= Rajiv Sabherwal,Anand Jeyaraj,
|source= MIS QUARTERLY
|year= 2015
|abstract = Despite the importance of investing in information technology, research on business value of information technology (BVIT) shows contradictory results, raising questions about the reasons for divergence. Kohli and Devaraj (2003) provided valuable insights into this issue based on a meta-analysis of 66 BVIT studies. This paper extends Kohli and Devaraj by examining the influences on BVIT through a meta-analysis of 303 studies published between 1990 and 2013. We found that BVIT increases when the study does not consider IT investment, does not use profitability measure of value, and employs primary data sources, fewer IT-related antecedents, and larger sample size. Considerations of IT alignment, IT adoption and use, and interorganizational IT strengthen the relationship between IT investment on BVIT, whereas the focus on environmental theories dampens the same relationship. However, the use of productivity measures of value, the number of dependent variables, the economic region, the consideration of IT assets and IT infrastructure or capability, and the consideration of IT sophistication do not affect BVIT. Finally, BVIT increases over time with IT progress. Implications for future research and practice are discussed.
|keyword = Information technology,business value of information technology,firm performance,meta-analysis,investment,payoff,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT DO SYSTEMS USERS HAVE TO FEAR? USING FEAR APPEALS TO ENGENDER THREATS AND FEAR THAT MOTIVATE PROTECTIVE SECURITY BEHAVIORS'''
{{header}}
{{article
|author= Scott R. Boss,Dennis F. Galletta,Paul Benjamin Lowry,Gregory D. Moody,Peter Polak,
|source= MIS QUARTERLY
|year= 2015
|abstract = Because violations of information security (ISec) and privacy have become ubiquitous in both personal and work environments, academic attention to ISec and privacy has taken on paramount importance. Consequently, a key focus of ISec research has been discovering ways to motivate individuals to engage in more secure behaviors. Over time, the protection motivation theory (PMT) has become a leading theoretical foundation used in ISec research to help motivate individuals to change their security-related behaviors to protect themselves and their organizations. Our careful review of the foundation for PMT identified four opportunities for improving ISec PMT research. First, extant ISec studies do not use the full nomology of PMT constructs. Second, only one study uses fear-appeal manipulations, even though these are a core element of PMT. Third, virtually no ISec study models or measures fear. Fourth, whereas these studies have made excellent progress in predicting security intentions, none of them have addressed actual security behaviors. This artticle describes the theoretical foundation of these four opportunities for improvement. We tested the nomology of PMT, including manipulated fear appeals, in two different ISec contexts that model the modern theoretical treatment of PMT more closely than do extant ISec studies. The first data collection was a longitudinal study in the context of data backups. The second study was a short-term cross-sectional study in the context of anti-malware software. Our new model demonstrated better results and stronger fit than the existing models and confirms the efficacy of the four potential improvements we identified.
|keyword = Information security,protection motivation theory,system backups,model comparison,fear appeals,threat,coping,intentions,behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FIT AND MISFIT OF PLURAL SOURCING STRATEGIES AND IT-ENABLED PROCESS INTEGRATION CAPABILITIES: CONSEQUENCES OF FIRM PERFORMANCE IN THE US ELECTRIC UTILITY INDUSTRY'''
{{header}}
{{article
|author= Arun Rai,Ilgaz Arikan,Jessica Pye,Amrit Tiwana,
|source= MIS QUARTERLY
|year= 2015
|abstract = Recent work has shown that a firm's plural sourcing strategy, which determines how much it chooses to make versus how much it chooses to buy, requires consideration of the complementarities and constraints that affect the differential advantages of making and buying. Elaborating on this perspective, we theorize how (mis)fit between a firm's plural sourcing strategy of simultaneously making and buying and its development of information technology (IT) enabled interfirm and intrafirm process integration capabilities influences firm performance in deregulated markets. We position our theory development and empirical tests in the context of the power-generation segment of the U.S. electric utility industry (EUI), an asset-intensive industry that has been deregulated to promote the separation of key value chain activities (i.e., generation, transmission, and distribution) and the development of wholesale energy markets. We draw on the transaction cost economics, coordination costs, and IT capabilities perspectives to theorize that a firm achieves fit (realizing performance benefits) by increasing market sourcing intensity (MSI)-or, how much it buys relative to how much it makes- and developing IT-enabled interfirm process integration capability for external coordination with the market, or misfit (realizing performance penalties) by increasing MSI and developing IT-enabled intrafirm process integration capability for coordinating internal production. We collated data from archival sources for 342 utility firms in the power-generation segment to construct a panel dataset for the period 1994-2004 on (1) firms' MSI from wholesale electricity markets, (2) firms' IT investment decisions to develop interfirm and intrafirm process integration capabilities, (3) measures of firm performance, and (4) several control variables related to exogenous shocks (i.e., regulatory change, oil crisis), region of operation, and firm-level factors. Our results suggest that fit between MSI and the development of IT-enabled interfirm process integration capability improves firm profitability, assessed by return on assets, and misfit between MSI and the development of IT-enabled intrafirm process integration capability extracts penalties in firm profitability. We also find evidence that fit between MSI and the development of IT-enabled interfirm process integration capability improves market valuation, assessed by Tobin's Q, and asset turnover, assessed by operating revenue/total assets. We discuss the implications of our findings for the development of IT capabilities to accompany a firm's plural sourcing strategy and the literature on IT business value.
|keyword = Plural sourcing,market sourcing intensity,IT-enabled process integration,IT capabilities,firm performance,IT business value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STRIKE A HAPPY MEDIUM: THE EFFECT OF IT KNOWLEDGE ON VENTURE CAPITALISTS' OVERCONFIDENCE IN IT INVESTMENTS'''
{{header}}
{{article
|author= Harpreet Singh,Rohit Aggarwal,Irina Cojuharenco,
|source= MIS QUARTERLY
|year= 2015
|abstract = In this article, the effect of IT knowledge on the overconfidence of venture capitalists (VCs) in their IT investments is examined. Our findings show that the effect of IT knowledge on overconfidence is nonlinear. VCs with moderate levels of IT knowledge are least overconfident. At the same time, VCs with moderate levels of IT knowledge are most resistant to the biasing effects of past successes. Past failures show a negative association with overconfidence independent of the level of the VC's IT knowledge. Finally, the negative association between stakes and VC overconfidence is stronger with greater levels of IT knowledge. These results shed light on the highly disputed role of IT knowledge in the domain of IT investments.
|keyword = IT knowledge,overconfidence,IT startups,IT investments,venture capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CAMPUS EMERGENCY NOTIFICATION SYSTEMS: AN EXAMINATION OF FACTORS AFFECTING COMPLIANCE WITH ALERTS'''
{{header}}
{{article
|author= Wencui Han,Serkan Ada,Raj Sharman,H. Raghav Rao,
|source= MIS QUARTERLY
|year= 2015
|abstract = The increasing number of campus-related emergency incidents, in combination with the requirements imposed by the Clery Act, have prompted college campuses to develop emergency notification systems to inform community members of extreme events that may affect them. Merely deploying emergency notification systems on college campuses, however, does not guarantee that these systems will be effective; student compliance plays a very important role in establishing such effectiveness. Immediate compliance with alerts, as opposed to delayed compliance or noncompliance, is a key factor in improving student safety on campuses. This paper investigates the critical antecedents that motivate students to comply immediately with messages from campus emergency notification systems. Drawing on Etzioni's compliance theory, a model is developed. Using a scenario-based survey method, the model is tested in five types of events-snowstorm, active shooter, building fire, health-related, and robbery-and with more than 800 college students from the Northern region of the United States. The results from this study suggest that subjective norm and information quality trust are, in general, the most important factors that promote immediate compliance. This research contributes to the literature on compliance, emergency notification systems, and emergency response policies.
|keyword = Compliance,campus alerts,emergency notification systems,information quality trust,scenario-based survey,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CULTURAL SENSEMAKING IN OFFSHORE INFORMATION TECHNOLOGY SERVICE SUPPLIERS: A CULTURAL FRAME PERSPECTIVE'''
{{header}}
{{article
|author= Ning Su,
|source= MIS QUARTERLY
|year= 2015
|abstract = In today's global IT outsourcing relationships, individual employees need to operate effectively in culturally diverse environments. Such intercultural interactions can be especially challenging for members of IT service suppliers based in offshore locations. Through an in-depth qualitative case study of one of the largest China-based IT service firms with diverse clients from Japan, the United States, and China, this research elaborates the cultural sensemaking activities of the supplier's individual employees. Specifically, drawing on the dynamic constructivist view of culture, this study develops the construct of "cultural frames" in the context of global IT outsourcing to characterize the knowledge structures guiding an individual's collaboration with diverse clients. A portfolio of cultural frames emerges and evolves through the individual's cultural sensemaking activities, which consist of the iterative enactment, alignment, and retention of cultural frames. In the cultural sensemaking process, the activity of frame bridging, in particular, creates significant value for the outsourcing relationship, and is especially salient among bicultural employees.
|keyword = IT outsourcing,IT supplier,cultural sensemaking,cultural frame,China,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Informing Privacy Research Through Information Systems, Psychology, and Behavioral Economics: Thinking Outside the "APCO" Box'''
{{header}}
{{article
|author= Tamara Dinev,Allen R. McConnell,H. Jeff Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Recently, several researchers provided overarching macromodels to explain individuals' privacy-related decision making. These macromodels-and almost all of the published privacy-related information systems (IS) studies to date-rely on a covert assumption: responses to external stimuli result in deliberate analyses, which lead to fully informed privacy-related attitudes and behaviors. The most expansive of these macromodels, labeled "Antecedents- Privacy Concerns-Outcomes" (APCO), reflects this assumption. However, an emerging stream of IS research demonstrates the importance of considering principles from behavioral economics (such as biases and bounded rationality) and psychology (such as the elaboration likelihood model) that also affect privacy decisions. We propose an enhanced APCO model and a set of related propositions that consider both deliberative (high-effort) cognitive responses (the only responses considered in the original APCO model) and low-effort cognitive responses inspired by frameworks and theories in behavioral economics and psychology. These propositions offer explanations of many behaviors that complement those offered by extant IS privacy macromodels and the information privacy literature stream. We discuss the implications for research that follow from this expansion of the existing macromodels.
|keyword = privacy,macromodels,elaboration likelihood model,behavioral economics,psychology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Discriminating IT Governance'''
{{header}}
{{article
|author= Amrit Tiwana,Stephen K. Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = The information technology (IT) governance literature predominantly explains firms' IT governance choices, but not their strategic consequences. We develop the idea that a firm's IT governance choices induce adeptness at strategically exploiting IT only when they are discriminatingly aligned with its departments' knowledge outside their specialty. Discriminating means that governing the two undertheorized classes of IT assets-apps and infrastructure-requires "peripheral" knowledge in different departments. Analyses of data from 105 firms support our middle-range theory.
|keyword = IT governance,IT infrastructure,IT applications,discriminating alignment,IT agility,Garen method,IT strategy,endogeneity,IT asset classes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Embarrassing Exposures in Online Social Networks: An Integrated Perspective of Privacy Invasion and Relationship Bonding'''
{{header}}
{{article
|author= Ben C. F. Choi,Zhenhui (Jack) Jiang,Bo Xiao,Sung S. Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Online social networks greatly facilitate social exchange among friends. At times, for amusement, individuals may be targeted by friends' playful teases, which often involve exposing individuals' private embarrassing information, such as information that reveals their past indecent behavior, mischief, or clumsiness. Although individuals sometimes do enjoy the humor, they might also be offended by the involuntary exposure. Drawing on social exchange theory, this paper elucidates the consequences of an embarrassing exposure in online social networks. Specifically, this study examines the effects of information dissemination and network commonality on individuals' exchange assessment as well as how this assessment shapes their behavioral responses. The results of our experiment provide strong evidence that information dissemination and network commonality jointly influence individuals' perceived privacy invasion and perceived relationship bonding. In addition, whereas perceived privacy invasion increases transactional avoidance, it reduces approach behavior. Furthermore, whereas perceived relationship bonding impedes both transactional avoidance and interpersonal avoidance, it leads to approach behavior. The theoretical and practical implications of the findings are discussed.
|keyword = online social networks,embarrassing exposure,privacy invasion,relationship bonding,inaction,avoidance behavior,approach behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do Organic Results Help or Hurt Sponsored Search Performance?'''
{{header}}
{{article
|author= Ashish Agarwal,Kartik Hosanagar,Michael D. Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = We study the impact of changes in the competitors' listings in organic search results on the performance of sponsored search advertisements. Using data from an online retailer's keyword advertising campaign, we measure the impact of organic competition on both click-through rate and conversion rate of sponsored search advertisements. We find that an increase in organic competition leads to a decrease in the click performance of sponsored advertisements. However, organic competition helps the conversion performance of sponsored ads and leads to higher revenue. We also find that organic competition has a higher negative effect on click performance than does sponsored competition. Our results inform advertisers on how the presence of organic results influences the performance of their sponsored advertisements. Specifically, we show that organic competition acts as a substitute for clicks, but has a complementary effect on the conversion performance.
|keyword = sponsored search,organic search,ad placement,hierarchical Bayesian estimation,online advertising,online auctions,search engine marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design of Consumer Review Systems and Product Pricing'''
{{header}}
{{article
|author= Yabing Jiang,Hong Guo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Consumer review systems have become an important marketing communication tool through which consumers share and learn product information. Although there is abundant evidence that consumer reviews have a significant impact on product sales, the design of consumer review systems and its impact on review outcomes and product sales have not yet been well examined. This paper analyzes firms' review system design and product pricing strategies. We formally model two review system design decisions-what rating scale cardinality to use and whether to offer granular review reports. We show that firms' optimal design and pricing strategies critically depend on contextual characteristics such as product valuation, product mainstream level, and consumer misfit cost. Our results suggest that it is beneficial to host a review system only when the product valuation is higher than a threshold. Furthermore, firms should choose low rating scale cardinality for niche products and high rating scale cardinality for mainstream products. When consumers' misfit cost is relatively high, including granular reports in the review system enables firms to attract the favorable consumer segment. Different pricing strategies should be deployed during the initial sale period for different product types. For niche products, firms are advised to adopt lower-bound pricing for high-quality products to take advantage of the positive word of mouth. For mainstream products, firms are advised to adopt upper-bound pricing for high-quality products to enjoy the direct profit from the initial sale period, even after taking into account the negative impact of high price on consumer reviews.
|keyword = economics of IS,electronic commerce,consumer reviews,online word-of-mouth systems,product uncertainty,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Versioning in the Software Industry: Heterogeneous Disutility from Underprovisioning of Functionality'''
{{header}}
{{article
|author= Shivendu Shivendu,Zhe (James) Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Literature has identified factors such as piracy, network externality, or concave cost of producing quality as key drivers of software versioning. However, software firms adopt versioning strategies that are often invariant across different market settings. To explain universal business practice of software versioning, we focus on "inconvenience" or disutility that users experience when software has lower functionality than what they require to accomplish tasks. In our model, users are heterogeneous on marginal valuation for functionality and the required level of functionality such that those with higher valuation have a higher required level of functionality. Users do not derive any additional utility if the software has more functionality than what they require. We show that heterogeneous disutility from underprovisioning of functionality is a sufficient condition for optimality of versioning under fairly general conditions. We also show that, as high-type users' required level of functionality increases, the firm increases the functionality level of the high version. Yet surprisingly, the firm may decrease the functionality level of the low version if the proportion of high-type users is moderate. On the other hand, as the required level of functionality of low-type users increases, the firm may reduce the functionality level of the low version when the proportion of high-type users is high, though the functionality level of the high version remains the same. Counterintuitively, an increase in the high-type (low-type) users' required level of functionality negatively (positively) impacts high-type users' consumer surplus.
|keyword = vertical differentiation,versioning,pricing,disutility from underprovisioning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Can Payment-per-Click Induce Improvements in Click Fraud Identification Technologies?'''
{{header}}
{{article
|author= Min Chen,Varghese S. Jacob,Suresh Radhakrishnan,Young U. Ryu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Pay-per-click (PPC) is a common pricing model used to pay for ads on the Web and is open to the possibility for click fraud, where clicks are not from a legitimate user. Identifying click fraud is generally done in a three-stage process: the service provider (SP) first classifies clicks as fraudulent or not, then the advertiser does the same with a different technology, and if there is a disagreement, the SP examines further and his conclusions are considered binding. The advertiser pays for clicks that are identified as valid in the first two stages or confirmed as valid in the last stage. We model the choice of the identification technologies as a double moral hazard problem. We analyze the case where the PPC is incentive compatible to overcome the moral hazard problem, and examine the question of whether the incentive compatible PPC is sufficient to incentivize the two parties to unilaterally make further improvements to their identification technologies and simultaneously increase their profits. We show that when the cost of the third-stage identification technology is large, which is likely to be the case because of its complexity and use of expensive human experts, the incentive compatible PPC does not support unilateral technological improvements. We then examine a setting where the third-stage identification is delegated to a third party and find that this arrangement can induce unilateral improvements to the identification technologies in the first two stages. Collectively our results show that although the PPC model itself may not induce improvements in the first two stages of click fraud identification, a common arrangement espoused of having a third party resolve disagreements helps make PPC support unilateral technological improvements. Accordingly, we show an indirect benefit to the third-party arrangement.
|keyword = click fraud,online advertising,game theory,double moral hazard,incentives,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What Motivates Contributors vs. Lurkers? An Investigation of Online Feedback Forums'''
{{header}}
{{article
|author= Chee Wei Phang,Atreyi Kankanhalli,Bernard C. Y. Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Organizations are setting up online forums to obtain inputs and feedback from key stakeholders, such as employees, customers, and citizens. Examples of such virtual spaces are online policy deliberation forums (OPDFs) initiated by government organizations to garner citizens' views on policy issues. Incorporating the inputs from these forums can result in more inclusive policies for societal benefit. Yet, as with other such forums, a common issue facing OPDFs is the sustainability of participation. When examining this issue, previous research has mostly explored the participation antecedents of existing contributors. However, engaging lurkers is also important, because these forums need to compensate for contributor attrition and become more effective with greater reach. Thus motivated, this study develops a model to explain the antecedents of both contributors' and lurkers' participation deriving from public participation and information technology-enabled public goods theories. It hypothesizes differences in the antecedents for contributors versus lurkers based primarily on construal level theory. The model was empirically validated through a survey of contributors and lurkers in a nationwide OPDF. The results reveal significant differences in the participation antecedents of the two groups as hypothesized. Specifically, contributors are influenced by political career benefit and political efficacy motives, whereas lurkers' future participation intention is driven by collective benefits, possession of civic skills, and mobilization. Furthermore, perceived connectivity of the OPDF directly influences participation intention for contributors and indirectly impacts participation intention for both groups via perceived communality. Perceived communality, on the other hand, influences collective and persuasion benefits for both contributors and lurkers. These findings are useful for understanding and promoting participation through differential strategies for contributors and lurkers in OPDFs in particular, and by extension, other feedback or online forums.
|keyword = online policy deliberation forums,online feedback forums,contributors,lurkers,public participation theories,IT-enabled public goods theory,construal level theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing Warning Messages for Detecting Biased Online Product Recommendations: An Empirical Investigation'''
{{header}}
{{article
|author= Bo Xiao,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = The increasing adoption of product recommendation agents (PRAs) by e-commerce merchants makes it an important area of study for information systems researchers. PRAs are a type of Web personalization technology that provides individual consumers with product recommendations based on their product-related needs and preferences expressed explicitly or implicitly. Whereas extant research mainly assumes that such recommendation technologies are designed to benefit consumers and focuses on the positive impact of PRAs on consumers' decision quality and decision effort, this study represents an early effort to examine PRAs that are designed to produce their recommendations on the basis of benefiting e-commerce merchants (rather than benefiting consumers) and to investigate how the availability and the design of warning messages (a potential detection support mechanism) can enhance consumers' performance in detecting such biased PRAs. Drawing on signal detection theory, the literature on warning messages, and the literature on message framing, we identified two content design characteristics of warning messages-the inclusion of risk-handling advice and the framing of risk-handling advice-and investigated how they influence consumers' detection performance. The results of an online experiment reveal that a simple warning message without accompanying advice on how to detect bias is a double-edged sword, because it increases correct detection of biased PRAs (hits) at the cost of increased incorrect detection (false alarms). By contrast, including in warning messages risk-handling advice about how to check for bias (particularly when the advice is framed to emphasize the loss from not following the advice) increases correct detection and, more importantly, also decreases incorrect detection. The patterns of findings are in line with the predictions of signal detection theory. With an enriched understanding of how the availability and the content design of warning messages can assist consumers in the context of PRA-assisted online shopping, the results of this study serve as a basis for future theoretical development and yield valuable insights that can guide practice and the design of effective warning messages.
|keyword = electronic commerce,product recommendation agent,personalization,bias,signal detection theory,manipulative practices,warning,message framing,online experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Tigerblood: Newspapers, Blogs, and the Founding of Information Technology Firms'''
{{header}}
{{article
|author= Brad N. Greenwood,Anand Gopal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = In this paper, we study the impact of increases in media coverage from two sources, newspapers and blogs, on firm founding rates in the context of technology-based entrepreneurship. Although increasing work in information systems (IS) has begun to investigate the effect of user-generated content on entrepreneurial behavior, limited attention has been devoted to how media affects firm founding or the boundary conditions of such an effect. Arguing for the direct effect of increased discourse in traditional and user-generated media in the information technology (IT) industry, results suggest that discourse in traditional media and blogs strongly influences IT firm founding rates. We further consider the differential impacts of media discourse on firm founding in different IT subsectors, over time, and in different locations. We test our hypotheses using entrepreneurial firm founding data from VentureXpert from 1998 to 2007, social media data from the three largest blogging platforms, and traditional media coverage from 11 major U.S. newspapers. Our work contributes to a better understanding of the concurrent effects of multiple forms of media on decision making and adds to the small but emerging literature addressing entrepreneurship-related research questions in IS.
|keyword = entrepreneurship,information technology firms,firm founding,media,newspapers,blogs,user-generated media,econometric analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Parenting New Acquisitions: Acquirers' Digital Resource Redeployment and Targets' Performance Improvement in the US Hospital Industry'''
{{header}}
{{article
|author= Kui Du,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This paper examines how information technology ( IT) can contribute to value creation in horizontal acquisitions. We propose that acquisition value can be created when an acquirer redeploys its digital resources to its newly acquired businesses and consequently improves their operations. However, not all acquirers are equally capable of redeploying their digital resources. In this study, we propose two enabling factors pertaining to an acquirer's IT resource base: IT extensiveness and IT standardization. We argue that the magnitude of digital resource redeployment increases when the acquirer has had extensive use of IT systems within its existing business units and has standardized IT systems across its business units. Moreover, the relative strength of the IT resources of the acquirer, as compared to those of the acquired business, also affects the reuse of the acquirer's IT resources in its digital resource redeployment activities. We empirically test these hypotheses by tracking the IT and performance changes in 108 U.S. hospitals before and after they were acquired across a seven-year study timeframe.
|keyword = acquisitions,resource redeployment,IT extensiveness,IT standardization,IT gap,hospitals,efficiency and quality of healthcare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Patching the Cloud: The Impact of SaaS on Patching Strategy and the Timing of Software Release'''
{{header}}
{{article
|author= Vidyanand Choudhary,Zhe (James) Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This paper extends prior research on the software vendors' optimal release time and patching strategy in the context of cloud computing and software as a service (SaaS). Traditionally, users are responsible for running on-premises software; by contrast, a vendor is responsible for running SaaS software, and the SaaS vendor incurs a larger proportion of defect-related costs than a vendor of on-premises software. We examine the effect of this difference on a vendor's choice of when to release software and the proportion of software defects to fix. Surprisingly, we find that, despite incurring a larger proportion of defect-related costs, it is optimal for the SaaS vendor to release software earlier and with more defects, and to patch a smaller proportion of defects, than the on-premises software vendor. Even though the SaaS vendor incurs higher defect-related costs, he obtains a larger profit than the traditional vendor. In addition, we find that for a vendor who uses the SaaS model, the optimal number of defects after patching may be lower than the socially efficient outcome. This occurs despite the fact that the number of defects after patching in the SaaS model is higher than in the traditional on-premises model.
|keyword = software security,cloud,software as a service,patch management,software release time,software maintenance,defect-related costs,economics of information systems,monopoly,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Can't Buy Me Love ... Or Can I? Social Capital Attainment Through Conspicuous Consumption in Virtual Environments'''
{{header}}
{{article
|author= Oliver Hinz,Martin Spann,Il-Horn Hann,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Conspicuous consumption affects anyone who cares about social status; it has intrigued sociologists and economists for more than 100 years. The idea that conspicuous consumption can increase social status, as a form of social capital, has been broadly accepted, yet researchers have not been able to test this effect empirically. In this work, we provide empirical evidence by analyzing the digital footprints of purchases and social interactions in different virtual worlds. We use a multimethod approach, such that we both analyze transactional data and conduct a randomized field experiment. Virtual worlds, as artificial laboratories, offer the opportunity to analyze the social capital of their inhabitants, subsequent to their purchase of virtual prestige goods, which provides a means to empirically test hypotheses that would be nearly impossible to test in real-world settings. Our results are consistent with the notion that conspicuous consumption represents an investment in social capital.
|keyword = social status,social capital,conspicuous consumption,prestige goods,virtual worlds,randomized field experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FAIRNESS IN THE INSTITUTIONAL VALUATION OF BUSINESS JOURNALS'''
{{header}}
{{article
|author= Gary F. Templeton,Bruce R. Lewis,
|source= MIS QUARTERLY
|year= 2015
|abstract = The fairness of performance evaluation is a concern for all professions, and the appraisal of research output is of particular interest to business scholars and academic administrators. We describe research assessment as a process of social construction that is heavily influenced by journal valuation in business schools. Using journal quality data from multiple sources, we empirically investigate whether the journals in each of eight business disciplines (Accounting, Economics, Finance, Information Systems, Management, Marketing, Operations Management, and Quantitative Methods) are treated evenly across the board. Specifically, we explore whether each business discipline exhibits recognition fairness (i.e., actual institutional journal evaluations are the same as market expectations) and inclusion fairness (i.e., actual availability of publication space in top journals being the same as market expectations). Our findings indicate that faculty in some disciplines enjoy an advantage, while faculty in other fields are disadvantaged. Consequently, we offer recommendations to ameliorate this inequity.
|keyword = Journal valuation fairness,institutional and public business journal lists,citation metrics,research evaluation,social constructionism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GENRES OF INQUIRY IN DESIGN-SCIENCE RESEARCH: JUSTIFICATION AND EVALUATION OF KNOWLEDGE PRODUCTION'''
{{header}}
{{article
|author= Richard L. Baskerville,Mala Kaul,Veda C. Storey,
|source= MIS QUARTERLY
|year= 2015
|abstract = Recognizing that design is at the core of information systems development has led to a design-science research paradigm where differing kinds of knowledge goals give form to differing kinds of knowledge processes within a single study. This paper analyzes knowledge production in design-science research to explain how an endogenous form of pluralism characterizes such studies, making it problematic to associate any design-science study with a single view of knowledge production. Instead, a design-science research study exhibits up to four different modes of reasoning, called genres of inquiry. These genres are derived from two dualities that contrast differing knowledge goals and differing knowledge scope in the knowledge production process. The first duality arises from the sometimes seemingly contradictory knowledge goals of science versus design. The second duality reflects the contradiction between the scope of the knowledge produced, which may be idiographic or nomothetic. The evolutionary and iterative nature of a design-science study compels different knowledge goals and scope at different moments throughout a project. Because of this momentary nature, a single design-science study can be associated with multiple genres of inquiry. This understanding of the variety in the genres of inquiry advances the discourse on the nature of design-science research and the justification and evaluation of its outcomes. Consequently, a corresponding set of criteria for knowledge justification and evaluation is provided for each genre of inquiry.
|keyword = Design science research,genres of inquiry,evaluation,duality,knowledge scope,knowledge goal,knowledge moment,centrality of knowledge,idiographic science,nomothetic science,endogenous pluralism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''VOCAL MINORITY AND SILENT MAJORITY: HOW DO ONLINE RATINGS REFLECT POPULATION PERCEPTIONS OF QUALITY'''
{{header}}
{{article
|author= Guodong (Gordon) Gao,Brad N. Greenwood,Ritu Agarwal,Jeffrey S. McCullough,
|source= MIS QUARTERLY
|year= 2015
|abstract = Consumer-generated ratings typically share an objective of illuminating the quality of a product or service for other buyers. While ratings have become ubiquitous and influential on the Internet, surprisingly little empirical research has investigated how these online assessments reflect the opinion of the population at large, especially in the domain of professional services where quality is often opaque to consumers. Building on the word-of-mouth literature, we examine the relationship between online ratings and population perceptions of physician quality. We leverage a unique dataset which includes direct measures of both the offline population's perception of physician quality and consumer-generated online reviews. As a result, we are able to examine how online ratings reflect patients' opinions about physician quality. In sharp contrast to the widely voiced concerns by medical practitioners, we find that physicians who are rated lower in quality by the patient population are less likely to be rated online. Although ratings provided online are positively correlated with patient population opinions, the online ratings tend to be exaggerated at the upper end of the quality spectrum. This study is the first to provide empirical evidence of the relationship between online ratings and the underlying consumer-perceived quality, and extends prior research on online word-of-mouth to the domain of professional services.
|keyword = Online ratings,physician quality,online word-of-mouth,professional services,informativeness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''JAMMING WITH SOCIAL MEDIA: HOW COGNITIVE STRUCTURING OF ORGANIZING VISION FACETS AFFECTS IT INNOVATION DIFFUSION'''
{{header}}
{{article
|author= Shaila M. Miranda,Inchan Kim,Jama D. Summers,
|source= MIS QUARTERLY
|year= 2015
|abstract = Organizing vision theory is an institutional alternative to the economic-rationality view of IT innovation diffusion. Institutional theorists have called for more attention to cognitive processes and structures in order to understand institutional mechanisms. Our objective was to unpack the cognitive structure of an organizing vision to understand its role in the diffusion of IT innovations. We focus on the know-why component of organizing visions and on social media as an IT innovation. In a two-stage study, Stage I leveraged schema theory, the "orders of worth" framework's six justificatory principles, and relational class analysis to discover the hierarchical structure of the social media organizing vision. This resulted in a view of the organizing vision as comprised of four schemas, which we conceptualized as visions-in-use, and ten nested business use cases, each comprised of different combinations of the six principles. Based on this understanding, Stage II explored how community appropriations of visions-in-use and business use cases from the repertoire provided by an organizing vision shape four facets of an organizing vision-coherence, continuity, clarity, and diversity-and how these facets influence diffusion of the IT innovation. We found that the two vision facets we surfaced-clarity and diversity-are essential to understanding diffusion and how and why coherence and continuity matter to diffusion. Much as the "vision" of a musical jam session emerges from players' multivocal performances, an organizing vision emerges from community members' multivocal discourse about an IT innovation. Just as a jam session depends on a structure of rules and individual player creativity, diffusion of an IT innovation depends on an organizing vision that offers prospective adopters a well-defined repertoire of moves to choose from, yet affords them the freedom to improvise.
|keyword = IT innovation,organizing vision,social media,schema,relational class analysis,quantitative grounded theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY USE AS A LEARNING MECHANISM: THE IMPACT OF IT USE ON KNOWLEDGE TRANSFER EFFECTIVENESS, ABSORPTIVE CAPACITY, AND FRANCHISEE PERFORMANCE'''
{{header}}
{{article
|author= Kishen Iyengar,Jeffrey R. Sweeney,Ramiro Montealegre,
|source= MIS QUARTERLY
|year= 2015
|abstract = This study aims to contribute to the literature through the theoretical development and empirical investigation of the role of information technology use in organizational learning. We develop a theoretical framework that unpacks organizational learning into mechanisms and outcomes. The outcomes of organizational learning are distinguished at two levels: first-order and second-order. Based on the framework, we propose a research model set in the franchising context. We conceptualize franchisee use of IT provided by the franchisor as an important learning mechanism that impacts knowledge transfer effectiveness (first-order outcome) and absorptive capacity (second-order outcome). Further, the influence of IT use on financial performance is mediated through absorptive capacity. The model was tested on a sample of 783 independently owned real-estate franchisees using a comprehensive dataset comprised of primary and secondary data. The results indicate that IT use is an important learning mechanism for franchisees by impacting knowledge transfer effectiveness and absorptive capacity. In turn, absorptive capacity mediates the relationship between IT use and financial performance. The empirical support for the research model serves to affirm the underlying learning mechanisms-outcomes framework. The results are stable across the choice of statistical method and the operationalization of financial performance. Theoretical contributions, implications for practice, and limitations of the study are discussed.
|keyword = IT use,organizational learning,knowledge transfer effectiveness,absorptive capacity,IT value,franchising,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ORGANIZATIONAL PATH CONSTITUTION IN TECHNOLOGICAL INNOVATION: EVIDENCE FROM RURAL TELEHEALTH'''
{{header}}
{{article
|author= Rajendra Singh,Lars Mathiassen,Abhay Mishra,
|source= MIS QUARTERLY
|year= 2015
|abstract = Path constitution theory has emerged as a promising combination of two contrasting perspectives on technological innovation: path dependence, which focuses on historically embedded, contingent processes that are more or less beyond the control of actors, and path creation, which emphasizes mindful contributions from powerful actors. However, the current path constitution literature focuses on macro- and multi-level inquiry without addressing the specific processes, opportunities, and challenges related to organizational (micro-level) technological innovation. Against this backdrop, we draw on the innovation and path literature as well as a case study of telehealth innovation in a public health organization to theorize how technological innovation paths constitute in organizational contexts. The proposed theory distinguishes between innovation path status and innovation path trajectory to help researchers understand and explain how organizations transform and reinforce path constitution patterns, how innovation paths may merge with or separate from other paths, and how organizations may arrive at a lock-in that challenges them to break out from dominant and seemingly irreversible action patterns.
|keyword = Technological innovation,innovation path constitution,path creation,path dependence,rural telehealth,social construction of technology,public health,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPARING POTENTIAL AND ACTUAL INNOVATORS: AN EMPIRICAL STUDY OF MOBILE DATA SERVICES INNOVATION'''
{{header}}
{{article
|author= Atreyi Kankanhalli,Hua (Jonathan) Ye,Hock Hai Teo,
|source= MIS QUARTERLY
|year= 2015
|abstract = Firms are increasingly opening up their innovation efforts to allow users to tap into the benefits they can offer, such as mobile data service (MDS) innovation on iOS and Google Android platforms. For this purpose, platforms typically provide toolkits to facilitate user participation, aiming to create an ecosystem for sustainable innovation. However, with the barriers to user innovation and attrition of existing innovators, it could be challenging for firms to attract and sustain users' MDS innovation. With the possible benefits from user innovation, and considering the challenges faced, firms need to understand how to influence potential user innovators to take part and to encourage extant user innovators to innovate again. However, there is a lack of comprehensive research and understanding of what drives users' intentions to innovate services and the differences in the antecedents of such intention between potential and actual user innovators. Further, although prior studies have suggested that toolkits can support user innovation, little research has theorized and empirically tested their influence. Motivated thus, this study proposes a model based on (1) user innovation theory to explain the antecedents (including toolkit support) of user MDS innovation intention and (2) construal level theory to explain the differential effects of the antecedents for actual and potential user innovators. We tested the model through survey data from potential and actual MDS user innovators on Google Android and iOS platforms. We find that trend leadership and anticipated extrinsic reward influence both potential and actual user innovators' intentions to innovate. However, anticipated recognition and toolkit support affect only actual user innovators, while anticipated enjoyment affects only potential user innovators. Interestingly, toolkit support strengthens the influence of anticipated enjoyment for actual user innovators but weakens its influence for potential user innovators. Further, potential user innovators value anticipated extrinsic rewards less than actual innovators do. The implications for research and practice are discussed.
|keyword = User service innovation,mobile data services,user innovation theory,construal level theory,potential and actual user innovators,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPETING FOR ATTENTION: AN EMPIRICAL STUDY OF ONLINE REVIEWERS' STRATEGIC BEHAVIOR'''
{{header}}
{{article
|author= Wenqi Shen,Yu Jeffrey Hu,Jackie Rees Ulmer,
|source= MIS QUARTERLY
|year= 2015
|abstract = Top online reviewers who reliably gain consumers' attention stand to make significant financial gains and monetize the amount of attention and reputation they have earned. This study explores how online reviewers strategically choose the right product to review and the right rating to post so that they can gain attention and enhance reputation. Using book reviews from Amazon and Barnes & Noble (BN), we find that reviewers on Amazon, where a reviewer ranking system quantifies reviewers' online reputations, are sensitive to the competition among existing reviews and thus tend to avoid crowded review segments. However, on BN, which does not include such a ranking mechanism, reviewers do not respond to the competition effect. In addition, reviewers on Amazon post more differentiated ratings compared with reviewers on BN since the competition for attention on Amazon is more intense than on BN. Overall, reviewers on Amazon behave more strategically than reviewers on BN. This study yields important managerial implications for companies to improve their design of online review systems and enhance their understanding of reviewers' strategic behaviors.
|keyword = Online attention,scarcity of attention,competing for attention,online product reviews,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXTENDING ICT4D STUDIES: THE VALUE OF CRITICAL RESEARCH'''
{{header}}
{{article
|author= Cecilia I. C. Lin,Feng-Yang Kuo,Michael D. Myers,
|source= MIS QUARTERLY
|year= 2015
|abstract = The purpose of this paper is to demonstrate the value of critical research for information and communications technology for development (ICT4D) studies. Most previous IS research on ICT4D projects is interpretive and has focused on the immediate organizational context, but there are very few critical studies that have engaged in macro sociopolitical analyses regarding institutional change. Hence we extend previous IS research on ICT4D by adopting a critical research perspective on the macro sociopolitical context within which most ICT4D projects take place. We illustrate this with an ethnographic study of a project that was intended to improve the education and social welfare of the aboriginal people in Taiwan. On the surface the project was tremendously successful; it became a showcase on national radio and TV showing how ICT could be used to support underprivileged children. However, our research uncovered a different story altogether-a story of the aboriginal people themselves feeling marginalized and without much of a voice. We use concepts from postcolonial theory to make sense of these two contradictory stories. We found that the interrelationship between the macro sociopolitical context and the local organizational context of the ICT4D project is the key to understanding what went wrong, something which we would not have discovered if we had taken the traditional approach. The postcolonial context is powerful and pervasive, hampering any real progress.
|keyword = ICT4D,development,critical research,postcolonial theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXHAUSTION FROM INFORMATION SYSTEM CAREER EXPERIENCE: IMPLICATIONS FOR TURN-AWAY INTENTION'''
{{header}}
{{article
|author= Deborah J. Armstrong,Nita G. Brooks,Cynthia K. Riemenschneider,
|source= MIS QUARTERLY
|year= 2015
|abstract = While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turnaway intentions. Building on Ahuja et al.'s (2007) work on turnover intentions and using the job demands-resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual's professional career. Findings indicate that IS professionals' perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.
|keyword = Information systems,IS personnel,workforce,burnout,exhaustion,affective commitment,turnaway intention,occupational turnover,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FRIENDSHIPS IN ONLINE PEER-TO-PEER LENDING: PIPES, PRISMS, AND RELATIONAL HERDING'''
{{header}}
{{article
|author= De Liu,Daniel J. Brass,Yong Lu,Dongyu Chen,
|source= MIS QUARTERLY
|year= 2015
|abstract = This paper investigates how friendship relationships act as pipes, prisms, and herding signals in a large online, peer-to-peer (P2P) lending site. By analyzing decisions of lenders, we find that friends of the borrower, especially close offline friends, act as financial pipes by lending money to the borrower. On the other hand, the prism effect of friends' endorsements via bidding on a loan negatively affects subsequent bids by third parties. However, when offline friends of a potential lender, especially close friends, place a bid, a relational herding effect occurs as potential lenders are likely to follow their offline friends with a bid.
|keyword = Peer-to-peer lending,friendship relationships,social networks,prism effect,herding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network Dynamics: How Can We Find Patients Like Us?'''
{{header}}
{{article
|author= Lu (Lucy) Yan,Jianping Peng,Yong Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Social networks have been shown to affect health. Because online social networking makes it easier for individuals to interact with experientially similar others in regard to health issues and to exchange social support, there has been an increasing effort to understand how networks function. Nevertheless, little attention has been paid to how these networks are formed. In this paper, we examine the driving forces behind patients' social network formation and evolution. We argue that patients' health-related traits influence their social connections and that the patients' network layout is shaped by their cognitive capabilities and their network embeddedness. By studying longitudinal data from 1,322 individuals and their communication ties in an online healthcare social network, we find that firsthand disease experience, which provides knowledge of the disease, increases the probability that patients will find experientially similar others and establish communication ties. Patients' cognitive abilities, including the information load that they can process and the range of social ties that they can manage, however, limit their network growth. In addition, we find that patients' efforts to reach out for additional social resources are associated with their embeddedness in the network and the cost of maintaining connections. Practical implications of our findings are discussed.
|keyword = social networks,healthcare,network dynamics,homophily,cognitive capabilities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT-Enabled Broadcasting in Social Media: An Empirical Study of Artists' Activities and Music Sales'''
{{header}}
{{article
|author= Hailiang Chen,Prabuddha De,Yu Jeffrey Hu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = With the emergence of social media and Web 2.0, broadcasting in the online environment has evolved into a new form of marketing due to the much broader reach enabled by information technology. This paper quantifies the effect of artists' broadcasting activities on a well-known social media site for music, MySpace, on music sales. We employ a panel vector autoregression model to investigate the interrelationship between broadcasting promotions in social media and music sales, while controlling for influential factors such as advertising in traditional media channels, album prices, new music releases, user-generated content, and artist popularity. We characterize two types of broadcast messages in the MySpace context, personal and automated. We find that broadcasting in social media has a significant effect on sales even after controlling for the aforementioned factors, and more important, the effect mainly comes from personal messages rather than automated messages. We also show that the timing and content of personal messages play a role in affecting sales. Our findings point to the importance of conducting captivating conversations with customers in social media marketing.
|keyword = broadcasting,social media marketing,music sales,panel vector autoregression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Recommendations Using Information from Multiple Association Rules: A Probabilistic Approach'''
{{header}}
{{article
|author= Abhijeet Ghoshal,Syam Menon,Sumit Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Business analytics has evolved from being a novelty used by a select few to an accepted facet of conducting business. Recommender systems form a critical component of the business analytics toolkit and, by enabling firms to effectively target customers with products and services, are helping alter the e-commerce landscape. A variety of methods exist for providing recommendations, with collaborative filtering, matrix factorization, and association-rule-based methods being the most common. In this paper, we propose a method to improve the quality of recommendations made using association rules. This is accomplished by combining rules when possible and stands apart from existing rule-combination methods in that it is strongly grounded in probability theory. Combining rules requires the identification of the best combination of rules from the many combinations that might exist, and we use a maximum-likelihood framework to compare alternative combinations. Because it is impractical to apply the maximum likelihood framework directly in real time, we show that this problem can equivalently be represented as a set partitioning problem by translating it into an information theoretic context-the best solution corresponds to the set of rules that leads to the highest sum of mutual information associated with the rules. Through a variety of experiments that evaluate the quality of recommendations made using the proposed approach, we show that (i) a greedy heuristic used to solve the maximum likelihood estimation problem is very effective, providing results comparable to those from using the optimal set partitioning solution; (ii) the recommendations made by our approach are more accurate than those made by a variety of state-of-the-art benchmarks, including collaborative filtering and matrix factorization; and (iii) the recommendations can be made in a fraction of a second on a desktop computer, making it practical to use in real-world applications.
|keyword = personalization,Bayesian estimation,maximum likelihood,information theory,data analytics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contemporaneous and Delayed Sales Impact of Location-Based Mobile Promotions'''
{{header}}
{{article
|author= Zheng Fang,Bin Gu,Xueming Luo,Yunjie Xu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Can location-based mobile promotion (LMP) trigger contemporaneous and delayed sales purchases? As mobile technologies can reach users anywhere and anytime, LMP becomes a promising new channel. We unravel the dynamic sales impact of LMP on the basis of a randomized field experiment with 22,000 mobile users sponsored by one of the largest mobile service providers in the world. Our identification strategy is to gauge the marginal increases in consumer purchases of the geo-fenced treatment group of users who received LMP, above and beyond the baseline control groups. There are two controls: one group who received the same LMP but not in the virtual geo-fencing locational range (nongeo-fenced control), and the other who did not receive the LMP but in the geo-fencing range (geo-fenced control). The latter control serves as an organic holdout baseline from the similar population, i.e., counterfactual test of what if without the mobile LMP intervention, to identify the actual "lift" of incremental purchases caused by the treatment with the mobile LMP intervention. Findings suggest that LMP treatment has a significantly stronger impact on contemporaneous (same-day) purchases and delayed (subsequent-days) purchases than the controls. The randomized experiment design renders these findings robust to alternative explanations such as mobile usage behavior heterogeneity, product effects heterogeneity, nonrandomized sample-selection bias, and endogeneity concerns. Follow-up surveys with the field experiment users explore the nuanced mechanisms via which LMP may induce the impulsive, same-day purchases, and create product awareness for the planned subsequent-days purchases. LMP can generate six times more purchases than nongeo-fenced control with the LMP intervention, and 12 times more than geo-fenced control without the LMP intervention. LMP has a delayed sales effect for 12 days after the mobile promotions. The total sales impact of LMP could be underestimated by 54% if excluding the delayed sales impact and only including the contemporaneous impact. These findings are new to the literature and often neglected in mobile promotion practices, proffering novel implications on the sales value of LMP in the mobile era.
|keyword = mobile computing,mobile promotion,location-based mobile promotion,advertising,dynamic impact,randomized field experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Disclosure and the Diffusion of Information Security Attacks'''
{{header}}
{{article
|author= Sabyasachi Mitra,Sam Ransbotham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = With the nearly instantaneous dissemination of information in the modern era, policies regarding the disclosure of sensitive information have become the focus of significant discussion in several contexts. The fundamental debate centers on trade-offs inherent in disclosing information that society needs, but that can also be used for nefarious purposes. Using information security as a research context, our empirical study examines the adoption of software vulnerabilities by a population of attackers. We compare attacks based on software vulnerabilities disclosed through full-disclosure and limited-disclosure mechanisms. We find that full disclosure accelerates the diffusion of attacks, increases the penetration of attacks within the target population, and increases the risk of first attack after the vulnerability is reported. Interestingly, the effect of full disclosure is greater during periods when there are more overall vulnerabilities reported, indicating that attackers may strategically focus on busy periods when the effort of security professionals is spread across many vulnerabilities. Although the aggregate volume of attacks remains unaffected by full disclosure, attacks occur earlier in the life cycle of the vulnerability. Building off our theoretical insights, we discuss the implications of our findings in more general contexts.
|keyword = information security,information disclosure,software vulnerability,diffusion of innovation,negative innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why Following Friends Can Hurt You: An Exploratory Investigation of the Effects of Envy on Social Networking Sites among College-Age Users'''
{{header}}
{{article
|author= Hanna Krasnova,Thomas Widjaja,Peter Buxmann,Helena Wenninger,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Research findings on how participation in social networking sites (SNSs) affects users' subjective well-being are equivocal. Some studies suggest a positive impact of SNSs on users' life satisfaction and mood, whereas others report undesirable consequences such as depressive symptoms and anxiety. However, whereas the factors behind the positive effects have received significant scholarly attention, little is known about the mechanisms that underlie the unfavorable consequences. To fill this gap, this study uses social comparison theory and the responses of 1,193 college-age Facebook users to investigate the role of envy in the SNS context as a potential contributor to those undesirable outcomes. Arising in response to social information consumption, envy is shown to be associated with reduced cognitive and affective well-being as well as increased reactive self-enhancement. These preliminary findings contribute to the growing body of information systems research investigating the dysfunctional consequences of information technology adoption in general and social media participation in particular.
|keyword = envy,self-enhancement,social comparison theory,social media,social networking sites,subjective well-being,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Migration of Service to the Internet: Evidence from a Federal Natural Experiment'''
{{header}}
{{article
|author= Kai-Lung Hui,I. P. L. Png,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Previous research into consumer choice of service channels studied the impact of online access as an addition to conventional service. Here, we study the impact of a compulsory migration to an online channel. We exploit a natural experiment in the implementation of a new federal government service to identify the causal effect of access channel on consumer choice. The government served western states through the Internet and telephone at all times. However, for the first 10 days, the government served the East through the Internet only. Comparing consumer responses in the East (only Internet service available) and West (both Internet and telephone service available), we find robust evidence that some consumers preferred telephone access. The unavailability of telephone service in the first 10 days resulted in a 4.3% loss of consumers who were otherwise interested in the service.
|keyword = service migration,channel choice,Do Not Call Registry,natural experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Exploration of Risk Characteristics of Information Security Threats and Related Public Information Search Behavior'''
{{header}}
{{article
|author= Jingguo Wang,Nan Xiao,H. Raghav Rao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Information security (IS) threats are increasingly pervasive, and search engines are being used by the public as the primary tool for searching for relevant information. This research investigates the following two questions: (1) How can different IS threats be characterized and distinguished in terms of their risk characteristics? and (2) how are risk characteristics related to public searches for information on IS threats? Applying psychometric analysis, our analyses of survey data first show that unknown risk and dread risk are two underlying dimensions that can characterize different IS threats. Drawing broadly on the literature of information foraging theory, we examine the influence of risk characteristics on public searches for information on these threats. We utilize a search engine log to extract searches related to IS threats. We develop and estimate a system of equations with correlated individual-specific error terms using the Markov Chain Monte Carlo method. We find that the two risk characteristics exert differential impacts on information search behavior (including types of information sought, number of pages viewed, and length of query). The implications for IS research and practice are discussed.
|keyword = information-seeking behavior,information security threats,risk characteristics,psychometric analysis,Markov Chain Monte Carlo,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NEW STATE OF PLAY IN INFORMATION SYSTEMS RESEARCH: THE PUSH TO THE EDGES'''
{{header}}
{{article
|author= Varun Grover,Kalle Lyytinen,
|source= MIS QUARTERLY
|year= 2015
|abstract = The dominant way of producing knowledge in information systems (IS) seeks to domesticate high-level reference theory in the form of mid-level abstractions involving generic and atheoretical information technology (IT) components. Enacting such epistemic scripts squeezes IS theory to the middle range, where abstract reference theory concepts are directly instantiated or slightly modified to the IS context, whereas IT remains exogenous to theory by being treated as an independent variable, mediator, or moderator. In this design, IT is often operationalized using proxies that detect the presence of IT or its variation in use or cost. Our analysis of 143 articles published in MIS Quarterly and Information Systems Research over the past 15 years demonstrates that over 70 percent of published theory conforms to this mode of producing IS knowledge. This state of play has resulted in two negative consequences: the field (1) agonizes over the dearth of original and bold theorizing over IT and (2) satisfices when integrating theory with empirics by creating incommensurate mid-range models that are difficult to consolidate. We propose that one way to overcome these challenges is to critically examine and debate the negative impacts of the field's dominant epistemic scripts and relax them by permitting IS scholarship that more fluidly accommodates alternative forms of knowledge production. This will push IS inquiry to the "edges" and emphasize, on the one hand, inductive, rich inquiries using innovative and extensive data sets and, on the other hand, novel, genuine, high-level theorizing around germane conceptual relationships between IT, information and its (semiotic) representations, and social behaviors. We offer several exemplars of such inquiries and their results. To promote this push, we invite alternative institutionalized forms of publishing and reviewing. We conclude by inviting individual scholars to be more open to practices that permit richer theorizing. These recommendations will broaden the field's knowledge ecology and permit the creation of good IS knowledge over just getting "hits." We surmise that, if such changes are carried out, the field can look confidently toward its future as one of the epicenters of organizational inquiry that deal with the central forces shaping human enterprise in the 21st century.
|keyword = Information systems discipline,research inquiry,epistemic scripts,IS theory,IT artifact,middle-range theory,theory borrowing,institutional analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONSISTENT PARTIAL LEAST SQUARES PATH MODELING'''
{{header}}
{{article
|author= Theo K. Dijkstra,Jorg Henseler,
|source= MIS QUARTERLY
|year= 2015
|abstract = This paper resumes the discussion in information systems research on the use of partial least squares (PLS) path modeling and shows that the inconsistency of PLS path coefficient estimates in the case of reflective measurement can have adverse consequences for hypothesis testing. To remedy this, the study introduces a vital extension of PLS: consistent PLS (PLSc). PLSc provides a correction for estimates when PLS is applied to reflective constructs: The path coefficients, inter-construct correlations, and indicator loadings become consistent. The outcome of a Monte Carlo simulation reveals that the bias of PLSc parameter estimates is comparable to that of covariance-based structural equation modeling. Moreover, the outcome shows that PLSc has advantages when using non-normally distributed data. We discuss the implications for IS research and provide guidelines for choosing among structural equation modeling techniques.
|keyword = PLS,consistent partial least squares,SEM,variance-based structural equation modeling,Monte Carlo simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DISASTER EXPERIENCE AND HOSPITAL INFORMATION SYSTEMS: AN EXAMINATION OF PERCEIVED INFORMATION ASSURANCE, RISK, RESILIENCE, AND HIS USEFULNESS'''
{{header}}
{{article
|author= Insu Park,Raj Sharman,H. Raghav Rao,
|source= MIS QUARTERLY
|year= 2015
|abstract = This paper examines how an individual's disaster experience affects his or her perceptions of sociotechnical safety factors (risk, information assurance, resilience) and perceived usefulness of hospital information systems (HIS). This paper consists of two studies focusing on different aspects: a quasi-field experiment conducted with employees in three hospitals affected by a severe snowstorm (labeled a federal disaster) (N = 103), where we compare the perceptual factors in the context of the disaster experience (with versus without recall), and a comparative study between a first sample group (with disaster experience) and a second, contrast sample group (with no disaster experience) of hospital employees (N = 179) from two similar hospitals. The results show that the disaster experience changes the relationships among the perceptual factors that affect perceived usefulness. Individuals tend to perceive negative factors (such as risk) as having greater effects when they actually have direct experience in a disaster situation than in a normal situation. Positive factors (such as information assurance and resilience) have a lesser impact among individuals who have disaster experience (with versus without recall).
|keyword = Disaster experience,information assurance,perceived system risk,perceived resilience,quasi-experiment,comparative study,hospital information systems usefulness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INCREASING ACCOUNTABILITY THROUGH USER-INTERFACE DESIGN ARTIFACTS: A NEW APPROACH TO ADDRESSING THE PROBLEM OF ACCESS-POLICY VIOLATIONS'''
{{header}}
{{article
|author= Anthony Vance,Paul Benjamin Lowry,Dennis Eggett,
|source= MIS QUARTERLY
|year= 2015
|abstract = Access-policy violations are a growing problem with substantial costs for organizations. Although training programs and sanctions have been suggested as a means of reducing these violations, evidence shows the problem persists. It is thus imperative to identify additional ways to reduce access-policy violations, especially for systems providing broad access to data. We use accountability theory to develop four user-interface (UI) design artifacts that raise users' accountability perceptions within systems and in turn decrease access-policy violations. To test our model, we uniquely applied the scenario-based factorial survey method to various graphical manipulations of a records system containing sensitive information at a large organization with over 300 end users who use the system daily. We show that the UI design artifacts corresponding to four submanipulations of accountability can raise accountability and reduce access policy violation intentions. Our findings have several theoretical and practical implications for increasing accountability using UI design. Moreover, we are the first to extend the scenario-based factorial survey method to test design artifacts. This method provides the ability to use more design manipulations and to test with fewer users than is required in traditional experimentation and research on human-computer interaction. We also provide bootstrapping tests of mediation and moderation and demonstrate how to analyze fixed and random effects within the factorial survey method optimally.
|keyword = Accountability theory,identifiability,expectation of evaluation,awareness of monitoring,social presence,factorial survey method,user-interface design,information security policy violations,unauthorized access,mediation,moderation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COPING WITH INFORMATION TECHNOLOGY: MIXED EMOTIONS, VACILLATION, AND NONCONFORMING USE PATTERNS'''
{{header}}
{{article
|author= Mari-Klara Stein,Sue Newell,Erica L. Wagner,Robert D. Galliers,
|source= MIS QUARTERLY
|year= 2015
|abstract = Achieving the promised business benefits of an IT system is intimately tied to the continued incorporation of the system into the work practices it is intended to support. While much is known about different social, cognitive, and technical factors that influence initial adoption and use, less is known about the role of emotional factors in users' behaviors. Through an in-depth field study conducted in two North American universities, we examine the role of emotions in how specific IT use patterns emerge. We find that there are five different characteristics of an IT stimulus event (cues) that, when interacting in a reinforcing manner, elicit a single class of emotions (uniform affective responses) and, when interacting in an oppositional manner, elicit mixed emotions (ambivalent affective responses). While users respond to uniform emotions with clear adaptation strategies, they deal with ambivalent emotions by combining different adaptation behaviors, a vacillating strategy between emphasizing positive and negative aspects of the stimulus. Surprisingly, these ambivalent emotions and vacillating strategies can lead to active and positive user engagement, exhibited in task and tool adaptation behaviors and improvisational use patterns that, despite their nonconformity to terms of use, can have positive organizational implications.
|keyword = Emotions,IT use patterns,adaptation behaviors,ambivalence,qualitative research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''LEADING COLLABORATION IN ONLINE COMMUNITIES'''
{{header}}
{{article
|author= Samer Faraj,Srinivas Kudaravalli,Molly Wasko,
|source= MIS QUARTERLY
|year= 2015
|abstract = Despite the growing importance of online communities in creating knowledge and facilitating collaboration, there has been limited research examining the role of leaders in such settings. In this paper, we propose a framework that integrates behavioral and structural approaches to explore the antecedents of leadership in online communities focused on knowledge work. Specifically, we propose that sociability and knowledge contribution behaviors as well as structural social capital lead to being identified as a leader by members of the online community. We test this framework using social network, survey, and message-level content analysis data collected from three different online communities focused on technical topics. The results from our zero inflated negative binomial models, with 6,709 messages from 976 individuals, provide strong support for the framework that is developed in this study. Our study contributes to both theory and practice by identifying the behavioral and structural antecedents of leadership in online communities.
|keyword = Online leadership,leader behaviors,online communities,structural social capital,knowledge collaboration,network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PATTERNS IN INFORMATION SYSTEMS PORTFOLIO PRIORITIZATION: EVIDENCE FROM DECISION TREE INDUCTION'''
{{header}}
{{article
|author= Prasanna Karhade,Michael J. Shaw,Ramanath Subramanyam,
|source= MIS QUARTERLY
|year= 2015
|abstract = Questions pertaining to the locus of information systems (IS) governance have been extensively examined in existing research. However, questions pertaining to the decision rationale applied for IS portfolio prioritization (why are certain initiatives approved, and why are certain others rejected), noted to be a critical component of IS governance, need further investigation. We submit that the IS strategy of a firm is likely to explain the decision rationale it applies to IS portfolio prioritization and maintain that it is critical to ensure this decision rationale is in congruence with the firm's IS strategy. By extending prior theoretical work on IS strategy types, we develop theoretical profiles of the decision rationale applied to IS portfolio prioritization using three attributes: communicability of decision rationale, consistency in applying decision rationale, and risk appropriateness of decision rationale. Since the decision rationale applied for IS portfolio prioritization is often tacit, unknown even to the decision makers themselves, we employ the decision tree induction methodology to discover this tacit decision rationale. We analyze over 150 IS portfolio prioritization decisions on a multimillion dollar IS portfolio of a multibusiness, Fortune 50 firm and our findings, which support our propositions, indicate that firms that adopt different IS strategies rely on systematically different profiles of decision rationale for IS portfolio prioritization. Implications for IS governance practices are developed.
|keyword = IS strategy,IS portfolio prioritization,IT portfolio management,IS governance,IT governance,decision making,decision tree induction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MOBILE APPLICATION USABILITY: CONCEPTUALIZATION AND INSTRUMENT DEVELOPMENT'''
{{header}}
{{article
|author= Hartmut Hoehle,Viswanath Venkatesh,
|source= MIS QUARTERLY
|year= 2015
|abstract = This paper presents a mobile application usability conceptualization and survey instrument following the 10step procedure recommended by MacKenzie et al. (2011). Specifically, we adapted Apple's user experience guidelines to develop our conceptualization of mobile application usability that we then developed into 19 first-order constructs that formed 6 second-order constructs. To achieve our objective, we collected four datasets: content validity (n = 318), pretest (n = 440), validation (n = 408), and cross-validation (n = 412). The nomological validity of this instrument was established by examining its impact on two outcomes: continued intention to use and mobile application loyalty. We found that the constructs that represented our mobile application usability conceptualization were good predictors of both outcomes and compared favorably to an existing instrument based on Microsoft's usability guidelines. In addition to being an exemplar of the recent procedure of MacKenzie et al. to validate an instrument, this work provides a rich conceptualization of an instrument for mobile application usability that can serve as a springboard for future work to understand the impacts of mobile application usability and can be used as a guide to design effective mobile applications.
|keyword = Usability,mobile applications,survey instrument development,continued use,mobile application loyalty,mobility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SUPPORT STRUCTURES AND THEIR IMPACTS ON EMPLOYEE OUTCOMES: A LONGITUDINAL FIELD STUDY OF AN ENTERPRISE SYSTEM IMPLEMENTATION'''
{{header}}
{{article
|author= Tracy Ann Sykes,
|source= MIS QUARTERLY
|year= 2015
|abstract = Despite the impressive progress in understanding the benefits and challenges related to enterprise system (ES) implementations-such as enterprise resource planning (ERP) systems-little is known about how the support structures traditionally used by organizations to help employees cope with a new ES affect employee outcomes related to the system and their jobs. Likewise, little is known about how existing peer advice ties in the business unit influence these outcomes after an ES implementation. Understanding employee outcomes is critical because of their ramifications for long-term ES success. This paper examines the impacts of four traditional support structures (namely, training, online support, help desk support, and change management support), and peer advice ties on four key employee outcomes (namely, system satisfaction, job stress, job satisfaction, and job performance). This paper also seeks to show that it is peer advice ties that best fill the complex informational needs of employees after an ES implementation by providing the right information at the right time and in the right context. The proposed model was tested in a field study conducted in one business unit of a large telecommunications company and gathered data from 120 supplier liaisons over the course of a year. Both traditional support structures and peer advice ties were found to influence the various outcomes, even after controlling for pre-implementation levels of the dependent variables. In all cases, peer advice ties was the strongest predictor, thus underscoring the importance of this critical internal resource.
|keyword = Enterprise systems,ES implementation,peer advice ties,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW INFORMATION TECHNOLOGY GOVERNANCE MECHANISMS AND STRATEGIC ALIGNMENT INFLUENCE ORGANIZATIONAL PERFORMANCE: INSIGHTS FROM A MATCHED SURVEY OF BUSINESS AND IT MANAGERS'''
{{header}}
{{article
|author= Shelly Ping-Ju Wu,Detmar W. Straub,Ting-Peng Liang,
|source= MIS QUARTERLY
|year= 2015
|abstract = Previous research has proposed different types for and contingency factors affecting information technology governance. Yet, in spite of this valuable work, it is still unclear through what mechanisms IT governance affects organizational performance. We make a detailed argument for the mediation of strategic alignment in this process. Strategic alignment remains a top priority for business and IT executives, but theory-based empirical research on the relative importance of the factors affecting strategic alignment is still lagging. By consolidating strategic alignment and IT governance models, this research proposes a nomological model showing how organizational value is created through IT governance mechanisms. Our research model draws upon the resource-based view of the firm and provides guidance on how strategic alignment can mediate the effectiveness of IT governance on organizational performance. As such, it contributes to the knowledge bases of both alignment and IT governance literatures. Using dyadic data collected from 131 Taiwanese companies (cross-validated with archival data from 72 firms), we uncover a positive, significant, and impactful linkage between IT governance mechanisms and strategic alignment and, further, between strategic alignment and organizational performance. We also show that the effect of IT governance mechanisms on organizational performance is fully mediated by strategic alignment. Besides making contributions to construct and measure items in this domain, this research contributes to the theory base by integrating and extending the literature on IT governance and strategic alignment, both of which have long been recognized as critical for achieving organizational goals.
|keyword = IT governance mechanisms,strategic alignment,organizational performance,degree-symmetric measures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Finding Similar Mobile Consumers with a Privacy-Friendly Geosocial Design'''
{{header}}
{{article
|author= Foster Provost,David Martens,Alan Murray,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This paper focuses on finding the same and similar users based on location-visitation data in a mobile environment. We propose a new design that uses consumer-location data from mobile devices (smartphones, smart pads, laptops, etc.) to build a "geosimilarity network" among users. The geosimilarity network (GSN) could be used for a variety of analytics-driven applications, such as targeting advertisements to the same user on different devices or to users with similar tastes, and to improve online interactions by selecting users with similar tastes. The basic idea is that two devices are similar, and thereby connected in the GSN, when they share at least one visited location. They are more similar as they visit more shared locations and as the locations they share are visited by fewer people. This paper first introduces the main ideas and ties them to theory and related work. It next introduces a specific design for selecting entities with similar location distributions, the results of which are shown using real mobile location data across seven ad exchanges. We focus on two high-level questions: (1) Does geosimilarity allow us to find different entities corresponding to the same individual, for example, as seen through different bidding systems? And (2) do entities linked by similarities in local mobile behavior show similar interests, as measured by visits to particular publishers? The results show positive results for both. Specifically, for (1), even with the data sample's limited observability, 70%-80% of the time the same individual is connected to herself in the GSN. For (2), the GSN neighbors of visitors to a wide variety of publishers are substantially more likely also to visit those same publishers. Highly similar GSN neighbors show very substantial lift.
|keyword = design science,mobile computing,analytical modeling,network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evolutionary Competition in Platform Ecosystems'''
{{header}}
{{article
|author= Amrit Tiwana,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Intraplatform competition has received scant attention in prior studies, which predominantly study interplatform competition. We develop a middle-range theory of how complementarity between input control and a platform extension's modularization-by inducing evolution-influences its performance in a platform market. Primary and archival data spanning five years from 342 Firefox extensions show that such complementarity fosters performance by accelerating an extension's perpetual evolution.
|keyword = platforms,ecosystems,evolution,modularity,input control,platform governance,Garen,endogeneity,apps,platform extensions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Extra-Role Behaviors and Social Controls in Information Security Policy Effectiveness'''
{{header}}
{{article
|author= Jack Shih-Chieh Hsu,Sheng-Pao Shih,Yu Wen Hung,Paul Benjamin Lowry,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Although most behavioral security studies focus on organizational in-role behaviors such as information security policy (ISP) compliance, the role of organizational extra-role behaviors-security behaviors that benefit organizations but are not specified in ISPs-has long been overlooked. This study examines (1) the consequences of organizational in-role and extra-role security behaviors on the effectiveness of ISPs and (2) the role of formal and social controls in enhancing in-role and extra-role security behaviors in organizations. We propose that both in-role security behaviors and extra-role security behaviors contribute to ISP effectiveness. Furthermore, based on social control theory, we hypothesize that social control can boost both in-and extra-role security behaviors. Data collected from practitioners-including information systems (IS) managers and employees at many organizations-confirmed most of our hypotheses. Survey data from IS managers substantiated the importance of extra-role behaviors in improving ISP effectiveness. Paired data, collected from managers and employees in the same organizations, indicated that formal control and social control individually and interactively enhance both in-and extra-role security behaviors. We conclude by discussing the implications of this research for academics and practitioners, along with compelling future research possibilities.
|keyword = IS security,behavioral security,in-role behaviors,extra-role behaviors,social control theory,SCT,security management,information security policy,ISP,formal control,social control,organizations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cable News Wars on the Internet: Competition and User-Generated Content'''
{{header}}
{{article
|author= Gaurav Sabnis,Rajdeep Grewal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Academics and practitioners alike recognize that user-generated content (UGC), such as blog posts, help not only predict but also boost performance (e.g., sales). However, the role of competition in the UGC domain is not well understood. Building on extant research pertaining to the UGC-performance relationship, the authors document empirical evidence for a relationship between competitor UGC and focal firm performance. Data from a 30-week period describe the viewership of competing cable news shows on Fox News, CNN, and MSNBC during the 7:00 P.M.-9:00 P.M. time slots. They find evidence of a statistically significant relationship between competitor UGC and viewership and of heterogeneity in the direction of these competitive relationships, positive in some time slots and negative in others. The predictive power of UGC for viewership is enhanced by 3% to 5% simply by incorporating competitors' UGC, in addition to a show's own UGC. Thus, the study, as well as formulation of UGC-related marketing strategies, should incorporate competitive relationships.
|keyword = user-generated content,social media,competition,cable news,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Net Neutrality, Exclusivity Contracts, and Internet Fragmentation'''
{{header}}
{{article
|author= Frago Kourandi,Jan Kraemer,Tommaso Valletti,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Net neutrality (NN) is believed to prevent the emergence of exclusive online content, which yields Internet fragmentation. We examine the relationship between NN regulation and Internet fragmentation in a game-theoretic model that considers the interplay between termination fees, exclusivity, and competition between two Internet service providers (ISPs) and between two content providers (CPs). An exclusivity arrangement between an ISP and a CP reduces the CP's exposure to some end users, but it also reduces competition over ads among the CPs. Fragmentation arises in equilibrium when competition over ads among the CPs is very strong, the CPs' revenues from advertisements are very low, the content of the CPs is highly complementary, or the termination fees are high. We find that the absence of fragmentation is always beneficial for consumers, because they can enjoy all available content. Policy interventions that prevent fragmentation are thus good for consumers. However, results for total welfare are more mixed. A zero-price rule on traffic termination is neither a sufficient nor a necessary policy instrument to prevent fragmentation. In fact, regulatory interventions may be ineffective or even detrimental to welfare and are only warranted under special circumstances.
|keyword = net neutrality,Internet fragmentation,exclusivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Learning User Real-Time Intent for Optimal Dynamic Web Page Transformation'''
{{header}}
{{article
|author= Amy Wenxuan Ding,Shibo Li,Patrali Chatterjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Many e-commerce websites struggle to turn visitors into real buyers. Understanding online users' real-time intent and dynamic shopping cart choices may have important implications in this realm. This study presents an individual-level, dynamic model with concurrent optimal page adaptation that learns users' real-time, unobserved intent from their online cart choices, then immediately performs optimal Web page adaptation to enhance the conversion of users into buyers. To suggest optimal strategies for concurrent page adaptation, the model analyzes each individual user's browsing behavior, tests the effectiveness of different marketing and Web stimuli, as well as comparison shopping activities at other sites, and performs optimal Web page transformation. Data from an online retailer and a laboratory experiment reveal that concurrent learning of the user's unobserved purchase intent and real-time, intent-based optimal interventions greatly reduce shopping cart abandonment and increase purchase conversions. If the concurrent, intent-based optimal page transformation for the focal site starts after the first page view, shopping cart abandonment declines by 32.4% and purchase conversion improves by 6.9%. The optimal timing for the site to intervene is after three page views, to achieve efficient learning of users' intent and early intervention simultaneously.
|keyword = real-time learning,shopping intent,optimization,concurrent page adaptation,website productivity,hierarchical Bayes models,hidden Markov models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Analyzing Software as a Service with Per-Transaction Charges'''
{{header}}
{{article
|author= Dan Ma,Abraham Seidmann,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Software as a Service (SaaS) delivers a bundle of applications and services through the Web. Its on-demand feature allows users to enjoy full scalability and to handle possible demand fluctuations at no risk. In recent years, SaaS has become an appealing alternative to purchasing, installing, and maintaining modifiable off-the-shelf (MOTS) software packages. We present a game-theoretical model to study the competitive dynamics between the SaaS provider, who charges a variable per-transaction fee, and the traditional MOTS provider. We characterize the equilibrium conditions under which the two coexist in a competitive market and those under which each provider will fail and exit the market. Decreasing the lack-of-fit (or the cross-application data integration) costs of SaaS results in four structural regimes in the market. These are MOTS Dominance! Segmented Market! Competitive Market! SaaS Dominance. Based on our findings, we recommend distinct competitive strategies for each provider. We suggest that the SaaS provider should invest in reducing both its lack-of-fit costs and its per-transaction price so that it can offer increasing economies of scale. The MOTS provider, by contrast, should not resort to a price-cutting strategy; rather, it should enhance software functionality and features to deliver superior value. We further examine this problem from the software life-cycle perspective, with multiple stages over which users can depreciate the fixed costs of installing and customizing their MOTS solutions on site. We then present an analysis that characterizes the competitive outcomes when future technological developments could change the relative levels of the lack-of-fit costs. Specifically, we explain why the SaaS provider will always use a forward-looking pricing strategy: When lack-of-fit costs are expected to decrease (increase) in the future, the SaaS provider should reduce (increase) its current price. This is in contrast with the MOTS provider, who will use the forward-looking pricing strategy only when lack-of-fit costs are expected to increase. Surprisingly, when such costs are expected to decrease, the MOTS provider should ignore this expectation and use the same pricing strategy as in the benchmark with invariant lack-of-fit costs.
|keyword = software as a service,game theory model,pricing based on transactions,competitive strategies,lack-of-fit costs,economies of scale,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Capital and Contract Duration in Buyer-Supplier Networks for Information Technology Outsourcing'''
{{header}}
{{article
|author= Kiron Ravindran,Anjana Susarla,Deepa Mani,Vijay Gurbaxani,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This paper presents new evidence on the role of embeddedness in predicting contract duration in the context of information technology outsourcing. Contract duration is a strategic decision that aligns interests of clients and vendors, providing the benefits of business continuity to clients and incentives to undertake relationship specific investments for vendors. Considering the salience of this phenomenon, there has been limited empirical scrutiny of how contract duration is awarded. We posit that clients and vendors obtain two benefits from being embedded in an interorganizational network. First, the learning and experience accumulated from being embedded in a client-vendor network could mitigate the challenges in managing longer term contracts. Second, the network serves as a reputation system that can stratify vendors according to their trustworthiness and reliability, which is important in longer term arrangements. In particular, we attempt to make a substantive contribution to the literature by theorizing about embeddedness at four distinct levels: structural embeddedness at the node level, relational embeddedness at the dyad level, contractual embeddedness at the level of a neighborhood of contracts, and finally, positional embeddedness at the level of the entire network. We analyze a data set of 22,039 outsourcing contracts implemented between 1989 and 2008. We find that contract duration is indeed associated with structural and positional embeddedness of participant firms, with the relational embeddedness of the buyer-seller dyad, and with the duration of other contracts to which it is connected through common firms. Given the nature of our data, identification using traditional ordinary least squares based approaches is difficult given the unobserved errors clustered along two nonnested dimensions and the autocorrelation in a firm's decision (here the contract) with those of contracts in its reference group. We use a multiway cluster robust estimation and a network auto-regressive estimation to address these issues. Implications for literature and practice are discussed.
|keyword = social capital,reputation,IT outsourcing,contract design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Does IT Ambidexterity Impact Organizational Agility?'''
{{header}}
{{article
|author= One-Ki (Daniel) Lee,Vallabh Sambamurthy,Kai H. Lim,Kwok Kee Wei,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Organizational agility is a significant business capability. Though there have been numerous studies about the effects of information technology (IT) capabilities on organizational agility, there has been limited attention on the enabling effects of IT ambidexterity, namely, the dual capacity to explore and exploit IT resources and practices. We propose that IT ambidexterity enhances organizational agility by facilitating operational ambidexterity, and that the magnitude of facilitation depends on the level of environmental dynamism. We test these relationships utilizing data from a large-scale, matched-pair field survey of business and IT executives. The results confirm that a firm's IT ambidexterity does enhance its organizational agility through the mediated effects of operational ambidexterity, and that the dynamism of a firm's environment affects these relationships.
|keyword = agility,IT ambidexterity,operational ambidexterity,environmental dynamism,moderated-mediation analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Road to Early Success: Impact of System Use in the Swift Response Phase'''
{{header}}
{{article
|author= Yu Tong,Sharon Swee-Lin Tan,Hock-Hai Teo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = When an enterprise system is introduced, system users often experience a performance dip as they struggle with the unfamiliar system. Appropriately managing this phase, which we term as the swift response phase (SRP), is vital given its prominent impact on the eventual success of the system. Yet, there is a glaring lack of studies that examine the SRP. Drawing on sensemaking theory and early postadoptive literature, this study seeks to propose a theory-driven model to understand how different support structures facilitate different forms of use-related activities to induce a positive performance in the SRP. The model was tested through a two-stage survey involving 329 nurses. The results demonstrated the discriminating alignment between information system (IS) use-related activity and support structures in enhancing system users' work performance in the SRP. Specifically, suitability of impersonal support moderated the effects of standardized system use and individual adaption on performance, whereas availability of personal support only moderated the effect of nonstandardized system use on performance. For moderating role of personal support, IS specialists support had a lower influence than peer-champion support and peer-user support. This study contributes to the extant literature by (1) conceptualizing the turbulent SRP, (2) applying sensemaking theory to the initial postadoptive stage, (3) adding to the theoretical debate on the value of system use, and (4) unveiling the distinct roles of support structures under different types of use activities. Practical suggestions are provided for organizational management and policy makers to deal with the complexities in the SRP.
|keyword = early system success,sensemaking in organization,swift response phase,shakedown phase,system use,support structure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Deliberation Without Attention: The Latent Benefits of Distracting Website Features for Online Purchase Decisions'''
{{header}}
{{article
|author= Barney Tan,Cheng Yi,Hock C. Chan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Early studies on Web design typically caution against the use of distracting website features in electronic commerce, such as animated banners, pop-ups, and floating advertisements, because they may cause annoyance for online consumers and disrupt information processing, leading to poorer purchase decisions. Yet, the recently uncovered deliberation-without-attention (D-W-A) effect suggests that distracting consumers from the decision-making process may improve their decision quality when there are a large number of decision parameters to consider. To ascertain whether the D-W-A effect can be triggered through the use of distracting website features in the context of online shopping, two experiments are conducted. The first experiment reveals that the presence of distracting website features, in the form of pop-ups, gives rise to annoyance in general, but also leads to better purchase decisions when the decision to be made is complex. The second experiment supports the findings of the first and sheds further light on the underlying mode of thought triggered by these features. In particular, by eliminating a number of potential alternative mechanisms, including online judgments, the mere disruption of decision-related thought, and cognitively constrained conscious deliberation, the second experiment demonstrates that unconscious deliberation is likely to be the underlying cause of superior decision making. With these findings, this research supports a more balanced view in the recent human-computer interaction literature, which suggests that the usual advice to minimize the use of distracting website features should be examined more carefully. The research also uncovers evidence that contributes to the ongoing debate surrounding the D-W-A effect and unconscious thought theory.
|keyword = Web design,human-computer interaction,unconscious thought theory,deliberation without attention,online shopping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating Firm Strategies on Offering Consumer-Customizable Products'''
{{header}}
{{article
|author= Zheyin (Jane) Gu,Giri K. Tayi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Advances in the digital economy have driven the trend among manufacturers, particularly those in the information technology (IT) industry, to offer products that consumers can self-customize to satisfy their idiosyncratic needs. This study examines firm strategies on offering such consumer-customizable products. Our analysis shows that a monopolistic firm obtains a greater profit from offering a consumer-customizable product than from offering a preconfigured standardized product only if consumers are sufficiently capable to conduct the customization task; otherwise, it is more profitable for the firm to offer a standardized product. Moreover, consumers obtain a greater surplus when the firm offers the customizable product. We also consider the case where the firm is capable of offering both a customizable product and a standardized product and find that the firm benefits more from offering both products than offering either product if consumer customizing capability and the customization cost are not too high. Interestingly, when the firm offers both products, its effort in enhancing consumer customizability (e.g., offering free consumer training) always benefits both the firm and consumers, but its effort in increasing the value of the standardized product (e.g., offering more functions) can hurt both the firm profit and consumer surplus. Our theoretical results explain many interesting business practices and provide useful insights for marketing practitioners.
|keyword = consumer customization,consumer-customizable product,standardized product,IT products,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Self-Control in Information Security Violations: Insights from a Cognitive Neuroscience Perspective'''
{{header}}
{{article
|author= Qing Hu,Robert West,Laura Smarandescu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Self-control has been identified as a major factor influencing individual behavior in the social science, neuroscience, criminology, and information security literatures. In this study, we first developed and validated a novel paradigm suitable for use with event-related potentials (ERPs) in scenario-based laboratory experiments of decision making in the context of information security. We then used this paradigm to examine the association between individual differences in self-control and ERPs elicited while individuals deliberated over violations of information security policies. Our results show that the left and right hemispheres of the brain were involved in decision making, and that the participants with low self-control had lower levels of neural recruitment in both hemispheres relative to those with high self-control. This was especially the case for regions in or near the dorsal lateral prefrontal cortex (DLPFC) and inferior frontal cortex (IFC). These results extend the findings in neuroscience literature related to the role of self-control in decision making in general, and validate a new paradigm for use with the electroencephalography/event-related potentials (EEG/ERP) technique to examine theoretical questions in information security and criminology research.
|keyword = information security,neuroscience,self-control,policy compliance,neural correlates,electroencephalography (EEG),event-related potentials (ERPs),NeuroIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Behavioral Roots of Information Systems Security: Exploring Key Factors Related to Unethical IT Use'''
{{header}}
{{article
|author= Sutirtha Chatterjee,Suprateek Sarker,Joseph S. Valacich,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Unethical information technology (IT) use, related to activities such as hacking, software piracy, phishing, and spoofing, has become a major security concern for individuals, organizations, and society in terms of the threat to information systems (IS) security. While there is a growing body of work on this phenomenon, we notice several gaps, limitations, and inconsistencies in the literature. In order to further understand this complex phenomenon and reconcile past findings, we conduct an exploratory study to uncover the nomological network of key constructs salient to this phenomenon, and the nature of their interrelationships. Using a scenario-based study of young adult participants, and both linear and nonlinear analyses, we uncover key nuances of this phenomenon of unethical IT use. We find that unethical IT use is a complex phenomenon, often characterized by nonlinear and idiosyncratic relationships between the constructs that capture it. Overall, ethical beliefs held by the individuals, along with economic, social, and technological considerations are found to be relevant to this phenomenon. In terms of practical implications, these results suggest that multiple interventions at various levels may be required to combat this growing threat to IS security.
|keyword = unethical IT use,ethics,information ethics,information systems security,nonlinear analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Chasing Lemmings: Modeling IT-Induced Misperceptions About the Strategic Situation as a Reason for Flash Crashes'''
{{header}}
{{article
|author= Tobias Brandt,Dirk Neumann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Flash crashes, perceived as sharp drops in market prices that rebound shortly after, have turned the public eye toward the vulnerability of information technology-based stock trading. In this paper, we explain flash crashes as the result of actions made by rational agents. We argue that the advancement of information technology (IT), which has long been associated with competitive advantages, may cause ambiguities with respect to the game form that give rise to a hypergame. We employ hypergame theory to demonstrate that a market crash constitutes an equilibrium state if players misperceive the true game. Once the ambiguity is resolved, prices readjust to the appropriate level, creating the characteristic flash-crash effect. By analyzing the interaction with herd behavior, we find that flash crashes may be an unavoidable systemic problem of modern financial markets.
|keyword = flash crash,game theory,impact of IT,hypergames,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information'''
{{header}}
{{article
|author= Ahmed Abbasi,Fatemeh Mariam Zahedi,Daniel Zeng,Yan Chen,Hsinchun Chen,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools' warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the "what" but rather on operationalizing the "why" into salient detection cues.
|keyword = design science,data mining,phishing websites,genre theory,Internet fraud,website genres,credibility assessment,phishing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring Knowledge Filtering Processes in Electronic Networks of Practice'''
{{header}}
{{article
|author= Kelly J. Fadel,Thomas O. Meservy,Matthew L. Jensen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Electronic networks of practice (ENPs) have become an important mechanism for knowledge exchange among loosely connected individuals who share common knowledge interests. While prior research has explored factors that influence knowledge contribution in such networks, less is understood about the process by which individuals evaluate and ultimately adopt knowledge from ENPs. This study examines the process of knowledge filtering in online ENP forums. Drawing from dual process and information-evaluation theories, we hypothesize that performance on a knowledge-filtering task will be influenced by the constancy and directionality of search patterns employed by knowledge seekers. Hypotheses are tested in an experiment that utilized an eye tracker to record gaze data from professional software developers using an experimental ENP forum. By combining information-evaluation and dual process theory perspectives, our results deepen the insights offered in extant information-processing literature by showing that higher filtering accuracy is associated with (a) constant evaluation of some types of information attributes (solution content) but not others (peripheral cues), and (b) increasing attribute-based processing over time.
|keyword = electronic networks of practice,filtering,knowledge evaluation,constancy,directionality,field experiment,eye tracking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Bundling Effects on Variety Seeking for Digital Information Goods'''
{{header}}
{{article
|author= Gediminas Adomavicius,Jesse Bockstedt,Shawn P. Curley,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Prior research with consumable goods has consistently found that consumers have a preference for greater variety when selecting items simultaneously as a bundle, rather than as a sequential series of individual decisions. However, digital information goods have a number of important differences from consumable goods that may impact variety-seeking behavior. In three experiments, we address two general research questions. First, as a precursor to studying digital goods, we disentangle the role of bundle cohesion (i.e., item relatedness) from the role of timing (simultaneous vs. sequential choice) as factors in variety seeking with consumable goods. Next, based on differences between digital and consumable goods, we theorize differences in the behavioral effects of bundle cohesion and timing on variety preferences for digital goods. The results show a reduction of influences upon variety-seeking behavior with digital goods, providing important implications for the sellers of such goods in contrast to what has been suggested for consumable goods. Therefore, a key takeaway is that, for digital goods such as music, the use of consumer-driven bundling variations does not suggest an advantage in terms of their ability to affect consumers' variety-seeking behavior.
|keyword = variety seeking,digital goods,information goods,bundled goods,simultaneous choice,sequential choice,bundle cohesion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enticing and Engaging Consumers via Online Product Presentations: The Effects of Restricted Interaction Design'''
{{header}}
{{article
|author= Cheng Yi,Zhenhui (Jack) Jiang,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = This work investigates the effects of three different online product presentation formats, namely, a noninteractive video presentation and two virtual product experience (VPE) presentations (full interaction and restricted interaction), on engaging users in online product experience as well as enticing users to try products offline. The experimental results show that restricted interaction, which deprives users of part of the interactive product experience, is more enticing than both the noninteractive and fully interactive design for users with more product-class knowledge. In addition, restricted interaction is generally as good as full interaction in engaging users. Both engagement and enticement positively affect users' purchase intentions. This study contributes to the information systems literature by extending the theory in curiosity formation to the interaction design context and advocating designs for enticement. It contributes to design practice by revealing that less interactive and less costly presentations can be more effective in attracting consumers toward the products.
|keyword = virtual product experience (VPE),restricted interaction,full interaction,enticement,engagement,purchase intention,online product presentation,online video,online selling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Recommender System on Competition Between Personalizing and Non-Personalizing Firms'''
{{header}}
{{article
|author= Abhijeet Ghoshal,Subodha Kumar,Vijay Mookerjee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = How do recommender systems affect prices and profits of firms under competition? To explore this question, we model the strategic behavior of customers who make repeated purchases at two competing firms: one that provides personalized recommendations and another that does not. When a customer intends to purchase a product, she obtains recommendations from the personalizing firm and uses this recommendation to eventually purchase from one of the firms. The personalizing firm profiles the customer (based on past purchases) to recommend products. Hence, if a customer purchases less frequently from the personalizing firm, the recommendations made to her become less relevant. While considering the impact on the quality of recommendations received, a customer must balance two opposing forces: (1) the lower price charged by the non-personalizing firm, and (2) an additional fit cost incurred when purchasing from the non-personalizing firm and the increased cost due to recommendations of reduced quality in the future. An outcome of the analysis is that the customers should distribute their purchases across both firms to maximize surplus over a planning horizon. Anticipating this response, the firms simultaneously choose prices. We study the sensitivity of the equilibrium prices and profits of the firms with respect to the effectiveness of the recommender system and the profile deterioration rate. We also analyze some interesting variants of the base model in order to study how its key results could be influenced. One of the key takeaways of this research is that the recommender system can influence the price and profit of not only the personalizing firm but also the non-personalizing firm.
|keyword = recommender systems,duopoly,pricing,dynamic optimization,online competition,Nash equilibrium,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Are You Feeling Lonely? The Impact of Relationship Characteristics and Online Social Network Features on Loneliness'''
{{header}}
{{article
|author= Sabine Matook,Jeff Cummings,Hillol Bala,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = In contemporary society, many people move away from their personal networks for extended periods to reach professional and/or educational goals. This separation can often lead to feelings of loneliness, which can be stressful and sometimes debilitating for the individual. We seek to understand how a person's use of online social networks (OSNs)-technology-enabled tools that assist users with creating and maintaining their relationships-might affect their perceptions of loneliness. Prior research has offered mixed results about how OSNs affect loneliness-reporting both positive and negative effects. We argue in this study that a clearer perspective can be gained by taking a closer look at how individuals approach their relationship management in OSNs. Building on theoretical works on loneliness, we develop a model to explain the effects of relationship characteristics (i.e., relationship orientation, self-disclosure, and networking ability) and OSN features (i.e., active or passive) on perceived loneliness. Our findings show that OSN can be linked to both more and less perceived loneliness, that is, individuals' relationship orientation significantly affects their feelings of loneliness, which are further moderated by their degree of self-disclosure within the OSN. Furthermore, how users engage in the OSN (either actively or passively) influences their perceptions of loneliness. Practical implications regarding perceived loneliness include recommendations for firms to encourage mobile workers to utilize OSNs when separated from others, for education providers to connect with their new students before they arrive, and for users to utilize OSNs as a social bridge to others they feel close with.
|keyword = social media,online social networks,loneliness,relationship management,communal orientation,social exchange theory,self-disclosure,networking ability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the Influence of Instant Messaging on Ending Concessions During Negotiations'''
{{header}}
{{article
|author= Norman A. Johnson,Randolph B. Cooper,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = In many online price negotiations, instant messaging (IM) and audio channels rival each other in use, but IM's use is on the rise. In these contexts, people who are strangers to each other are inclined to act more competitively. They are driven by self-interest and strive for greater gains from agreement in the form of what is termed "ending concessions." To date, however, researchers have focused on striving for agreement as the main goal of negotiators. This study uses a selfishness theory to explain how individuals using IM, as compared to those using audio, can encourage their negotiation partners to make ending concessions, and thereby increase individuals' gains when agreement is reached. We use data from a negotiation laboratory experiment to test a model of ending concessions that is predicted by unrestricted offers and comments that negotiators make over IM and audio. We base our analyses on the contents of the resulting communications. The objects of negotiation are simulated lottery tickets. Our results provide three key insights. First, when using IM, partners appear to interpret offers that include concessions from individuals as attempts to manipulate partners into accepting non-equitable agreements; as a result partners decrease their ending concessions. These interpretations do not appear to occur when using audio, and as a result individuals' concessions do not decrease partners' ending concessions. Second, using IM, when individuals disagree with anger directed at partners' bidding behaviors, partners respond by increasing their ending concessions. Ending concessions are further increased when using audio. Third, and in contrast, using IM, when individuals disagree with emotion that does not include anger, partners respond by decreasing their ending concessions. Ending concessions are further decreased when using audio. These insights provide guidance for practice, and are bases for future research on the use of IM and audio for negotiation.
|keyword = instant messaging,audio,selfishness,agreement,ending concession,self-interest,deindividuation,media,negotiation experiment,lottery,negative reciprocation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Health Information Exchange as a Multisided Platform: Adoption, Usage, and Practice Involvement in Service Co-Production'''
{{header}}
{{article
|author= Niam Yaraghi,Anna Ye Du,Raj Sharman,Ram D. Gopal,Ram Ramesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Health Information Exchanges (HIE) are becoming integral parts of the national healthcare reform efforts, chiefly because of their potential impact on cost reduction and quality enhancement in healthcare services. However, the potential of an HIE platform can only be realized when its multiple constituent users actively participate in using its variety of services. In this research, we model HIE systems as multisided platforms that incorporate self-service technologies whose value to the users depends on both user-specific and network-specific factors. We develop a model of adoption, use, and involvement of clinical practices in the coproduction of the HIE services. This model is grounded in social network theory, service operations theory, and institutional isomorphism theory. A longitudinal study of actual adoption and use behaviors of 2,054 physicians within 430 community medical practices in Western New York over a three-year period has been carried out to evaluate the proposed model. This study has been supported by HEALTHeLINK, the Regional Health Information Organization of Western New York, which has an extensive database comprising over half a million transactions on patient records by the HIE users. We extracted panel data on adoption, use, and service coproduction behaviors from this database and carried out a detailed analysis using metrics derived from the foundational theories. Positioning practices within two distinct but interrelated networks of patients and practitioners, we show that adoption, use, and service coproduction behaviors are influenced by the topographies of the two networks, isomorphic effects of large practices on the smaller ones, and practice labor inputs in HIE use. Our findings provide a comprehensive view of the drivers of HIE adoption and use at the level of medical practices. These results have implications for marketing and revenue management of HIE platforms, as well as public health and national/regional healthcare policy making.
|keyword = health information exchange,multisided platforms,network externalities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predictive Analytics for Readmission of Patients with Congestive Heart Failure'''
{{header}}
{{article
|author= Indranil Bardhan,Jeong-ha (Cath) Oh,Zhiqiang (Eric) Zheng,Kirk Kirksey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Mitigating preventable readmissions, where patients are readmitted for the same primary diagnosis within 30 days, poses a significant challenge to the delivery of high-quality healthcare. Toward this end, we develop a novel, predictive analytics model, termed as the beta geometric Erlang-2 (BG/EG) hurdle model, which predicts the propensity, frequency, and timing of readmissions of patients diagnosed with congestive heart failure (CHF). This unified model enables us to answer three key questions related to the use of predictive analytics methods for patient readmissions: whether a readmission will occur, how often readmissions will occur, and when a readmission will occur. We test our model using a unique data set that tracks patient demographic, clinical, and administrative data across 67 hospitals in North Texas over a four-year period. We show that our model provides superior predictive performance compared to extant models such as the logit, BG/NBD hurdle, and EG hurdle models. Our model also allows us to study the association between hospital usage of health information technologies (IT) and readmission risk. We find that health IT usage, patient demographics, visit characteristics, payer type, and hospital characteristics, are significantly associated with patient readmission risk. We also observe that implementation of cardiology information systems is associated with a reduction in the propensity and frequency of future readmissions, whereas administrative IT systems are correlated with a lower frequency of future readmissions. Our results indicate that patient profiles derived from our model can serve as building blocks for a predictive analytics system to identify CHF patients with high readmission risk.
|keyword = patient readmissions,healthcare information technologies,congestive heart failure,predictive healthcare analytics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Infrastructure Development and Governance as Collective Action'''
{{header}}
{{article
|author= Panos Constantinides,Michael Barrett,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = In this paper, we examine the challenges around the development and scalability of information infrastructures. We identify two possible solutions proposed in the literature, one emphasizing more top-down control and the need for a clear IT governance framework, and a second arguing for a more flexible approach since absolute control is impossible and only leads to drift and unintended outcomes. We suggest that there is a clear gap in the literature in better understanding how to govern the development of information infrastructures using a bottom-up approach. We build on research that approaches IS development as a collective action problem and focus on how different actors frame the infrastructure as a public and private good, and how the framing process is underpinned by actors' different ideologies. We use our theoretical approach to examine the framing of the development of a regional health information infrastructure in Crete. Our analysis examines how different actors frame the infrastructure as a collective action good and explore their ideological positioning. We explore the struggle around meanings attributed to the good over time as being a public or private one in establishing or sustaining relations of power, and how legitimacy is challenged or reinforced. Finally, we develop contributions on the collective action challenges in infrastructure development and suggest how a polycentric approach to governance might be further developed to promote the ongoing cultivation of information infrastructures from the bottom up.
|keyword = information infrastructure,collective action,longitudinal research,healthcare,polycentric governance,framing,ideology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Paradoxes and the Nature of Ambidexterity in IT Transformation Programs'''
{{header}}
{{article
|author= Robert Wayne Gregory,Mark Keil,Jan Muntermann,Magnus Mahring,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Though information technology (IT) transformation programs are gaining in importance, we know little about the nature of the challenges involved in such programs and how to manage them. Using grounded theory methodology, we conducted a multiyear case study of a large IT transformation program in a major commercial bank, during which we encountered the interrelated themes of paradoxes and ambidexterity. Grounded in our case, we construct a substantive theory of ambidexterity in IT transformation programs that identifies and explains the paradoxes that managers need to resolve in IT transformation programs. The ambidexterity areas we identified are (1) IT portfolio decisions (i.e., IT efficiency versus IT innovation), (2) IT platform design (i.e., IT standardization versus IT differentiation), (3) IT architecture change (i.e., IT integration versus IT replacement), (4) IT program planning (i.e., IT program agility versus IT project stability), (5) IT program governance (i.e., IT program control versus IT project autonomy), and (6) IT program delivery (i.e., IT program coordination versus IT project isolation). What weaves these six areas together is the combined need for IT managers to employ ambidextrous resolution strategies to ensure short-term IT contributions and continuous progress of IT projects while simultaneously working toward IT transformation program success as a foundation for IT-enabled business transformation. However, in addition to this commonality, we find that the nature of paradoxical tensions differs across the six areas and requires slightly different management strategies for paradox resolution. Ambidexterity areas (1), (2), and (3) are associated with IT transformation strategizing and, in addition to balancing short-and long-term goals, require the mutual accommodation and blending of business and IT interests in the spirit of IT-business partnering to achieve IT-enabled business change and IT-based competitiveness. Ambidexterity areas (4), (5), and (6) are associated with IT program and project execution and, in addition to balancing short-and long-term requirements, require a recurrent and dynamic act of balancing "local" needs at the IT project level and "global" needs at the IT program level.
|keyword = information technology,transformation programs,ambidexterity,paradoxical tensions,balancing,blending,grounded theory methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Control, Incentive Contracts, and Knowledge Transfer in Offshore Business Process Outsourcing'''
{{header}}
{{article
|author= Ying Liu,Ravi Aron,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = We study the determinants of output quality in offshore business process outsourcing (BPO). Firms can exert control over output quality through incentives formally written into contracts and allow both clients and providers to manage the offshore agents creating a dual governance mechanism. We use a combination of two data sets, a cross sectional data set of 139 processes and a balanced panel data set comprising 21 processes with 36 observations per process, to investigate the impact of different factors on the quality of output of offshore BPO providers. Our findings point to the strong moderating effect of process codifiability on the dual governance mechanism. Process codifiability is not only associated with higher output quality, it also moderates the functioning of the dual governance mechanism and determines when the managerial efforts of the client and provider are substitutes and when they are complementary. We show that contractual incentives tied to quality are generally associated with a higher quality of output. Finally, we show that the use of process-level inter-organizational information systems also has a positive impact on the output quality of offshore BPO providers.
|keyword = business process outsourcing,offshoring,output quality,organizational control,empirical research,agency theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Market Positioning by IT Service Vendors Through Imitation'''
{{header}}
{{article
|author= Karen Ruckman,Nilesh Saraf,Vallabh Sambamurthy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Information technology (IT) services vendors operate in a highly competitive but also institutional environment that render their service-line offerings mutually observable. This suggests that imitation of rivals' decisions can be an efficient means for IT vendors when reconfiguring their service-line offerings. To explore how such imitation unfolds in this sector, we estimate a series of logistic regression models of 116 IT vendors' service-line choices over three time periods. First, from the strategic imitation literature we identify the key imitation "referents," which is a group of firms or a single firm with specific traits, and we test the relative influence of each referent. All of our analysis includes these referents as predictors of service-line choice. Next, we tested more nuanced models using theoretically guided subsamples as follows. One, based on information systems (IS) literature, we consider the IT vendors as embedded in three distinct " institutional spheres," each corresponding to a knowledge domain, namely, technical, functional, and vertical industry domains. We separately examine imitation in each subsample corresponding to the three types of service lines. Two, based on strategy literature, we consider that the influence of the imitation referents differs when the choice under consideration is the addition of a new service line versus a withdrawal. Our results across all of these subsamples uncover a nuanced pattern of imitation that sometimes contrasts the full-sample results. The most prominent result is that although imitation is highly salient, the different imitation referents are not universally influential across all knowledge domains and between development versus withdrawal decisions. Specifically, the imitation of similar firms is widespread, whereas the imitation of largest firms or offering popular service-lines, which indicates bandwagon effects, are at play only selectively. This study contributes to the IS literature by laying a basis for a variety of research directions including resource spillovers and vicarious learning in IT sectors.
|keyword = IT outsourcing,institutional aspects of information systems,strategic management of IT,institutional theory,firm-level imitation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Early to Adopt and Early to Discontinue: The Impact of Self-Perceived and Actual IT Knowledge on Technology Use Behaviors of End Users'''
{{header}}
{{article
|author= Rohit Aggarwal,David Kryscynski,Vishal Midha,Harpreet Singh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = For organizations to achieve the benefits of new information technology (IT) systems, their users must adopt and then actually use these new systems. Recent models help to articulate the potentially different explanations for why some users will adopt and then continue using new technologies, but these models have not explicitly incorporated IT knowledge. This is particularly important in contexts where the user base may be non-IT professionals-i.e., the users may vary substantially in their basic IT knowledge. We draw on psychology to argue that in situations where there is a wide variance in actual IT knowledge, there will often exist a U-shaped relationship between actual and self-perceived IT knowledge such that the least knowledgeable believe themselves to be highly knowledgeable. We then draw on individual-level adoption theories to argue that users with high self-perceived IT knowledge will be more likely to adopt new technologies and do so faster. We also draw on individual-level continuance theories to argue that users with low actual IT knowledge will be more likely to discontinue using new technologies and do so faster. We test our expectations using a proprietary data set of 225 sales professionals in a large Indian pharmaceutical company that is testing a new customer relationship management system. We find strong support for our hypotheses.
|keyword = IT knowledge,non-IT professionals,adoption,continuance,econometric analysis,healthcare,pharma,CRM,SaaS,cloud computing,self-assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Turnover or Turnaway? Competing Risks Analysis of Male and Female IT Professionals' Job Mobility and Relative Pay Gap'''
{{header}}
{{article
|author= Damien Joseph,Soon Ang,Sandra A. Slaughter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This study draws on distributive justice, human capital, and stigmatization theories to hypothesize relationships between relative pay gap and patterns of job mobility. Our study also expands the criterion space of job mobility by contrasting different job destinations when information technology (IT) professionals make job moves. We examine three job moves: (a) turnover to another IT job in a different firm, (b) turnaway-within to a non-IT job, and (c) turnaway-between to a different firm and a non-IT job. We analyze work histories spanning 28 years for 359 IT professionals drawn from the National Longitudinal Survey of Youth. We report three major findings. First, as hypothesized, larger relative pay gaps significantly increase the likelihood of job mobility. Second, IT males and IT females have different job mobility patterns. IT males are more likely to turn over than turn away-between when faced with a relative pay gap. Further, and contrary to predictions from human capital theory, IT males are more likely to turn away-within than turn over. This surprising finding suggests that the ubiquitous use of IT in other business functions may have increased the value of IT skills for non-IT jobs and reduced the friction of moving from IT to other non-IT positions. Third, and consistent with stigmatization arguments, IT females are more likely to turn away from IT than to turn over when faced with a relative pay gap. In fact, to reduce relative pay gaps, IT females tend to take on lower-status jobs that pay less than their IT jobs. We conclude this study with important theoretical, practical, and policy implications.
|keyword = information technology professionals,relative pay gap,turnover,turnaway,job mobility,stigmatization,human capital,survival analysis,competing risks,longitudinal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Emergence of Online Community Leadership'''
{{header}}
{{article
|author= Steven L. Johnson,Hani Safadi,Samer Faraj,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Compared to traditional organizations, online community leadership processes and how leaders emerge are not well studied. Previous studies of online leadership have often identified leaders as those who administer forums or have high network centrality scores. Although communication in online communities occurs almost exclusively through written words, little research has addressed how the comparative use of language shapes community dynamics. Using participant surveys to identify leading online community members, this study analyzes a year of communication network history and message content to assess whether language use differentiates leaders from other core community participants. We contribute a novel use of textual analysis to develop a model of language use to evaluate the utterances of all participants in the community. We find that beyond communication network position-in terms of formal role, centrality, membership in the core, and boundary spanning-those viewed as leaders by other participants, post a large number of positive, concise posts with simple language familiar to other participants. This research provides a model to study online language use and points to the emergent and shared nature of online community leadership.
|keyword = online communities,leadership,natural language processing,knowledge management,network analysis,computer-mediated communication and collaboration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web Footprints of Firms: Using Online Isomorphism for Competitor Identification'''
{{header}}
{{article
|author= Gautam Pant,Olivia R. L. Sheng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Competitive isomorphism refers to the phenomenon of competing firms becoming similar as they mimic each other under common market forces. With the growing presence of firms as well as their consumers and suppliers on the Web, we discover a parallel phenomenon of online isomorphism wherein the Web footprints of competing firms are found to overlap. We propose new online metrics based on the content, in-links, and outlinks of firms' websites to measure the presence of online isomorphism as well as uncover its utility in predicting competitor relationships. Through rigorous analysis involving more than 2,600 firms, we find that predictive models for competitor identification based on online metrics are largely superior to those using offline data such as Standard Industrial Classification codes and market values of firms. In addition, combining online and offline metrics can boost the predictive performance. We also find that such models are valuable for identifying nuances of competitor relationships such as asymmetry and the role of industrial divisions. Furthermore, the suggested predictive models can effectively rank firms in an industrial division by their likelihood of being competitors to a focal firm as well as identify new future competitors, thus adding to a portfolio of evidence indicating their utility for managers and analysts.
|keyword = isomorphism,competitor identification,Web metrics,predictive models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Role of Social Media in Social Change: An Analysis of Collective Sense Making During the 2011 Egypt Revolution'''
{{header}}
{{article
|author= Onook Oh,Chanyoung Eom,H. R. Rao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = This study explores the role of social media in social change by analyzing Twitter data collected during the 2011 Egypt Revolution. Particular attention is paid to the notion of collective sense making, which is considered a critical aspect for the emergence of collective action for social change. We suggest that collective sense making through social media can be conceptualized as human-machine collaborative information processing that involves an interplay of signs, Twitter grammar, humans, and social technologies. We focus on the occurrences of hashtags among a high volume of tweets to study the collective sense-making phenomena of milling and keynoting. A quantitative Markov switching analysis is performed to understand how the hashtag frequencies vary over time, suggesting structural changes that depict the two phenomena. We further explore different hashtags through a qualitative content analysis and find that, although many hashtags were used as symbolic anchors to funnel online users' attention to the Egypt Revolution, other hashtags were used as part of tweet sentences to share changing situational information. We suggest that hashtags functioned as a means to collect information and maintain situational awareness during the unstable political situation of the Egypt Revolution.
|keyword = social media,social change,2011 Egypt Revolution,Twitter,hashtag,sociomateriality,collective sense making,human-machine collaborative information process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Perceived Firm Attributes and Intrinsic Motivation in Sponsored Open Source Software Projects'''
{{header}}
{{article
|author= Sebastian Spaeth,Georg von Krogh,Fang He,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2015
|abstract = Voluntary contributions are crucial to the success of open source software (OSS) projects. Firms sponsoring OSS projects may face substantial challenges in soliciting such contributions, since volunteer participants are neither regulated by an employment contract nor offered financial incentives. Although prior work has shown the positive impact of motivation on the effort expended by volunteer participants, there is limited understanding of how specific firm attributes shape volunteers' intrinsic motivation. We offer a theoretical model of how the perceived community-based credibility and openness of the sponsoring firm have a positive impact on the intrinsic motivation of volunteer participants. The model is explored using survey data on volunteer participants from two sponsored OSS projects. Results show that a sponsoring firm's community-based credibility (OSS developers' perception of its expertise and trustworthiness) and openness (its mutual knowledge exchange with the community) strengthen the volunteer participants' social identification with the firm-sponsored community, which in turn reinforces their intrinsic motivation to participate. Moreover, the perceived community-based credibility of a sponsoring firm directly enhances volunteer participants' intrinsic motivation, whereas perceived openness fails to affect motivation without the mediating mechanism of social identification. Implications for firms seeking voluntary contributions for their sponsored OSS projects are discussed.
|keyword = open source software,firm sponsorship,firm attributes,intrinsic motivation,voluntary contributions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MOTIVATING EMPLOYEES TO EXPLORE COLLABORATION TECHNOLOGY IN TEAM CONTEXTS'''
{{header}}
{{article
|author= Likoebe M. Maruping,Massimo Magni,
|source= MIS QUARTERLY
|year= 2015
|abstract = Firms are increasing their investments in collaboration technologies in order to leverage the intellectual resources embedded in their employees. Research on post-adoption use of technology suggests that the true gains from such investments are realized when users explore various system features and attempt to incorporate them into their work practices. However, the literature has been silent about how to promote such behavior when individuals are embedded in team settings, where members' actions are interdependent. This research develops a multilevel model that theorizes the cross-level influence of team empowerment on individual exploration of collaboration technology. Further, it identifies two cognitions-intention to continue exploring and expectation to continue exploring-that are oriented toward exploring ways to incorporate implemented technology into daily work routines over time. A 12-month field study of 212 employees in 48 organizational work teams was conducted to test the multilevel research model. The results provide support for the hypotheses, with team empowerment having a positive cross-level influence on intention to continue exploring and expectation to continue exploring and these, in turn, mediating the cross-level influence of team empowerment on individual exploration of collaboration technology.
|keyword = Collaboration technology,IT exploration,extended use,multilevel theory,cross-level mediation,teams,technology use,empowerment,post-implementation,post-adoption use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW DOES THE INTERNET AFFECT THE FINANCIAL MARKET? AN EQUILIBRIUM MODEL OF INTERNET-FACILITATED FEEDBACK TRADING'''
{{header}}
{{article
|author= Xiaoquan (Michael) Zhang,Lihong Zhang,
|source= MIS QUARTERLY
|year= 2015
|abstract = The ease of Internet stock trading has lured relatively inexperienced investors into the financial markets. This paper is a study of the consequences of the influx of these uninformed traders with a dynamic equilibrium framework. The results show that these strategic, uninformed online traders who adopt feedback strategies cannot outperform those who do not follow feedback strategies and that feedback trading cannot affect market equilibrium. The results also show that an informed trader's equilibrium strategy and expected profit remain unchanged with or without feedback trading. The presence of feedback trading in the market does not affect the speed at which information gets incorporated into prices. If uninformed traders aggregately adopt a more aggressive feedback trading strategy, they bear a higher risk. It is therefore important to manage and contain these uninformed traders' risks. The implications for regulating and designing such Internet trading systems are also discussed.
|keyword = Financial information,financial market,feedback strategy,market stability,online trading,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW DO ENTERPRISE RESOURCE PLANNING SYSTEMS AFFECT FIRM RISK? POST-IMPLEMENTATION IMPACT'''
{{header}}
{{article
|author= Feng Tian,Sean Xin Xu,
|source= MIS QUARTERLY
|year= 2015
|abstract = Managing firm risk, or firm performance volatility, is a key task for contemporary firms. Although information technology ( IT) has been generally viewed as an effective information processing tool that enables firms to better cope with uncertainty, thus holding the potential to mitigate firm performance volatility, evidence to support this view is lacking in the literature. We theorize that enterprise resource planning (ERP) systems, a major type of enterprise IT applications, can help reduce firm risk and, in particular, we argue that, to uncover the risk reduction effect of ERP systems, a research focus on the post-implementation stage is needed. Based on a sample of 2,127 firm-year observations, we found that ERP systems in the post-implementation stage were associated with reduced firm risk, and that the risk reduction effect was stronger for ERP systems with a greater scope of functional and operational modules, especially functional modules. We further found that, on average, the risk reduction effect of ERP systems became greater when firms' operating environments feature higher uncertainty, while the risk reduction associated with fully deploying ERP system modules seem to level off as environmental uncertainty increases. These findings extend our understanding of the business value of ERP systems by shedding light on the risk reduction benefit of ERP systems.
|keyword = ERP systems,firm risk,performance volatility,post-implementation,environmental uncertainty,ERP system scope,business value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INVESTING IN INFORMATION SYSTEMS: ON THE BEHAVIORAL AND INSTITUTIONAL SEARCH MECHANISMS UNDERPINNING HOSPITALS' IS INVESTMENT DECISIONS'''
{{header}}
{{article
|author= Torsten Oliver Salge,Rajiv Kohli,Michael Barrett,
|source= MIS QUARTERLY
|year= 2015
|abstract = This study integrates tenets of the behavioral theory of the firm and neo-institutional theory to identify four recurring search mechanisms that are expected to influence hospital managers' information systems investment decisions. To account for the critical role of regulation in healthcare, senior managers' reliance on each of these four search mechanisms is hypothesized to be contingent upon their hospital's regulative legitimacy. Analyses of panel data from all 153 public nonspecialist hospital organizations in England reveal that hospital managers invest in IS not only to find solutions to performance shortfalls (problemistic search), but also to achieve continuity and predictability in resource allocation (institutionalized search) and signal conformity with external norms and expectations (mimetic search). We find that the desire to make adequate use of uncommitted financial resources (slack search) is salient only among hospitals with low levels of regulative legitimacy. These new insights into the motives that trigger-and constrain-senior managers' IS investment decisions will help IS managers to strengthen their case for IS investment and guide policy makers in how best to allocate resources to IS in healthcare and possibly beyond.
|keyword = Decision making,IS investment,business value of IT,behavioral theory of the firm,institutional theory,regulative legitimacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INSIDER THREATS IN A FINANCIAL INSTITUTION: ANALYSIS OF ATTACK-PRONENESS OF INFORMATION SYSTEMS APPLICATIONS'''
{{header}}
{{article
|author= Jingguo Wang,Manish Gupta,H. Raghav Rao,
|source= MIS QUARTERLY
|year= 2015
|abstract = This study investigates the risk of insider threats associated with different applications within a financial institution. Extending routine activity theory (RAT) from criminology literature to information systems security, hypotheses regarding how application characteristics, namely value, inertia, visibility, accessibility, and guardians, cause applications to be exposed to insider threats are developed. Routine activity theory is synthesized with survival modeling, specifically a Weibull hazard model, and users' system access behavior is investigated using seven months of field data from the institution. The inter-arrival times of two successive unauthorized access attempts on an application are employed as the measurement of risk. For a robustness check, the daily number of unauthorized attempts experienced by an application as an alternative measurement of risk are introduced and a zero-inflated Poisson-Gamma model is developed. The Markov chain Monte Carlo (MCMC) method is used for model estimations. The results of the study support the empirical application of routine activity theory in understanding insider threats, and provide a picture of how different applications have different levels of exposure to such threats. Theoretical and practical implications for risk management regarding insider threats are discussed. This study is among the first that uses behavioral logs to investigate victimization risk and attack proneness associated with information assets.
|keyword = Information security,insider threats,routine activity theory,information systems applications,MCMC,risk quantification,dark side of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ENHANCED FEAR APPEAL RHETORICAL FRAMEWORK: LEVERAGING THREATS TO THE HUMAN ASSET THROUGH SANCTIONING RHETORIC'''
{{header}}
{{article
|author= Allen C. Johnston,Merrill Warkentin,Mikko Siponen,
|source= MIS QUARTERLY
|year= 2015
|abstract = Fear appeals, which are used widely in information security campaigns, have become common tools in motivating individual compliance with information security policies and procedures. However, empirical assessments of the effectiveness of fear appeals have yielded mixed results, leading IS security scholars and practitioners to question the validity of the conventional fear appeal framework and the manner in which fear appeal behavioral modeling theories, such as protection motivation theory (PMT), have been applied to the study of information security phenomena. We contend that the conventional fear appeal rhetorical framework is inadequate when used in the context of information security threat warnings and that its primary behavioral modeling theory, PMT, has been misspecified in the extant information security research. Based on these arguments, we propose an enhanced fear appeal rhetorical framework that leverages sanctioning rhetoric as a secondary vector of threats to the human asset, thereby adding the dimension of personal relevance, which is critically absent from previous fear appeal frameworks and PMT-grounded security studies. Following a hypothetical scenario research approach involving the employees of a Finnish city government, we validate the efficacy of the enhanced fear appeal framework and determine that informal sanction rhetoric effectively enhances conventional fear appeals, thus providing a significant positive influence on compliance intentions.
|keyword = Fear appeals,protection motivation theory,deterrence theory,information security,threats,responses,sanctions,rhetoric,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SERVICE INNOVATION: A SERVICE-DOMINANT LOGIC PERSPECTIVE'''
{{header}}
{{article
|author= Robert F. Lusch,Satish Nambisan,
|source= MIS QUARTERLY
|year= 2015
|abstract = In this article, we offer a broadened view of service innovation-one grounded in service-dominant logic-that transcends the tangible-intangible and producer-consumer divides that have plagued extant research in this area. Such a broadened conceptualization of service innovation emphasizes (1) innovation as a collaborative process occurring in an actor-to-actor (A2A) network, (2) service as the application of specialized competences for the benefit of another actor or the self and as the basis of all exchange, (3) the generativity unleashed by increasing resource liquefaction and resource density, and (4) resource integration as the fundamental way to innovate. Building on these core themes, we offer a tripartite framework of service innovation: (1) service ecosystems, as emergent A2A structures actors create and recreate through their effectual actions and which offer an organizing logic for the actors to exchange service and cocreate value; (2) service platforms, which enhance the efficiency and effectiveness of service exchange by liquefying resources and increasing resource density (facilitating easy access to appropriate resource bundles) and thereby serve as the venue for innovation; and (3) value cocreation, which views value as cocreated by the service offer(er) and the service beneficiary (e. g., customer) through resource integration and indicate the need for mechanisms to support the underlying roles and processes. In discussing these components, we consider the role of information technology-both as an operand resource and as an operant resource-and then examine the implications for research and practice in digitally enabled service innovation.
|keyword = Service innovation,S-D logic,platforms,ecosystems,value cocreation,collaboration,resource integration,institutions,architecture,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE VALUE OF SELF-SERVICE: LONG-TERM EFFECTS OF TECHNOLOGY-BASED SELF-SERVICE USAGE ON CUSTOMER RETENTION'''
{{header}}
{{article
|author= Anne Scherer,Nancy V. Wuenderlich,Florian von Wangenheim,
|source= MIS QUARTERLY
|year= 2015
|abstract = Advancements in information technology have changed the way customers experience a service encounter and their relationship with service providers. Especially technology-based self-service channels have found their way into the 21st century service economy. While research embraces these channels for their cost-efficiency, it has not examined whether a shift from personal to self-service affects customer-firm relationships. Drawing from the service-dominant logic and its central concept of value-in-context, we discuss customers' value creation in self-service and personal service channels and examine the long-term impact of these channels on customer retention. Using longitudinal customer data, we investigate how the ratio of self-service versus personal service use influences customer defection over time. Our findings suggest that the ratio of self-service to personal service used affects customer defection in a U-shaped manner, with intermediate levels of both self-service and personal service use being associated with the lowest likelihood of defection. We also find that this effect mitigates over time. We conclude that firms should not shift customers toward self-service channels completely, especially not at the beginning of a relationship. Our study underlines the importance of understanding when and how self-service technologies create valuable customer experiences and stresses the notion of actively managing customers' cocreation of value.
|keyword = Self-service,e-service,value-in-context,customer retention,customer defection,longitudinal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ALGORITHM AND THE CROWD: CONSIDERING THE MATERIALITY OF SERVICE INNOVATION'''
{{header}}
{{article
|author= Wanda J. Orlikowski,Susan V. Scott,
|source= MIS QUARTERLY
|year= 2015
|abstract = This special issue acknowledges important innovations in the world of service and within this domain we are particularly interested in exploring the rise and influence of web-based crowd-sourcing and algorithmic rating and ranking mechanisms. We suggest that a useful way to make sense of these digital service innovations and their novel implications is to recognize that they are materialized in practice. We thus need effective conceptual and analytical tools that allow us to take materiality seriously in our studies of service innovation. To this end, we propose some theoretical ideas relating to a sociomaterial perspective, and then highlight empirically how this perspective helps us analyze the specific service materializations enacted through the algorithmic configuring of crowd-sourced data, and how these make a difference in practice to the outcomes produced.
|keyword = Algorithms,crowds,innovation,materiality,performativity,practice,sociomateriality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DISTRIBUTED TUNING OF BOUNDARY RESOURCES: THE CASE OF APPLE'S IOS SERVICE SYSTEM'''
{{header}}
{{article
|author= Ben Eaton,Silvia Elaluf-Calderwood,Carsten Sorensen,
|source= MIS QUARTERLY
|year= 2015
|abstract = The digital age has seen the rise of service systems involving highly distributed, heterogeneous, and resource-integrating actors whose relationships are governed by shared institutional logics, standards, and digital technology. The cocreation of service within these service systems takes place in the context of a paradoxical tension between the logic of generative and democratic innovations and the logic of infrastructural control. Boundary resources play a critical role in managing the tension as a firm that owns the infrastructure can secure its control over the service system while independent firms can participate in the service system. In this study, we explore the evolution of boundary resources. Drawing on Pickering's (1993) and Barrett et al.'s (2012) conceptualizations of tuning, the paper seeks to forward our understanding of how heterogeneous actors engage in the tuning of boundary resources within Apple's iOS service system. We conduct an embedded case study of Apple's iOS service system with an in-depth analysis of 4,664 blog articles concerned with 30 boundary resources covering 6 distinct themes. Our analysis reveals that boundary resources of service systems enabled by digital technology are shaped and reshaped through distributed tuning, which involves cascading actions of accommodations and rejections of a network of heterogeneous actors and artifacts. Our study also shows the dualistic role of power in the distributed tuning process.
|keyword = Service system innovation,mobile platform,ecosystem,digital infrastructure,boundary resource dynamics,tuning,sociomateriality,iOS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BRIDGING THE SERVICE DIVIDE THROUGH DIGITALLY ENABLED SERVICE INNOVATIONS: EVIDENCE FROM INDIAN HEALTHCARE SERVICE PROVIDERS'''
{{header}}
{{article
|author= Shirish C. Srivastava,G. Shainesh,
|source= MIS QUARTERLY
|year= 2015
|abstract = The digital divide is usually conceptualized through goods-dominant logic, where bridging the divide entails providing digital goods to disadvantaged segments of the population. This is expected to enhance their digital capabilities and thus to have a positive influence on the digital outcomes (or services) experienced. In contrast, this study is anchored in an alternative service-dominant logic and posits that viewing the divide from a service perspective might be better suited to the context of developing countries, where there is a huge divide across societal segments in accessing basic services such as healthcare and education. This research views the prevailing differences in the level of services consumed by different population segments (service divide) as the key issue to be addressed by innovative digital tools in developing countries. The study posits that information and communication technologies (ICTs) can be leveraged to bridge the service divide to enhance the capabilities of service-disadvantaged segments of society. But such service delivery requires an innovative assembly of ICT as well as non-ICT resources. Building on concepts from service-dominant logic and service science, this paper aims to understand how such service innovation efforts can be orchestrated. Specifically, adopting a process view, two Indian enterprises that have developed sustainable telemedicine healthcare service delivery models for the rural population in India are examined. The study traces the configurations of three interactional resources-knowledge, technology, and institutions-through which value-creating user-centric objectives of increasing geographical access and reducing cost are achieved. The theoretical contributions are largely associated with unearthing and understanding how the three interactional resources were orchestrated for service-centric value creation in different combinative patterns as resource exploitation, resource combination, and value reinforcement. The analysis also reveals the three distinct stages of service innovation evolution (idea and launch, infancy and early growth, and late growth and expansion), with a distinct shift in the dominant resource for each stage. Through an inductive process, the study also identifies four key enablers for successfully implementing these ICT-enabled service innovations: obsessive customer empathy, belief in the transformational power of ICT, continuous recursive learning, and efficient network orchestration.
|keyword = Service innovation,developing countries,service divide,healthcare,process view,service science,service systems,social entrepreneurship,society,digital divide,India,institutions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How the Use of Big Data Analytics Affects Value Creation in Supply Chain Management'''
{{header}}
{{article
|author= Daniel Q. Chen,David S. Preston,Morgan Swink,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Despite numerous testimonials of first movers, the underlying mechanisms of organizations' big data analytics (BDA) usage deserves close investigation. Our study addresses two essential research questions: (1) How does organizational BDA usage affect value creation? and (2) What are key antecedents of organizational-level BDA usage? We draw on dynamic capabilities theory to conceptualize BDA use as a unique information processing capability that brings competitive advantage to organizations. Furthermore, we employ the technology-organization-environment (TOE) framework to identify and theorize paths via which factors influence the actual usage of BDA. Survey data collected from 161 U.S.-based companies show that: organizational-level BDA usage affects organizational value creation; the degree to which BDA usage influences such creation is moderated by environmental dynamism; technological factors directly influence organizational BDA usage; and organizational and environmental factors indirectly influence organizational BDA usage through top management support. Collectively, these findings provide a theory-based understanding of the impacts and antecedents of organizational BDA usage, while also providing guidance regarding what managers should expect from usage of this rapidly emerging technology.
|keyword = big data,data analytics,dynamics capability theory,structural equation modeling,survey research,TOE framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Platform Desertion by App Developers'''
{{header}}
{{article
|author= Amrit Tiwana,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Platform desertion, or a developer's stopping the development of an app for a platform, is a widespread phenomenon to the detriment of platforms. However, the extant literature focuses primarily on why app developers join-not leave-a platform. This app-level study develops two ideas: (a) coordination costs borne by an app's developer are associated with platform desertion, and (b) these costs are, in turn, shaped by a nuanced interplay between app decision rights and app "microarchitecture" introduced here. We use survey and snapshot archival data spanning 2009-2014 on over 300 apps in the Mozilla Firefox ecosystem to test these ideas. Our novel contribution shows how, by influencing coordination costs, the previously invisible interplay between app decision rights and app microarchitecture shapes an app's platform desertion. We find that delegating app decision rights to its developer weakens the coordination cost-reducing benefits of decoupling an app from the platform but strengthens those of standardizing its interfaces to the platform. The key theoretical implication is that app decision rights and app microarchitecture symbiotically influence the coordination costs borne by an app's developer. The key practical implication for platform designers is that the choices about who ought to make what decisions are intertwined with the architecture of the governed information technology artifact.
|keyword = app developers,apps,architecture,coordination cost,decision rights,ecosystems,mobile apps,modularity,platforms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Two Formulas for Success in Social Media: Learning and Network Effects'''
{{header}}
{{article
|author= Liangfei Qiu,Qian Tang,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Recent years have witnessed an unprecedented explosion in information technology that enables dynamic diffusion of user-generated content in social networks. Online videos, in particular, have changed the landscape of marketing and entertainment, competing with premium content and spurring business innovations. In the present study, we examine how learning and network effects drive the diffusion of online videos. While learning happens through informational externalities, network effects are direct payoff externalities. Using a unique data set from YouTube, we empirically identify learning and network effects separately, and find that both mechanisms have statistically and economically significant effects on video views; furthermore, the mechanism that dominates depends on the video type. Specifically, although learning primarily drives the popularity of quality-oriented content, network effects also make it possible for attention-grabbing content to go viral. Theoretically, we show that, unlike the diffusion of movies, it is the combination of both learning and network effects that generate the multiplier effect for the diffusion of online videos. From a managerial perspective, providers can adopt different strategies to promote their videos accordingly, that is, signaling the quality or featuring the viewer base depending on the video type. Our results also suggest that YouTube can play a much greater role in encouraging the creation of original content by leveraging the multiplier effect.
|keyword = learning,network effects,social contagion,social media,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Thumbs Up, Sales Up? The Contingent Effect of Facebook Likes on Sales Performance in Social Commerce'''
{{header}}
{{article
|author= Kyunghee Lee,Byungtae Lee,Wonseok Oh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = In this study we investigate whether social reference systems, such as Facebook "likes" (FBLs), promote sales in social commerce, wherein adverse selection and quality uncertainty often severely damage consumer trust and impede efforts to achieve sustainable growth. We also examine the extent to which product characteristics (product uncertainty and product franchising) and deal characteristics (tipping points, discount rates, and deal durations) moderate the social selling stimulated by FBLs. On the basis of 1,327 samples collected from a major social commerce platform provider, we identify several interesting empirical regularities regarding the relationship between FBLs and social commerce sales. The findings suggest that FBLs drive traffic and increase sales. Information technology artifacts and social technologies, such as FBLs, can endow a consumer's shopping experience with a socialization component and induce social selling in collective buying platforms. Nevertheless, significant variations occur across products and deals. For example, consumers who purchase experience goods more frequently depend on FBLs than do those who buy search goods. FBLs exert a far greater influence on the sales of goods from independent stores than those from franchise chains. Social commerce consumers are unaffected by heavy discount rates as they make purchase decisions, but they extensively rely on FBLs, particularly when purchasing products that have low tipping points. Our results suggest that social commerce can be a powerful marketplace when the economic utility that is driven by price incentives is further strengthened and protected by the social utility that originates from trust and sharing.
|keyword = deal characteristics,Facebook likes,information asymmetry,online sales,product characteristics,social commerce,social network sites,social utility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Multilevel Approach to Examine Employees' Loyal Use of ERP Systems in Organizations'''
{{header}}
{{article
|author= Hsiuju Rebecca Yen,Paul Jen-Hwa Hu,Sheila Hsuan-Yu Hsu,Eldon Y. Li,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = A successful enterprise resource planning (ERP) system ultimately requires loyal use-proactive, extended use and willingness to recommend such uses to others-by employees. Building on interactional psychology literature and situational strength theory, we emphasize the importance of psychological commitment, in addition to behavioral manifestation, in a multilevel model of loyal use. Our empirical test of the model uses data from 485 employees and 166 information system professionals in 47 large Taiwanese organizations. Individual-level analyses suggest that perceived benefits and workload partially mediate the effects of perceived information quality (IQ) and system quality (SQ) on loyal use. Cross-level analyses show that IQ at the organizational level alleviates the negative effect of an employee's perceived workload on loyal use; organization-level SQ and service-oriented organizational citizenship behaviors (SOCBs) of internal information systems staff reduce the influence of employees' perceived benefits. Overall, our findings suggest that IQ, SQ, and SOCBs at the organizational level influence employees' loyal use in ways different from their effects at the individual level, and seem to affect individuals' cost-benefit analyses. This study contributes to extant literature by considering the SOCBs of the internal information systems group that have been overlooked by most prior research. Our findings offer insights for managers who should find ways to create positive, salient, shared views of IQ, SQ, and SOCBs in the organization to nourish and foster employees' loyal use of an ERP system, including clearly demonstrating the system's utilities and devising viable means to reduce the associated workload.
|keyword = information quality,loyal use,multilevel analysis,perceived benefits,perceived workload,service-oriented organizational citizenship behaviors,system quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Organizational Commitment on Insiders' Motivation to Protect Organizational Information Assets'''
{{header}}
{{article
|author= Clay Posey,Tom L. Roberts,Paul Benjamin Lowry,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Insiders may act to sustain and improve organizational information security, yet our knowledge of what motivates them to do so remains limited. For example, most extant research relies on mere portions of protection motivation theory (PMT) and has focused on isolated behaviors, thus limiting the generalizability of findings to isolated issues, rather than addressing the global set of protective security behaviors. Here, we investigate the motivations surrounding this larger behavioral set by assessing maladaptive rewards, response costs, and fear alongside traditional PMT components. We extend PMT by showing that: (1) security education, training, and awareness (SETA) efforts help form appraisals; (2) PMT's applicability to organizational rather than personal contexts depends on insiders' organizational commitment levels; and (3) response costs provide the link between PMT's appraisals. We show in detail how organizational commitment is the mechanism through which organizational security threats become personally relevant to insiders and how SETA efforts influence many PMT-based components.
|keyword = coping appraisal,information security,MIMIC model,organizational commitment,protection-motivated behaviors,protection motivation theory,security,structural equation modeling,threat appraisal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Robustness of Multiple Indicators in Automated Screening Systems for Deception Detection'''
{{header}}
{{article
|author= Nathan W. Twyman,Jeffrey Gainer Proudfoot,Ryan M. Schuetzler,Aaron C. Elkins,Douglas C. Derrick,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = This study investigates the effectiveness of an automatic system for detection of deception by individuals with the use of multiple indicators of such potential deception. Deception detection research in the information systems discipline has postulated increased accuracy through a new class of screening systems that automatically conduct interviews and track multiple indicators of deception simultaneously. Understanding the robustness of this new class of systems and the limitations of its theoretical improved performance is important for refinement of the conceptual design. The design science proof-of-concept study presented here implemented and evaluated the robustness of these systems for automated screening for deception detection. A large experiment was used to evaluate the effectiveness of a constructed multiple-indicator system, both under normal conditions and with the presence of common types of countermeasures (mental and physical). The results shed light on the relative strength and robustness of various types of deception indicators within this new context. The findings further suggest the possibility of increased accuracy through the measurement of multiple indicators if classification algorithms can compensate for human attempts to counter effectiveness.
|keyword = automated screening systems,deception countermeasures,deception detection,design science,human-computer interaction,human risk assessment,human screening,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Paradoxes of Word of Mouth in Electronic Commerce'''
{{header}}
{{article
|author= Zhijie Lin,Cheng-Suang Heng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Challenging conventional wisdom, we unravel three paradoxes of word of mouth (WOM) in e-commerce. Specifically, the WOM valence paradox contends that higher WOM valence of a product results in a larger subsequent decrease in the WOM valence of the product, the WOM volume paradox propounds that higher WOM volume of a product results in a smaller subsequent increase in the WOM volume of the product, and the WOM spillover paradox proposes that an improvement in the WOM of a product also improves the WOM of connected products in a product network. These paradoxes caution online retailers that superior WOM may at times backfire and not boost further sales. Drawing theoretical support from expectation-confirmation theory and network theory, we collect data from China's largest business-to-consumer platform, Tmall. com, and use linear panel data models to examine WOM evolution in a product network, controlling for relevant factors at the individual product, product network, and time unit levels. Importantly, we base our identification strategies on the use of instrumental variables and the differencein- differences estimation approach. Numerous statistical checks confirm the robustness and consistency of our findings. We contribute to a much richer theoretical understanding of WOM, by extending the applicability of expectation-confirmation theory and network theory to novel predictions and contexts, adding a dynamic perspective, unveiling three important WOM paradoxes, and offering practical insights.
|keyword = econometric analysis,electronic commerce,e-tail,eWoM,product network,product review,word of mouth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Corporate Blogging and Job Performance: Effects of Work-related and Nonwork-related Participation'''
{{header}}
{{article
|author= Benjiang Lu,Xunhua Guo,Nianlong Luo,Guoqing Chen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Corporate blogs are expected to facilitate communication, knowledge sharing, and collaborative innovation within organizations. However, empirical evidence has yet to be found illustrating whether and how such applications have affected job performance. Drawing upon social network theory, we postulate a conceptual model suggesting that employees' online social relationships accumulated through work-and nonwork-related blog participation will engender different effects on job performance. The model is empirically tested using digital trace and archival data collected from two in-practice systems of a large telecommunications company. The results reveal that, in the workrelated blog network, the structural and cognitive dimensions of social relationships positively affect job performance, whereas the relational dimension shows a negative influence. Meanwhile, participation in nonwork-related blog network benefits job performance for employees with a high level of performance in the previous time period, but is detrimental for other employees. The findings uncover the influencing mechanism of corporate blogging on job performance and offer practical advice for managers to better exploit the value of intraorganizational social media.
|keyword = blogs,corporate blogs,digital traces,job performance,organizational social media,social capital,social network,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Last Research Mile: Achieving Both Rigor and Relevance in Information Systems Research'''
{{header}}
{{article
|author= Jr. Jay F. Nunamaker,Robert O. Briggs,Douglas C. Derrick,Gerhard Schwabe,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = From our desk chairs it may be tempting to work up an idea, build a quick prototype, test it in a lab, and say, "Our work here is done; the rest is merely details." More scholarly knowledge awaits discovery, however, by researchers who shepherd an information systems (IS) solution through the last research mile, that is, through successful transition to the workplace. Going the last research mile means using scientific knowledge and methods to address important unsolved classes of problems for real people with real stakes in the outcomes. The last research mile proceeds in three stages: proof-of-concept research to demonstrate the functional feasibility of a solution; proof-of-value research to investigate whether a solution can create value across a variety of conditions; and proof-of-use research to address complex issues of operational feasibility. The last research mile ends only when practitioners routinely use a solution in the field. We argue that going the last research mile negates the assumption that one must trade off rigor and relevance, showing it to be it a false dilemma. Systems researchers who take their solutions through the last research mile may ultimately have the greatest impact on science and society. We demonstrate the last research mile with cases from our own work and the work of others spanning more than forty years.
|keyword = design science,prototypes,research methodology,research rigor,systems research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Barriers to Interorganizational Knowledge Transfer in Post-Hospital Care Transitions: Review and Directions for Information Systems Research'''
{{header}}
{{article
|author= Shi Ying Lim,Sirkka L. Jarvenpaa,Holly J. Lanham,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Post-hospital care transitions involve coordination and continuity of care from hospital providers to patients and community providers. These care transitions represent a domain of high-risk interorganizational collaborations. However, a conversation about how health information technology (HIT) can enhance interorganizational knowledge transfer during care transitions is largely absent in the information systems literature. We conducted a review of qualitative studies of post-hospital care transitions to better understand barriers to knowledge transfer in high-risk interorganizational collaborations. Our analysis highlights how time pressures inhibit multilateral knowledge transfers, accommodation of fluctuating absorptive capacity, and reconciliation of knowledge and goal conflicts. We advance research questions that focus on HIT capabilities to ease these barriers.
|keyword = absorptive capacity,health information technology,health-care transitions,interorganizational knowledge transfer,IS design,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Anatomy of Successful Business Models for Complex Services: Insights from the Telemedicine Field'''
{{header}}
{{article
|author= Christoph Peters,Ivo Blohm,Jan Marco Leimeister,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Telemedicine services may improve the quality of life of individuals while also reducing the costs of service provisioning. They represent an important but as yet understudied type of complex services that integrates many stakeholders acting in service value networks. These complex services typically comprise a combination of information technology (IT) services and highly person-oriented, non-IT services, and are characterized by long service delivery periods. In such an environment, it is particularly difficult to generate successful and sustainable business models, which are necessary for the widespread provision of telemedicine services. Following a design research approach, we develop and evaluate the CompBizMod framework, a morphological box allowing for: (1) the analysis, description, and classification of telemedicine business models, (2) the identification of white spots for future business opportunities, (3) and the identification of patterns for successful business models. We contribute to the literature by presenting a specific business model framework and identifying three business model patterns in the telemedicine industry. We exhibit how business models for complex services can be decomposed into their constituent elements and present an easy and replicable approach for identifying business model patterns in a given industry.
|keyword = analysis frameworks,business model,business model pattern,complex service,telemedicine,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''It Is Not Just About Competition with "Free": Differences Between Content Formats in Consumer Preferences and Willingness to Pay'''
{{header}}
{{article
|author= Benedikt Berger,Christian Matt,Dennis M. Steininger,Thomas Hess,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = While consumption of content in offline formats continues to decline, many providers are still struggling to monetize their content online, because consumers' willingness to pay (WTP) for content in online formats is low. The availability of free content on the Internet is often considered the primary reason for this issue. However, we hold that the lower WTP is also related to a lower appraisal of online formats per se. Using a conjoint analysis and the example of newspaper subscriptions, we explore differences in consumer preferences and WTP between offline and online formats. Our results show that after price, format is the second-most important attribute of a newspaper subscription. While consumers still prefer the printed newspaper to any online format, WTP differs across online formats and is strongly associated with device ownership. Our study provides a novel understanding of the previously neglected factor content format and its importance for content providers.
|keyword = conjoint analysis,consumer preferences,content formats,digital device ownership,digital media,media industries,willingness to pay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating Team Collaboration Quality: The Development and Field Application of a Collaboration Maturity Model'''
{{header}}
{{article
|author= Imed Boughzala,Gert-Jan De Vreede,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = The quality of collaboration directly affects the quality of an organization's outcomes and performance. Trends like globalization and increased product and service complexity have pushed organizations to become more and more reliant on collaboration in distributed, cross-disciplinary, cross-cultural, virtual teams. The present research is based on an applied science/engineering (AS/E) research paradigm to address an important class of unsolved problems-measuring the quality of collaboration within and across organizational boundaries. This paper presents a collaboration maturity model (Col-MM) to assess an organization's team collaboration maturity as a first step toward a generalizable solution to that class of problems. The Col-MM is intended to be sufficiently generic to be applied to different organizational and team settings and usable by practitioners for conducting self-assessments. The Col-MM was developed during a series of focus group meetings with professionals (business unit managers). The model was then piloted and subsequently applied in a field study in an automotive company. This paper reports on the development and field application of the Col-MM. It contributes to the collaboration science literature, theory, and practice through a detailed AS/E study that develops a maturity model and a system for administering it that provides proof of value and effective use in the field.
|keyword = collaboration maturity,collaboration maturity model,collaboration quality,collaboration technology,online collaboration,organizational performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Strategic Relevance of Organizational Virtues Enabled by Information Technology in Organizational Innovation'''
{{header}}
{{article
|author= Sutirtha Chatterjee,Gregory Moody,Paul Benjamin Lowry,Suranjan Chakraborty,Andrew Hardin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = The central theme of this paper is that information technology (IT) can serve to create ethical organizations endowed with virtuous characteristics, and that such ethical organizations can innovate better in today's dynamic market environment. Drawing on the notion of virtue ethics propounded by the Greek philosopher Aristotle, we theorize that core organizational IT affordances influence the development of organizational virtues, which in turn influence organizational improvisational capabilities and innovation. We propose the "IT-virtues-innovation" (IVI) model and test it using a cross-organizational survey of 250 employees from various organizations in the United States. Our findings largely support our proposal that IT affordances positively influence organizational virtues, which then influence organizational improvisational capabilities, thus improving organizational innovation. This paper contributes to the understanding of organizational innovation by articulating the strategic usefulness of IT-enabled organizational ethics, and it explains how IT-enabled ethical competence (virtues) influences strategic competence (improvisational capabilities and innovation).
|keyword = ethical organizations,information technology affordances,information technology strategy,organizational capabilities,organizational courage,organizational innovation,organizational justice,organizational memory affordance,organizational temperance,organizational virtues,organizational wisdom,process management affordance,virtue ethics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Machiavellianism or Morality: Which Behavior Pays Off In Online Innovation Contests?'''
{{header}}
{{article
|author= Katja Hutter,Johann Fueller,Julia Hautz,Volker Bilgram,Kurt Matzler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Prior research on user behavior in online innovation contests has mainly focused on factors that positively impact prosocial, collaborative behavior, which should ultimately lead to innovative outcomes. However, little is known about the effects of more negative personal characteristics that might result in more competitive, antisocial, and even unethical behavior. This paper considers Machiavellianism as one of the traits that constitute the "dark triad of personality" and explores the relationship between Machiavellianism and participants' contribution behavior in online innovation contests. Specifically we investigate how Machiavellian characteristics influence individuals' contribution intensity, communication, and interaction behavior within the contest community as well as the quality and kind of their contributions. This study relies on multisource individual-level data from a large innovation contest in the field of public transportation. We find that the three dimensions of Machiavellianism-distrust of others, amorality, and desire for status-have very distinct behavioral consequences in the context of online innovation contests. Specifically, the oppositional consequences of amoral manipulation and striving for status on the one hand and showing distrust of others on the other hand concerning contribution quantity and contribution quality are found. This study contributes to a deeper understanding of negative personality traits such as Machiavellianism as powerful predictors of behavior and of success within competitive innovation environments and leads to important managerial implications regarding the design and management of innovation contests.
|keyword = business ethics,co-creation,competitive behavior,crowd-sourcing,innovation,innovation contests,Machiavellianism,online contests,personality traits,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Taxonomy of Evaluation Methods for Information Systems Artifacts'''
{{header}}
{{article
|author= Nicolas Prat,Isabelle Comyn-Wattiau,Jacky Akoka,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Artifacts, such as software systems, pervade organizations and society. In the field of information systems (IS) they form the core of research. The evaluation of IS artifacts thus represents a major issue. Although IS research paradigms are increasingly intertwined, building and evaluating artifacts has traditionally been the purview of design science research (DSR). DSR in IS has not reached maturity yet. This is particularly true of artifact evaluation. This paper investigates the "what" and the "how" of IS artifact evaluation: what are the objects and criteria of evaluation, the methods for evaluating the criteria, and the relationships between the "what" and the "how" of evaluation? To answer these questions, we develop a taxonomy of evaluation methods for IS artifacts. With this taxonomy, we analyze IS artifact evaluation practice, as reflected by ten years of DSR publications in the basket of journals of the Association for Information Systems (AIS). This research brings to light important relationships between the dimensions of IS artifact evaluation, and identifies seven typical evaluation patterns: demonstration; simulation-and metric-based benchmarking of artifacts; practice-based evaluation of effectiveness; simulation- and metric-based absolute evaluation of artifacts; practice-based evaluation of usefulness or ease of use; laboratory, student-based evaluation of usefulness; and algorithmic complexity analysis. This study also reveals a focus of artifact evaluation practice on a few criteria. Beyond immediate usefulness, IS researchers are urged to investigate ways of evaluating the long-term organizational impact and the societal impact of artifacts.
|keyword = artifact evaluation,content analysis,design evaluation,design science,system design,taxonomy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competition Between Open Source and Proprietary Software: Strategies for Survival'''
{{header}}
{{article
|author= Michael Sacks,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = There are two puzzles in the software competition literature: whether both proprietary and open source software will survive and how producers of proprietary software differentiate themselves from open source competition. I address both puzzles by analyzing competition between a firm producing proprietary software and a community producing open source software. If the firm faces no competition, then the software caters to less technologically savvy individuals. When facing competition, the open source software caters to the most technologically savvy individuals, leading the firm to target even less savvy individuals than it would when acting as a monopolist in order to differentiate its software from the open source option. The open source movement, then, may not be an unalloyed success as the growth in open source can be tied to deterioration in the proprietary software. Given that both types of software survive by catering to different segments of the market, an important avenue for research will be to analyze the stability of the underlying segments and the corresponding welfare implications.
|keyword = differentiation,endogenous fixed costs,heterogeneous consumers,Hotelling competition,mixed duopoly,open source software,proprietary software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Task Division for Team Success in Crowdsourcing Contests: Resource Allocation and Alignment Effects'''
{{header}}
{{article
|author= Indika Dissanayake,Jie Zhang,Bin Gu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Advances in information technology bring changes to the nature of work by facilitating companies to go beyond the wisdom of their workforce and tap into the "wisdom of the crowd" via online crowdsourcing contests. In these contests, active and motivated individuals collaborate in the form of self-organized teams that compete for rewards. Using a rich data set of 732 teams in 52 contests collected from the crowdsourcing platform, Kaggle.com, from its launch in April 2010 to July 2012, we studied how the allocation of members' social and intellectual capital within a virtual team affects team performance in online crowdsourcing contests. Our econometric analysis uses a rank-ordered logistic regression model, and suggests that the effect of a member's social and intellectual capital on team performance varies depending on his or her roles. Though a team leader's social capital and a team expert's intellectual capital significantly influence team performance, a team leader's intellectual capital and a team expert's social capital do not. Further, we found that the alignment of a member's social and intellectual capital within a team has a significant influence on team performance. Moreover, the intensity of the competition moderates the impact. When a contest is highly competitive, the social and intellectual capital alignment negatively affects team performance, and when the competitive intensity is low, this alignment positively affects team performance. Our findings provide insights into improving performance in team-based competitions in crowdsourcing communities.
|keyword = crowdsourcing,crowdsourcing contests,econometrics,intellectual capital,social capital,social network analysis,team competition,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Family Preferences Concerning Online Privacy, Data Mining, and Targeted Ads: Regulatory Implications'''
{{header}}
{{article
|author= Eric K. Clemons,Joshua S. Wilson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Young Internet users engage in risky or inappropriate behavior online that could either be embarrassing or harmful to their future. As importantly, young Internet users engage in online activities that reveal a great deal about the cost to serve them and their willingness to pay for goods and services, which could be used against them by well-informed sellers. Educational applications that collect users' information are becoming ubiquitous in the classroom, presenting the opportunity for students' data to be mined. We are not aware of prior studies that examine parental or students' attitudes and preferences toward data mining of educational application accounts, and how these attitudes differ across several countries. We used three survey instruments to measure parents' and students' attitudes toward data mining of educational applications. Parents in all countries studied prefer far less data mining of students' online activities than seems to be the current practice. Most importantly, aversion to data mining does not seem to be correlated with awareness of current practices of data mining of teens' activities. This study highlights regulatory alternatives and suggests future research and future data requirements for designing appropriate regulatory interventions. The nature of the intervention will be guided by the nature of the causes of inappropriate online behavior and inappropriate selection of educational software. Intervention could range from no regulation needed, through providing greater transparency, to new and detailed legal requirements that software providers must meet.
|keyword = data mining,educational software,online behavior,online teen behavior,privacy,privacy regulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Industry-Level Analysis of Information Technology Return and Risk: What Explains the Variation?'''
{{header}}
{{article
|author= Fei Ren,Sanjeev Dewan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Motivated by the wide dispersion in the returns on the use of information technology (IT) across industries, we conduct an industry-level examination of IT return and risk, focusing on the moderating role of industry competition, regulation, and technological change. We address the following research questions: What is the impact of IT investment on the return and risk dimensions of industry financial performance? How do industry characteristics moderate the relationship between IT investment and industry performance? Our analysis of these questions finds that higher levels of industry competition are associated with higher IT productivity (contribution of IT to value-added output), lower IT profitability (contribution of IT to industry average return on assets [ROA]), and higher IT risk (contribution of IT to ex ante variability of ROA). This is consistent with the notion that competition induces riskier IT investments, despite the fact that returns tend to be competed away. Higher levels of industry regulation are associated with lower IT returns in both productivity and profitability, but also lower IT risk. Finally, a higher rate of technological change induces both higher IT returns and higher IT risk. A variety of tests indicate that our results are robust. Our results shed light on factors that drive variation in IT performance across industries, and provide useful industry-level performance benchmarks of the return and risk impacts of IT investments.
|keyword = competition,industry-level analysis,information technology investment,IT return and risk,IT value,productivity,profitability,regulation,technological change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the Dynamics of Service-Oriented Architecture Implementation'''
{{header}}
{{article
|author= Xitong Li,Stuart E. Madnick,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Despite the potential benefits, many organizations have failed in service-oriented architecture (SOA) implementation projects. Prior research often used a variance perspective and neglected to explore the complex interactions and timing dependencies between the critical success factors. This study adopts a process perspective to capture the dynamics while providing a new explanation for the mixed outcomes of SOA implementation. We develop a system dynamics model and use simulation analysis to demonstrate the phenomenon of "tipping point." That is, under certain conditions, even a small reduction in the duration of normative commitment can dramatically reverse, from success to failure, the outcome of an SOA implementation. The simulation results also suggest that (1) the duration of normative commitment can play a more critical role than the strength, and (2) the minimal duration of normative commitment for a successful SOA implementation is associated positively with the information delay of organizational learning of SOA knowledge. Finally, we discuss the theoretical causes and organizational traps associated with SOA implementation to help IT managers make better decisions about their implementation projects.
|keyword = normative commitment,organizational traps,service-oriented architecture (SOA),system dynamics,tipping point,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contextualized Relationship Between Knowledge Sharing and Performance in Software Development'''
{{header}}
{{article
|author= Muammer Ozer,Doug Vogel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = We study how the knowledge that software developers receive from other software developers in their company impacts their performance. We also study the boundary conditions of this relationship. The results of our empirical study indicate that receiving knowledge from other software developers in the company is positively related to the performance of the knowledge-receiving software developers. Moreover, this relationship was stronger when the software developers had high rather than low task autonomy, when they had high- rather than low-quality social exchanges with their supervisors, and when the software development firms used formal knowledge utilization processes. Theoretically, these results contribute to a better understanding of the processes through which software developers utilize the knowledge that they receive from their peers in the firm. Practically, they show software development firms how emphasizing the task, social, and institutional dimensions of the software development process can help them increase knowledge utilization and performance in software development.
|keyword = knowledge sharing,software developers,software development,software-development performance,systems design and implementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Members' Active Participation in Online Question-and-Answer Communities: A Theory and Empirical Analysis'''
{{header}}
{{article
|author= Lara Khansa,Xiao Ma,Divakaran Liginlal,Sung S. Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Community-based question-and-answer (Q&A) websites have become increasingly popular in recent years as an alternative to general-purpose Web search engines for open-ended complex questions. Despite their unique contextual characteristics, only a handful of Q&A websites have been successful in sustaining members' active participation that, unlike lurking, consists of not only posting questions but also answering others' inquiries. Because the specific design of the information technology artifacts on Q&A websites can influence their level of success, studying leading Q&A communities such as Yahoo! Answers (YA) provides insights into more effective design mechanisms. We tested a goal-oriented action framework using data from 2,920 YA users, and found that active online participation is largely driven by artifacts (e.g., incentives), membership (e.g., levels of membership and tenure), and habit (e.g., past behavior). This study contributes to the information systems literature by showing that active participation can be understood as the setting, pursuit, and automatic activation of goals.
|keyword = active participation,dynamic panel data analysis,goal-oriented action,habit,incentives,online community,online question-and-answer community,system-generated data,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Media and Brand Purchase: Quantifying the Effects of Exposures to Earned and Owned Social Media Activities in a Two-Stage Decision Making Model'''
{{header}}
{{article
|author= Karen Xie,Young-Jin Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = This study investigates the effects of exposures to earned and owned social media activities and their interaction on brand purchase in a two-stage decision model (i.e., likelihood to purchase and the amount purchased offline). Our study is instantiated on a unique single-source dataset of 12-month home-scanned brand purchase records of a group of fast-moving consumer good brands and Facebook brand Fan Page messages related to the brands. We first find that exposures to earned and owned social media activities for brands have significant and positive impacts on consumers' likelihood to purchase the brands. Their effects are, surprisingly, suppressive on each other. Second, exposures to earned and owned social media activities have almost no impact on the amount purchased offline with presence of in-store promotions. Our study contributes to our knowledge body of social media marketing by demonstrating that social media activities for a brand can foster the consumer base of the brand, but that effort is not necessarily sales-oriented. In addition, our study is conducive to guiding marketers onto the strategic allocation of advertising dollars to online social channels featuring a mixture of earned and owned social media.
|keyword = brand community,brand purchase,Facebook Fan Page,multilevel modeling,social media,social media marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Role of Affect in Self-Disclosure on Social Network Websites: A Test of Two Competing Models'''
{{header}}
{{article
|author= Jongtae Yu,Paul Jen-Hwa Hu,Tsang-Hsiang Cheng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = This study examines how affect influences self-disclosure on social network (SN) websites. We test two competing models that build on direct causation theory and affect heuristic theory, respectively. In a direct effect model, affect steers self-disclosure, independent of cognitive cost-benefit appraisals. The indirect effect model instead suggests that affect influences self-disclosure by adjusting perceptions of benefits and costs. The empirical comparison of the models relies on survey data from more than 500 university students. Overall, affect influences self-disclosure indirectly by adjusting the benefits people perceive. In particular, affect toward self-disclosure and toward SN websites relate positively to self-disclosure motivators; their perceived values appear amplified in the presence of positive affect. We also offer a plausible, alternative explanation of the observed positive relationship between privacy risk and self-disclosure according to an indirect effect model, in which self-disclosure is driven mainly by motivators, whereas the effects of inhibitors depend a posteriori on self-disclosure. These findings call for a reconsideration of any exclusive focus on the direct impacts of affect on technology use, as is common in previous research, and suggest the importance of affective factors for understanding social technology uses and managing customer relationships.
|keyword = affect,dual processing approach,online privacy,online self-disclosure,social network sites,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design Theory for Market Surveillance Systems'''
{{header}}
{{article
|author= Xin Li,Sherry X. Sun,Kun Chen,Terrance Fung,Huaiqing Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Market surveillance systems (MSSs) are information systems that monitor financial markets to combat market abuses. Existing MSSs focus mainly on analyzing trading activities and are often developed through a trial-and-error approach by screening data mining algorithms and features. The void of theoretical direction limits the effectiveness of MSSs and calls for the development of a design theory based on a thorough examination of the meta-requirements of MSSs. Based on the efficient market hypothesis and text understanding theory, this paper argues that market information analysis should be incorporated into MSSs and common-sense knowledge should be employed to connect related events to transactions and provide reference concepts for understanding market context and assessing transaction risk. We show the effectiveness of this proposed design theory through developing and evaluating a prototype system in the context of a real-world stock exchange market. By taking a theory-driven approach, this research shows the possibility and provides guidelines on the use of market information analysis to alleviate the market surveillance problem, which has significant implications for financial markets and the economy given the explosive growth of illegal trading activities worldwide.
|keyword = design theory,market surveillance systems,text mining,financial markets,efficient market hypothesis,text understanding theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Estimating the Contextual Risk of Data Breach: An Empirical Approach'''
{{header}}
{{article
|author= Ravi Sen,Sharad Borle,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Data breach incidents are on the rise, and have resulted in severe financial and legal implications for the affected organizations. We apply the opportunity theory of crime, the institutional anomie theory, and institutional theory to identify factors that could increase or decrease the contextual risk of data breach. We investigate the risk of data breach in the context of an organization's physical location, its primary industry, and the type of data breach that it may have suffered in the past. Given the location of an organization, the study finds support for application of the opportunity theory of crime and the institutional anomie theory in estimating the risk of data breach incidents within a state. In the context of the primary industry in which an organization operates, we find support for the institutional theory and the opportunity theory of crime in estimating risk of data breach incidents within an industry. Interestingly though, support for the opportunity theory of crime is partial. We find that investment in information technology (IT) security corresponds to a higher risk of data breach incidents within both a state and an industry, a result contrary to the one predicted by the opportunity theory of crime. A possible explanation for the contradiction is that investments in IT security are not being spent on the right kind of data security controls, a fact supported by evidence from the industry. The work has theoretical and practical implications. Theories from criminology are used to identify the risk factors of data breach incidents and the magnitude of their impact on the risk of data breach. Insights from the study can help IT security practitioners to assess the risk environment of their firm (in terms of data breaches) based on the firm's location, its industry sector, and the kind of breaches that the firm may typically be prone to.
|keyword = computer crime,computer security,data breach,data theft,information security,IT security risks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Firm Learning on Value Creation in Strategic Outsourcing Relationships'''
{{header}}
{{article
|author= Deepa Mani,Anitesh Barua,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Information technology (IT) is central to the process execution and management of an ongoing relationship in outsourcing, both of which are fraught with challenges, and often lead to poor business outcomes. Thus, it is important for IT groups in organizations to understand how to deal with such difficulties for improved outsourcing performance. We study whether firms learn over time to deal with these two related but distinct issues in IT and business process outsourcing. Does such learning affect financial value appropriation through outsourcing? We build on the literature in information systems and strategy to investigate whether value creation in outsourcing depends on relational learning that results from prior association with the vendor, and procedural learning that results from prior experience in managing interfirm relationships. We estimate value in terms of long-term abnormal stock returns to the client relative to an industry, size, and book-to-market matched sample of control firms following the implementation of the outsourcing contract. We also analyze announcement period returns and allied wealth effects. Using data from the hundred largest outsourcing deals between 1996 and 2005, we find that whereas relational learning influences value creation in both simple and complex outsourcing engagements, procedural learning impacts value only in complex initiatives. Financial markets are slow to price the value of learning. The results suggest that caution should be exercised when firms without the experience of managing interfirm relationships externalize complex tasks to vendors they have not worked with in the past. Furthermore, IT groups can help improve learning-based outcomes by developing processes and systems that enable a firm to improve outsourcing procedures in a cumulative manner and also to coordinate and collaborate with the vendor.
|keyword = abnormal returns,business process outsourcing,contracts,financial value,market efficiency,organizational learning,outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Dynamic Capabilities in Responding to Digital Disruption: A Factor-Based Study of the Newspaper Industry'''
{{header}}
{{article
|author= Jahangir Karimi,Zhiping Walter,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Internet and digitization are fundamentally changing and disrupting newspaper companies' traditional operating models. Disruptive innovation theory offers explanations for why companies succeed or fail to respond to disruptive innovations. This study builds on disruptive innovation theory by ascertaining the role of dynamic capabilities in the performance of response to digital disruption. Empirical results suggest that first-order dynamic capabilities that are created by changing, extending, or adapting a firm's existing resources, processes, and values are positively associated with building digital platform capabilities, and that these capabilities impact the performance of response to digital disruption. For information systems (IS) researchers, this study clarifies the role of first-order dynamic capabilities in responding to digital disruption. For IS practice, it helps managers to focus on the most promising factors for creating first-order dynamic capabilities, for building digital platform capabilities, and for reinventing their core functions to accelerate digitization.
|keyword = autonomous growth group,common language,digital disruption,digital platforms,digital transformation,digitization,disruptive innovation,innovative culture,multimedia mindset,organizational capabilities,resources-processes-values (RPV) framework,response performance,staged resource allocation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge Integration in Outsourced Software Development: The Role of Sentry and Guard Processes'''
{{header}}
{{article
|author= Nikhil Mehta,Anandhi Bharadwaj,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = We examine the role of sentry and guard activities in outsourced software development. Sentry activities are designed to regulate the inflow of external information to the project teams and guard activities are designed to manage the outflow of teams' information and resources to external sources. The use of sentry and guard activities has been examined in teams in other contexts such as new product development, but their role and relationship to performance in software development teams is not well understood. We hypothesize and test curvilinear relationships between these activities and knowledge integration in vendor development teams. We also examine how these effects vary under conditions of greater project uncertainty. We tested the hypotheses using data from 139 vendor development teams drawn from sixteen Indian software companies. Results highlight complex curvilinear associations among sentry and guard activities, and knowledge integration, which are further impacted by the level of uncertainty that the project team faces. We recommend that carefully calibrating sentry and guard processes will help vendor development teams enhance project outcomes.
|keyword = guard processes,knowledge integration,project uncertainty,sentry processes,software development,software development teams,software projects,team boundary,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploitation and Exploration Networks in Open Source Software Development: An Artifact-Level Analysis'''
{{header}}
{{article
|author= Orcun Temizkan,Ram L. Kumar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Open source software (OSS) development is an increasingly important paradigm of software development. However, key aspects of OSS such as the determinants of project success and motivations of developers in joining these projects are not well understood. Based on organizational theory, we propose that OSS activities of patch development and feature request can be classified as exploitation (implementation-oriented) and exploration (innovation-oriented) activities, respectively. We empirically examine how the structure of social network affects the success of patch-development and feature-request networks in OSS projects, using a data set collected from the SourceForge database. Our results provide empirical support for the view that patch development and feature request are exploitation and exploration activities, respectively. Network structures differ due to team formation differences and have a differential impact on development success based on the type of activity. The concepts of ambidextrous developers and ambidexterity are explored in the context of OSS projects. Collectively, our results indicate that studying OSS projects at the artifact level could improve our understanding of OSS project success and team formation. This, in turn, could lead to better management of OSS projects.
|keyword = exploitation and exploration,open source software development,project success,social networks,software development,team formation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Temporal Distance, Communication Patterns, and Task Performance in Teams'''
{{header}}
{{article
|author= J. Alberto Espinosa,Ning Nan,Erran Carmel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Drawing on theories on dispersed teamwork, computer-mediated communications, and organizations, we examine the direct associations between temporal distance and team performance as well as the mediating role of team interaction. We tested our research model in a laboratory experiment with four temporal distance conditions. Results show that the direct associations between temporal distance and team performance are substantially diminished when we enter the intervening team communication variables (communication frequency and turn-taking) into the analysis model. We find that communication frequency and turn-taking have differentiated effects on conveyance of information and convergence on its meaning. Conveyance is positively associated with production speed, whereas convergence is positively associated with higher product quality (i.e., accuracy). These findings speak to the theoretical significance of communication patterns and information exchange behaviors in dispersed team research. They also transcend the common wisdom that temporal distance is good for speed and bad for quality.
|keyword = geographically dispersed teams,global teams,team performance,temporal distance,time-zone differences,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Lost in Cyberspace: The Impact of Information Scent and Time Constraints on Stress, Performance, and Attitudes Online'''
{{header}}
{{article
|author= Gregory D. Moody,Dennis F. Galletta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = As competition online increases, website owners investigate ways in which they can attract and retain more users. One avenue is to reduce frustration and stress for the users. Furthermore, many website users are rushed when browsing for specific information on a website. To save time and prevent frustration, website owners should try to maximize information scent, that is, visual, audio, and semantic cues that are meant to lead or guide the user to his or her informational goal. This paper presents and tests a model to predict how information scent can reduce the amount of stress that consumers experience when seeking information under time constraints. The study also demonstrates the relationships between information scent, time constraints, stress, performance, and attitudes toward the website. Results demonstrate that high information scent is an important design goal for a website, and latent semantic analysis can be a useful tool for measuring scent. In addition, rather than an attribute of an overall site, the concept of scent is demonstrated to be dependent on both the website and the task(s) being performed by the user. This finding demonstrates that to maximize users' satisfaction and ability to accomplish their goals, website designers need to determine what tasks users need to accomplish, and to make sure that the links on each page point clearly to the appropriate destination to meet those goals. The latent semantic analysis tool can provide an indication of strength and clarity of the links. Clear links gain even more importance when considering the time constraints of users. Measurable stress explains some of the variance in performance and attitudes.
|keyword = human-computer interaction,information scent,Internet,latent semantic analysis,online anxiety,online stress,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web Personalization Cues and Their Differential Effects on User Assessments of Website Value'''
{{header}}
{{article
|author= Alexander Benlian,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Although various kinds of personalization cues are pervasively used on websites, previous research studies have treated web personalization primarily as a coarse-grained, monolithic block (e.g., by comparing personalization vs. nonpersonalization or personalization vs. privacy) rather than as a combination of salient types of personalization cues that may create-either jointly or separately-different effects on user assessments of website value. Based on the stimulus-organism-response framework, we develop a research model that proposes users' preference fit and perceived enjoyment as two key intervening mechanisms that carry over the differential effects of content and design personalization cues on users' willingness to stick to a website and to pay for website offerings. In a field experiment with 206 subjects using a real-life news aggregator website, our findings provide evidence in support of different effect paths emanating from content and design personalization cues. Furthermore, we show that the effects of content personalization cues on website stickiness and users' willingness to pay (WTP) are mediated by both preference fit and perceived enjoyment, whereas design personalization cues exert their effects on website stickiness only through perceived enjoyment. Counter to intuition, we find that a combination of content and design personalization cues is ineffective-or even counterproductive-in increasing preference fit and users' WTP above and beyond the levels generated by content cues alone. With regard to perceived enjoyment and website stickiness, however, content and design personalization cues exhibit synergistic properties indicating that the combination of both cues are more than the sum of the individual cues alone. Recommendations are provided as to how online managers and web designers can use web personalization cues to positively influence website stickiness and to strengthen their digital business model.
|keyword = field experiment,online news aggregators,perceived enjoyment,personalization cues,preference fit,web personalization,website stickiness,website value,willingness to pay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Price Dispersion Revisited: How Do Transaction Prices Differ from Listing Prices?'''
{{header}}
{{article
|author= Kexin Zhao,Xia Zhao,Jing Deng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Price dispersion of a homogeneous product reflects market efficiency and has significant implications on sellers' pricing strategies. Two different perspectives, the supply and demand perspectives, can be adopted to examine this phenomenon. The former focuses on listing prices posted by sellers, and the latter uses transaction prices that consumers pay to obtain the product. However, no prior research has systematically compared both perspectives, and it is unclear whether different perspectives will generate different insights. Using a unique data set collected from an online market, we find that the dispersion of listing prices is three times higher than the dispersion of transaction prices. More interestingly, the drivers of price dispersion differ significantly between listing and transaction data. The dispersion of listing prices reflects sellers' perception of market environment and their pricing strategies, and it may not fully capture consumer behavior manifested through the variation of transaction prices. Our study indicates that the difference in perspectives taken on the online prices yields different results as to their dispersion.
|keyword = listing prices,online markets,online prices,price dispersion,transaction prices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Push or Pull? A Website's Strategic Choice of Content Delivery Mechanism'''
{{header}}
{{article
|author= Dan Ma,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Really simple syndication (RSS) technology enables an alternative delivery mechanism for online content. Instead of waiting passively for users to pull online content out, websites can push it to potential users through RSS. This is expected to significantly affect user behavior, website profitability, and market equilibrium. This research uses an economic model to study the impact of RSS adoption and examine whether it increases a website's profit and competitive advantage. The findings are intriguing: they demonstrate that RSS can either increase or decrease website profit. In a competitive context, RSS adoption can actually be a disadvantage; in some cases, it hurts the adopter but benefits the competitor. Moreover, under certain conditions, the first mover will be worse off when the competitor mimics its adoption decision, which discourages the earlier adoption and thus creates an obstacle to using RSS. Derivation of the adoption equilibria in sequential and simultaneous games shows that multiple market outcomes may result. Finally, regardless of whether or not a website operator adopts RSS, it will still benefit by increasing user awareness of RSS technology, but only up to a certain level. Once this critical awareness level has been reached, websites will not gain by continuing to promote RSS to users. As a whole, these results show how technology adoption will have an impact on firm performance and market outcome, and illustrate the complexity of technology adoption strategy in a competitive setting.
|keyword = competition,e-commerce,game theory,information economics,online content delivery,RSS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Employees' Exploration of Complex Systems: An Integrative View'''
{{header}}
{{article
|author= Huigang Liang,Zeyu Peng,Yajiong Xue,Xitong Guo,Nengmin Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2015
|abstract = Based on the theory of effective use and adaptive structuration theory, we propose that employees' system exploration behavior can be affected by factors related to three major components: task, system, and organizational environment. Specifically, we examine how task characteristics (job autonomy and task variety), system complexity, and innovation climate jointly affect employees' exploration, which, in turn, leads to extended use of enterprise systems. A field survey of enterprise resource planning (ERP) users yields several interesting findings. First, job autonomy and task variety directly enhance system exploration. Second, system complexity plays a moderating role by strengthening the relationship between job autonomy and exploration and weakening the relationship between task variety and exploration. Third, innovation climate, also acting as a moderator, strengthens both the impact of job autonomy on exploration and the impact of system exploration on extended use. This research contributes to information systems (IS) research by theoretically articulating that system exploration is subject to the simultaneous influences of task, system, and organizational environment factors and empirically testing these factors' main effects and interactions to shed new light on system exploration research. It also contributes to IS practice by suggesting that organizations could enhance employees' system exploration and facilitate the transition from exploration to extended use by increasing job autonomy and task variety, designing personalized training programs to reduce system complexity, and developing organizational climates that foster innovations.
|keyword = autonomy,ERP,innovation climate,IS jobs,system exploration,task complexity,task variety,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Healthcare IT Adoption: An Analysis of Knowledge Transfer in Socioeconomic Networks'''
{{header}}
{{article
|author= Gang Peng,Debabrata Dey,Atanu Lahiri,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Despite the potential of health information technology (HIT) systems to significantly reduce medical errors, streamline clinical processes, contain healthcare costs, and ultimately improve the quality of healthcare, their adoption by hospitals in the United States has been rather slow. To study this adoption process and get insights into the underlying mechanisms, in this work we synthesize the theories on social networks and knowledge transfer. We propose a research framework in which the absorptive capacity of a potential adopter and the collective disseminative capacity of connected adopters act as two key determinants of knowledge transfer in a socioeconomic network, and these two capacities substitute for each other in affecting HIT adoption. We also propose that, in a network setting, the mechanism of knowledge transfer manifests quite differently from that of social contagion in its impact on the diffusion process at different stages of adoption. Using a large longitudinal data set covering adoption decisions of more than five thousand hospitals across a thirteen-year horizon, we find strong support for our hypotheses. Our analysis shows that knowledge flow in provider networks plays a key role in fostering technology diffusion in initial years, allowing the contagion effect to set in sooner for quicker adoption in later years. Therefore, recent efforts at multiple levels to form integrated healthcare delivery networks should accelerate HIT adoption.
|keyword = absorptive capacity,disseminative capacity,healthcare,healthcare information system,healthcare information technology,healthcare information technology adoption,integrated healthcare delivery system,knowledge transfer,social network,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Intermediation in a Sharing Economy: Insurance, Moral Hazard, and Rent Extraction'''
{{header}}
{{article
|author= Thomas A. Weber,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = A key impediment to sharing is a lender's concern about damage to a lent item due to unobservable actions by a renter, usually resulting in moral hazard. This paper shows how an intermediary can eliminate the moral hazard problem by providing optimal insurance to the lender and first-best incentives to the renter to exert care, as long as market participants are risk neutral. The solution is illustrated for the collaborative housing market but applies in principle to any sharing market with vertically differentiated goods. A population of renters, heterogeneous both in their preferences for housing quality and with respect to the amount of care they exert in a rental situation, face a choice between collaborative housing and staying at a local hotel. The private hosts choose their prices strategically, and the intermediary sets commission rates on both sides of the market as well as insurance terms for the rental agreement. The latter are set to eliminate moral hazard. The intermediary is able to extract the gains the hosts would earn if transacting directly. Finally, even if hotels set their prices at the outset so as to maximize collusive profits, collaborative housing persists at substantial market shares, regardless of the difference between the efficiencies of hosts and hotels to reduce renters' cost of effort. The aggregate of hosts, intermediary, and hotels benefits from (a variety in) these effort costs, which indicates that the intermediated sharing of goods is an economically viable, robust phenomenon.
|keyword = collaborative consumption,digital economy,incentive contracting,intermediation,moral hazard,optimal insurance,sharing economy,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Augmenting Conflict Resolution with Informational Response: A Holistic View of Governance Choice in Business Process Outsourcing'''
{{header}}
{{article
|author= Anitesh Barua,Deepa Mani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = We develop a holistic model of governance choice in business process outsourcing (BPO) that represents a highly information-intensive form of outsourcing. We integrate perspectives from neoinstitutional economics and the information-processing view (IPV) of the firm. We argue that the governance structure in BPO is chosen not only to address opportunism concerns arising from relational uncertainty to and encourage cooperation, as suggested by institutional economics, but also as an informational response to task and relational uncertainty to encourage coordination between exchange partners. Using the lens of IPV, we posit that uncertainty in the outsourced task increases the information requirements (IR) of the BPO relationship, which, in turn, leads to more hierarchical governance structures. We also suggest that in addition to directly influencing governance choice, relational uncertainty, a key construct in transaction cost economics (TCE), increases IR, and hence has an indirect impact on governance choice. Furthermore, we hypothesize that technological capabilities enable more hierarchical governance in response to increasing IR needs. Data on 130 BPO initiatives provide empirical support for our hypotheses regarding the drivers of IR, its impact on governance choice, and the moderating role of technological capabilities. Our study contributes to theory by integrating the premises of TCE and IPV in the context of BPO, and to practice by underscoring the need to consider information requirements in designing appropriate coordination and collaboration processes.
|keyword = business process outsourcing,cooperation,coordination,governance,hierarchy,information requirements,uncertainty,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Autonomous Scientifically Controlled Screening Systems for Detecting Information Purposely Concealed by Individuals'''
{{header}}
{{article
|author= Nathan W. Twyman,Paul Benjamin Lowry,Judee K. Burgoon,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Screening individuals for concealed information has traditionally been the purview of professional interrogators investigating crimes. However, the ability to detect when a person is hiding important information would have high value in many other applications if results could be reliably obtained using an automated and rapid interviewing system. Unfortunately, this ideal has thus far been stymied by practical limitations and inadequate scientific control in current interviewing systems. This study proposes a new class of systems, termed autonomous scientifically controlled screening systems (ASCSS), designed to detect individuals' purposely hidden information about target topics of interest. These hidden topics of interest could cover a wide range, including knowledge of concealed weapons, privacy violations, fraudulent organizational behavior, organizational security policy violations, preemployment behavioral intentions, organizational insider threat, leakage of classified information, or even consumer product use information. ASCSS represent a systematic synthesis of structured interviewing, orienting theory, defensive response theory, noninvasive psychophysiological measurement, and behavioral measurement. To evaluate and enhance the design principles, we built a prototype automated screening kiosk system and configured it for a physical security screening scenario in which participants constructed and attempted to smuggle a fake improvised explosive device. The positive results provide support for the proposition that ASCSS may afford more widespread application of credibility assessment screening systems.
|keyword = automated screening kiosk,autonomous scientifically controlled screening system,concealed information test,credibility assessment,deception detection,defensive response,design science,eye-tracking measures,orienting response,physical security,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital Natives or Digital Immigrants? The Impact of User Characteristics on Online Trust'''
{{header}}
{{article
|author= Christian Pieter Hoffmann,Christoph Lutz,Miriam Meckel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Previous research suggests that user characteristics such as web experience and demographics may affect online trust. Drawing on social cognitive theory, we explore the moderating effect of user characteristics on online trust. Based on a survey of German Internet users, we differentiate three groups by age, web experience, and education. We term these groups digital natives, digital immigrants, and naturalized digitals. A multiple-group analysis reveals significant differences in trust formation, particularly in the cues considered in the evaluation of online services. Whereas a large user base inspires confidence in digital natives, naturalized digitals are more geared toward familiar brands and recommendations. Digital immigrants most critically weigh the risks of a transaction against its benefits. We argue that specific user characteristics are associated with distinct cognitive schemata, implying distinct interests and evaluations in online transactions. Online services should differentiate their signaling efforts according to the targeted customer group.
|keyword = e-business,online trust,trust cues,web user characteristics,website signaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Drivers of Quantity and Quality of Participation in Online Policy Deliberation Forums'''
{{header}}
{{article
|author= Chee Wei Phang,Atreyi Kankanhalli,Lihua Huang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Online policy deliberation forums (OPDFs) have been increasingly initiated by governments to allow citizens to provide their input and discuss policy issues. Yet, failure to garner participation, in terms of both quantity and quality, prevents the realization of their benefits. In this regard, prior research has suggested different antecedents for the quantity and quality of participation in online forums, but without systematically considering their differences. To address this research gap, in this study we develop a theoretical model to explain the antecedents of quantity and quality of OPDF participation and test the model using a survey and content analysis of forum logs. The results indicate that quantity of participation is enhanced by the information-technology-enabled resource factor of communality but negatively influenced by collective incentives. In contrast, the antecedents of the quality of participation include both motivational and resource factors. Furthermore, communality accentuates the perceived collective incentives and persuasion benefit of participation. This study contributes to the research by proposing and testing a theoretical model that explains the different antecedents of the quantity and quality of participation in OPDFs. More broadly, the findings inform research and practice on how outcomes of web-enabled cocreation, such as those generated through OPDF participation, can be evaluated and enhanced in these online communities.
|keyword = cocreation,online policy deliberation forum,participation theory,public policy deliberation,quality of participation,quantity of participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Classifying, Measuring, and Predicting Users' Overall Active Behavior on Social Networking Sites'''
{{header}}
{{article
|author= Aihui Chen,Yaobin Lu,Patrick Y. K. Chau,Sumeet Gupta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Although understanding the role of users' overall active behavior on a social networking site (SNS) is of significant importance for both theory and practice, the complexity and difficulty involved in measuring such behavior has inhibited research attention. To understand users' active behaviors on an SNS, it is important that we identify and classify various types of online behaviors before measuring them. In this paper we holistically examine users' active behaviors on an SNS. Toward this end, we conduct three studies. First, we classify active behaviors on an SNS into four categories using the Delphi method. Then, we develop a measurement model and validate it using the data collected from an online survey of 477 SNS users. The measures of the developed instrument exhibit satisfactory reliability and validity and are used as indicators of the latent constructs. This instrument is then used in a predictive model based on commitment theory and tested using data from 1,242 responses. The results of data analysis suggest that affective commitment and continuance commitment are good predictors of overall active behavior on an SNS. This study complements the existing research on social media, cocreation, and social commerce. Most important, this study provides a theoretically sound measurement instrument that addresses the complex characteristic of overall active behavior on an SNS and which should be useful for future research. The findings of this study have important implications for practice as they highlight managing and stimulating users' active behaviors on an SNS.
|keyword = Delphi study,measurement model,social media,social networking site,user commitment,users' active behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Delivery Consolidation and Service Competition Among Internet Service Providers'''
{{header}}
{{article
|author= I. Robert Chiang,Jhih-Hua Jhang-Li,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = The infrastructure of the Internet, by and large, is maintained by Internet service providers (ISPs) that cater to regional customers and by Internet backbone providers (IBPs) that serve large organizations and ISPs. Some IBPs have recently branched into content delivery network (CDN) services; separately, other ISPs have started offering on-demand video streaming to compete with pure-play content providers. These developments have intensified the competition in both content delivery and media-streaming markets. For the content delivery market, we study the competition equilibriums by analyzing factors such as market share, cost structure, service pricing, and subscriber preference. Our approach helps identify conditions under which a content provider should choose an IBP over the incumbent CDN for content distribution. We also show how an IBP's CDN venture affects its interconnection relationship with ISPs. For the streaming service market, we examine conditions under which a content provider would partner with an ISP to lower operating and marketing costs while providing a more streamlined subscriber experience. Analytically, our game-theoretical models can optimize key contracting and pricing strategies for multiple classes of service providers; empirically, insights derived from the proposed models have anticipated events that coincide with several recent developments in content delivery and streaming service markets.
|keyword = content delivery network,Internet service provider,media streaming,peering and transit,service pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the Drivers of Unethical Programming Behavior: The Inappropriate Reuse of Internet-Accessible Code'''
{{header}}
{{article
|author= Manuel Sojer,Oliver Alexy,Sven Kleinknecht,Joachim Henkel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Programming is riddled with ethical issues. Although extant literature explains why individuals in IT would act unethically in many situations, we know surprisingly little about what causes them to do so during the creative act of programming. To address this issue, we look at the reuse of Internet-accessible code: software source code legally available for gratis download from the Internet. Specifically, we scrutinize the reasons why individuals would unethically reuse such code by not checking or purposefully violating its accompanying license obligations, thus risking harm for their employer. By integrating teleological and deontological ethical judgments into a theory of planned behavior model-using elements of expected utility, deterrence, and ethical work climate theory-we construct an original theoretical framework to capture individuals' decision-making process leading to the unethical reuse of Internet-accessible code. We test this framework with a unique survey of 869 professional software developers. Our findings advance the theoretical and practical understanding of ethical behavior in information systems. We show that programmers use consequentialist ethical judgments when carrying out creative tasks and that ethical work climates influence programmers indirectly through their peers' judgment of what is appropriate behavior. For practice, where code reuse promises substantial efficiency and quality gains, our results highlight that firms can prevent unethical code reuse by informing developers of its negative consequences, building a work climate that fosters compliance with laws and professional codes, and making sure that excessive time pressure is avoided.
|keyword = code reuse,ethical behavior,information systems ethics,Internet-accessible code,open source software,partial least squares,programming ethics,theory of planned behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Freemium Strategy in the Mobile App Market: An Empirical Study of Google Play'''
{{header}}
{{article
|author= Charles Zhechao Liu,Yoris A. Au,Hoon Seok Choi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = This paper examines the effect of the freemium strategy on Google Play, an online marketplace for Android mobile applications. By analyzing a large panel data set consisting of 711 ranked mobile apps, we found that the freemium strategy is positively associated with increased sales of the paid mobile apps. Positive trial experience as represented by high review rating of the free version of a mobile app leads to higher sales of its paid version, whereas high visibility of the free version of a mobile app as represented by its product rank does not have a significant impact on the sales of its paid version. This finding suggests that although offering a free trial version is a viable way to improve the visibility of a mobile app, offering a quality free app is more important in boosting sales of the paid app. Moreover, we found that the impact of review rating is reduced when the free version is offered, or when the mobile app is a hedonic app, because consumers have the ability to experience the app themselves before purchase. These findings extend understanding of the freemium business model to include a market characterized by simultaneous intra-market competition for both the freemium and paid products and demonstrate how such dynamics may influence sales of the paid products.
|keyword = Android,freemium,Google Play,mobile apps,online product rank,online reviews,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERNET'S DIRTY SECRET: ASSESSING THE IMPACT OF ONLINE INTERMEDIARIES ON HIV TRANSMISSION'''
{{header}}
{{article
|author= Jason Chan,Anindya Ghose,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online platforms offer access to a larger social group than is generally available through offline contacts, making the Internet an emerging venue for seeking casual sex partners. The ease of seeking sex partners through classified ad sites may promote risky behaviors that increase the transmission of STDs. In this paper, using a natural experiment setup, we investigate whether the entry of a major online personals ad site, Craigslist, increases the prevalence of HIV over a 10 year period from 1999 to 2008 across 33 states in the United States. After controlling for extraneous factors, our results suggest that the entry of Craigslist is related to a 15.9 percent increase in HIV cases. Our analysis suggests that the site entry produces an average of 6,130 to 6,455 cases of HIV infection in the United States each year, mapping out to between $62 million and $65.3 million in annual treatment costs. In addition, the analyses reveal that nonmarket-related casual sex is the primary driver of the increase in HIV cases, in contrast to paid transactions solicited on the site (e.g., escort services and prostitution), which has a negative relationship with HIV trends. These findings are essential to understanding the social routes through which HIV transmission takes place and the extent to which site entry can influence HIV trends. Implications for healthcare practitioners and policy makers are discussed.
|keyword = Classified ad sites,HIV,Internet,online intermediaries,transmission route,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MULTIHOMING USERS' PREFERENCES FOR TWO-SIDED EXCHANGE NETWORKS'''
{{header}}
{{article
|author= Tat Koon Koh,Mark Fichman,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online business-to-business (B2B) exchanges are proliferating, giving firms numerous platforms from which to choose. Many firms are also multihoming, using competing platforms concurrently. In this study, we examine how selling and buying activities on B2B exchanges affect multihoming buyers' preferences for exchanges. We posit that these activities influence buyers' perceived returns and risks of using the exchanges, and impact buyers' preferences. Using a unique dataset of 118 buyers' participation in two B2B exchanges over seven months, we find that buyers prefer exchanges with more selling activities. However, buyers' preferences and buying levels on the exchanges are non-monotonically related. At low buying levels, an increase in buying by others positively affects buyers' preferences. This effect may result from observational learning, where individual buyers learn from other buyers' behaviors. On the other hand, as buying level increases further on the exchange, competition among buyers also increases. Consequently, buyers lower their preferences for the exchange. In addition, we find that the effects of selling and buying activities on buyers' preferences change over time. Our results highlight the need to correctly model buyers' homing behavior; failing to do so could bias the picture of competitive dynamics between platforms and lead to suboptimal strategies by exchanges.
|keyword = Online platforms,B2B exchanges,multihoming,network effects,observational learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ONTOLOGY-BASED EVALUATION OF NATURAL DISASTER MANAGEMENT WEBSITES: A MULTISTAKEHOLDER PERSPECTIVE'''
{{header}}
{{article
|author= Chen-Huei Chou,Fatemeh Mariam Zahedi,Huimin Zhao,
|source= MIS QUARTERLY
|year= 2014
|abstract = In recent years, the world has witnessed a number of severe natural disasters, causing heavy losses to families, communities, and even nations. Natural disaster management (NDM) websites play an important role in assisting people through various disaster stages. However, such websites are complex and there is little research on standards and guidelines for their development and evaluation. In this paper, we develop an ontology-based evaluation tool to assess the utility of NDM websites. Two main groups of stakeholders-experts who are in charge of NDM websites and potential users of such websites-contributed to the process. A total of 73 experts validated the ontology developed for NDM web elements through a Delphi study. These experts also provided importance ratings for web elements in the ontology. In a survey of the second major group of stakeholders-potential users-818 participants provided another set of importance ratings for web elements in the ontology. The design theory in this work is based on utility theory. The metrics for the evaluation of websites are relative utility and absolute utility. Using the evaluation tool, we evaluated the NDM websites of the 50 U.S. states from the perspectives of the two groups of stakeholders. The results indicate a lack of readiness in most of these websites.
|keyword = Ontology,utility theory,natural disaster management,Delphi method,website evaluation,web elements,analytic hierarchy process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ANALYSIS OF PRICING MODELS IN THE ELECTRONIC BOOK MARKET'''
{{header}}
{{article
|author= Lin Hao,Ming Fan,
|source= MIS QUARTERLY
|year= 2014
|abstract = In this paper, we develop a game theoretic model to study the pricing of e-books and e-readers under two pricing models: wholesale and agency. We analyze pricing strategies for a publisher and a retailer. We identify the complementary relationship between e-books and e-readers as the main reason for the retailer to set a low e-book price in the wholesale model. Comparing the wholesale and the agency models, we find, in a wide range of market conditions, the price for e-book readers is lower in the agency model, leading to a higher e-book market share. However, a higher e-book price in the agency model lowers e-book consumption. Overall social welfare is lower in the agency model than in the wholesale model. While total consumer surplus is slightly higher in the agency model, largely because of a lower e-reader price, business profit is lower. The publisher, surprisingly, is worse off under the agency model.
|keyword = Electronic book,e-reader,pricing,agency model,wholesale model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STRATEGIC BEHAVIOR IN ONLINE REPUTATION SYSTEMS: EVIDENCE FROM REVOKING ON EBAY'''
{{header}}
{{article
|author= Shun Ye,Guodong (Gordon) Gao,Siva Viswanathan,
|source= MIS QUARTERLY
|year= 2014
|abstract = This study examines how sellers respond to changes in the design of reputation systems on eBay. Specifically, we focus on one particular strategic behavior on eBay's reputation system: sellers' explicit retaliation against negative feedback provided by buyers to coerce buyers into revoking their negative feedback. We examine how these strategic sellers respond to removal of their ability to retaliate against buyers. We utilize one key policy change of eBay's reputation system, which provides a natural experimental setting that allows us to infer the causal impact of the reputation system on seller behavior. Our results show that coercing buyers to revoke their negative feedback through retaliation enables low-quality sellers to manipulate their reputations and masquerade as high-quality sellers. We find that these sellers reacted strongly to eBay's announcement of a proposed ban on revoking. Interestingly, after the power of these strategic sellers is curtailed, we find evidence that they exert more efforts to improve their reputation scores. This study provides valuable insights about the relationship between reputation system and seller behavior, which have important implications for the design of online reputation mechanisms.
|keyword = Reputation mechanisms,online ratings,quality transparency,online auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION DISCOVERY AND THE LONG TAIL OF MOTION PICTURE CONTENT'''
{{header}}
{{article
|author= Anuj Kumar,Michael D. Smith,Rahul Telang,
|source= MIS QUARTERLY
|year= 2014
|abstract = Recent papers have shown that, in contrast to the long tail theory, movie sales remain concentrated in a small number of hits. These papers have argued that concentrated sales can be explained, in part, by heterogeneity in quality and increasing returns from social effects. Our research analyzes an additional explanation: how incomplete information may skew sales patterns. We use the movie broadcast on pay-cable channels as an exogenous shock to the availability of information, and analyze how this shock changes the resulting sales distribution. Our data show that the pay-cable broadcast shifts the distribution of DVD sales toward long tail movies, suggesting an information spillover from the broadcast. We develop a learning-based movie discovery model to precisely quantify the two mechanisms of movie discovery: word-of-mouth from previous sales and information spillover from broadcast. We use this model to estimate the lost DVD sales due to incomplete information. Our study contributes to the literature by analyzing how information provided in one channel can change the assortment of the same products demanded in another channel.
|keyword = Incomplete information,product discovery,multichannel distribution,movie industry,cannibalization,movie broadcast,DVD sales and rental,long tail,sales distribution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY AND ADMINISTRATIVE EFFICIENCY IN US STATE GOVERNMENTS: A STOCHASTIC FRONTIER APPROACH'''
{{header}}
{{article
|author= Min-Seok Pang,Ali Tafti,M. S. Krishnan,
|source= MIS QUARTERLY
|year= 2014
|abstract = This paper explores value creation from government use of information technologies (IT). While the majority of studies in the information systems (IS) discipline have focused on discovering IT business value in for-profit organizations, the performance effects of IT in the public sector have not been extensively studied in either the IS or the public administration literature. We examine whether IT improves administrative efficiency in U. S. state governments. Utilizing IT budget data in state governments, the census data on state government expenditures, and a variety of information on public services that states provide, we measure technical efficiency with a stochastic frontier analysis and a translog cost function and estimate the effect of IT spending on efficiency. Our analyses provide evidence for a positive relationship between IT spending and cost efficiency and indicate that, on average, a $1 increase in per capita IT budget is associated with $1.13 in efficiency gains. This study contributes to the IS literature by expanding the scope of IT value research to public sector organizations and provides meaningful implications for elected officials and public sector managers.
|keyword = IT value,public sector,US state governments,stochastic frontier analysis,translog cost function,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PEER INFLUENCE IN THE DIFFUSION OF IPHONE 3G OVER A LARGE SOCIAL NETWORK'''
{{header}}
{{article
|author= Miguel Godinho de Matos,Pedro Ferreira,David Krackhardt,
|source= MIS QUARTERLY
|year= 2014
|abstract = In this paper, we study the effect of peer influence in the diffusion of the iPhone 3G across a number of communities sampled from a large dataset provided by a major European Mobile carrier in one country. We identify tight communities of users in which peer influence may play a role and use instrumental variables to control for potential correlation between unobserved subscriber heterogeneity and friends' adoption. We provide evidence that the propensity of a subscriber to adopt increases with the percentage of friends who have already adopted. During a period of 11 months, we estimate that 14 percent of iPhone 3Gs sold by this carrier were due to peer influence. This result is obtained after controlling for social clustering, gender, previous adoption of mobile Internet data plans, ownership of technologically advanced handsets, and heterogeneity in the regions where subscribers move during the day and spend most of their evenings. This result remains qualitatively unchanged when we control for changes over time in the structure of the social network. We provide results from several policy experiments showing that, with this level of effect of peer influence, the carrier would have hardly benefitted from using traditional marketing strategies to seed the iPhone 3G to benefit from viral marketing.
|keyword = Peer influence,homophily,diffusion,community identification,viral marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HARNESSING THE POWER OF SELF-ORGANIZATION IN AN ONLINE COMMUNITY DURING ORGANIZATIONAL CRISIS'''
{{header}}
{{article
|author= Ning Nan,Yong Lu,
|source= MIS QUARTERLY
|year= 2014
|abstract = Organizational crisis management has traditionally favored a centralized plan-and-control approach. This study explores the possibility for an orderly crisis management process to arise unintentionally from decentralized and spontaneous actions in an online community (i.e., self-organization). Based on complex adaptive systems theory, a multilevel model is developed to account for the logical relation between individual-level actions and interactions in an online community and an organizational-level orderly and rational crisis management process, as described by the organizational crisis management literature. We apply this multilevel model to an analysis of 89,596 posts from an online community that was deeply embedded in an earthquake-induced organizational crisis. Results indicate that fluctuation of message content themes in this online community served to energize continuous input from ordinary organization members. These input actualized new possibilities offered by the technology platform for crisis management actions (i.e., actualized IT affordances). Concatenation of immediate impacts of message content themes and actualized IT affordances formed feedback loops that moderated the crisis management activities toward an efficient trajectory. Our findings challenge the traditional assumption that macro-level order requires micro-level order-seeking behaviors. They suggest the viability of self-organization as a new source of organizational order that complements the traditional centralized plan-and-control approach. Theoretical and empirical implications for harnessing the power of ordinary organization members connected by today's technology platforms are discussed.
|keyword = Organizational crisis,self-organization,complex adaptive systems,online community,information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''LOOKING TOWARD THE FUTURE OF IT-BUSINESS STRATEGIC ALIGNMENT THROUGH THE PAST: A META-ANALYSIS'''
{{header}}
{{article
|author= Jennifer E. Gerow,Varun Grover,Jason Thatcher,Philip L. Roth,
|source= MIS QUARTERLY
|year= 2014
|abstract = Research examining the relationship between IT-business strategic alignment (hereafter referred to as alignment) and firm performance (hereafter referred to as performance) has produced apparently conflicting findings (i.e., an alignment paradox). To examine the alignment paradox, we conducted a meta-analysis that probed the interrelationships between alignment, performance, and context constructs. We found the alignment dimensions (intellectual, operational, and cross-domain) demonstrate unique relationships with the different performance types (financial performance, productivity, and customer benefit) and with many of the other constructs in alignment's nomological network. All mean corrected correlations between dimensions of alignment and dependent variables were positive and most of the credibility interval values in these analyses were also positive. Overall, the evidence gathered from the extant literature suggests there is not much of an alignment paradox. This study contributes to the literature by clarifying the relationships between alignment and performance outcomes and offering insight into sources of inconsistencies in alignment research. By doing so, this paper lays a foundation for more consistent treatment of alignment in future IT research.
|keyword = Alignment,business-IT strategic alignment,alignment paradox,IT value,productivity paradox,meta-analysis,review,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SYMBOLIC ACTION RESEARCH IN INFORMATION SYSTEMS: INTRODUCTION TO THE SPECIAL ISSUE'''
{{header}}
{{article
|author= Mark Aakhus,Par J. Agerfalk,Kalle Lyytinen,Dov Te'eni,
|source= MIS QUARTERLY
|year= 2014
|abstract = This special issue introduction explores the need to study information systems as symbolic action systems, defines broadly the research domain and related assumptions, notes the origins of this perspective, articulates its key lines of study, and discusses the state of the field in light of published research. The essay also positions the three papers of the special issue in the broader Information Systems (IS) discourse and notes their specific contribution in bridging so far unconnected streams of research and expanding research methods amenable to symbolic action research. This introductory essay furthermore observes some unique challenges in pulling together the special issue that invited the editors to combat against the tendency to approach communicative processes associated with information systems as primarily psychological processes. In closing we note several lines of inquiry that can strengthen future studies of symbolic action including better design theories, more flexible and open use of methods, and attentive use of rich traditions that inform symbolic action research in IS.
|keyword = Symbolic action,information system,semiotics,speech act,symbol action,communication,collaboration,design,research methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TAKE THEIR WORD FOR IT: THE SYMBOLIC ROLE OF LINGUISTIC STYLE MATCHES IN USER COMMUNITIES'''
{{header}}
{{article
|author= Stephan Ludwig,Ko de Ruyter,Dominik Mahr,Martin Wetzels,Elisabeth Bruggen,
|source= MIS QUARTERLY
|year= 2014
|abstract = User communities are increasingly becoming an essential element of companies' business processes. However, reaping the benefits of such social systems does not always prove effective, often because companies fail to stimulate members' collaboration continuously or neglect their social integration. Following communication accommodation theory, the authors posit that members' communication style alignment symbolically reflects their community identification and affects subsequent participation behavior. This research uses text mining to extract the linguistic style properties of 74,246 members' posts across 37 user communities. Two mixed multilevel Poisson regression models show that when members' linguistic style matches with the conventional community style, it signals their community identification and affects their participation quantity and quality. Drawing on an expanded view of organizational identification, the authors consider dynamics in members' social identification by examining trends and reversals in linguistic style match developments. Whereas a stronger trend of alignment leads to greater participation quantity and quality, frequent reversals suggest lower participation quantity. At a community level, greater synchronicity in the linguistic style across all community members fosters individual members' participation behavior.
|keyword = Linguistic style match (LSM),user communities,text mining,organizational identification,argument development quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BEYOND BEING THERE: THE SYMBOLIC ROLE OF COMMUNICATION AND IDENTIFICATION IN PERCEPTIONS OF PROXIMITY TO GEOGRAPHICALLY DISPERSED COLLEAGUES'''
{{header}}
{{article
|author= Michael Boyer O'Leary,Jeanne M. Wilson,Anca Metiu,
|source= MIS QUARTERLY
|year= 2014
|abstract = Using a mixed-methods approach, we develop the concept of perceived proximity, which is created through communication, shared identity, and the symbolic aspects thereof. Building on previous theoretical work, we create and validate measures of perceived proximity. Then, we compare how perceived proximity and objective distance relate to relationship quality for collocated and geographically dispersed work colleagues. Our results show that perceived proximity (i.e., a cognitive and affective sense of relational closeness) and not physical proximity (i.e., geographic closeness measured in miles or kilometers) affects relationship quality in an international survey of more than 600 people and 1,300 dyadic work relationships. We also find that people's perceptions of proximity mediate the effects of communication and identification on relationship quality. Using qualitative data (2,289 comments from 1,188 respondents coded into 9 themes), we explore the symbolic meaning of perceived proximity. We show how people can form strong bonds despite being separated by large distances and continue to shift the emphasis from information systems as "pipes" or channels to information systems as vehicles for conveying shared meaning and symbolic value. Our findings have important implications for scholars, managers, systems designers, and members of virtual teams, teleworkers, and other geographically dispersed contexts.
|keyword = Proximity,distance,relationships,symbolic action,geographically dispersed work,virtual work,telework,virtual teams,dyads,mixed methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''KNOWLEDGE EXCHANGE AND SYMBOLIC ACTION IN SOCIAL MEDIA-ENABLED ELECTRONIC NETWORKS OF PRACTICE: A MULTILEVEL PERSPECTIVE ON KNOWLEDGE SEEKERS AND CONTRIBUTORS'''
{{header}}
{{article
|author= Roman Beck,Immanuel Pahlke,Christoph Seebach,
|source= MIS QUARTERLY
|year= 2014
|abstract = Organizational knowledge is one of the most important assets of an enterprise. Therefore, many organizations invest in enterprise social media (ESM) to establish electronic networks of practice and to foster knowledge exchange among employees. ESM improves interaction transparency and can be regarded as a sociotechnical system that provides a language for communication and symbolic action as well as a better sense of others' social identity. Accordingly, the individual characteristics of knowledge seekers and contributors determine why and how interactions occur. However, existing studies tend to focus only on knowledge contributors' characteristics and to treat knowledge as an object that needs to be transferred. To address this gap, this study conceptualizes and empirically tests a multilevel model of knowledge exchange in electronic networks of practice (ENoP) that includes the characteristics of knowledge seekers and knowledge contributors as well as their dyadic relationship from an activity-centered language/action point of view. A dataset of 15,505 enterprise microblogging messages reveals that knowledge seekers' characteristics and relational factors drive knowledge exchanges in social media-enabled ENoP. Focusing on organizations with knowledge exchanges supported by information technology, our research extends prior findings by providing the first evidence that the communicative act expressed by question-answer pairs impacts the quality of knowledge exchanged.
|keyword = Knowledge exchange,enterprise social media,language action view,electronic networks of practice,hierarchical linear modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The IQ of the Crowd: Understanding and Improving Information Quality in Structured User-Generated Content'''
{{header}}
{{article
|author= Roman Lukyanenko,Jeffrey Parsons,Yolanda F. Wiersma,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = User-generated content (UGC) is becoming a valuable organizational resource, as it is seen in many cases as a way to make more information available for analysis. To make effective use of UGC, it is necessary to understand information quality (IQ) in this setting. Traditional IQ research focuses on corporate data and views users as data consumers. However, as users with varying levels of expertise contribute information in an open setting, current conceptualizations of IQ break down. In particular, the practice of modeling information requirements in terms of fixed classes, such as an Entity-Relationship diagram or relational database tables, unnecessarily restricts the IQ of user-generated data sets. This paper defines crowd information quality (crowd IQ), empirically examines implications of class-based modeling approaches for crowd IQ, and offers a path for improving crowd IQ using instance-and-attribute based modeling. To evaluate the impact of modeling decisions on IQ, we conducted three experiments. Results demonstrate that information accuracy depends on the classes used to model domains, with participants providing more accurate information when classifying phenomena at a more general level. In addition, we found greater overall accuracy when participants could provide free-form data compared to a condition in which they selected from constrained choices. We further demonstrate that, relative to attribute-based data collection, information loss occurs when class-based models are used. Our findings have significant implications for information quality, information modeling, and UGC research and practice.
|keyword = systems design and implementation,laboratory experiments,information quality,conceptual modeling,crowdsourcing,social media,citizen science,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Feeling Blue? Go Online: An Empirical Study of Social Support Among Patients'''
{{header}}
{{article
|author= Lu Yan,Yong Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = In this paper, we investigate whether social support exchanged in an online healthcare community benefits patients' mental health. We propose a nonhomogeneous Partially Observed Markov Decision Process (POMDP) model to examine the latent health outcomes for online health community members. The transition between different health states is modeled as a probability function that incorporates different forms of social support that patients exchange via discussion board posts. We find that patients benefit from learning from others and that their participation in the online community helps them to improve their health and to better engage in their disease self-management process. Our results also reveal differences in the influence of various forms of social support exchanged on the evolution of patients' health conditions. We find evidence that informational support is the most prevalent type in the online healthcare community. Nevertheless, emotional support plays the most significant role in helping patients move to a healthier state. Overall, the influence of social support is found to vary depending on patients' health conditions. Finally, we demonstrate that our proposed POMDP model can provide accurate predictions for patients' health states and can be used to recover missing or unavailable information on patients' health conditions.
|keyword = healthcare,social networks,social support,partially observed Markov decision process,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of ICT Service Innovation and Complementary Strategies on Brand Equity and Customer Loyalty in a Consumer Technology Market'''
{{header}}
{{article
|author= Xin Xu,James Y. L. Thong,Viswanath Venkatesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper examines the effects of information and communication technology (ICT) service innovation and its complementary strategies on brand equity and customer loyalty toward ICT service providers. We draw from research on brand equity and customer loyalty, ICT innovation management, and strategy complementarity to propose a model that includes new constructs representing ICT service innovation, i.e., service leadership, and its two complementary strategies, i.e., customization-personalization control and technology leadership, and how their interactions influence customer loyalty through customer-based brand equity. We test our model using data from an online survey of 1,210 customers of mobile data services. The results show that service leadership and customization-personalization control have significant direct impacts on ICT service providers' brand equity. Moreover, when either the level of technology leadership or the level of customization-personalization control is high, the impact of service leadership on brand equity is enhanced. In turn, brand equity has significant impacts on consumers' affective loyalty and conative loyalty, but not on cognitive loyalty. Our study contributes to the literature on service management and service science, and in particular to the management of ICT service innovation in a consumer technology market.
|keyword = ICT service innovation,ICT service management,service leadership,technology leadership,customization,personalization,brand equity,customer loyalty,strategy complementarity,mobile data services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal Management of Digital Content on Tiered Infrastructure Platforms'''
{{header}}
{{article
|author= Anna Ye Du,Sanjukta Das,Ram D. Gopal,Ram Ramesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Media firms are increasingly using tiered infrastructure to cost-effectively manage heterogeneous resources by strategically allocating digital content across multiple tiers to avoid overcapacitating high-performance, expensive infrastructure tiers. Complex migrations in tiered environments are currently possible in a seamless, nondisruptive manner. We model digital content as a network capturing the inter-item impacts and use this network structure to develop optimal migration policies that partition media content into tiers. Addressing the context of large content providers such as video-on-demand providers that employ infrastructure platforms for content storage and delivery, we develop a bilevel programming model to maximize the profits of a price-setting platform and a tiered allocation-setting content provider. We model two fundamental effects with digital content: a revenue effect emanating from the tiered architecture and a traffic generating effect among media objects. Using a detailed longitudinal simulation study, we demonstrate the effectiveness of the proposed provisioning policy and pricing strategy and illustrate the existence and impact of these effects in media markets. Finally, we show that repeated execution of the model can help providers respond effectively to a changing environment and thus better manage the risk from demand fluctuations.
|keyword = data migration,information lifecycle,digital content,networks,tiered infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Lateral Coordination Mechanisms and the Moderating Role of Arrangement Characteristics in Information Systems Development Outsourcing'''
{{header}}
{{article
|author= S. Balaji,Carol V. Brown,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Although increased information systems (IS) development outsourcing is the trend, many of these arrangements fail to meet client expectations. We take a coordination perspective and adopt an information processing lens used by prior organization theorists to conceptualize sets of formal structural and informal nonstructural mechanisms, and predict their positive impacts on the strategic IT benefits achieved by the client. Utilizing a strategic alliance lens, we also predict that two characteristics of the client-vendor arrangement will moderate the impacts of both sets of coordination mechanisms. We test our hypotheses using hierarchical regression techniques on field survey data collected from 141 IS managers in client firms, responsible for IS development outsourcing arrangements. We found that the implementation of both structural and informal mechanisms positively impact the client's strategic IT benefits. In arrangements with greater resource provisioning by the vendor, the positive impacts of informal governance mechanisms are strengthened. In arrangements with higher values similarity, the positive impacts of structural governance mechanisms are strengthened, but the positive impacts of informal mechanisms are weakened. A post-hoc analysis of a mediation model reveals that values similarity also has a positive relationship to both structural and informal governance mechanisms. This study therefore provides empirical support for the validity of an information processing lens to theorize lateral mechanism solutions to the coordination challenges of IS development outsourcing. Implications for research and practice are discussed, including the need for future research to better understand how client managers evolve sets of formal and informal mechanisms over the life of an outsourcing arrangement to achieve strategic objectives for their IT organization.
|keyword = IS outsourcing,lateral coordination mechanisms,structural governance,informal governance,outsourcing arrangements,outsourcing performance benefits,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Partial Least Squares and Models with Formatively Specified Endogenous Constructs: A Cautionary Note'''
{{header}}
{{article
|author= Miguel I. Aguirre-Urreta,George M. Marakas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Information systems researchers have recently begun to propose models that include formatively specified constructs, and largely rely on partial least squares (PLS) to estimate the parameters of interest in those models. In this research, we focus on those cases where the formatively specified constructs are endogenous to other constructs in the research model in addition to their own manifest indicators, which are quite common in published research in the discipline, and analyze whether PLS is a valid statistical technique for estimating those models. Although there is evidence that covariance-based approaches can accurately estimate them, this is the first research that examines whether PLS can indeed do so. Through a theoretical analysis based on the inner workings of the PLS algorithm, which is later validated and extended through a series of Monte Carlo simulations, we conclude that is not the case. Specifically, estimates obtained from PLS are capturing something other than the relationship of interest when the formatively specified constructs are endogenous to others in the model. We show how our results apply more generally to a class of models, and discuss implications for future research practice.
|keyword = formative specification,partial least squares,research methods,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Conflating Antecedents and Formative Indicators: A Comment on Aguirre-Urreta and Marakas'''
{{header}}
{{article
|author= Edward E. Rigdon,Jan-Michael Becker,Arun Rai,Christian M. Ringle,Adamantios Diamantopoulos,Elena Karahanna,Detmar W. Straub,Theo K. Dijkstra,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Aguirre-Urreta and Marakas [Aguirre-Urreta MI, Marakas GM (2014) Research note-Partial least squares and models with formatively specified endogenous constructs: A cautionary note. Inform. Systems Res. 25(4): 761-778] aim to evaluate the performance of partial least squares (PLS) path modeling when estimating models with formative endogenous constructs, but their ability to reach valid conclusions is compromised by three major flaws in their research design. First, their population data generation model does not represent "formative measurement" as researchers generally understand that term. Second, their design involves a PLS path model that is misspecified with respect to their population model. Third, although their aim is to estimate a composite-based PLS path model, their design uses simulation data generated via a factor analytic procedure. In consequence of these flaws, Aguirre-Urreta and Marakas' (2014) study does not support valid inference about the behavior of PLS path modeling with respect to endogenous formatively measured constructs.
|keyword = formative indicators,partial least squares,endogenous constructs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Rejoinder to Rigdon et al. (2014)'''
{{header}}
{{article
|author= Miguel I. Aguirre-Urreta,George M. Marakas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We appreciate the interest shown by Rigdon et al. [Rigdon EE, Becker J-M, Rai A, Ringle CM, Diamantopoulos A, Karahanna E, Straub DW, Dijkstra TK (2014) Conflating antecedents and formative indicators: A comment on Aguirre-Urreta and Marakas. Inform. Systems Res. 25(4):780-784.] in our recent work and for the time and effort spent in carefully considering it and offering their comments and concerns. In what follows, and within the limitations of a short rejoinder, we offer our response to their comments, highlighting points of agreement and noting where more research is necessary.
|keyword = formative specification,partial least squares,structural equation model,data generation,endogenous constructs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information, Technology, and the Changing Nature of Work'''
{{header}}
{{article
|author= Chris Forman,John Leslie King,Kalle Lyytinen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = The information systems field started with the expectation that information and technology will significantly shape the nature of work. The topic provides ample scope for significant scholarly inquiry. Work content, process, and organization are now different from what they were in the 1960s and 1970s, which provided a foundation for theories and understanding. Although investigations about the changing nature of work have been made for years, this special section recognizes that the time of reckoning has come again. There is a growing need for deeper understanding of information, technology, and work. The specific contributions of this special section are at the heart of new frontiers of research in information, technology, and work. We observe a continued need to study their relationships, and to separate short-term and long-term effects. We expect continued surprises and conclude that patience is required to achieve increased understanding in this important domain.
|keyword = information technology,work,information,effects,control,productivity,skills,work organization,sociotechnical systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Media, Knowledge Sharing, and Innovation: Toward a Theory of Communication Visibility'''
{{header}}
{{article
|author= Paul M. Leonardi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper offers a theory of communication visibility based on a field study of the implementation of a new enterprise social networking site in a large financial services organization. The emerging theory suggests that once invisible communication occurring between others in the organization becomes visible for third parties, those third parties could improve their metaknowledge (i.e., knowledge of who knows what and who knows whom). Communication visibility, in this case made possible by the enterprise social networking site, leads to enhanced awareness of who knows what and whom through two interrelated mechanisms: message transparency and network translucence. Seeing the contents of other's messages helps third-party observers make inferences about coworkers' knowledge. Tangentially, seeing the structure of coworkers' communication networks helps third-party observers make inferences about those with whom coworkers regularly communicate. The emerging theory further suggests that enhanced metaknowledge can lead to more innovative products and services and less knowledge duplication if employees learn to work in new ways. By learning vicariously rather than through experience, workers can more effectively recombine existing ideas into new ideas and avoid duplicating work. Moreover, they can begin to proactively aggregate information perceived daily rather than engaging in reactive search after confronting a problem. I discuss the important implications of this emerging theory of communication visibility for work in the knowledge economy.
|keyword = social networking,innovation,knowledge sharing,metaknowledge,computer-mediated communication and collaboration,knowledge management,ethnographic research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Patient Data as Medical Facts: Social Media Practices as a Foundation for Medical Knowledge Creation'''
{{header}}
{{article
|author= Jannis Kallinikos,Niccolo Tempini,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper investigates a web-based, medical research network that relies on patient self-reporting to collect and analyze data on the health status of patients, mostly suffering from severe conditions. The network organizes patient participation in ways that break with the strong expert culture of medical research. Patient data entry is largely unsupervised. It relies on a data architecture that encodes medical knowledge and medical categories, yet remains open to capturing details of patient life that have as a rule remained outside the purview of medical research. The network thus casts the pursuit of medical knowledge in a web-based context, marked by the pivotal importance of patient experience captured in the form of patient data. The originality of the network owes much to the innovative amalgamation of networking and computational functionalities built into a potent social media platform. The arrangements the network epitomizes could be seen as a harbinger of new models of organizing medical knowledge creation and medical work in the digital age, and a complement or alternative to established models of medical research.
|keyword = medical practice,medical knowledge,social data,social media,computation,patient participation,networking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Infrastructuring Work: Building a State-Wide Hospital Information Infrastructure in India'''
{{header}}
{{article
|author= Margunn Aanestad,Bob Jolliffe,Arunima Mukherjee,Sundeep Sahay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Information and communication technologies that strengthen knowledge-based governance in low and middle-income countries (LMIC) will affect work processes and organizations on a massive scale. This paper draws attention to demands on public sector organizations in resource-constrained contexts that face different challenges than in high-income societies. This paper from the Indian public healthcare sector reports on design, development, implementation, and scaling of a free and open-source software-based hospital information system for district hospitals. The paper focuses on the implications for work, competencies, and organization, building on and extending the concepts of "automate" and "informate." The paper focuses on the emerging and recursive interplay between information infrastructure and work within the context of organizational realities of a district hospital in an LMIC context, captured by the concepts of "infrastructuring of work" and "work of infrastructuring."
|keyword = information systems and organizational change,inter-organizational information systems,management of IS projects,action research,health,India,developing countries,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Efficacy of R&D Work in Offshore Captive Centers: An Empirical Study of Task Characteristics, Coordination Mechanisms, and Performance'''
{{header}}
{{article
|author= Deepa Mani,Kannan Srikanth,Anandhi Bharadwaj,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Seizing the latest technological advances in distributed work, an increasing number of firms have set up offshore captive centers (CCs) in emerging economies to carry out sophisticated R&D work. We analyze survey data from 132 R&D CCs established by foreign multinational companies in India to understand how firms execute distributed innovative work. Specifically, we examine the performance outcomes of projects using different technology-enabled coordination strategies to manage their interdependencies across multiple locations. We find that modularization of work across locations is largely ineffective when the underlying tasks are less routinized, less analyzable, and less familiar to the CC. Coordination based on information sharing across locations is effective when the CC performs tasks that are less familiar to it. A key contribution of our work is the explication of the task contingencies under which coordination based on modularization versus information sharing yield differential performance outcomes.
|keyword = offshoring,captive centers,R&D,coordination,distributed work,modularization,information sharing,performance,knowledge-intensive work,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Doing Business with Strangers: Reputation in Online Service Marketplaces'''
{{header}}
{{article
|author= Antonio Moreno,Christian Terwiesch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Online service marketplaces allow service buyers to post their project requests and service providers to bid for them. To reduce the transactional risks, marketplaces typically track and publish previous seller performance. By analyzing a detailed transactional data set with more than 1,800,000 bids corresponding to 270,000 projects posted between 2001 and 2010 in a leading online intermediary for software development services, we empirically study the effects of the reputation system on market outcomes. We consider both a structured measure summarized in a numerical reputation score and an unstructured measure based on the verbal praise left by previous buyers, which we encode using text mining techniques. We find that buyers trade off reputation (both structured and unstructured) and price and are willing to accept higher bids posted by more reputable bidders. Sellers also respond to changes in their own reputation through three different channels. They increase their bids with their reputation score (price effect) but primarily use a superior reputation to increase their probability of being selected (volume effect) as opposed to increasing their bid prices. Negative shocks in seller reputation are associated to an increase in the probability of seller exit (exit effect), but this effect is moderated by the investment that the seller has made in the site. We conclude that participants in this market are very responsive to the numerical reputation score and also to the unstructured reputational information, which behaves in a similar way to the structured numerical reputation score but provides complementary information.
|keyword = auctions,online service marketplaces,procurement,reputation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Physical and Electronic Wholesale Markets: An Empirical Analysis of Product Sorting and Market Function'''
{{header}}
{{article
|author= Eric Overby,Sabyasachi Mitra,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Markets can yield significant economic benefits by improving transaction efficiency, but effective design is necessary to achieve these benefits. We compare a physical market to a discrete electronic market in the wholesale used vehicle industry to evaluate how their different designs work for different types of transactions. We find that buyers and sellers balance adverse selection costs and other transaction costs when using the two markets, with the physical market serving as the general exchange and the electronic market serving as a spot market for vehicles with low adverse selection risk. These findings increase our understanding of how sellers and buyers distribute supply and demand between physical and electronic markets in industries in which they coexist. They also increase our understanding of how information technology can improve market function in wholesale environments.
|keyword = adverse selection,automotive sector,electronic markets,market design,online markets,physical markets,quality sorting,transaction costs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Launching Successful E-Markets: A Broker-Level Order-Routing Analysis of Two Options Exchanges'''
{{header}}
{{article
|author= Chris Parker,Bruce W. Weber,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = New e-markets try in a number of ways to attract a critical mass of participation and usage. Two innovative, all-electronic options exchanges, the International Securities Exchange (ISE) and the Boston Options Exchange (BOX), opened for trading in 2000 and 2004. In contrast to rival floor markets, they offer immediate order execution, direct user access, and reduced costs. As a result, ISE and BOX grew trading volumes and won market share from four incumbent exchanges in the United States. We observe significant differences between broker order-routing practices across ISE and BOX, leading to the markets' different growth patterns. We develop and test hypotheses about new market growth using a panel of six years of quarterly disclosures from 24 major brokerage firms. We find that membership affiliations are the dominant force in predicting brokers' order-routing patterns. In contrast to prior research, network externalities, as measured by an exchange's previous quarter market share, are not significant predictors after controlling for temporal heterogeneity. From our results, executives of new electronic exchanges should concentrate on developing broker exchange affiliation and incentive schemes in order to achieve sustainable order levels. Furthermore, keeping a keen eye on the competitive landscape and reacting to changes in current and prospective competitors' affiliation structures may prove the most beneficial way to ensure continued success. Top management must identify the relative advantages of new entrants' affiliation structures and respond accordingly. A new entrant that provides incentives through a novel affiliation structure can be routed significant orders if the incumbent exchange does not react swiftly and effectively. The results are not limited to analyzing electronic exchanges but, we expect, to many situations where competing information technology platforms also benefit from user affiliation and network effects.
|keyword = competitive effects of IS,electronic auctions,electronic financial markets,electronic market assessment,electronic market design,IT effects on industry structure,longitudinal research,market structure,network economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Buy-Now Features in Pay-per-Bid Auctions'''
{{header}}
{{article
|author= Jochen Reiner,Martin Natter,Bernd Skiera,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Pay-per-bid auctions require all bidders to pay for every bid. However, paying bidding fees without receiving the auction item in return often causes high dissatisfaction among losers, resulting in heated discussions and high churn rates. To reduce these negative reactions, pay-per-bid auctioneers created the Buy-Now feature, which allows losers to put all or part of the bidding fees that they paid during an auction toward buying the auction item. Using unique data, including individual customer bidding histories and cost data from more than 6,800 pay-per-bid auctions, we find that, overall, the Buy-Now feature leads to more aggressive bidding behavior, attracts more bidders, increases loyalty, and results in a higher profit per auction. However, for voucher auctions that represent common value auctions, the Buy-Now feature causes a decrease in the number of bidders and the profit per auction, although we find an increase in the average number of bids per bidder. We also show theoretically that a bidder can pursue a risk-free bidding strategy. However, we find empirically that bidders rarely use this strategy.
|keyword = Buy-Now feature,Buy-Now prices,electronic auctions,online marketing,online retailing,pay-per-bid auctions,penny auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Customized Bundling and Consumption Variety of Digital Information Goods'''
{{header}}
{{article
|author= Jese C. Bockstedt,Kim Huat Goh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Customized bundling retail strategies have become increasingly popular online. In customized bundling, consumers decide the bundle's components, and the effects of this change on consumption variety have important implications for information goods retailers. Although reduction in transaction and search costs increases supply-side product variety, customized bundling can introduce new types of friction in the consumption process. We show that customization of information good bundles reduces consumption variety through two effects: design cost effects and compromise effects. We present the results of three behavioral experiments and an empirical study using sales data from a national music retailer. This study contributes to the theoretical understanding of the effects of customized bundling on search costs and demand-side dynamics. The results provide insights for information goods retailers on the effects of design and search costs on consumer purchasing behavior. Implications for the design of retail platforms for customizable information goods are discussed.
|keyword = behavioral economics,bundling,content bundling,customization,e-commerce,information goods,mass customization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of Mobile Apps' Success: Evidence from the App Store Market'''
{{header}}
{{article
|author= Gunwoong Lee,T. S. Raghu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Mobile applications markets with app stores have introduced a new approach to define and sell software applications with access to a large body of heterogeneous consumer population. This research examines key seller-and app-level characteristics that impact success in an app store market. We tracked individual apps and their presence in the top-grossing 300 chart in Apple's App Store and examined how factors at different levels affect the apps' survival in the top 300 chart. We used a generalized hierarchical modeling approach to measure sales performance, and confirmed the results with the use of a hazard model and a count regression model. We find that broadening app offerings across multiple categories is a key determinant that contributes to a higher probability of survival in the top charts. App-level attributes such as free app offers, high initial ranks, investment in less-popular (less-competitive) categories, continuous quality updates, and high-volume and high-user review scores have positive effects on apps' sustainability. In general, each diversification decision across a category results in an approximately 15 percent increase in the presence of an app in the top charts. Survival rates for free apps are up to two times more than that for paid apps. Quality (feature) updates to apps can contribute up to a threefold improvement in survival rate as well. A key implication of the results of this study is that sellers must utilize the natural segmentation in consumer tastes offered by the different categories to improve sales performance.
|keyword = app markets,apps,m-commerce,mobile software sustainability,product portfolio management,survival analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Gifting and Status in Virtual Worlds'''
{{header}}
{{article
|author= Sigi Goode,Greg Shailer,Mark Wilson,Jaroslaw Jankowski,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = While profitable business models elude many virtual worlds, sales of virtual products are a potentially lucrative source of revenue. One new addition to this strategy is virtual gifting, whereby users purchase virtual products to give to other users. The monetary value of such virtual good transactions is economically significant but no prior study has examined this phenomenon in a strictly virtual context. We apply theory from the economics literature to examine gifting behavior in a virtual world in which users' social status is reflected in observable social connections (friendships) and interactions (personal messages). We find strong evidence that gifting is associated with future enhancements of the gift giver's social status, consistent with a social status-seeking motivation, thus confirming a theorized behavior that is difficult to study in the real world. Our study has implications for system proprietors and managers because we show that gift giving increases system use continuance. We identify various antecedents of gift giving, which may assist a manager in identifying users who are most inclined to give gifts and enable the manager to signal the social exchange benefits to users as a way of improving their social connections.
|keyword = gift economy,MMOG,status in virtual worlds,virtual gifts,virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital Piracy, Teens, and the Source of Advice: An Experimental Study'''
{{header}}
{{article
|author= Matthew J. Hashim,Karthik N. Kannan,Sandra Maximiano,Jackie Rees Ulmer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = The objective of our paper is to determine the effect of piracy advice from various sources on the behavior of the music consumer. Specifically, does it matter if the source of advice has a stake in the outcome of the piracy decision? Does it matter if the source of advice has a social tie with the advisee? Accordingly, we conduct a laboratory experiment using teenagers and their parents as subjects, increasing the realism of the context by sampling potential pirates and their parents. Treatments represent various sources of piracy advice (e.g., the teen's parent, a record label, or an external regulator). Subjects make decisions playing our new experimental game-The Piracy Game-extended from the volunteer's dilemma literature. Interestingly, subjects respond negatively to advice from record labels over time, purchasing fewer songs as compared to other sources such as the subject's parent. The existence of a social tie between the adviser and the subject assists in mitigating piracy, especially when a parent is facing potential penalties due to his or her child's behavior. An external regulator, having no social tie or stake in the decision, provides the least credible source of advice, leading to the greatest amount of piracy. Our analyses not only provide managerial insights but also develop theoretical understanding of the role of social ties in the context of advice.
|keyword = advice effectiveness,experimental economics,music consumers,music markets,music piracy,online piracy,volunteer's dilemma,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaborative Demand Forecasting: Toward the Design of an Exception-Based Forecasting Mechanism'''
{{header}}
{{article
|author= Yan Dong,Xiaowen Huang,Kingshuk K. Sinha,Kefeng Xu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Sharing of truthful information involving business intelligence between supply chain partners is a challenge on account of the asymmetric nature of the information, where one party possesses information such as market intelligence that is neither available in the public domain nor verifiable through third parties. While busesinss-to-business (B2B) technology solutions, such as CPFR (collaborative planning, forecasting, and replenishment), facilitate the sharing of historical information (e.g., transaction records), business intelligence (e.g., potential customer demand) is considered private. Central to CPFR is collaborative demand forecasting (CDF) that allows supply chain partners to share private demand information and incorporate the jointly derived demand forecast into production planning and product replenishment decisions. Implementing CDF, however, is a challenge because of the high costs of the laborious collaboration effort (e.g., to resolve forecast differences). Hence, companies are unable to realize the benefits of CDF and, in turn, the full potential of CPFR. Typically, the issues of information truthfulness and collaboration cost are addressed through an exception management mechanism that defines a range of forecast updates within which collaboration is automated without any human intervention in B2B trading partners. In this paper, we develop incentive-based contracts that explicitly consider the truth-telling behavior and exception resolution in decisions related to the threshold values of demand information. Our first contribution to B2B information management is in establishing the strategic value of exception management and resolution mechanisms in B2B relationships, leading to truthful revelation of demand information. Our second contribution is in developing exception-based incentive contracts, especially in light of the advances in today's business practices and technology, to address issues associated with unobservable and asymmetric demand information. Specifically, we propose a resolution contract to coordinate the supply chain that directly incorporates both exceptions and resolution in an incentive mechanism. We show that these alternative contracts are all viable solutions in assuring truthful exchange of demand information but excel individually in specific situations and, thus, provide practitioners with alternative demand collaboration tools when price negotiation is not an option.
|keyword = B2B commerce,collaborative commerce,collaborative demand forecasting,collaborative planning,exception-based incentive mechanism,forecasting and replenishment,information sharing,supply chain management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Employee Responses to Stressful Information Security Requirements: A Coping Perspective'''
{{header}}
{{article
|author= John D'Arcy,Tejaswini Herath,Mindy K. Shoss,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = We use coping theory to explore an underlying relationship between employee stress caused by burdensome, complex, and ambiguous information security requirements (termed "security-related stress" or SRS) and deliberate information security policy (ISP) violations. Results from a survey of 539 employee users suggest that SRS engenders an emotion-focused coping response in the form of moral disengagement from ISP violations, which in turn increases one's susceptibility to this behavior. Our multidimensional view of SRS-comprised of security-related overload, complexity, and uncertainty-offers a new perspective on the workplace environment factors that foster noncompliant user behavior and inspire cognitive rationalizations of such behavior. The study extends technostress research to the information systems security domain and provides a theoretical framework for the influence of SRS on user behavior. For practitioners, the results highlight the incidence of SRS in organizations and suggest potential mechanisms to counter the stressful effects of information security requirements.
|keyword = coping theory,ethical orientation,information security,moral disengagement theory,sanctions,security compliance,security policies,security policy violation,social cognitive theory,technostress,workplace stress,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relational Contracts, Growth Options, and Heterogeneous Beliefs: A Game-Theoretic Perspective on Information Technology Outsourcing'''
{{header}}
{{article
|author= Xiaotong Li,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = More companies have realized that information technology (IT) outsourcing, once viewed as a cost reduction tool, could facilitate and even enable the transformation of their core business processes. The benefits from a potential outsourcing relationship expansion have strategic implications for relational incentive provision. Modeling "information poaching" in IT outsourcing as an incentive problem with contractibility constraints, our analysis shows that this problem could be mitigated in a repeated game where the outsourcing client and the service provider agree on a relational contract. When the two partners share the belief that they can potentially benefit from a future relationship expansion, they are more likely to behave cooperatively during the early stages of their relationship. However, when they disagree about the likelihood of the future relationship expansion, they will have different preferences on a set of otherwise equivalent relational bonus contracts. Specifically, they will adopt a relational contract with large but infrequent bonuses when the client is more optimistic than the service provider about the potential of their relationship. Because these results hold even when the sourcing partners' beliefs are very close to each other, our analysis sheds fresh light on the issue of equilibrium selection in relational contract theory. In the context of IT outsourcing, the results of this study suggest that, because salient forms of relational bonuses are often not adopted, relational incentive provision is likely more pervasive than what we can observe.
|keyword = contractibility,equilibrium selection,growth options,heterogeneous beliefs,IT outsourcing,outsourcing contracts,relational contracts,repeated games,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''KNOW YOURSELF AND KNOW YOUR ENEMY: AN ANALYSIS OF FIRM RECOMMENDATIONS AND CONSUMER REVIEWS IN A COMPETITIVE ENVIRONMENT'''
{{header}}
{{article
|author= Wael Jabr,Zhiqiang (Eric) Zheng,
|source= MIS QUARTERLY
|year= 2014
|abstract = Reviews and product recommendations at online stores enable customers to readily evaluate alternative products prior to purchase. In this context, firms generate recommendations referring customers to a wider variety of products. They also display customer-generated online reviews in order to facilitate evaluation of those recommended products. This study integrates these two IT artifacts to investigate consumer choice vis-a-vis competing products. We use a dataset we collected from Amazon.com consisting of books, sales ranks, recommendations, reviews, and reviewers. We derive the granular impact of reviews, product referrals, and reviewer opinions on product sale dynamics within a competitive market using comprehensive econometric analyses.
|keyword = Online review,eWOM,competition,recommendation system,instrument variable,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIFFERENTIAL EFFECTS OF PRIOR EXPERIENCE ON THE MALWARE RESOLUTION PROCESS'''
{{header}}
{{article
|author= Seung Hyun Kim,Byung Cho Kim,
|source= MIS QUARTERLY
|year= 2014
|abstract = Despite growing interest in the economic and policy aspects of information security, little academic research has used field data to examine the development process of a security countermeasure provider. In this paper, we empirically examine the learning process a security software developer undergoes in resolving a malware problem. Using the data collected from a leading antivirus software company in Asia, we study the differential effects of experience on the malware resolution process. Our findings reveal that general knowledge from cross-family experience has greater impact than specific knowledge from within-family experience on performance in the malware resolution process. We also examine the factors that drive the differential effects of prior experience. Interestingly, our data show that cross-family experience is more effective than within-family experience in malware resolution when malware targets the general public than when a specific victim is targeted. Similar results-for example, the higher (lower) effect of cross-family (within-family) experience-were observed in the presence of information sharing among software vendors or during a disruption caused by a catastrophe. Our study contributes to a better understanding of the specific expertise required for security countermeasure providers to be able to respond under varying conditions to fast-evolving malware.
|keyword = Information security,economics of information systems,learning curve,antivirus software,malware,targeted attack,information sharing,catastrophe,knowledge retention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGRESSION AND VALUE CONCATENATION TO ENABLE PRIVACY-PRESERVING REGRESSION'''
{{header}}
{{article
|author= Xiao-Bai Li,Sumit Sarkar,
|source= MIS QUARTERLY
|year= 2014
|abstract = Regression techniques can be used not only for legitimate data analysis, but also to infer private information about individuals. In this paper, we demonstrate that regression trees, a popular data-analysis and data-mining technique, can be used to effectively reveal individuals' sensitive data. This problem, which we call a regression attack, has not been addressed in the data privacy literature, and existing privacy-preserving techniques are not appropriate in coping with this problem. We propose a new approach to counter regression attacks. To protect against privacy disclosure, our approach introduces a novel measure, called digression, which assesses the sensitive value disclosure risk in the process of building a regression tree model. Specifically, we develop an algorithm that uses the measure for pruning the tree to limit disclosure of sensitive data. We also propose a dynamic value-concatenation method for anonymizing data, which better preserves data utility than a user-defined generalization scheme commonly used in existing approaches. Our approach can be used for anonymizing both numeric and categorical data. An experimental study is conducted using real-world financial, economic, and healthcare data. The results of the experiments demonstrate that the proposed approach is very effective in protecting data privacy while preserving data quality for research and analysis.
|keyword = Privacy,data analytics,data mining,regression,regression trees,anonymization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ATTRACTION-SELECTION-ATTRITION THEORY OF ONLINE COMMUNITY SIZE AND RESILIENCE'''
{{header}}
{{article
|author= Brian S. Butler,Patrick J. Bateman,Peter H. Gray,E. Ilana Diamant,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online discussion communities play an important role in the development of relationships and the transfer of knowledge within and across organizations. Their underlying technologies enhance these processes by providing infrastructures through which group-based communication can occur. Community administrators often make decisions about technologies with the goal of enhancing the user experience, but the impact of such decisions on how a community develops must also be considered. To shed light on this complex and under-researched phenomenon, we offer a model of key latent constructs influenced by technology choices and possible causal paths by which they have dynamic effects on communities. Two important community characteristics that can be impacted are community size (number of members) and community resilience (membership that is willing to remain involved with the community in spite of variability and change in the topics discussed). To model community development, we build on attraction-selection-attrition (ASA) theory, introducing two new concepts: participation costs (how much time and effort are required to engage with content provided in a community) and topic consistency cues (how strongly a community signals that topics that may appear in the future will be consistent with what it has hosted in the past). We use the proposed ASA theory of online communities (OCASA) to develop a simulation model of community size and resilience that affirms some conventional wisdom and also has novel and counterintuitive implications. Analysis of the model leads to testable new propositions about the causal paths by which technology choices affect the emergence of community size and community resilience, and associated implications for community sustainability.
|keyword = Online communities,social media,benefits,costs,emergent systems,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPECTATION CONFIRMATION IN INFORMATION SYSTEMS RESEARCH: A TEST OF SIX COMPETING MODELS'''
{{header}}
{{article
|author= Susan A. Brown,Viswanath Venkatesh,Sandeep Goyal,
|source= MIS QUARTERLY
|year= 2014
|abstract = Expectation confirmation research in general, and in information systems (IS) in particular, has produced conflicting results. In this paper, we discuss six different models of expectation confirmation: assimilation, contrast, generalized negativity, assimilation-contrast, experiences only, and expectations only. Relying on key constructs from the technology acceptance model (TAM), we test each of these six models that suggests different roles for expectations and experiences of the key predictor-here, perceived usefulness-and their impacts on key outcomes-here, behavioral intention, use, and satisfaction. Data were collected in a field study from 1,113 participants at two points in time. Using polynomial modeling and response surface analysis, we provide the analytical representations for each of the six models and empirically test them to demonstrate that the assimilation-contrast is the best existing model in terms of its ability to explain the relationships between expectations and experiences of perceived usefulness and important dependent variables-namely, behavioral intention, use, and satisfaction-in individual-level research on IS implementations.
|keyword = Expectations,disconfirmation,software use,polynomial modeling,response surface analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ESTIMATING RETURNS TO TRAINING IN THE KNOWLEDGE ECONOMY: A FIRM-LEVEL ANALYSIS OF SMALL AND MEDIUM ENTERPRISES'''
{{header}}
{{article
|author= Amit Mehra,Nishtha Langer,Ravi Bapna,Ram Gopal,
|source= MIS QUARTERLY
|year= 2014
|abstract = The ongoing digitization of multiple industries has drastically reduced the half-life of skills and capabilities acquired by knowledge workers through formal education. Thus, firms are forced to make significant ongoing investments in training their employees to remain competitive. Existing research has not examined the role of training in improving firm-level productivity of knowledge firms. This paper provides an innovative econometric framework to estimate returns to such employee training investments made by firms. We use a panel dataset of small-to medium-sized Indian IT services firms and assess how training enhances human capital, a critical input for such firms, thereby improving firm revenues. We use econometric approaches based on optimization of the firm's profit function to eliminate the endogenous choice of inputs common in production function estimations. We find that an increase in training investments is significantly linked to an increase in revenue per employee. Further, marginal returns to training are increasing firm size. Therefore, relatively speaking, large firms benefit more from training. For the median company in our data, we find that a dollar invested in training yields a return of $4.67, and this effect approximately grows 2.5 times for the 75th percentile-sized firm. A variety of robustness checks, including the use of data envelopment analysis, are used to establish the veracity of our results.
|keyword = IT services,nonlinear growth,ROI of training,productivity,human capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CULTURAL DIFFERENCES AND GEOGRAPHY AS DETERMINANTS OF ONLINE PROSOCIAL LENDING'''
{{header}}
{{article
|author= Gordon Burtch,Anindya Ghose,Sunil Wattal,
|source= MIS QUARTERLY
|year= 2014
|abstract = In this paper, we analyze patterns of transaction between individuals using data drawn from Kiva.org, a global online crowdfunding platform that facilitates prosocial, peer-to-peer lending. Our analysis, which employs an aggregate dataset of country-to-country lending volumes based on more than three million individual lending transactions that took place between 2005 and 2010, considers the dual roles of geographic distance and cultural differences on lenders' decisions about which borrowers to support. While cultural differences have seen extensive study in the Information Systems literature as sources of friction in extended interactions, here, we argue and demonstrate their role in individuals' selection of a transaction partner. We present evid ence that lenders do prefer culturally similar and geographically proximate borrowers. An analysis of the marginal effects indicates that an increase of one standard deviation in the cultural differences between lender and borrower countries is associated with 30 fewer lending actions, while an increase of one standard deviation in physical distance is associated with 0.23 fewer lending actions. We also identify a substitution effect between cultural differences and physical distance, such that a 50 percent increase in physical distance is associated with an approximate 30 percent decline in the effect of cultural differences. Considering approaches to overcoming the observed cultural effect, we offer some empirical evidence of the potential of IT-based trust mechanisms, focusing on Kiva's reputation rating system for microfinance intermediaries. We discuss the implications of our findings for prosocial lending, online crowdfunding, and electronic markets more broadly.
|keyword = Prosocial lending,microfinance,cultural differences,geography,crowdfunding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EMERGENCE OF POWER LAWS IN ONLINE COMMUNITIES: THE ROLE OF SOCIAL MECHANISMS AND PREFERENTIAL ATTACHMENT'''
{{header}}
{{article
|author= Steven L. Johnson,Samer Faraj,Srinivas Kudaravalli,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online communities bring together individuals with shared interest in joint action or sustained interaction. Power law distributions of user popularity appear ubiquitous in online communities but their formation mechanisms are not well understood. This study tests for the emergence of power law distributions via the mechanisms of preferential attachment, least efforts, direct reciprocity, and indirect reciprocity. Preferential attachment, where new entrants favor connections with already popular participants, is the predominant explanation suggested by prior literature. Yet, the attribution of preferential attachment or any other mechanism as a single unitary reason for the emergence of power law distributions runs contrary to the social nature of online communities and does not account for diversity of participants' motivation. Agent-based modeling is used to test if a single social mechanism alone or multiple mechanisms together can generate power law distributions observed in online communities. Data from 28 online communities is used to calibrate, validate, and analyze the simulation. Simulated communication networks are randomly generated according to parameters for each hypothesis. The fit of the power law distribution in the model testing subset is then compared against the fit for these simulated networks. The major finding is that, in contrast to research in more general network settings, neither preferential attachment nor any other single mechanism alone generates a power law distribution. Instead, a blended model of preferential attachment with other social network formation mechanisms was most consistent with power law distributions seen in online communities. This suggests the need to move away from stylized explanations of network emergence that rely on single theories toward more highly socialized and multitheoretic explanations of community development.
|keyword = Online communities,scale-free,power law distribution,preferential attachment,social exchange,reciprocity,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DYNAMIC RECONFIGURATION IN PLANETARY EXPLORATION: A SOCIOMATERIAL ETHNOGRAPHY'''
{{header}}
{{article
|author= Melissa Mazmanian,Marisa Cohn,Paul Dourish,
|source= MIS QUARTERLY
|year= 2014
|abstract = In taking into account the ways in which material and social realms are constitutively entangled within organizations, it is rhetorically tempting to say that technologies and social structures reconfigure each other. But what does it mean to reconfigure? How does one "figure" the other and how do we fully embrace a mutually constitutive relationship when examining fluid relations? This paper delves into these questions by exploring how physical, social, material, technological, and organizational arrangements dynamically reconfigure each other in the duration of organizational practice. Using the venue of space exploration, we present three empirical examples from an ethnographic engagement with a NASA mission orbiting an outer planet in the solar system to examine various configurations and sociomaterial relations. In this endeavor, we suggest that theoretical and empirical traction can be gained by focusing attention on the dynamic reconfigurations between social and material realms. In so doing, we call attention to the ways in which current sociomaterial perspectives have difficulty articulating the shifting, figural, asymmetric and dynamic negotiations between people, social structures, information technologies, and representational objects. This paper contributes to current discussions of sociomaterial relations in information systems research by presenting an empirical treatment of entangled and shifting reconfigurations and providing language for engaging with this perspective.
|keyword = Sociomateriality,dynamic reconfiguration,organizational processes,software studies,representational practices,outer planetary exploration,qualitative empirical analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD GENERALIZABLE SOCIOMATERIAL INQUIRY: A COMPUTATIONAL APPROACH FOR ZOOMING IN AND OUT OF SOCIOMATERIAL ROUTINES'''
{{header}}
{{article
|author= James Gaskin,Nicholas Berente,Kalle Lyytinen,Youngjin Yoo,
|source= MIS QUARTERLY
|year= 2014
|abstract = In this paper, a computational, mixed methods approach that combines qualitative analysis with a novel approach to sequence analysis for studying the entanglement of human activities and digital capabilities in organizational routines is described. The approach is scalable across multiple contexts and complements the dominant idiographic modes of sociomaterial inquiry. The approach is rooted in the epistemology of a "rational reconstruction" consistent with the interpretive stance underlying the sociomaterial position. It arms researchers with the means to seek and uncover regularities in the ways human activities and digital capabilities become entangled across contexts by enabling the identification and articulation of generalizable patterns of sociomaterial activity. The computational approach is founded on sequence-analytic techniques that originated from the field of computational biology (genetics), but are now gaining popularity in the study of temporally ordered social phenomena such as organizational routines. These techniques are extended by drawing upon theoretical insights gained within sociomaterial scholarship on how the digital and the social become entangled. By detecting the variation in activities, actors, artifacts, and affordances that comprise what we denote a sociomaterial routine, the approach directly attends to ways in which human actors and the material features of technology become entangled in patterns of practice. Beyond motivating and describing the approach, the different insights that researchers can generate through its application in the study of the digitalization of organizational routines are illustrated. We conclude by suggesting several lines of inquiry that can enrich sociomaterial research.
|keyword = Sociomaterial,sociotechnical,rational reconstruction,computational social science,methodology,sequence analysis,routines,mixed methods,organizational routines,generative grammar,lexicon,lexical notation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ENTANGLEMENTS IN PRACTICE: PERFORMING ANONYMITY THROUGH SOCIAL MEDIA'''
{{header}}
{{article
|author= Susan V. Scott,Wanda J. Orlikowski,
|source= MIS QUARTERLY
|year= 2014
|abstract = Information systems researchers have shown an increasing interest in the notion of sociomateriality. In this paper, we continue this exploration by focusing specifically on entanglement: the inseparability of meaning and matter. Our particular approach is differentiated by its grounding in a relational and performative ontology, and its use of agential realism. We explore some of the key ideas of entanglement through a comparison of two phenomena in the travel sector: an institutionalized accreditation scheme offered by the AA and an online social media website hosted by TripAdvisor. Our analysis centers on the production of anonymity in these two practices of hotel evaluation. By examining how anonymity is constituted through an entanglement of matter and meaning, we challenge the predominantly social treatments of anonymity to date and draw attention to the uncertainties and outcomes generated by specific performances of anonymity in practice. In closing, we consider what the particular agential realist concept of entanglement entails for understanding anonymity, and discuss its implications for research practice.
|keyword = Anonymity,entanglement,agential realism,social media,materiality,sociomateriality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MATTER OF LIFE AND DEATH: EXPLORING CONCEPTUALIZATIONS OF SOCIOMATERIALITY IN THE CONTEXT OF CRITICAL CARE'''
{{header}}
{{article
|author= Matthew Jones,
|source= MIS QUARTERLY
|year= 2014
|abstract = Sociomateriality has been attracting growing attention in the Organization Studies and Information Systems literatures since 2007, with more than 140 journal articles now referring to the concept. Over 80 percent of these articles have been published since January 2011 and almost all cite the work of Orlikowski (2007, 2010; Orlikowski and Scott 2008) as the source of the concept. Only a few, however, address all of the notions that Orlikowski suggests are entailed in sociomateriality, namely materiality, inseparability, relationality, performativity, and practices, with many employing the concept quite selectively. The contribution of sociomateriality to these literatures is, therefore, still unclear. Drawing on evidence from an ongoing study of the adoption of a computer-based clinical information system in a hospital critical care unit, this paper explores whether the notions, individually and collectively, offer a distinctive and coherent account of the relationship between the social and the material that may be useful in Information Systems research. It is argued that if sociomateriality is to be more than simply a label for research employing a number of loosely related existing theoretical approaches, then studies employing the concept need to pay greater attention to the notions entailed in it and to differences in their interpretation.
|keyword = Sociomateriality,practice,empirical,adoption,interpretive,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A TRICHORDAL TEMPORAL APPROACH TO DIGITAL COORDINATION: THE SOCIOMATERIAL MANGLING OF THE CERN GRID'''
{{header}}
{{article
|author= Will Venters,Eivor Oborn,Michael Barrett,
|source= MIS QUARTERLY
|year= 2014
|abstract = This paper develops a sociomaterial perspective on digital coordination. It extends Pickering's mangle of practice by using a trichordal approach to temporal emergence. We provide new understanding as to how the nonhuman and human agencies involved in coordination are embedded in the past, present, and future. We draw on an in-depth field study conducted between 2006 and 2010 of the development, introduction, and use of a computing grid infrastructure by the CERN particle physics community. Three coordination tensions are identified at different temporal dimensions, namelyobtaining adequate transparency in the present, modeling a future infrastructure, and the historical disciplining of social and material inertias. We propose and develop the concept of digital coordination, and contribute a trichordal temporal approach to understanding the development and use of digital infrastructure as being orientated to the past and future while emerging in the present.
|keyword = Grid computing,coordination,development,case study,mangle of practice,temporality,digital infrastructure,transparency,sustainable change,performativity,sociomaterial,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Income Accounting as the Theoretical Basis for Measuring IT Productivity'''
{{header}}
{{article
|author= Dennis O. Kundisch,Neeraj Mittal,Barrie R. Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We use the under-recognized income accounting identity to provide an important theoretical basis for using the Cobb-Douglas production function in IT productivity analyses. Within the income accounting identity we partition capital into non-IT and IT capital and analytically derive an accounting identity (AI)-based Cobb-Douglas form that both nests the three-input Cobb-Douglas and provides additional terms based on wage rates and rates of return to non-IT and IT capital. To empirically confirm the theoretical derivation, we use a specially constructed data set from a subset of the U. S. manufacturing industry that involve elaborate calculations of rates of return-a data set that is infeasible to obtain for most productivity studies-to estimate the standard Cobb-Douglas and our AI-based form. We find that estimates from our AI-based form correspond with those of the Cobb-Douglas, and our AI-based form has significantly greater explanatory power. In addition, empirical estimation of both forms is relatively robust to the assumption of intertemporally stable input shares required to derive the AI-based form, although there may be limits. Thus, in the context of future research the Cobb-Douglas form and its application in IT productivity work have a theoretically and empirically supported basis in the accounting identity. A poor fit to data or unexpected coefficient estimates suggests problems with data quality or intertemporally unstable input shares. Our work also shows how some returns to IT that do not show up in output elasticities can be found in total factor productivity (TFP)-the novel ways inputs are combined to produce output. The critical insight for future research is that many unobservables that have been considered part of TFP can be manifested in rates of return to IT capital, non-IT capital, and labor-rates of return that are separated from TFP in our AI-based form. Finally, finding that the additional rates of return terms partially explain TFP confirms the need for future IT productivity researchers to incorporate time-varying TFP in their models.
|keyword = information systems,IT policy and management,economics of IS,income accounting identity,IT productivity,production function,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Distinction and Status Production on User-Generated Content Platforms: Using Bourdieu's Theory of Cultural Production to Understand Social Dynamics in Online Fields'''
{{header}}
{{article
|author= Natalia Levina,Manuel Arriaga,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = In this paper, we propose an analytical lens for studying social status production processes across a wide variety of user-generated content (UGC) platforms. Various streams of research, including those focused on social network analysis in social media, online communities, reputation systems, blogs, and multiplayer games, have discussed social status production online in ways that are diverse and incompatible. Drawing on Bourdieu's theory of fields of cultural production, we introduce the notion of an online field and associated sociological concepts to help explain how diverse types of producers and consumers of content jointly generate unique power relations online. We elaborate on what role external resources and status markers may play in shaping social dynamics in online fields. Using this unifying theory we are able to integrate previous research findings and propose an explanation of social processes behind both the similarity across UGC platforms, which all offer multiple ways of pursuing distinction through content production, as well as the differences across such platforms in terms of which distinctions matter. We elaborate what role platform design choices play in shaping which forms of distinction count and how they are pursued as well as implications these have for status gaining strategies. We conclude the paper by suggesting how our theory can be used in future qualitative and quantitative research studies.
|keyword = electronic commerce,social media,user-generated content,status,power,Bourdieu practice theory,network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cloud Implications on Software Network Structure and Security Risks'''
{{header}}
{{article
|author= Terrence August,Marius Florin Niculescu,Hyoduk Shin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = By software vendors offering, via the cloud, software-as-a-service (SaaS) versions of traditionally on-premises application software, security risks associated with usage become more diversified. This can greatly increase the value associated with the software. In an environment where negative security externalities are present and users make complex consumption and patching decisions, we construct a model that clarifies whether and how SaaS versions should be offered by vendors. We find that the existence of version-specific security externalities is sufficient to warrant a versioned outcome, which has been shown to be suboptimal in the absence of security risks. In high security-loss environments, we find that SaaS should be geared to the middle tier of the consumer market if patching costs and the quality of the SaaS offering are high, and geared to the lower tier otherwise. In the former case, when security risk associated with each version is endogenously determined by consumption choices, strategic interactions between the vendor and consumers may cause a higher tier consumer segment to prefer a lower inherent quality product. Relative to on-premises benchmarks, we find that software diversification leads to lower average security losses for users when patching costs are high. However, when patching costs are low, surprisingly, average security losses can increase as a result of SaaS offerings and lead to lower consumer surplus. We also investigate the vendor's security investment decision and establish that, as the market becomes riskier, the vendor tends to increase investments in an on-premises version and decrease investments in a SaaS version. On the other hand, in low security-loss environments, we find that SaaS is optimally targeted to a lower tier of the consumer market, average security losses decrease, and consumer surplus increases as a result. Security investments increase for both software versions as risk increases in these environments.
|keyword = cloud computing,software-as-a-service,network economics,security,versioning,on-premises software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Gambling Behavior: The Impacts of Cumulative Outcomes, Recent Outcomes, and Prior Use'''
{{header}}
{{article
|author= Xiao Ma,Seung Hyun Kim,Sung S. Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = The objective of this work is to examine various psychological forces underlying the behavior of people's online gambling, an increasingly popular form of entertainment in the gaming industry. Drawing on extant theories, we first developed a model of how cumulative outcomes, recent outcomes, and prior use affect online gambling behavior differently. We empirically tested the model using longitudinal panel data collected over eight months from 22,304 actual users of a gambling website. The results of a multilevel panel data analysis strongly supported our hypotheses. First, consistent with gambling theory, individuals' online gambling was found to increase with any increase in a cumulative net gain or cumulative net loss. Second, as the availability heuristic prescribes, a recent loss reduced online gambling, whereas a recent gain increased it. Third, consistent with the literature on repeated behavior, regular use and extended use moderated the relationship between current and subsequent gambling. Taken together, the present study clarifies how people react differently to immediate and cumulative outcomes and also how regular use and extended use facilitate routine behavior in the context of online gambling. In general, our findings suggest that the three perspectives, i.e., gambling theory, the availability heuristic, and repeated behavior, should be taken into account to understand online gambling, which is in essence a series of risk-taking attempts with the potential of eventually becoming routine behavior. This study is expected to offer valuable insights into other types of online games that could engage people in risking real or cyber money and, at the same time, could be easily enmeshed with everyday life (e. g., fantasy sports, online virtual worlds).
|keyword = online user behavior,online gambling,repeated behavior,decision making under uncertainty,panel data,multilevel analysis,hierarchical analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Central Role of Engagement in Online Communities'''
{{header}}
{{article
|author= Soumya Ray,Sung S. Kim,James G. Morris,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Online communities are new social structures dependent on modern information technology, and they face equally modern challenges. Although satisfied members regularly consume content, it is considerably harder to coax them to contribute new content and help recruit others because they face unprecedented social comparison and criticism. We propose that engagement-a concept only abstractly alluded to in information systems research-is the key to active participation in these unique sociotechnical environments. We constructed and tested a framework that demonstrates what engagement is, where it comes from, and how it powerfully explains both knowledge contribution and word of mouth. Our results show that members primarily contribute to and revisit an online community from a sense of engagement. Nonetheless, word of mouth is partly influenced by prior satisfaction. Therefore, engagement and satisfaction appear to be parallel mediating forces at work in online communities. Both mediators arise from a sense of communal identity and knowledge self-efficacy, but engagement also emerges from validation of self-identity. Nevertheless, we also found signs that the contributions of the most knowledgeable users are not purely from engagement, but also from a competing sense of self-efficacy. Our findings significantly contribute to the area of information systems by highlighting that engagement is a concrete phenomenon on its own, and it can be directly modeled and must be carefully managed.
|keyword = online communities,engagement,self-identity verification,knowledge self-efficacy,community identification,knowledge contribution,word of mouth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Latent Growth Modeling for Information Systems: Theoretical Extensions and Practical Applications'''
{{header}}
{{article
|author= Zhiqiang (Eric) Zheng,Paul A. Pavlou,Bin Gu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper presents and extends Latent Growth Modeling (LGM) as a complementary method for analyzing longitudinal data, modeling the process of change over time, testing time-centric hypotheses, and building longitudinal theories. We first describe the basic tenets of LGM and offer guidelines for applying LGM to Information Systems (IS) research, specifically how to pose research questions that focus on change over time and how to implement LGM models to test time-centric hypotheses. Second and more important, we theoretically extend LGM by proposing a model validation criterion, namely "d-separation," to evaluate why and when LGM works and test its fundamental properties and assumptions. Our d-separation criterion does not rely on any distributional assumptions of the data; it is grounded in the fundamental assumption of the theory of conditional independence. Third, we conduct extensive simulations to examine a multitude of factors that affect LGM performance. Finally, as a practical application, we apply LGM to model the relationship between word-of-mouth communication (online product reviews) and book sales over time with longitudinal 26-week data from Amazon. The paper concludes by discussing the implications of LGM for helping IS researchers develop and test longitudinal theories.
|keyword = latent growth modeling,LGM,longitudinal data,d-separation,word of mouth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The "Most Popular News" Recommender: Count Amplification and Manipulation Resistance'''
{{header}}
{{article
|author= Shankar Prawesh,Balaji Padmanabhan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = A broad motivation for our research is to build manipulation resistant news recommender systems. There are several algorithms that can be used to generate news recommendations, and the strategies for manipulation resistance are likely specific to the algorithm (or class of algorithm) used. In this paper, we will focus on a common method used on the front page by many media sites of recommending the N most popular articles (e. g., New York Times, BBC, CNN, Wall Street Journal all prominently use this). We show that whereas recommendation of the N most read articles is easily susceptible to manipulation, a probabilistic variant is more robust to common manipulation strategies. Furthermore, for the "N most popular" recommender, probabilistic selection has other desirable properties. Specifically, the (N+1)th article, which may have just missed making the cut-off, is unduly penalized under common user models. Small differences are easily amplified initially, an observation that can be used by manipulators. Probabilistic selection, on the other hand, creates no such artificial penalty. We use classical results from urn models to derive theoretical results for special cases and study specific properties of the probabilistic recommender.
|keyword = recommender systems,news recommendation,polya urn,zipf,power law,sampling,manipulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Analysis of the Impact of Pre-Release Movie Piracy on Box Office Revenue'''
{{header}}
{{article
|author= Liye Ma,Alan L. Montgomery,Param Vir Singh,Michael D. Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Digital distribution channels raise many new challenges for managers in the media industry. This is particularly true for movie studios where high-value content can be stolen and released through illegitimate digital channels, even prior to the release of the movie in legal channels. In response to this potential threat, movie studios have spent millions of dollars to protect their content from unauthorized distribution throughout the lifecycle of films. They have focused their efforts on the pre-release period under the assumption that pre-release piracy could be particularly harmful for a movie's success. However, surprisingly, there has been little rigorous research to analyze whether, and how much, pre-release movie piracy diminishes legitimate sales. In this paper, we analyze this question using data collected from a unique Internet file-sharing site. We find that, on average, pre-release piracy causes a 19.1% decrease in revenue compared to piracy that occurs post-release. Our study contributes to the growing literature on piracy and digital media consumption by presenting evidence of the impact of Internet-based movie piracy on sales and by analyzing pre-release piracy, a setting that is distinct from much of the existing literature.
|keyword = movies,box office revenue,piracy,forecasting,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Allure of Homophily in Social Media: Evidence from Investor Responses on Virtual Communities'''
{{header}}
{{article
|author= Bin Gu,Prabhudev Konana,Rajagopal Raghunathan,Hsuanwei Michelle Chen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Millions of people participate in online social media to exchange and share information. Presumably, such information exchange could improve decision making and provide instrumental benefits to the participants. However, to benefit from the information access provided by online social media, the participant will have to overcome the allure of homophily-which refers to the propensity to seek interactions with others of similar status (e. g., religion, education, income, occupation) or values (e. g., attitudes, beliefs, and aspirations). This research assesses the extent to which social media participants exhibit homophily (versus heterophily) in a unique context-virtual investment communities (VICs). We study the propensity of investors in seeking interactions with others with similar sentiments in VICs and identify theoretically important and meaningful conditions under which homophily is attenuated. To address this question, we used a discrete choice model to analyze 682,781 messages on Yahoo! Finance message boards for 29 Dow Jones stocks and assess how investors select a particular thread to respond. Our results revealed that, despite the benefits from heterophily, investors are not immune to the allure of homophily in interactions in VICs. The tendency to exhibit homophily is attenuated by an investor's experience in VICs, the amount of information in the thread, but amplified by stock volatility. The paper discusses important implications for practice.
|keyword = virtual communities,social media,homophily,heterophily,financial markets,psychological biases,psychological benefits,instrumental benefits,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Industry-Specific Human Capital and Wages: Evidence from the Business Process Outsourcing Industry'''
{{header}}
{{article
|author= Keongtae Kim,Sunil Mithas,Jonathan Whitaker,Prasanto K. Roy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Human capital is becoming more critical as the global economy becomes more information intensive and service intensive. Although information systems (IS) researchers have studied some dimensions of human capital, the role of industry-specific human capital has remained understudied. The information technology enabled business process outsourcing (BPO) industry provides an ideal setting to study returns to human capital, because jobs in this industry are standardized and many professionals in this new industry have come from other industries. We build on IS and economics literature to theorize returns to human capital in the BPO industry, and we test the theory using data for over 2,500 BPO professionals engaged in call center work and other nonvoice services (e. g., accounting, finance, human resources, etc.) in India during the 2006-2008 time period. We find higher returns to industry-specific human capital than to firm-specific and general human capital. We also find that junior-level professionals, whose jobs are relatively more standardized, have higher returns to industry-specific human capital than senior-level professionals. We discuss implications for further research and practice in the global economy where inter-industry transfers and migration of skills are becoming increasingly common.
|keyword = global disaggregation,globalization,services,industry,human capital,BPO,outsourcing,professionals,wages,compensation,industry-specific human capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Heuristic Theorizing: Proactively Generating Design Theories'''
{{header}}
{{article
|author= Robert Wayne Gregory,Jan Muntermann,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Design theories provide explicit prescriptions, such as principles of form and function, for constructing an artifact that is designed to meet a set of defined requirements and solve a problem. Design theory generation is increasing in importance because of the increasing number and diversity of problems that require the participation and proactive involvement of academic researchers to build and test artifact-based solutions. However, we have little understanding of how design theories are generated. Drawing on key contributions by Herbert A. Simon, including the ideas of satisfice and bounded rationality and reviewing a large body of information systems and problem-solving literature, we develop a normative framework for proactive design theorizing based on the notion of heuristic theorizing. Heuristics are rules of thumb that provide a plausible aid in structuring the problem at hand or in searching for a satisficing artifact design. An example of a problem-structuring heuristic is problem decomposition and an example of an artifact design heuristic is analogical design. We define heuristic theorizing as the process of proactively generating design theory for prescriptive purposes from problem-solving experiences and prior theory by constantly iterating between the search for a satisficing problem solution, i.e., heuristic search, and the synthesis of new information that is generated during heuristic search, i.e., heuristic synthesis. Heuristic search involves alternating between structuring the problem at hand and generating new artifact design components, whereas heuristic synthesis involves different ways of thinking, including reflection and learning and forms of reasoning, that complement the use of heuristics for theorizing purposes. We illustrate the effectiveness of our heuristic theorizing framework through a detailed example of a multiyear design science research program in which we proactively generated a design theory for solving problems in the area of intelligent information management and so-called big data in the finance domain. We propose that heuristic theorizing is a useful alternative to established theorizing approaches, i.e., reasoning-based approaches. Heuristic theorizing is particularly relevant for proactive design theorizing, which emphasizes problem solving as being highly intertwined with theorizing, involves a greater variety of ways of thinking than other theorizing approaches, and assumes an engaged relationship between academics and practitioners.
|keyword = generating design theories,design science,proactive design theorizing,problem solving,heuristics,heuristic theorizing,heuristic search,heuristic synthesis,sciences of the artificial,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Discriminant Analysis with Strategically Manipulated Data'''
{{header}}
{{article
|author= Juheng Zhang,Haldun Aytug,Gary J. Koehler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We study the problem where a decision maker uses a linear classifier over attribute values (e. g., age, income, etc.) to classify agents into classes (e. g., creditworthy or not). Sometimes the attribute values are altered and/or hidden by agents to obtain a favorable but undeserved classification. Our main goal is to develop methods to thwart agents from hiding or distorting attribute values to obtain a favorable but incorrect classification. Intentionally altered attributes to obtain strategic goals have been studied. In this paper we develop methods that handle strategic hiding (i.e., nondisclosure) and then merge them with methods to thwart strategic distortion in the context of classification.
|keyword = classification,support vector machines,data imputation,missing values,adversarial learning,strategically hidden information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring How IT Professionals Experience Role Transitions at the End of Successful Projects'''
{{header}}
{{article
|author= Line Dube,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = In an increasingly fluid working environment, workers often find themselves in a state of transition and must be capable of adapting to rapid changes. This study focuses on intrafirm temporary role transitions, and more specifically on the case of information technology (IT) professionals transitioning out of a successful project and returning to their functional unit. In-depth interviews were conducted with 19 IT professionals. All of the respondents reported that they had experienced an adaptation period, albeit minor for some. Others, however, felt an important re-entry shock. A shock of high magnitude is mainly experienced when the successful project becomes a referent to which all other work assignments are compared. This idealized project work environment is the result of the decisions top management makes about the project structure, management, and governance. The results show that all people do not react the same way to a shock of high magnitude: some either adapt or change their new role, but others resist. This study highlights the need to better understand role transitions by further investigating the moderating variables at play in the relationships between actor, experience, magnitude of shock, and reaction. The study contributes to practice by questioning the widely shared assumption that IT professionals effortlessly navigate between project and functional work environments, and by highlighting the need to consider successful projects as a potential source of turmoil for team members. Finally, it raises the question of where the responsibility of managing transitions lies in the organization.
|keyword = IT professionals,IT projects,job satisfaction,project governance,project management,qualitative study,role theory,role transition,socialization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Talk Before It's Too Late: Reconsidering the Role of Conversation in Information Systems Project Management'''
{{header}}
{{article
|author= Stefano Mastrogiacomo,Stephanie Misonier,Riccardo Bonazzi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Effective team coordination is essential for the information systems (IS) projects' success. We present a four-year study, based on design science research, to develop and instantiate a conceptual model-called Coopilot-to improve real-time coordination in IS projects. Coopilot is a simple conversational guide to help IS project managers minimize the number of coordination surprises that arise for teams during their project meetings. Drawing on coordination literature outside the IS research field, we have adapted and instantiated the theory of joint activity developed by psycholinguist Herbert Clark. The results illustrate the value Clark's theory can add to the IS field and both the importance of conversation intended as a new theoretical construct in IS team project coordination as well as the importance of reaching a sufficient level of understanding. Project managers involved in this study who used Coopilot reported both higher levels of confidence that their projects were on a successful path and overall higher levels of team motivation.
|keyword = conversation for coordination,IS project management,IS teams,project management,team coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Achieving IT Program Goals with Integrative Conflict Management'''
{{header}}
{{article
|author= James J. Jiang,Jamie Y. T. Chang,Houn-Gee Chen,Eric T. G. Wang,Gary Klein,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Information technology (IT) programs are collections of projects structured to meet goals established by top management regarding the use of technology. Prior research has established the importance of commitment to the organizational goals set by top management and a shared understanding of the goals among the project teams. However, conflicts occur among project teams due to pursuit of their own goals, their unique approaches to completion of required tasks, and their individual need for limited resources. These conflicts need to be resolved in a fashion that leads to the pursuit of program goals, not the independent goals lodged in individual projects. We develop a model of an IT program environment to study the effects of goal interdependence among projects and shared understanding of organizational goals on promoting integrative conflict management (ICM). ICM techniques yield agreement on decisions in the face of conflicting ideas. In turn, ICM promotes arrival at an agreement about implementation means and commitment to the IT program goals, which are better achieved as a result. The model presents a new perspective for research on conflict that considers the specific resolution process to be a key component in the attainment of goals. Practitioners should instill integrative conflict resolution techniques into program and project processes as a fundamental means of achieving goals critical to the organization.
|keyword = conflict management,goal commitment,goal consensus,goal understanding,IT program,IT projects,means consensus project integration,project management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Creating Shared Understanding in Heterogeneous Work Groups: Why It Matters and How to Achieve It'''
{{header}}
{{article
|author= Eva Alice Christiane Bittner,Jan Marco Leimeister,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Shared understanding has been claimed to be crucial for effective collaboration of researchers and practitioners. Heterogeneity in work groups further strengthens the challenge of integrating understanding among diverse group members. Nevertheless, shared understanding and especially its formation are largely unexplored. After conceptualizing shared understanding, we apply collaboration engineering to derive a validated collaboration process module (compound thinkLet " MindMerger") to systematically support heterogeneous work groups in building shared understanding. We conduct a large-scale action research study at a German car manufacturing company. The evaluation indicates that with the use of MindMerger, team learning behaviors occur, and shared understanding of the tasks in complex work processes increases among experienced diverse tool and dye makers. Thus, the validated compound thinkLet MindMerger provides designers of collaborative work practices with a reusable module of activities to solve clarification issues in group work early on. Furthermore, findings from the field study contribute to the conceptualization of the largely unexplored phenomenon of shared understanding and its formation.
|keyword = collaboration engineering,group collaboration,heterogeneous groups,knowledge integration,shared understanding,thinkLet,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Social Network Structures on Prediction Market Accuracy in the Presence of Insider Information'''
{{header}}
{{article
|author= Liangfei Qiu,Huaxia Rui,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = This paper examines the effects of social network structures on prediction market accuracy in the presence of insider information through a randomized laboratory experiment. In the experiment, insider information is operationalized as signals on the state of nature with high precision. Motivated by the literature on insider information in the context of financial markets, we test and confirm two characterizations of insider information in the context of prediction markets: abnormal performance and less diffusion. Experimental results suggest that a more balanced social network structure is crucial to the success of prediction markets, whereas network structures akin to star networks are ill suited to prediction markets. As compared with other network structures, insider information has less positive effects on prediction market accuracy in star networks. We also find that the bias of the public information has a larger negative effect on prediction market accuracy in star networks.
|keyword = controlled experiment,insider information,prediction markets,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Rigidity Detection System for Automated Credibility Assessment'''
{{header}}
{{article
|author= Nathan W. Twyman,Aaron C. Elkins,Jude K. Burgoon,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Credibility assessment is an area in which information systems research can make a major impact. This paper reports on two studies investigating a system solution for automatic, noninvasive detection of rigidity for automated interviewing. Kinesic rigidity has long been a phenomenon of interest in the credibility assessment literature, but until now was infeasible as a veracity indicator in practical use cases. An initial study unexpectedly revealed the occurrence of rigidity in a highly controlled concealed information test setting, prompting the design and implementation of an automated rigidity detection system for interviewing. A unique experimental evaluation supported the system concept. The results of the second study confirmed the kinesic rigidity found in the first, and provided further theoretical insights explaining the rigidity phenomenon. Although additional research is needed, the evidence from this investigation suggests that credibility assessment can benefit from a rigidity detection system.
|keyword = automated interviewing systems,computer vision,concealed information test,credibility assessment,deception detection,freeze response,kinesic rigidity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Posture-Profile Misalignment in IT Innovation Diminishes Returns: Conceptual Development and Empirical Demonstration'''
{{header}}
{{article
|author= Robert G. Fichman,Nigel P. Melville,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = We conceive of information technology (IT) innovation posture-profile misalignment as a condition that exists when a firm's innovation posture (the extent to which a firm leads with IT innovation) does not match up with its innovation resource profile (the firm's stock of resources conducive to effective innovation). We theorize that firms with a posture-profile misalignment will see diminished returns from IT adoption because they will be less likely to possess (and be less effective at exploiting) crucial innovation resources when they need them most. We demonstrate how misalignment conditions the link between IT innovation adoption and organizational performance using a data set comprising electronic networking technologies in over 25,000 U.S. manufacturing plants. Productivity regression estimations reveal a consistent pattern that the association between IT innovation adoption and productivity is substantially diminished among misaligned firms. These empirical results provide initial confirmation of the theoretical value of innovation posture, innovation resource profile, and innovation posture-profile misalignment. We consider the implications for research on business value and innovation as well as for the practice of management.
|keyword = innovation,innovation outcomes,innovation with information technology,IT adoption,IT business value,IT innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Social Capital Among Information Technology and Business Units Drives Operational Alignment and IT Business Value'''
{{header}}
{{article
|author= Heinz-Theo Wagner,Daniel Beimborn,Tim Weitzel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = It is widely acknowledged that information technology (IT) and business resources need to be well aligned to achieve organizational goals. Yet, year after year, chief information officers still name business-IT alignment a key challenge for IT executives. While alignment research has matured, we still lack a sound theoretical foundation for alignment. Transcending the predominantly strategic executive-level focus, we develop a model of "operational alignment" and IT business value that combines a social perspective of IT and business linkage with a view of interaction between business and IT at nonstrategic levels, such as in daily business operations involving regular staff. Drawing on social capital theory to explain how alignment affects organizational performance, we examine why common suggestions such as "communicate more" are insufficient to strengthen alignment and disclose how social capital between IT and business units drives alignment and ultimately IT business value. Empirical data from 136 firms confirms the profound impact of operational business-IT alignment, composed of social capital and business understanding of IT, on IT flexibility, IT utilization, and organizational performance. The results show that social capital theory is a useful theoretical foundation for understanding how business IT alignment works. The findings suggest that operational alignment is at least as important as strategic alignment for IT service quality; that managers need to focus on operational aspects of alignment beyond communication by fostering knowledge, trust, and respect; and that IT utilization and flexibility are appropriate intermediate goals for business-IT alignment governance.
|keyword = business-IT alignment,business value of IT,information systems alignment,operational alignment,social capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User Roles and Contributions in Innovation-Contest Communities'''
{{header}}
{{article
|author= Johan Fueller,Katja Hutter,Julia Hautz,Kurt Matzler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Organizations increasingly initiate Internet-based innovation-contest communities through which individuals can interact and contribute to the innovation process. To successfully manage these communities, organizations need to understand what roles members assume, how they communicate and vary in their contribution behavior. In this exploratory study, we investigate the heterogeneous roles of contest participants based on an international innovation-contest community. We identify six user types associated with various behavioral contribution patterns by using cluster and social network analysis. The six user types further differ in their communicative content and contribution quality. Our paper contributes to a better theoretical understanding of distinctive user types in innovation-contest communities, their role in the community, and their contribution to the success of innovation contests in the era of social software. From a managerial perspective, the study provides guidance for contest platform design and appropriate reward structures.
|keyword = co-creation,innovation contests,online communities,user contribution,user roles,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Supporting Creative Problem Solving with a Case-Based Reasoning System'''
{{header}}
{{article
|author= Niek Althuizen,Berend Wierenga,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Attention for the division of work between computers and humans is growing due to ever-increasing computer capabilities. Over the past decades, creativity support systems (CSSs) have gained ground as a means to enhance individual, group, and organizational creativity. Whereas prior research has focused primarily on the main effects of CSSs, we explore the interaction effects with the creative ability of the individual. In this paper, we investigate the use of the case-based reasoning (CBR) technology, which is based on the principle of analogical reasoning, to aid individuals in solving business problems creatively. The expectations as to why the CBR technology should enhance individual creativity, and under what conditions (i.e., the type and number of cases that are made available), are derived from creative cognition theory, and are tested empirically. In a series of studies, a CBR system loaded with a diverse set of cases was found to enhance the performance of individuals with lower creative ability, but it did not help the most creative individuals. Although the literature suggests that cases from remote problem domains should lead to more novel solutions, loading the CBR system only with cases closely related to the problem domain proved more effective than remote cases only. Finally, loading the CBR system with a larger set of diverse cases was found to positively influence the creativity of the solutions. These findings have the following implications for CSSs and creative cognition theory: (1) when considering the effectiveness of CSSs it is important to take into account the creative ability of the individual (i.e., "one size does not fit all"), (2) making a sufficiently large and diverse set of cases available is better for stimulating creativity, and (3) providing cases that are too remote may be counterproductive. On a practical note, organizations seeking to redesign their division of labor between individuals and machines can easily follow the CBR approach presented here using their own set of cases.
|keyword = case-based reasoning,creative problem solving,creativity support systems,creative cognition theory,domain knowledge,marketing campaigns,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGITAL INNOVATION AS A FUNDAMENTAL AND POWERFUL CONCEPT IN THE INFORMATION SYSTEMS CURRICULUM'''
{{header}}
{{article
|author= Robert G. Fichman,Brian L. Dos Santos,Zhiqiang (Eric) Zheng,
|source= MIS QUARTERLY
|year= 2014
|abstract = The 50-year march of Moore's Law has led to the creation of a relatively cheap and increasingly easy-to-use world-wide digital infrastructure of computers, mobile devices, broadband network connections, and advanced application platforms. This digital infrastructure has, in turn, accelerated the emergence of new technologies that enable transformations in how we live and work, how companies organize, and the structure of entire industries. As a result, it has become important for all business students to have a strong grounding in IT and digital innovation in order to manage, lead, and transform organizations that are increasingly dependent on digital innovation. Yet, at many schools, students do not get such grounding because the required information systems core class is stuck in the past. We present a vision for a redesigned IS core class that adopts digital innovation as a fundamental and powerful concept (FPC). A good FPC serves as both a foundational concept and an organizing principle for a course. We espouse a particularly broad conceptualization of digital innovation that allows for a variety of teaching styles and topical emphases for the IS core class. This conceptualization includes three types of innovation (i.e., process, product, and business model innovation), and four stages for the overall innovation process (i.e., discovery, development, diffusion, and impact). Based on this conceptualization, we examine the implications of adopting digital innovation as an FPC. We also briefly discuss broader implications relating to (1) the IS curriculum beyond the core class, (2) the research agenda for the IS field, and (3) the identity and legitimacy of IS in business schools.
|keyword = Fundamental and powerful concepts (FPC),digital innovation,IS core course,pedagogy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DATA COLLECTION IN THE DIGITAL AGE: INNOVATIVE ALTERNATIVES TO STUDENT SAMPLES'''
{{header}}
{{article
|author= Zachary R. Steelman,Bryan I. Hammer,Moez Limayem,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online crowdsourcing markets (OCM) are becoming more popular as a source for data collection. In this paper, we examine the consistency of survey results across student samples, consumer panels, and online crowdsourcing markets (specifically Amazon's Mechanical Turk) both within the United States and outside. We conduct two studies examining the technology acceptance model (TAM) and the expectation-disconfirmation theory (EDT) to explore potential differences in demographics, psychometrics, structural model estimates, and measurement invariances. Our findings indicate that (1) U.S.-based OCM samples provide demographics much more similar to our student and consumer panel samples than the non-U.S.-based OCM samples; (2) both U.S. and non-U.S. OCM samples provide initial psychometric properties (reliability, convergent, and divergent validity) that are similar to those of both student and consumer panels; (3) non-U.S. OCM samples generally provide differences in scale means compared to those of our students, consumer panels, and U.S. OCM samples; and (4) one of the non-U.S. OCM samples refuted the highly replicated and validated TAM model in the relationship of perceived usefulness to behavioral intentions. Although our post hoc analyses isolated some cultural and demographic effects with regard to the non-U.S. samples in Study 1, they did not address the model differences found in Study 2. Specifically, the inclusion of non-U.S. OCM respondents led to statistically significant differences in parameter estimates, and hence to different statistical conclusions. Due to these unexplained differences that exist within the non-U.S. OCM samples, we caution that the inclusion of non-U.S. OCM participants may lead to different conclusions than studies with only U.S. OCM participants. We are unable to conclude whether this is due to of cultural differences, differences in the demographic profiles of non-U.S. OCM participants, or some unexplored factors within the models. Therefore, until further research is conducted to explore these differences in detail, we urge researchers utilizing OCMs with the intention to generalize to U.S. populations focus on U.S.-based participants and exercise caution in using non-U.S. participants. We further recommend that researchers should clearly describe their OCM usage and design (e. g., demographics, participant filters, etc.) procedures. Overall, we find that U.S. OCM samples produced models that lead to similar statistical conclusions as both U.S. students and U.S. consumer panels at a considerably reduced cost.
|keyword = Data collection,crowdsourcing,sampling,online research,crowdsourcing market,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE NATURE AND CONSEQUENCES OF TRADE-OFF TRANSPARENCY IN THE CONTEXT OF RECOMMENDATION AGENTS'''
{{header}}
{{article
|author= Jingjun (David) Xu,Izak Benbasat,Ronald T. Cenfetelli,
|source= MIS QUARTERLY
|year= 2014
|abstract = That recommendation agents (RAs) can substantially improve consumers' decision making is well understood. Far less understood is the influence of specific design attributes of the RA interface on decision making and other outcome measures. We investigate a novel design for an RA interface that enables it to interactively demonstrate trade-offs among product attribute values (i.e., trade-off transparency feature) to improve consumers' perceived product diagnosticity and perceived enjoyment. We also examine the extent to which the trade-offs among product attribute values should be revealed to the user. Further, based on the stimulus organism-response model, we develop a theoretical model that extends the effort-accuracy framework by proposing perceived enjoyment and perceived product diagnosticity as two antecedents for perceived decision quality and perceived decision effort, respectively. In an experimental study, we find that (1) the trade-off transparency feature significantly affects perceived enjoyment and perceived product diagnosticity, (2) perceived enjoyment and perceived product diagnosticity follow an inverted U-shaped curve as the level of trade-off transparency increases, (3) although users spend more time understanding attribute trade-offs with the trade-off transparency feature, they are more efficient in selecting a product, (4) perceived enjoyment simultaneously leads to better perceived decision quality and lower perceived decision effort, and (5) perceived product diagnosticity leads to better perceived decision quality without compromising perceptions of decision effort. Theoretically, this study increases our understanding of how the design of an RA interface can improve consumers' product diagnosticity and enjoyment, and proposes two antecedents to improve perceived decision quality and reduce perceived decision effort. For design practitioners, our results indicate the importance of providing the trade-off transparency design feature to potential consumers.
|keyword = Interface design,task complexity,recommendation agents (RAs),trade-off transparency,perceived enjoyment,perceived product diagnosticity,perceived decision effort,perceived decision quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TRUST, SATISFACTION, AND ONLINE REPURCHASE INTENTION: THE MODERATING ROLE OF PERCEIVED EFFECTIVENESS OF E-COMMERCE INSTITUTIONAL MECHANISMS'''
{{header}}
{{article
|author= Yulin Fang,Israr Qureshi,Heshan Sun,Patrick McCole,Elaine Ramsey,Kai H. Lim,
|source= MIS QUARTERLY
|year= 2014
|abstract = The effects of e-commerce institutional mechanisms on trust and online purchase have traditionally been understood in the initial online purchase context. This study extends this literature by exploring the role of e-commerce institutional mechanisms in the online repurchase context. In doing so, it responds to the emerging call for understanding the institutional context under which customer trust operates in an e-commerce environment. Specifically, this study introduces a key moderator, perceived effectiveness of e-commerce institutional mechanisms (PEEIM), to the relationships between trust, satisfaction, and repurchase intention. Drawing on the theory of organizational trust, and based on a survey of 362 returning online customers, we find that PEEIM negatively moderates the relationship between trust in an online vendor and online customer repurchase intention, as it decreases the importance of trust to promoting repurchase behavior. We also find that PEEIM positively moderates the relationship between customer satisfaction and trust as it enhances the customer's reliance on past transaction experience with the vendor to reevaluate trust in the vendor. Consistent with the predictions made in the literature, PEEIM does not directly affect trust or repurchase intention. Academic and practical implications and future research directions are discussed.
|keyword = E-commerce,trust,online repurchase intention,e-commerce,institutional mechanisms,moderation analysis,partial least square modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ECONOMIC ANALYSIS OF ONLINE ADVERTISING USING BEHAVIORAL TARGETING'''
{{header}}
{{article
|author= Jianqing Chen,Jan Stallaert,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online publishers and advertisers have recently shown increasing interest in using targeted advertising online. Such targeting allows them to present users with advertisements that are a better match, based on their past browsing and search behavior and other available information (e.g., hobbies registered on a web site). This technique, known as behavioral targeting, has been hailed as the new "Holy Grail" in online advertising because of its potential effectiveness. In this paper, we study the economic implications when an online publisher engages in behavioral targeting. The publisher auctions off an advertising slot and is paid on a cost-perclick basis. Using a horizontal differentiation model to capture the fit between a user and an advertisement being displayed, we identify the factors that affect the publisher's revenue, the advertisers' payoffs, and social welfare. We show that revenue for the online publisher in some circumstances can double when behavioral targeting is used. However, increased revenue for the publisher is not guaranteed: in some cases, the prices of advertising and hence the publisher's revenue can be lower, depending on the degree of competition and the advertisers' valuations. We identify two effects associated with behavioral targeting: a competitive effect and a propensity effect. The relative strength of the two effects determines whether the publisher's revenue is positively or negatively affected. We also demonstrate that, although social welfare is increased and small advertisers are better off under behavioral targeting, the dominant advertiser might be worse off and reluctant to switch from traditional advertising.
|keyword = Behavioral targeting,targeted advertising,online advertising,pricing,competition,auctions,analytical modeling,economic modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PROACTIVE VERSUS REACTIVE SECURITY INVESTMENTS IN THE HEALTHCARE SECTOR'''
{{header}}
{{article
|author= Juhee Kwon,M. Eric Johnson,
|source= MIS QUARTERLY
|year= 2014
|abstract = This study identifies the effects of security investments that arise from previous failures or external regulatory pressure. Building on organizational learning theory, the study focuses on the healthcare sector where legislation mandates breach disclosure and detailed data on security investments are available. Using a Cox proportional hazard model, we demonstrate that proactive security investments are associated with lower security failure rates. Coupling that result with the economics of breach disclosure, we also show that proactive investments are more cost effective in healthcare security than reactive investments. Our results further indicate that this effect is amplified at the state level, supporting the argument that security investments create positive externalities. We also find that external pressure decreases the effect of proactive investments on security performance. This implies that proactive investments, voluntarily made, have more impact than those involuntarily made. Our findings suggest that security managers and policy makers should pay attention to the strategic and regulatory factors influencing security investment decisions.
|keyword = Security investment,organizational learning,proactive,reactive,healthcare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE BUSINESS OF BEING A USER: THE ROLE OF THE REFERENCE ACTOR IN SHAPING PACKAGED ENTERPRISE SYSTEM ACQUISITION AND DEVELOPMENT'''
{{header}}
{{article
|author= Neil Pollock,Sampsa Hyysalo,
|source= MIS QUARTERLY
|year= 2014
|abstract = The paper extends the concept of "user" to account for a new, more formalized role that some client organizations play in the diffusion of packaged enterprise systems. Package vendors are attempting to draw parts of their user base into activities related to the promotion, selling, and commodification of systems. Users, in turn, appear willing to help construct these systems as objects of consumption for others. This can appear to be rather idiosyncratic behavior. Information Systems scholars have argued that relations between packaged enterprise system vendors and users are attenuated. Why might the user help the vendor market its systems in this way? What benefits accrue from it? And what role are users performing in carrying out this work? To show how this is becoming a general facet of the work of some packaged enterprise system users, we develop the notion of "reference actor," which is an extension of the earlier Information Systems concept of "social actor." In combining insights from the social shaping of technology and the biography of artifacts, and drawing on long-term qualitative fieldwork, we analyze this new actor role in relation to expectations and commitments coming from the wider packaged enterprise system community. In return for the help provided to prospective adopters, reference actors are also able to gather various kinds of benefits for themselves and others. In particular, they build closer relations with vendors such that they can influence product development strategies.
|keyword = User,reference site,demonstration,testimonial,commodification,marketing,enterprise system,procurement,innovation,social actor,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECTS OF WEB PERSONALIZATION ON USER ATTITUDE AND BEHAVIOR: AN INTEGRATION OF THE ELABORATION LIKELIHOOD MODEL AND CONSUMER SEARCH THEORY'''
{{header}}
{{article
|author= Shuk Ying Ho,David Bodoff,
|source= MIS QUARTERLY
|year= 2014
|abstract = Web personalization can achieve two business goals: increased advertising revenue and increased sales revenue. The realization of the two goals is related to two kinds of user behavior: item sampling and item selection. Prior research does not provide a model of attitude formation toward a personalization agent nor of how attitudes relate to these two behaviors. This limits our understanding of how web personalization can be managed to increase advertising revenues and/or sales revenues. To fill this gap, the current research develops and tests a theoretical model of user attitudes and behaviors toward a personalization agent. The model is based on an integration of two theories: the elaboration likelihood model (ELM) and consumer search theory (CST). In the integrated model, a user's attitude toward a personalization agent is influenced by both the number of items he/she has sampled so far (from CST) and the degree to which he/she cognitively processes each one (from ELM). In turn, attitude is modeled to influence both behaviors-that is, item selection and any further item sampling. We conducted a lab study and a field study to test six hypotheses. This research extends the theory on web personalization by providing a more complete picture of how sampling and processing of personalized recommendations influence a user's attitude and behavior toward the personalization agent. For online merchants, this research highlights the trade-off between item sampling and item selection and provides practical guidance on how to steer users toward the attitudes and behaviors that will realize their business goals.
|keyword = Elaboration likelihood model,consumer search theory,web personalization,attitude persistence,attitude confidence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NATURE AND NURTURE: THE IMPACT OF AUTOMATICITY AND THE STRUCTURATION OF COMMUNICATION ON VIRTUAL TEAM BEHAVIOR AND PERFORMANCE'''
{{header}}
{{article
|author= Valerie L. Bartelt,Alan R. Dennis,
|source= MIS QUARTERLY
|year= 2014
|abstract = Much prior research on virtual teams has examined the impact of the features and capabilities of different communication tools (the nature of communication) on team performance. In this paper, we examine how the social structures (i.e., genre rules) that emerge around different communication tools (the nurture of communication) can be as important in influencing performance. During habitual use situations, team members enact genre rules associated with communication tools without conscious thought via automaticity. These genre rules influence how teams interact and ultimately how well they perform. We conducted an experimental study to examine the impact of different genre rules that have developed for two communication tools: instant messenger and discussion forum. Our results show that in habitual use situations, these tools triggered different genre rules with different behaviors, which in turn resulted in significantly different decision quality. We used heightened time pressure as a discrepant event to interrupt the automatic enactment of habitual genre rules and found that users adopted similar behaviors for both tools, which resulted in no significant differences in decision quality. These findings suggest that the automatic enactment of genre rules for a communication tool may have as powerful an effect on behavior and performance as the actual features of the tool itself. We believe that our results, taken together with past research showing the effects of social structures on communication, call for the expansion of task-technology fit theories to include the role of social structures in explaining the use of and performance from communication tools.
|keyword = Collaboration,virtual teams,genre rules,structuration theory,IM,discussion board,time pressure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REFRAMING SUCCESS AND FAILURE OF INFORMATION SYSTEMS: A PERFORMATIVE PERSPECTIVE'''
{{header}}
{{article
|author= Dubravka Cecez-Kecmanovic,Karlheinz Kautz,Rebecca Abrahall,
|source= MIS QUARTERLY
|year= 2014
|abstract = The paper questions common assumptions in the dominant representational framings of information systems success and failure and proposes a performative perspective that conceives IS success and failure as relational effects performed by sociomaterial practices of IS project actor-networks of developers, managers, technologies, project documents, methodologies, and other actors. Drawing from a controversial case of a highly innovative information system in an insurance company-considered a success and failure at the same time-the paper reveals the inherent indeterminacy of IS success and failure and describes the mechanisms by which success and failure become performed and thus determined by sociomaterial practices. This is explained by exposing ontological politics in the reconfiguration and decomposition of the IS project actor-network and the emergence of different agencies of assessment that performed both different IS realities and competing IS assessments. The analysis shows that the IS project and the implemented system as objects of assessment are not given and fixed, but are performed by the agencies of assessment together with the assessment outcomes of success and failure. The paper demonstrates that by reframing IS success and failure, the performative perspective provides some novel and surprising insights that have a potential to change conversations on IS assessments in both the IS literature and IS practice.
|keyword = IS project success and failure,IS success and failure,IS project assessment,IS benefits assessment,sociomaterial worldview,performative perspective,actor-network theory (ANT),ontological politics,agency of assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''QUALITY COMPETITION AND MARKET SEGMENTATION IN THE SECURITY SOFTWARE MARKET'''
{{header}}
{{article
|author= Debabrata Dey,Atanu Lahiri,Guoying Zhang,
|source= MIS QUARTERLY
|year= 2014
|abstract = In recent years, we have witnessed an unprecedented growth in the security software market. This market is now fiercely competitive with hundreds of nearly identical products; yet, the price is high and coverage low. Although recent research has examined such idiosyncrasies and found the existence of a negative network effect as a possible explanation, several important questions still remain: (1) What possibly discourages product differentiation in such a competitive market? (2) Why is versioning absent here? (3) How does the presence of free alternatives in this market impact its structure? We develop a comprehensive oligopoly model, with endogenous quality and versioning decisions, to address these issues. Our analyses reveal that, although the presence of numerous competitors leads to a greater need to differentiate, the network effect in this market works as a counterweight, incentivizing vendors to sacrifice differentiation in favor of collocating in the top end of the quality spectrum. We explain the reasons and implications of this important finding. We further show that this result is robust and applicable even when versioning by competing vendors or the presence of free software is taken into consideration. Furthermore, given that the presence of free software actually intensifies competitive pressure and heightens the need to differentiate, the role of the network effect in abating differentiation becomes even more discernible.
|keyword = Security software,network effect,negative network effect,quality competition,market structure,vertical differentiation,fulfilled expectations equilibrium,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COORDINATING EXPERTISE ACROSS KNOWLEDGE BOUNDARIES IN OFFSHORE-OUTSOURCING PROJECTS: THE ROLE OF CODIFICATION'''
{{header}}
{{article
|author= Julia Kotlarsky,Harry Scarbrough,Ilan Oshri,
|source= MIS QUARTERLY
|year= 2014
|abstract = The coordination of effort within and among different expert groups is a central feature of contemporary organizations. Within the existing literature, however, a dichotomy has emerged in our understanding of the role played by codification in coordinating expert groups. One strand of literature emphasizes codification as a process that supports coordination by enabling the storage and ready transfer of knowledge. In contrast, another strand highlights the persistent differences between expert groups that create boundaries to the transfer of knowledge, seeing coordination as dependent on the quality of the reciprocal interactions between groups and individuals. Our research helps to resolve such contested understandings of the coordinative role played by codification. By focusing on the offshore-outsourcing of knowledge-intensive services, we examine the role played by codification when expertise was coordinated between client staff and onsite and offshore vendor personnel in a large-scale outsourcing contract between TATA Consultancy Services (TCS) and ABN AMRO bank. A number of theoretical contributions flow from our analysis of the case study, helping to move our understanding beyond the dichotomized views of codification outlined above. First, our study adds to previous work where codification has been seen as a static concept by demonstrating the multiple, coexisting, and complementary roles that codification may play. We examine the dynamic nature of codification and show changes in the relative importance of these different roles in coordinating distributed expertise over time. Second, we reconceptualize the commonly accepted view of codification as focusing on the replication and diffusion of knowledge by developing the notion of the codification of the "knower" as complementary to the codification of knowledge. Unlike previous studies of expertise directories, codification of the knower does not involve representing expertise in terms of occupational skills or competences but enables the reciprocal interrelating of expertise required by more unstructured tasks.
|keyword = Offshore-outsourcing,codification,expertise coordination,knowledge boundaries,qualitative case study,outsourcing transition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Disciplines of Information: Lessons from the History of the Discipline of Medicine'''
{{header}}
{{article
|author= David G. Schwartz,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = In this research commentary we show that the discipline of information systems (IS) has much that can be learned from the history of the discipline of medicine. We argue that as interest in historical studies of information systems grows, there are important historical lessons to be drawn from disciplines other than IS, with the medical discipline providing fertile ground. Of particular interest are the circumstances that surrounded the practice of the medical craft in the 1800's-circumstances that drove a process of unification and specialization resulting in the modern conceptualization of medical education, research, and practice. In analyzing the history of the field of medicine, with its long-established methods for general practice, specialization, and sub-specialization we find that it serves as an example of a discipline that has dealt effectively with its initial establishment as a scientific discipline, exponential growth of knowledge and ensuing diversity of practice over centuries, and has much to say in regards to a number of discipline-wide debates of IS. Our objective is to isolate the key factors that can be observed from the writings of leading medical historians, and examine those factors from the perspective of the information systems discipline today. Through our analysis we identify the primary factors and structural changes which preceded a modern medical discipline characterized by unification and specialization. We identify these same historic factors within the present-day information systems milieu and discuss the implications of following a unification and specialization strategy for the future of the disciplines of information.
|keyword = critical perspectives on information technology,institutional aspects of information systems,medical discipline,specialization,history,academia,professions,DI,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Machine Learning Approach to Improving Dynamic Decision Making'''
{{header}}
{{article
|author= Georg Meyer,Gediminas Adomavicius,Paul E. Johnson,Mohamed Elidrisi,William A. Rush,JoAnn M. Sperl-Hillen,Patrick J. O'Connor,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Decision strategies in dynamic environments do not always succeed in producing desired outcomes, particularly in complex, ill-structured domains. Information systems often capture large amounts of data about such environments. We propose a domain-independent, iterative approach that (a) applies data mining classification techniques to the collected data in order to discover the conditions under which dynamic decision-making strategies produce undesired or suboptimal outcomes and (b) uses this information to improve the decision strategy under these conditions. In this paper, we formally develop this approach and illustrate it by providing detailed examples of its application to a chronic disease care problem in a healthcare management organization, specifically the treatment of patients with type 2 diabetes mellitus. In particular, the proposed iterative approach is used to improve treatment strategies by predicting and eliminating treatment failures, i.e., insufficient or excessive treatment actions, based on information that is available in electronic medical record systems. We also apply the proposed approach to a manufacturing task, resulting in substantial decision strategy improvements, which further demonstrates the generality and flexibility of the proposed approach.
|keyword = dynamic decision making,process control,data mining,process mining,machine learning,simulation,healthcare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Prediction in Economic Networks'''
{{header}}
{{article
|author= Vasant Dhar,Tomer Geva,Gal Oestreicher-Singer,Arun Sundararajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We define an economic network as a linked set of entities, where links are created by actual realizations of shared economic outcomes between entities. We analyze the predictive information contained in a specific type of economic network, namely, a product network, where the links between products reflect aggregated information on the preferences of large numbers of individuals to co-purchase pairs of products. The product network therefore reflects a simple "smoothed" model of demand for related products. Using a data set containing more than 70 million observations of a nonstatic co-purchase network over a period of two years, we predict network entities' future demand by augmenting data on their historical demand with data on the demand for their immediate neighbors, in addition to network properties, specifically, local clustering and PageRank. To our knowledge, this is the first study of a large-scale dynamic network that shows that a product network contains useful distributed information for demand prediction. The economic implications of algorithmically predicting demand for large numbers of products are significant.
|keyword = economic networks,prediction,co-purchase network,predictive modeling,neural networks,autoregressive models,network-based prediction,PageRank,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Selling vs. Profiling: Optimizing the Offer Set in Web-Based Personalization'''
{{header}}
{{article
|author= Monica Johar,Vijay Mookerjee,Sumit Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We study the problem of optimally choosing the composition of the offer set for firms engaging in web-based personalization. A firm can offer items or links that are targeted for immediate sales based on what is already known about a customer's profile. Alternatively, the firm can offer items directed at learning a customer's preferences. This, in turn, can help the firm make improved recommendations for the remainder of the engagement period with the customer. An important decision problem faced by a profit maximizing firm is what proportion of the offer set should be targeted toward immediate sales and what proportion toward learning the customer's profile. We study the problem as an optimal control model, and characterize the solution. Our findings can help firms decide how to vary the size and composition of the offer set during the course of a customer's engagement period with the firm. The benefits of the proposed approach are illustrated for different patterns of engagement, including the length of the engagement period, uncertainty in the length of the period, and the frequency of the customer's visits to the firm. We also study the scenario where the firm optimizes the size of the offer set during the planning horizon. One of the most important insights of this study is that frequent visits to the firm's website are extremely important for an e-tailing firm even though the customer may not always buy products during these visits.
|keyword = web-based personalization,electronic retailing,optimal control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Personal Communication Technologies for Commercial Communications: A Cross-Country Investigation of Email and SMS'''
{{header}}
{{article
|author= Chuan-Hoo Tan,Juliana Sutanto,Chee Wei Phang,Anar Gasimov,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = The widespread use of personal communication technologies (PCTs) for commercial message dissemination necessitates understanding that PCTs might lead to better commercial performance in different situations. Building primarily on apparatgeist and social construction theories, this research proposes that consumer responses to PCT-disseminated commercial messages are jointly influenced by the PCT (i.e., technology) that carries general symbolic meanings about its nature and purpose (its "spirit"), and the context culture (i.e., the cultural milieu) in which it is used. We began with focus groups' assessments of two commonly utilized PCTs-email and short message service-which revealed their comparative symbolic meanings in terms of intimacy or formality of communication-to be in line with extant literature. Then, in a commercial setting where retailers leverage PCTs to disseminate product discount coupons, we examined the difference between two distinct environments that differed in their context-cultural dimensions (their cultural milieus of social interaction and communication)-i.e., China (an environment of high context-cultural dimension) and Switzerland (an environment of low context-cultural dimension). To do so, we first validated the context-cultural differences through a survey (study 1) and conducted two matching field experiments in the two countries involving more than one thousand consumers (study 2). Results support our propositions, demonstrating favorable commercial performance for SMS use in the high context-cultural environment and for email use in the low context-cultural environment. Follow-up surveys (study 3) corroborated the results and provided deeper insights into how both PCTs' general meanings and pertinent values in the cultural milieus we studied led to consumer responses. Besides presenting empirical evidence to inform the selection of appropriate PCTs for commercial communications, this research contributes to the theoretical development of apparatgeist and social construction theories via its joint examination of technologies and consumers' environments.
|keyword = personal communication technology,apparatgeist theory,social construction,context-cultural dimension,multimethod investigation,field experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product Fit Uncertainty in Online Markets: Nature, Effects, and Antecedents'''
{{header}}
{{article
|author= Yili (Kevin) Hong,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Product fit uncertainty (defined as the degree to which a consumer cannot assess whether a product's attributes match her preference) is proposed to be a major impediment to online markets with costly product returns and lack of consumer satisfaction. We conceptualize the nature of product fit uncertainty as an information problem and theorize its distinct effect on product returns and consumer satisfaction (versus product quality uncertainty), particularly for experience (versus search) goods without product familiarity. To reduce product fit uncertainty, we propose two Internet-enabled systems-website media (visualization systems) and online product forums (collaborative shopping systems)-that are hypothesized to attenuate the effect of product type (experience versus search goods) on product fit uncertainty. Hypotheses that link experience goods to product returns through the mediating role of product fit uncertainty are tested with analyses of a unique data set composed of secondary data matched with primary direct data from numerous consumers who had recently participated in buy-it-now auctions. The results show the distinction between product fit uncertainty and quality uncertainty as two distinct dimensions of product uncertainty and interestingly show that, relative to product quality uncertainty, product fit uncertainty has a significantly stronger effect on product returns. Notably, whereas product quality uncertainty is mainly driven by the experience attributes of a product, product fit uncertainty is mainly driven by both experience attributes and lack of product familiarity. The results also suggest that Internet-enabled systems are differentially used to reduce product (fit and quality) uncertainty. Notably, the use of online product forums is shown to moderate the effect of experience goods on product fit uncertainty, and website media are shown to attenuate the effect of experience goods on product quality uncertainty. The results are robust to econometric specifications and estimation methods. The paper concludes by stressing the importance of reducing the increasingly prevalent information problem of product fit uncertainty in online markets with the aid of Internet-enabled systems.
|keyword = product fit uncertainty,product quality uncertainty,product returns,Internet-enabled systems,expectation confirmation theory,online markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer Informedness and Firm Information Strategy'''
{{header}}
{{article
|author= Ting Li,Robert J. Kauffman,Eric van Heck,Peter Vervest,Benedict G. C. Dellaert,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Consumer informedness plays a critical role in determining consumer choice in the presence of information technology deployed by competing firms in the marketplace. This paper develops a new theory of consumer informedness. Using data collected through a series of stated choice experiments in two different research contexts, we examine how consumer characteristics and observed behaviors moderate the influence of price and product informedness on consumer choice. The results indicate that different types of consumer informedness amplify different consumer behaviors in specific consumer segments. In particular, we found that price informedness is more influential among consumers in the commodity segment. They exhibit greater trading down behavior, which represents stronger preferences for choosing the products that provide the best price. In contrast, product informedness is more influential among consumers in the differentiated segment. This group exhibits greater trading out behavior, involving stronger preferences for choosing products that best suit their specific needs. These results suggest that firm information strategy should take into account consumers' characteristics, their past observed behaviors, and the impact of consumer informedness. We also discuss the theoretical contributions of this research and its broader implications for firm-level information strategy.
|keyword = consumer choice,information strategy,marketing and IS,price and product information,randomized experiment,stated choice experiment,theory of consumer informedness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Project Managers' Practical Intelligence and Project Performance in Software Offshore Outsourcing: A Field Study'''
{{header}}
{{article
|author= Nishtha Langer,Sandra A. Slaughter,Tridas Mukhopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = T his study examines the role of project managers' (PM) practical intelligence (PI) in the performance of software offshore outsourcing projects. Based on the extant literature, we conceptualize PI for PMs as their capability to resolve project related work problems, given their long-range and short-range goals; PI is targeted at resolving unexpected and difficult situations, which often cannot be resolved using established processes and frameworks. We then draw on the information processing literature to argue that software offshore outsourcing projects are prone to severe information constraints that lead to unforeseen critical incidents that must be resolved adequately for the projects to succeed. We posit that PMs can use PI to effectively address and resolve such incidents, and therefore the level of PMs' PI positively affects project performance. We further theorize that project complexity and familiarity contribute to its information constraints and the likelihood of critical incidents in a project, thereby moderating the relationship between PMs' PI and project performance. To evaluate our hypotheses, we analyze longitudinal data collected in an in-depth field study of a leading software vendor organization in India. Our data include project and personnel level archival data on 530 projects completed by 209 PMs. We employ the critical incidents methodology to assess the PI of the PMs who led these projects. Our findings indicate that PMs' PI has a significant and positive impact on project performance. Further, projects with higher complexity or lower familiarity benefit even more from PMs' PI. Our study extends the literatures on project management and outsourcing by conceptualizing and measuring PMs' PI, by theorizing its relationship with project performance, and by positing how that relationship is moderated by project complexity and familiarity. Our study provides unique empirical evidence of the importance of PMs' PI in software offshore outsourcing projects. Given that PMs with high PI are scarce resources, our findings also have practical implications for the optimal resource allocation and training of PMs in software offshore services companies.
|keyword = IT project management,practical intelligence,software offshore outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Influence Techniques in Phishing Attacks: An Examination of Vulnerability and Resistance'''
{{header}}
{{article
|author= Ryan T. Wright,Matthew L. Jensen,Jason Bennett Thatcher,Michael Dinger,Kent Marett,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Phishing is a major threat to individuals and organizations. Along with billions of dollars lost annually, phishing attacks have led to significant data breaches, loss of corporate secrets, and espionage. Despite the significant threat, potential phishing targets have little theoretical or practical guidance on which phishing tactics are most dangerous and require heightened caution. The current study extends persuasion and motivation theory to postulate why certain influence techniques are especially dangerous when used in phishing attacks. We evaluated our hypotheses using a large field experiment that involved sending phishing messages to more than 2,600 participants. Results indicated a disparity in levels of danger presented by different influence techniques used in phishing attacks. Specifically, participants were less vulnerable to phishing influence techniques that relied on fictitious prior shared experience and were more vulnerable to techniques offering a high level of self-determination. By extending persuasion and motivation theory to explain the relative efficacy of phishers' influence techniques, this work clarifies significant vulnerabilities and lays the foundation for individuals and organizations to combat phishing through awareness and training efforts.
|keyword = persuasion theory,influence techniques,motivation theory,self-determination,perceived locus of causality,social engineering,online deception,mediated deception,deception,field experiments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Semantics and Pragmatics Interact in Understanding Conceptual Models'''
{{header}}
{{article
|author= Palash Bera,Andrew Burton-Jones,Yair Wand,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Underlying the design of any information system is an explicit or implicit conceptual model of the domain that the system supports. Because of the importance of such models, researchers and practitioners have long focused on how best to construct them. Past research on constructing conceptual models has generally focused on their semantics (their meaning), to discover how to convey meaning more clearly and completely, or their pragmatics (the importance of context in model creation and use), to discover how best to create or use a model in a given situation. We join these literatures by showing how semantics and pragmatics interact. Specifically, we carried out an experiment to examine how the importance of clear semantics in conceptual models-operationalized in terms of ontological clarity-varies depending on the pragmatics of readers' knowledge of the domain shown in the model. Our results show that the benefit of ontological clarity on understanding is concave downward (follows an inverted-U) as a function of readers' prior domain knowledge. The benefit is greatest when readers have moderate knowledge of the domain shown in the model. When readers have high or low domain knowledge, ontological clarity has no apparent benefit. Our study extends the theory of ontological clarity and emphasizes the need to construct conceptual models with readers' knowledge in mind.
|keyword = conceptual modeling,domain familiarity,ontological clarity,semantics,pragmatics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influences of Online Service Technologies and Task Complexity on Efficiency and Personalization'''
{{header}}
{{article
|author= Jingjun (David) Xu,Izak Benbasat,Ronald T. Cenfetelli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Online retailers are increasingly providing service technologies, such as technology-based and human-based services, to assist customers with their shopping. Despite the prevalence of these service technologies and the scholarly recognition of their importance, surprisingly little empirical research has examined the fundamental differences among them. Consequently, little is known about the factors that may favor the use of one type of service technology over another. In this paper, we propose the Model of Online Service Technologies (MOST) to theorize that the capacity of a service provider to accommodate the variability of customer inputs into the service process is the key difference among various types of service technologies. We posit two types of input variability: Service Provider-Elicited Variability (SPEV), where variability is determined in advance by the service provider; and User-Initiated Variability (UIV), where customers determine variability in the service process. We also theorize about the role of task complexity in changing the effectiveness of service technologies. We then empirically investigate the impact of service technologies that possess different capacities to accommodate input variability on efficiency and personalization, the two competing goals of service adoption. Our empirical approach attempts to capture both the perspective of the vendor who may deploy such technologies, as well as the perspective of customers who might choose among service technology alternatives. Our findings reveal that SPEV technologies (i.e., technologies that can accommodate SPEV) are more efficient, but less personalized, than SPEUIV technologies (i.e., technologies that can accommodate both SPEV and UIV). However, when task complexity is high (vs. low), the superior efficiency of SPEV technologies is less prominent, while both SPEV and SPEUIV technologies have higher personalization. We also find that when given a choice, a majority of customers tend to choose to use both types of technologies. The results of this study further our understanding of the differences in efficiency and personalization experienced by customers when using various types of online service technologies. The results also inform practitioners when and how to implement these technologies in the online shopping environment to improve efficiency and personalization for customers.
|keyword = input variability,online service technologies,SPEV technologies,SPEUIV technologies,task complexity,efficiency,personalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''RELIABILITY GENERALIZATION OF PERCEIVED EASE OF USE, PERCEIVED USEFULNESS, AND BEHAVIORAL INTENTIONS'''
{{header}}
{{article
|author= Traci J. Hess,Anna L. McNab,K. Asli Basoglu,
|source= MIS QUARTERLY
|year= 2014
|abstract = A reliability generalization study (a meta-analysis of reliability coefficients) was conducted on three widely studied information systems constructs from the technology acceptance model (TAM): perceived ease of use, perceived usefulness, and behavioral intentions. This form of meta-analysis summarizes the reliability coefficients of the scores on a specified scale across studies and identifies the study characteristics that influence the reliability of these scores. Reliability is a critical issue in conducting empirical research as the reliability of the scores on well-established scales can vary with study characteristics, attenuating effect sizes. In conducting this study, an extensive literature search was conducted, with 380 articles reviewed and coded to perform reliability generalization. Study characteristics, including technology, sample, and measurement characteristics, for these articles were recorded along with effect size data for the relationships among these variables. After controlling for number of items, sample size, and sampling error, differences in reliability coefficients were found with several study characteristics for the three technology acceptance constructs. The reliability coefficients of PEOU and PU were lower in hedonic contexts than in utilitarian contexts, and were higher when the originally validated scales were used as compared to when other items were substituted. Only 27 percent of the studies that provided the measurement items used the original PEOU items, while 39 percent used the original PU items. Scales that were administered in English had higher reliability coefficients for PU and BI, with a marginal effect for PEOU. Reliability differences were also found for other study characteristics, including reliability type, subject experience, and gender composition. While average reliability coefficients were high, the results show that, on average, relationships among these constructs are attenuated by 12 percent with maximum attenuation in the range of 35 to 43 percent. Implications for technology acceptance research are discussed and suggestions for addressing variation in reliability coefficients across studies are provided.
|keyword = Reliability generalization,reliability,meta-analysis,technology acceptance model (TAM),ease of use,usefulness,behavioral intentions,effect size attenuation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COLLABORATION THROUGH OPEN SUPERPOSITION: A THEORY OF THE OPEN SOURCE WAY'''
{{header}}
{{article
|author= James Howison,Kevin Crowston,
|source= MIS QUARTERLY
|year= 2014
|abstract = This paper develops and illustrates the theory of collaboration through open superposition: the process of depositing motivationally independent layers of work on top of each other over time. The theory is developed in a study of community-based free and open source software (FLOSS) development, through a research arc of discovery (participant observation), replication (two archival case studies), and theorization. The theory explains two key findings: (1) the overwhelming majority of work is accomplished with only a single programmer working on any one task, and (2) tasks that appear too large for any one individual are more likely to be deferred until they are easier rather than being undertaken through structured team work. Moreover, the theory explains how working through open superposition can lead to the discovery of a work breakdown that results in complex, functionally interdependent, work being accomplished without crippling search costs. We identify a set of socio-technical contingencies under which collaboration through open superposition is likely to be effective, including characteristics of artifacts made from information as the objects being worked on. We demonstrate the usefulness of the theory by using it to analyze difficulties in learning from FLOSS in other domains of work and in the IS function of for-profit organizations.
|keyword = Open source,information systems development,materiality,socio-technical system,collaboration,coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ENTERPRISE SYSTEM IMPLEMENTATION AND EMPLOYEE JOB PERFORMANCE: UNDERSTANDING THE ROLE OF ADVICE NETWORKS'''
{{header}}
{{article
|author= Tracy Ann Sykes,Viswanath Venkatesh,Jonathan L. Johnson,
|source= MIS QUARTERLY
|year= 2014
|abstract = The implementation of enterprise systems, such as enterprise resource planning (ERP) systems, alters business processes and associated workflows, and introduces new software applications that employees must use. Employees frequently find such technology-enabled organizational change to be a major challenge. Although many challenges related to such changes have been discussed in prior work, little research has focused on post-implementation job outcomes of employees affected by such change. We draw from social network theory-specifically, advice networks-to understand a key post-implementation job outcome (i.e., job performance). We conducted a study among 87 employees, with data gathered before and after the implementation of an ERP system module in a business unit of a large organization. We found support for our hypotheses that workflow advice and software advice are associated with job performance. Further, as predicted, we found that the interactions of workflow and software get-advice, workflow and software give-advice, and software get-and give-advice were associated with job performance. This nuanced treatment of advice networks advances our understanding of post-implementation success of enterprise systems.
|keyword = Social networks,get-advice,give-advice,enterprise system implementation,job performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLAINING DATA-DRIVEN DOCUMENT CLASSIFICATIONS'''
{{header}}
{{article
|author= David Martens,Foster Provost,
|source= MIS QUARTERLY
|year= 2014
|abstract = Many document classification applications require human understanding of the reasons for data-driven classification decisions by managers, client-facing employees, and the technical team. Predictive models treat documents as data to be classified, and document data are characterized by very high dimensionality, often with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classifiers is very difficult. This paper begins by extending the most relevant prior theoretical model of explanations for intelligent systems to account for some missing elements. The main theoretical contribution is the definition of a new sort of explanation as a minimal set of words (terms, generally), such that removing all words within this set from the document changes the predicted class from the class of interest. We present an algorithm to find such explanations, as well as a framework to assess such an algorithm's performance. We demonstrate the value of the new approach with a case study from a real-world document classification task: classifying web pages as containing objectionable content, with the goal of allowing advertisers to choose not to have their ads appear on those pages. A second empirical demonstration on news-story topic classification shows the explanations to be concise and document-specific, and to be capable of providing understanding of the exact reasons for the classification decisions, of the workings of the classification models, and of the business application itself. We also illustrate how explaining the classifications of documents can help to improve data quality and model performance.
|keyword = Document classification,instance level explanation,text mining,comprehensibility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SOCIAL MEDIA, TRADITIONAL MEDIA, AND MUSIC SALES'''
{{header}}
{{article
|author= Sanjeev Dewan,Jui Ramaprasad,
|source= MIS QUARTERLY
|year= 2014
|abstract = Motivated by the growing importance of social media, this paper examines the relationship between new media, old media, and sales in the context of the music industry. In particular, we study the interplay between blog buzz, radio play, and music sales at both the album and song levels of analysis. We employ the panel vector autoregression (PVAR) methodology, an extension of vector autoregression to panel data. We find that radio play is consistently and positively related to future sales at both the song and album levels. Blog buzz, however, is not related to album sales and negatively related to song sales, suggesting that sales displacement due to free online sampling dominates any positive word-of-mouth effects of song buzz on sales. Further, the negative relationship between song buzz and sales is stronger for niche music relative to mainstream music, and for less popular songs within albums. We discuss the implications of these results for both research and practice regarding the role of new media in the music industry.
|keyword = Social media,traditional media,music industry,panel vector auto-regression,blog buzz,music sales,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTENT SHARING IN A SOCIAL BROADCASTING ENVIRONMENT: EVIDENCE FROM TWITTER'''
{{header}}
{{article
|author= Zhan Shi,Huaxia Rui,Andrew B. Whinston,
|source= MIS QUARTERLY
|year= 2014
|abstract = The rise of social broadcasting technologies has greatly facilitated open access to information worldwide, not only by powering decentralized information production and consumption, but also by expediting information diffusion through social interactions like content sharing. Voluntary information sharing by users in the context of Twitter, the predominant social broadcasting site, is studied by modeling both the technology and user behavior. A detailed data set about the official content-sharing function on Twitter, called retweet, is collected and the statistical relationships between users' social network characteristics and their retweeting acts are documented. A two-stage consumption-sharing model is then estimated using the conditional maximum likelihood estimatio (MLE) method. The empirical results convincingly support our hypothesis that weak ties (in the form of unidirectional links) are more likely to engage in the social exchange process of content sharing. Specifically, we find that after a median quality tweet (as defined in the sample) is consumed, the likelihood that a unidirectional follower will retweet is 3.1 percentage point higher than the likelihood that a bidirectional follower will do so.
|keyword = Content sharing,social broadcasting,information diffusion,Twitter,weak tie,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTRIBUTION BEHAVIOR IN VIRTUAL COMMUNITIES: COGNITIVE, EMOTIONAL, AND SOCIAL INFLUENCES'''
{{header}}
{{article
|author= Hsien-Tung Tsai,Richard P. Bagozzi,
|source= MIS QUARTERLY
|year= 2014
|abstract = The long-term viability of virtual communities depends critically on contribution behavior by their members. We deepen and extend prior research by conceptualizing contributions to virtual communities in terms of small friendship group-referent intentional actions. Specifically, we investigate cognitive, emotional, and social determinants of shared we-intentions and their consequences for member contribution behavior to the small friendship group to which they belong within a larger community. Using multiple measurement sources and a longitudinal quasi-experimental design, we show that group norms and social identity, as well as attitudes and anticipated emotions, contribute to the development of behavioral desires, which in turn influence we-intentions. In addition, subjective norms are less effective than either group norms or social identity in encouraging contribution behavior. Finally, members' experience levels positively moderate the relationship between we-intentions and contribution behaviors, and differences between collectivistic versus individualistic orientations moderate the effects of social identity and anticipated emotions on the desire to contribute to one's friendship group in the virtual community. Tests for methods biases were conducted, as well as rival hypotheses. These findings have significant research and managerial implications.
|keyword = Anticipated emotions,desire,individualism-collectivism,novice versus experienced members,social identity,social influence,virtual communities,we-intentions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THEORIZATION AND TRANSLATION IN INFORMATION TECHNOLOGY INSTITUTIONALIZATION: EVIDENCE FROM DANISH HOME CARE'''
{{header}}
{{article
|author= Jeppe Agger Nielsen,Lars Mathiassen,Sue Newell,
|source= MIS QUARTERLY
|year= 2014
|abstract = Although institutional theory has become a more dominant perspective in information systems research, studies have only paid scant attention to how field dynamics and organizational processes coevolve during information technology institutionalization. Against this backdrop, we present a new conceptualization based on the "traveling of ideas" metaphor that distinguishes between theorization of ideas about IT usage across an organizational field and translation of such ideas into practical use of IT within particular organizations. Drawing on these distinct analytical views, we posit that IT institutionalization is constituted through recursive intertwining of theorization and translation involving both linguistic and material objects. To illustrate the detailed workings of this conceptualization, we apply it to a longitudinal study of mobile IT institutionalization within Danish home care. We demonstrate how heterogeneous actors within the Danish home care field theorized ideas about mobile IT usage and how these ideas translated into different local arrangements. Further, our account reveals a complex institutionalization process in which mobile IT was first seen as a fashionable recipe for improvement but subsequently became the subject of controversy. The paper adds to the emerging process and discourse literature on IT institutionalization by shedding new light on how IT ideas travel across a field and within individual organizations, how they transform and become legitimized over time, and how they take on different linguistic and material forms across organizational settings.
|keyword = IT institutionalization,institutional theory,theorization,translation,mobile IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''LEVERAGING PHILANTHROPIC BEHAVIOR FOR CUSTOMER SUPPORT: THE CASE OF USER SUPPORT FORUMS'''
{{header}}
{{article
|author= Wael Jabr,Radha Mookerjee,Yong Tan,Vijay S. Mookerjee,
|source= MIS QUARTERLY
|year= 2014
|abstract = Online user forums for technical support are being widely adopted by IT firms to supplement traditional customer support channels. Customers benefit from having an additional means of product support, while firms benefit by lowering the costs of supporting a large customer base. Typically these forums are populated with content generated by users, consisting of questioners (solution seekers) and solvers (solution providers). While questioners can be expected to keep returning as long as they can find answers, firms must employ different means in order to recognize and encourage the contributions of solvers. We identify and compare the impact of two widely adopted recognition mechanisms on the philanthropic behavior of solvers. In the first mechanism, feedback-based recognition, solver contribution is evaluated by questioners. In the second mechanism, quantity-based recognition, all contributions are weighted equally regardless of questioner feedback. We draw on the pro-social behavior literature to identify four drivers of solver contribution: (1) peer recognition, (2) image motivation, (3) social comparison, and (4) social exposure. We show that the choice of recognition mechanism strongly influences a solver's problem-solving behavior, highlighting the importance of the firm's decision in this regard. We address issues of solvers self-selecting a type of recognition mechanism by using propensity score analysis in order to show that solver behavior is a result of forum conditioning. We also study the impact of the recognition mechanism on forum quality and the effectiveness of support to draw comparative analytics.
|keyword = User support forum,text mining,pro-social behavior,recognition mechanism,feedback,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SWIFT GUANXI IN ONLINE MARKETPLACES: THE ROLE OF COMPUTER-MEDIATED COMMUNICATION TECHNOLOGIES'''
{{header}}
{{article
|author= Carol Xiaojuan Ou,Paul A. Pavlou,Robert M. Davison,
|source= MIS QUARTERLY
|year= 2014
|abstract = The concept of guanxi (i.e., a close and pervasive interpersonal relationship) has received little attention in the literature on online marketplaces, perhaps due to their impersonal nature. However, we propose that computer-mediated communication (CMC) technologies can mimic traditional interactive face-to-face communications, thus enabling a form of guanxi in online marketplaces. Extending the literature on traditional guanxi, we herein introduce the concept of swift guanxi, conceptualized as the buyer's perception of a swiftly formed interpersonal relationship with a seller, which consists of mutual understanding, reciprocal favors, and relationship harmony. Integrating theories of CMC and guanxi, we develop a model that explains how a set of CMC tools (i.e., instant messaging, message box, feedback system) facilitate repeat transactions with sellers by building swift guanxi and trust through interactivity and presence (social presence and telepresence) with sellers. Longitudinal data from 338 buyers in TaoBao, China's leading online marketplace, support our structural model, showing that the buyers' effective use of CMC tools enable swift guanxi and trust by enhancing the buyers' perceptions of interactivity and presence. In turn, swift guanxi and trust predict buyers' repurchase intentions and their actual repurchases from sellers. We discuss the implications of swift guanxi in online marketplaces with the aid of CMC technologies.
|keyword = Guanxi,swift guanxi,computer-mediated communication (CMC) technologies,media synchronicity theory (MST),computer-mediated communication interactivity model (CMCIM),instant messenger (IM),online marketplaces,interactivity,presence,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPLEMENTARY ONLINE SERVICES IN COMPETITIVE MARKETS: MAINTAINING PROFITABILITY IN THE PRESENCE OF NETWORK EFFECTS'''
{{header}}
{{article
|author= Hila Etzion,Min-Seok Pang,
|source= MIS QUARTERLY
|year= 2014
|abstract = A growing number of firms are strategically utilizing information technology and the Internet to provide online services to consumers who buy their products. Online services differ from traditional services because they often promote interactivity among users and exhibit positive network effects. While the service increases the value obtained by consumers, network effects are known to intensify price competition and thus may reduce firms' profits. In this paper, we model the competition between two firms that sell a differentiated product when each firm can offer a complementary online service to its customers. We derive the market equilibrium and determine how firms should adjust their strategies to account for network effects. We find that when the service exhibits network effects, a firm's decision whether or not to offer the service depends on both the competitor's decision and the competitor's service quality. When the service does not exhibit network effects, this is not the case. In addition, we show that a firm can benefit from the technological ability to offer the service, and from an increase in the strength of network effects or in the market size of the service, only when the value customers derive from the direct functionalities (those that do not rely on the network) of the service are sufficiently high. As a result, a firm's investment in the direct functionalities of its service increases with the strength of network effects of the service as long as the marginal development cost is not too high. Finally, we show that inefficiencies in terms of the number of firms offering the service as well as the total number of service users may prevail.
|keyword = Online services,network effects,e-commerce,analytical modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DISTRIBUTED COGNITION IN SOFTWARE DESIGN: AN EXPERIMENTAL INVESTIGATION OF THE ROLE OF DESIGN PATTERNS AND COLLABORATION'''
{{header}}
{{article
|author= George Mangalaraj,Sridhar Nerur,RadhaKanta Mahapatra,Kenneth H. Price,
|source= MIS QUARTERLY
|year= 2014
|abstract = Software design is a knowledge intensive task that constitutes a critical part of the software development process. Using a controlled experiment involving software practitioners, this research examines two potentially useful mechanisms for improving the software design process. Specifically, this study examines the impact of structural distribution of cognition through design patterns and social distribution of cognition through collaborating pairs on design outcomes. The results indicate that the use of design patterns as external cognitive artifacts improves design quality, reduces time taken to solve a design problem, and leads to higher participant satisfaction. Collaborating pairs of software designers were compared to participants working alone but whose efforts were conjointly considered as the best and second-best members of nominal pairs. It was found that paired designers produced higher quality designs compared with the second-best members of nominal pairs, did not differ from the best member of a nominal pair, but took more time to complete a design task than either member of a nominal pair. The results also indicate that the availability of design patterns raises the performance level of the second-best member of a nominal pair, in terms of quality, and reduces task completion time when compared with a pair not using design patterns. Finally, paired designers were found to experience higher levels of task satisfaction when compared with individuals. Implications for research and practice are discussed.
|keyword = Software design,agile methodology,paired design,design pattern,nominal group,distributed cognition,codified knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT'S DIFFERENT ABOUT SOCIAL MEDIA NETWORKS? A FRAMEWORK AND RESEARCH AGENDA'''
{{header}}
{{article
|author= Gerald C. Kane,Maryam Alavi,Giuseppe (Joe) Labianca,Stephen P. Borgatti,
|source= MIS QUARTERLY
|year= 2014
|abstract = In recent years, we have witnessed the rapid proliferation and widespread adoption of a new class of information technologies, commonly known as social media. Researchers often rely on social network analysis (SNA) when attempting to understand these technologies, often without considering how the novel capabilities of social media platforms might affect the underlying theories of SNA, which were developed primarily through studies of offline social networks. This article outlines several key differences between traditional offline social networks and online social media networks by juxtaposing an established typology of social network research with a well-regarded definition of social media platforms that articulates four key features. The results show that at four major points of intersection, social media has considerable theoretical implications for SNA. In exploring these points of intersection, this study outlines a series of theoretically distinct research questions for SNA in social media contexts. These points of intersection offer considerable opportunities for researchers to investigate the theoretical implications introduced by social media and lay the groundwork for a robust social media agenda potentially spanning multiple disciplines.
|keyword = Social media,social network analysis,theory,blog,wiki,networks,framework,research agenda,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY CAPABILITY AND FIRM PERFORMANCE: CONTRADICTORY FINDINGS AND THEIR POSSIBLE CAUSES'''
{{header}}
{{article
|author= Ho-Chang Chae,Chang E. Koh,Victor R. Prybutok,
|source= MIS QUARTERLY
|year= 2014
|abstract = Several studies support the positive link between information technology capability and firm performance, including Bharadwaj (2000) and Santhanam and Hartono (2003), which appeared in MIS Quarterly. We conducted a study to see if this link is still statistically significant. It is now over a decade since the first study was published, during which several significant developments in the IT industry have taken place. Unlike the 1990s, when proprietary information systems prevailed, the 2000s are characterized by more standardized and homogeneous information systems and with the rapid adoption of ERP and web technologies. Thus, we attempted to reexamine the link between IT capability and firm performance with data from the 2000s. Surprisingly, the results of our current analysis showed no significant link between IT capability and firm performance. Contrary to earlier studies, IT leader firms in our study didn't show better financial performance than control firms. We discuss several possible causes for the change in findings and present an in-depth comparison in business performance between the two groups-IT leader and control-over a period extending from 1991 to 2007.
|keyword = IT capability,firm performance,IW 500,IT business value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Neuroscience and a Nomological Network for the Understanding and Assessment of Emotions in Information Systems Research'''
{{header}}
{{article
|author= Shirley Gregor,Aleck C. H. Lin,Tom Gedeon,Amir Riaz,Dingyun Zhu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Human emotions' role in phenomena related to information systems ( IS) is increasingly of interest to research and practice, and is now informed by a burgeoning literature in neuroscience. This study develops a nomological network with an overarching view of relationships among emotions and other constructs of interest in IS research. The resulting 3-emotion systems' nomological network includes three interacting emotion systems: language, physiology, and behavior. Two laboratory experiments were conducted to test the nomological network, with six online travel service Web pages used as stimuli. The first study used paper-based self-report measures and qualitative comments, whereas the second included both self-reports and electroencephalography (EEG) measures. An outcome measure of e-loyalty was included in each study. The results of both studies showed positive and negative emotion-inducing stimuli were related to positive and negative emotions when viewing the Web sites as indicated by both self-reports and EEG data. In turn, positive and negative emotions as measured by both self-reports and EEG measures were linked to e-loyalty to some degree. This research is novel and significant because it is possibly the first in-depth study to link the study of emotions in IS with a sound theory base and multiple measurement approaches, including neuroscience measures. It shows that an EEG measure has some predictive power for an outcome such as e-loyalty. Implications of the research are that IS studies should distinguish between the different emotion systems of language and physiology, choose emotion measures carefully, and also recognize the intertwining of the emotion systems and cognitive processing.
|keyword = emotions in information systems,measurement,NeuroIS,neuroscience,nomological net,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Putting on the Thinking Cap: Using NeuroIS to Understand Information Processing Biases in Virtual Teams'''
{{header}}
{{article
|author= Randall K. Minas,Robert F. Potter,Alan R. Denis,Valerie Bartelt,Soyoung Bae,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Virtual teams are increasingly common in today's organizations, yet they often make poor decisions. Teams that interact using text-based collaboration technology typically exchange more information than when they perform the same task face-to-face, but past results suggest that team members are more likely to ignore information they receive from others. Collaboration technology makes unique demands on individual cognitive resources that may change how individual team members process information in virtual settings compared to face-to-face settings. This experiment uses electroencephalography, electrodermal activity, and facial electromyography to investigate how team members process information received from text-based collaboration during a team decision-making process. Our findings show that information that challenges an individual's prediscussion decision preference is processed similarly to irrelevant information, while information that supports an individual's prediscussion decision preference is processed more thoroughly. Our results present neurological evidence for the underlying processes of confirmation bias in information processing during online team discussions.
|keyword = collaboration technology,electroencephalography,information processing bias,NeuroIS,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trusting Humans and Avatars: A Brain Imaging Study Based on Evolution Theory'''
{{header}}
{{article
|author= Rene Riedl,Peter N. C. Mohr,Peter H. Kening,Fred D. Davis,Hauke R. Hekeren,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Avatars, as virtual humans possessing realistic faces, are increasingly used for social and economic interaction on the Internet. Research has already determined that avatar-based communication may increase perceived interpersonal trust in anonymous online environments. Despite this trust-inducing potential of avatars, however, we hypothesize that in trust situations, people will perceive human faces differently than they will perceive avatar faces. This prediction is based on evolution theory, because throughout human history the majority of interaction among people has taken place in face-to-face settings. Therefore, unlike perception of an avatar face, perception of a human face and the related trustworthiness discrimination abilities must be part of the genetic makeup of humans. Against this background, we conducted a functional magnetic resonance imaging experiment based on a multiround trust game to gain insight into the differences and similarities of interactions between humans versus human interaction with avatars. Our results indicate that (1) people are better able to predict the trustworthiness of humans than the trustworthiness of avatars; (2) decision making about whether or not to trust another actor activates the medial frontal cortex significantly more during interaction with humans, if compared to interaction with avatars; this brain area is of paramount importance for the prediction of other individuals' thoughts and intentions (mentalizing), a notably important ability in trust situations; and (3) the trustworthiness learning rate is similar, whether interacting with humans or avatars. Thus, the major implication of this study is that although interaction on the Internet may have benefits, the lack of real human faces in communication may serve to reduce these benefits, in turn leading to reduced levels of collaboration effectiveness.
|keyword = agent,avatar,brain,cognitive neuroscience,evolutionary psychology,evolution theory,functional magnetic resonance imaging (fMRI),medial frontal cortex (MFC),mentalizing,NeuroIS,theory-of-mind (TOM),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enhancing User-Game Engagement Through Software Gaming Elements'''
{{header}}
{{article
|author= Mengxiang Li,Qiqi Jiang,Chuan-Ho Tan,Kwok-Kee Wei,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = User-game engagement is vital for building and retaining a customer base for software games. However, few studies have investigated such engagement during gameplay and the impact of gaming elements on engagement. Drawing on the theoretical foundation of engagement, we meticulously deduced two cognitive-related gaming elements of a software game, namely, game complexity and game familiarity, and argued that these elements have individual and joint effects on user-game engagement. This research adopted multimethod empirical investigations to validate our conceptions. The first investigation used electroencephalography and a self-report survey to study quantitatively the cognitive activities of user-game engagement. The second investigation adopted the qualitative interview method to triangulate the findings from the quantitative data. This research contributes to theory in two ways, namely, conceptualizing and empirically examining user-game engagement as well as theorizing and demonstrating how the two gaming elements affect user-game engagement. This work contributes to the gaming practice by providing a set of design principles for gaming elements.
|keyword = electroencephalography,NeuroIS,online games,software games,user-game engagement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Informational and Normative Social Influence in Group-Buying: Evidence from Self-Reported and EEG Data'''
{{header}}
{{article
|author= Kevin K. Y. Kuan,Yingqin Zhong,Patrick Y. K. Chau,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = This study examines two types of information commonly used by group-buying sites to induce purchasing. The first study indicates the number of people who have bought a deal ("buy" information). The second one indicates Facebook friends who "like" a deal ("like" information). The effects of the group-buying information on opinions (attitude and intention) and emotions were examined using a controlled experiment. Our results show that positive and negative "buy" information has an asymmetric influence on attitude and intention, whereas "like" information has a positive influence on intention. The presence of "buy" information is associated with EEG activity that is generally linked to negative emotions. However, the addition of "like" information is associated with EEG activity that is generally linked to positive emotions. The different effects of the two types of group-buying information can be explained by the different social influences exerted by the information.
|keyword = electroencephalography,emotion online,group buying,informational social influence,NeuroIS,normative social influence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Explicit and Implicit Antecedents of Users' Behavioral Beliefs in Information Systems: A Neuropsychological Investigation'''
{{header}}
{{article
|author= Ana Ortiz de Guinea,Ryad Titah,Pierre-Majorique Leger,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Behavioral beliefs-perceived usefulness and perceived ease of use-have been identified as the most influential antecedents of individuals' information systems use intentions and behaviors within the technology acceptance model. However, little research has been aimed at investigating the implicit (automatic or unconscious) determinants of such cognitive beliefs, and more importantly, the potential nonlinear relationships of such antecedents with explicit (perceptual) ones. As such, this paper theorizes that implicit neurophysiological states-memory load and distraction- and explicit-engagement and frustration-antecedents interact in the formation of perceived usefulness and perceived ease of use. To test the study's hypotheses, we conducted an experiment that measured neurophysiological states while individuals worked on instrumental and hedonic tasks using technology. The results show that, as theorized, implicit and explicit constructs interact together, and thus have a nonlinear effect on behavioral beliefs. Specifically, when engagement is high, neurophysiological distraction does not statistically significantly affect perceived usefulness, whereas when engagement is low, neurophysiological distraction has a negative and significant effect on usefulness. The results also show that when frustration is high, neurophysiological memory load has a negative effect on perceived ease of use, whereas when it is low, neurophysiological memory load has a positive effect on perceived ease of use. This study makes several contributions to acceptance research and the emerging field of NeuroIS, including demonstration of the importance of emotional perceptions for moderating the effects of neurophysiological states on behavioral beliefs.
|keyword = behavioral belief formation,cognitive beliefs,electroencephalography (EEG),emotion,IS acceptance,IS use,NeuroIS,nonlinear effects,TAM,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Guidelines for Neuroscience Studies in Information Systems Research'''
{{header}}
{{article
|author= Jan vom Brocke,Ting-Peng Liang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Neuroscience provides a new lens through which to study information systems (IS). These NeuroIS studies investigate the neurophysiological effects related to the design, use, and impact of IS. A major advantage of this new methodology is its ability to examine human behavior at the underlying neurophysiological level, which was not possible before, and to reduce self-reporting bias in behavior research. Previous studies that have revisited important IS concepts such as trust and distrust have challenged and extended our knowledge. An increasing number of neuroscience studies in IS have given researchers, editors, reviewers, and readers new challenges in terms of determining what makes a good NeuroIS study. While earlier papers focused on how to apply specific methods (e. g., functional magnetic resonance imaging), this paper takes an IS perspective in deriving six phases for conducting NeuroIS research and offers five guidelines for planning and evaluating NeuroIS studies: to advance IS research, to apply the standards of neuroscience, to justify the choice of a neuroscience strategy of inquiry, to map IS concepts to bio-data, and to relate the experimental setting to IS-authentic situations. The guidelines provide guidance for authors, reviewers, and readers of NeuroIS studies, and thus help to capitalize on the potential of neuroscience in IS research.
|keyword = NeuroIS,neuroscience,research guidelines,research methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Social Networks on Prediction Markets: Examination in a Controlled Experiment'''
{{header}}
{{article
|author= Liangfei Qiu,Huaxia Rui,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = This paper examines the effect of a social network on prediction markets using a controlled laboratory experiment that allows us to identify causal relationships between a social network and the performance of an individual participant, as well as the performance of the prediction market as a whole. Through a randomized experiment, we first confirm the theoretical predictions that participants with more social connections are less likely to invest in information acquisition from outside information sources, but perform significantly better than other participants in prediction markets. We further show that when the cost of information acquisition is low, a social network-embedded prediction market outperforms a nonnetworked prediction market. We find strong support for peer effects in prediction accuracy among participants. These results have direct managerial implications for the business practice of prediction markets and are critical to understanding how to use social networks to improve the performance of prediction markets.
|keyword = information exchange,prediction markets,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Forming Interoperability Through Interorganizational Systems Standards'''
{{header}}
{{article
|author= Kexin Zhao,Mu Xia,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Interoperability is a crucial organizational capability that enables firms to manage information systems (IS) from heterogeneous trading partners in a value network. While interoperability has been discussed conceptually in the IS literature, few comprehensive empirical studies have been conducted to conceptualize this construct and examine it in depth. For instance, it is unclear how interoperability is formed and whether it can improve organizational performance. To fill the gap, we argue that interorganizational systems (IOS) standards are a key information technology infrastructure facilitating formation of interoperability. As an organizational ability to work with external trading partners, interoperability's development depends not only on capability building within firm boundaries but also on community readiness across firm boundaries. Using data collected from 194 organizations in the geospatial industry, we empirically confirm that interoperability is formed via these two different paths. Furthermore, our results show that interoperability acts as a mediator by enabling firms to achieve performance gains from IOS standards adoption. Our study sheds new light on formation mechanisms as well as the business value of interoperability.
|keyword = business value,geospatial industry,interoperability,interorganizational systems standards,IT standards,network effects,standardization,standardized data infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Differential Effects of Keyword Selection in Search Engine Advertising on Direct and Indirect Sales'''
{{header}}
{{article
|author= Xianghua Lu,Xia Zhao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2014
|abstract = Product sales via sponsored keyword advertising on search engines rely on an effective selection of keywords that describe the offerings. In this study, we consider both the direct sales of the advertised products and indirect sales (i.e., cross-selling) of other products, and examine how specific keywords and general keywords influence these two types of sales differently. We also examine how the cross-selling effects may vary across different types of products (main products and accessories). Our results suggest that the use of specific keywords leans toward improving the direct sales of advertised products, while the use of general keywords leans toward improving the indirect sales of other products. The contribution of keywords to indirect sales is influenced by product type. For main products, the use of specific keywords generates a higher marginal contribution to indirect sales than that of general keywords. For accessory products, the use of general keywords generates a higher marginal contribution to indirect sales than that of specific keywords. The key implication of this study is that sellers focusing on different types of sales (direct or indirect sales) or products (main or accessory products) should consider using different types of keywords in search engine advertising to drive sales.
|keyword = cross-selling,keyword advertising,keywords selection,online advertising,search engines,sponsored search,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology-Enabled Business Models: A Conceptual Framework and a Coevolution Perspective for Future Research'''
{{header}}
{{article
|author= Arun Rai,Xinlin Tang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = There is growing recognition that firms' information technology (IT)-enabled business models (i.e., how inter-firm transactions with suppliers, customers, and partners are structured and executed) are a distinctive source of value creation and appropriation. However, the concept of business models' (BMs) "IT enablement" remains coarse in the information systems and strategic management literatures. Our objectives are to introduce a framework to elaborate the concept of IT-enabled BMs and to identify areas for future research that will enhance our understanding of the subject. We introduce the idea that two business-to-business (B2B) IT capabilities-dyadic IT customization and network IT standardization-are the mediating execution mechanisms between the strategic intent of interfirm collaboration and the (re) configuration of BMs to both create and appropriate value. We develop the logic that B2B IT capabilities for BM (re) configuration operate at two levels-IT customization at the dyadic relationship level and IT standardization at the interfirm network level-that together provide the complementary IT capabilities for firms to exchange content, govern relationships, and structure interconnections between products and processes with a diverse set of customers, suppliers, and partners. We discuss how these two complementary B2B IT capabilities are pivotal for firms to pursue different sources of value creation and appropriation. We identify how a firm's governance choices to engage in interfirm collaboration and its interfirm networks coevolve with its B2B IT capabilities as fruitful areas for future research.
|keyword = IT-enabled business model,B2B IT capabilities,interfirm collaboration,governance choices,design elements,value creation,value appropriation,coevolution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluation of Competing Candidate Solutions in Electronic Networks of Practice'''
{{header}}
{{article
|author= Thomas O. Meservy,Matthew L. Jensen,Kelly J. Fadel,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Electronic networks of practice have become a prevalent means for acquiring new knowledge. Knowledge seekers commonly turn to online repositories constructed by these networks to find solutions to domain-specific problems and questions. Yet little is understood about the process by which such knowledge is evaluated and adopted by knowledge seekers. This study examines how individuals filter knowledge encountered in online forums, a common platform for knowledge exchange in an electronic network of practice. Drawing on dual process theory, we develop research hypotheses regarding both central and peripheral evaluation of knowledge. These hypotheses are examined in a field experiment in which participants evaluate online solutions for computer programming problems. Results show that peripheral cues (source expertise and validation) have a greater influence on knowledge filtering decisions than does the content quality of the solution. Moreover, elaboration increases the effect of content quality but does not seem to attenuate the effect of peripheral cues. Implications for research and practice are discussed.
|keyword = electronic networks of practice,dual process theory,elaboration likelihood,knowledge management,mediated knowledge exchange,knowledge forums,information influence,knowledge filtering,field experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How to Attract and Retain Readers in Enterprise Blogging?'''
{{header}}
{{article
|author= Param Vir Singh,Nachiketa Sahoo,Tridas Mukhopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We investigate the dynamics of blog reading behavior of employees in an enterprise blogosphere. A dynamic model is developed and calibrated using longitudinal data from a Fortune 1,000 IT services firm. Our modeling framework allows us to segregate the impact of textual characteristics (sentiment and quality) of a post on attracting readers from retaining them. We find that the textual characteristics that appeal to the sentiment of the reader affect both reader attraction and retention. However, textual characteristics that reflect only the quality of the posts affect only reader retention. We identify a variety-seeking behavior of blog readers where they dynamically switch from reading on one set of topics to another. The modeling framework and findings of this study highlight opportunities for the firm to influence blog-reading behavior of its employees to align it with its goals. Overall, this study contributes to improved understanding of reading behavior of individuals in communities formed around user generated content.
|keyword = blogs,employee blogs,enterprise 2.0,blog reading,dynamic models,user generated content,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring Information Technology Spillovers'''
{{header}}
{{article
|author= Prasanna Tambe,Lorin M. Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = The measurement of the impact of IT spillovers on productivity is an important emerging area of research. Studies of IT spillovers often adopt a "production function" approach commonly used for measuring R&D spillovers, in which an external pool of IT investment is modeled using weighted measures of the IT investments of other firms, industries, or countries. We show that when using this approach, measurement error in a firm's own IT inputs can exert a significant upward bias on estimates of social returns to IT investment. This problem is particularly severe for IT spillovers because of the high levels of measurement error in most available IT data. The presence of the bias term can be demonstrated by using instrumental variable techniques to remove the effects of measurement error in a firm's own IT inputs. Using panel data on IT investment, we show that measurement error corrected estimates of IT spillovers are 40% to 90% lower than uncorrected estimates. This bias term is increasing in the correlation between the IT pool and firms' own IT investment. Therefore, estimates from models of spillover pools are less sensitive to the issues identified in this paper when the spillover paths minimize the correlation between a firm's own IT investment and the constructed external IT pool. Implications for researchers, policy makers, and managers are discussed.
|keyword = IT spillovers,IT productivity,measurement error,business value of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT-Enabled Coordination for Ambidextrous Interorganizational Relationships'''
{{header}}
{{article
|author= Ghiyoung Im,Arun Rai,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Contextual ambidexterity of an interorganizational relationship (IOR) is the ability of its management system to align partners' activities and resources for short-term goals and adapt partners' cognitions and actions for long-term viability. It is an alternative to structural ambidexterity in which separate units of the IOR pursue short-and long-term goals. We theorize that when utilized to coordinate the IOR, information technology (IT)-enabled operations and sensemaking, along with interdependent decision making, promote the IOR's contextual ambidexterity. We test our hypotheses on both sides of a customer-vendor relationship using data collected from (1) the account executives of one of the world's largest supply chain vendors (n = 76) and (2) its customers (n = 238). We find commonalities and differences in the influence coordination mechanisms have on contextual ambidexterity from the vendor's and the customer's perspectives. For both customers and vendors, contextual ambidexterity improves the quality and performance of the relationship, and decision interdependence promotes contextual ambidexterity. For customers, using operations support systems (OSSs) and interpretation support systems (ISSs) enhances contextual ambidexterity. For vendors, the impact of both OSS use and ISS use on contextual ambidexterity depends on the duration of the relationship. Our study shows that IT-enabled operations and sensemaking are key enablers of IOR ambidexterity and that vendors should combine these IT capabilities with relationship-specific knowledge that accumulates with relationship duration.
|keyword = interorganizational relationships,interorganizational systems,contextual ambidexterity,coordination,decision interdependence,sensemaking,operations support systems,interpretation support systems,relationship duration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Product Reviews: Implications for Retailers and Competing Manufacturers'''
{{header}}
{{article
|author= Young Kwark,Jianqing Chen,Srinivasan Raghunathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper studies the effect of online product reviews on different players in a channel structure. We consider a retailer selling two substitutable products produced by different manufacturers, and the products differ in both their qualities and fits to consumers' needs. Online product reviews provide additional information for consumers to mitigate the uncertainty about the quality of a product and about its fit to consumers' needs. We show that the effect of reviews on the upstream competition between the manufacturers is critical in understanding which firms gain and which firms lose. The upstream competition is affected in fundamentally different ways by quality information and fit information, and each information type has different implications for the retailer and manufacturers. Quality information homogenizes consumers' perceived utility differences between the two products and increases the upstream competition, which benefits the retailer but hurts the manufacturers. Fit information heterogenizes consumers' estimated fits to the products and softens the upstream competition, which hurts the retailer but benefits the manufacturers. Furthermore, reviews may also alter the nature of upstream competition from one in which consumers' own assessment on the quality dimension plays a dominant role in consumers' comparative evaluation of products to one in which fit dimension plays a dominant role. If manufacturers do not respond strategically to reviews and keep the same wholesale prices regardless of reviews (i.e., the upstream competition is assumed to be unaffected by reviews), then, we show that reviews never hurt the retailer and the manufacturer with favorable reviews, and never benefit the manufacturer with unfavorable reviews, a finding that demonstrates why reviews' effect on upstream competition is critical for firms in online marketplaces.
|keyword = online product reviews,competition,electronic commerce,analytical modeling,economics of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Framework and Guidelines for Context-Specific Theorizing in Information Systems Research'''
{{header}}
{{article
|author= Weiyin Hong,Frank K. Y. Chan,James Y. L. Thong,Lewis C. Chasalow,Gurpreet Dhillon,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = This paper discusses the value of context in theory development in information systems (IS) research. We examine how prior research has incorporated context in theorizing and develop a framework to classify existing approaches to contextualization. In addition, we expound on a decomposition approach to contextualization and put forth a set of guidelines for developing context-specific models. We illustrate the application of the guidelines by constructing and comparing various context-specific variations of the technology acceptance model (TAM)-i.e., the decomposed TAM that incorporates interaction effects between context-specific factors, the extended TAM with context-specific antecedents, and the integrated TAM that incorporates mediated moderation and moderated mediation effects of context-specific factors. We tested the models on 972 individuals in two technology usage contexts: a digital library and an agile Web portal. The results show that the decomposed TAM provides a better understanding of the contexts by revealing the direct and interaction effects of context-specific factors on behavioral intention that are not mediated by the TAM constructs of perceived usefulness and perceived ease of use. This work contributes to the ongoing discussion about the importance of context in theory development and provides guidance for context-specific theorizing in IS research.
|keyword = theory development,contextualization,context-specific model,general model,technology adoption,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Joint Product Improvement by Client and Customer Support Center: The Role of Gain-Share Contracts in Coordination'''
{{header}}
{{article
|author= Shantanu Bhattacharya,Alok Gupta,Sameer Hasija,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = We study the role of different contract types in coordinating the joint product improvement effort of a client and a customer support center. The customer support center's costly efforts at joint product improvement include transcribing and analyzing customer feedback, analyzing market trends, and investing in product design. Yet this cooperative role must be adequately incentivized by the client, since it could lead to fewer service requests and hence lower revenues for the customer support center. We model this problem as a sequential game with double-sided moral hazard in a principal-agent framework (in which the client is the principal). We follow the contracting literature in modeling the effort of the customer support center, which is the first mover, as either unobservable or observable; in either case, the efforts are unverifiable and so cannot be contracted on directly. We show that it is optimal for the client to offer the customer support center a linear gain-share contract when efforts are unobservable, even though it can yield only the second-best solution for the client. We also show that the cost-plus contracts widely used in practice do not obtain the optimal solution. However, we demonstrate that if efforts are observable then a gain-share and cost-plus options-based contract is optimal and will also yield the first-best solution. Our research provides a systematic theoretical framework that accounts for the prevalence of gain-share contracts in the IT industry's joint improvement efforts, and it provides guiding principles for understanding the increased role for customer support centers in product improvement.
|keyword = IT outsourcing,gain-share contract,cost-plus contract,joint product improvement,double-sided moral hazard,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Platform Performance Investment in the Presence of Network Externalities'''
{{header}}
{{article
|author= Jr. Edward G. Anderson,Geoffrey G. Parker,Burcu Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = Managers of emerging platforms must decide what level of platform performance to invest in at each product development cycle in markets that exhibit two-sided network externalities. High performance is a selling point for consumers, but in many cases it requires developers to make large investments to participate. Abstracting from an example drawn from the video game industry, we build a strategic model to investigate the trade-off between investing in high platform performance versus reducing investment in order to facilitate third party content development. We carry out a full analysis of three distinct settings: monopoly, price-setting duopoly, and price-taking duopoly. We provide insights on the optimum investment in platform performance and demonstrate how conventional wisdom about product development may be misleading in the presence of strong cross-network externalities. In particular, we show that, contrary to the conventional wisdom about "winner-take-all" markets, heavily investing in the core performance of a platform does not always yield a competitive edge. We characterize the conditions under which offering a platform with lower performance but greater availability of content can be a winning strategy.
|keyword = two-sided markets,network externality,product development,video game industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Economics of Free Under Perpetual Licensing: Implications for the Software Industry'''
{{header}}
{{article
|author= Marius F. Niculescu,D. J. Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2014
|abstract = In this paper, we explore the economics of free under perpetual licensing. In particular, we focus on two emerging software business models that involve a free component: feature-limited freemium (FLF) and uniform seeding (S). Under FLF, the firm offers the basic software version for free, while charging for premium features. Under S, the firm gives away for free the full product to a percentage of the addressable market uniformly across consumer types. We benchmark their performance against a conventional business model under which software is sold as a bundle (labeled as "charge for everything" or CE) without free offers. In the context of consumer bounded rationality and information asymmetry, we develop a unified two-period consumer valuation learning framework that accounts for both word-of-mouth (WOM) effects and experience-based learning, and use it to compare and contrast the three business models. Under both constant and dynamic pricing, for moderate strength of WOM signals, we derive the equilibria for each model and identify optimality regions. In particular, S is optimal when consumers significantly underestimate the value of functionality and cross-module synergies are weak. When either cross-module synergies are stronger or initial priors are higher, the firm decides between CE and FLF. Furthermore, we identify nontrivial switching dynamics from one optimality region to another depending on the initial consumer beliefs about the value of the embedded functionality. For example, there are regions where, ceteris paribus, FLF is optimal when the prior on premium functionality is either relatively low or high, but not in between. We also demonstrate the robustness of our findings with respect to various parameterizations of cross-module synergies, strength of WOM effects, and number of periods. We find that stronger WOM effects or more periods lead to an expansion of the seeding optimality region in parallel with a decrease in the seeding ratio. Moreover, under CE and dynamic pricing, second period price may be decreasing in the initial consumer valuation beliefs when WOM effects are strong and the prior is relatively low. However, this is not the case under weak WOM effects. We also discuss regions where price skimming and penetration pricing are optimal. Our results provide key managerial insights that are useful to firms in their business model search and implementation.
|keyword = software,freemium business models,versioning,seeding strategies,product sampling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Horizontal Allocation of Decision Rights for On-Premise Applications and Software-as-a-Service'''
{{header}}
{{article
|author= Till J. Winkler,Carol V. Brown,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = This study addresses a major gap in our knowledge about the allocation of information technology (IT) decision rights between business and IT units at the application level, including the governance of applications delivered on-premise versus those delivered with a software-as-a-service (SaaS) model. Building on the findings from a multicase qualitative study of organizations that had adopted the same SaaS application, we draw on three theoretical lenses (agency theory, transaction cost economics, and knowledge-based view) to develop a theoretically grounded model with three organization-level factors, three application-level factors, and application-level IT governance. Hypotheses derived from the model, as well as a set of differential hypotheses about factor influences due to on-premise versus SaaS delivery, are tested with survey responses from 207 firms in which application-level governance is operationalized with two dimensions: decision control rights (decision authority) and decision management rights (task responsibility). Three antecedents (origin of the application initiative, scope of application use, business knowledge of the IT unit) were significantly associated with application governance postimplementation, and the on-premise/SaaS subgroup analyses provide preliminary evidence for the mode of application delivery as a moderator of these relationships. Overall, this study contributes to a growing body of research that takes a more modular approach to studying IT governance and provides theoretical explanations for differing application-level governance designs.
|keyword = agency theory,application governance,cloud computing,field survey,IT governance,knowledge-based view,multigroup analysis,software-as-a-service,transaction cost economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Governance and Control of Open Source Software Projects'''
{{header}}
{{article
|author= Dany Di Tullio,D. Sandy Staples,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = A comprehensive set of governance mechanisms and dimensions were investigated to identify combinations of mechanisms that are effectively used together in on-going volunteer-based open source software (OSS) projects. Three configurations were identified: Defined Community, Open Community, and Authoritarian Community. Notably, Defined Community governance had the strongest coordination and project climate and had the most extensive use of outcome, behavior, and clan control mechanisms (controller driven). The controls in the Defined Community governance configuration appear to effectively enable open, coordinated contribution and participation from a wide variety of talented developers (one of the virtues of open source development) while managing the development process and outcomes. The results add to our theoretical understanding of control in different types of information systems projects, as the combination of control modes found in OSS projects is different from those found in previous research for internal or outsourced information systems development projects. This could be due to unique features of OSS projects, such as volunteer participation and the controller being part of the development team. The results provide guidance for practitioners about how to combine 19 identified governance mechanisms into effective project governance that stimulates productive participation.
|keyword = configuration theory,control modes and mechanisms,control theory,coordination,development activity,governance,IS development,open source software projects,OSS,project climate,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Process Model of Complementarity and Substitution of Contractual and Relational Governance in IS Outsourcing'''
{{header}}
{{article
|author= Thomas L. Huber,Thomas A. Fischer,Jens Dibbern,Rudy Hirschheim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = This paper develops a process model of how and why complementarity and substitution form over time between contractual and relational governance in the context of information systems outsourcing. Our analysis identifies four distinct process patterns that explain this formation as the outcome of interaction processes between key elements of both contractual and relational governance. These patterns unveil the dynamic nature of complementarity and substitution. In particular, we show that the relationship between contractual and relational governance oscillates between complementarity and substitution. Those oscillations are triggered mainly by three types of contextual events (goal fuzziness, goal conflict, and goal misalignment). Surprisingly, substitution of informal control did not occur as an immediate reaction to external events but emerged as a consequence of preceding complementarity. Thus, our study challenges the prevailing view of an either/or dichotomy of complementarity and substitution by showing that they are causally connected over time.
|keyword = contract,contractual governance,formal control,informal control,information systems outsourcing,outsourcing,process view,relational governance,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evolution of Governance: Achieving Ambidexterity in IT Outsourcing'''
{{header}}
{{article
|author= Lan Cao,Kanan Mohan,Balasubramaniam Ramesh,Sumantra Sarkar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Two types of information technology (IT) outsourcing governance-contractual and relational-are commonly employed to address different goals in IT service management in outsourcing arrangements. Contractual governance helps improve efficiency in an outsourcing relationship, whereas relational governance facilitates satisfying changing business needs. Past literature argues that both forms of governance are important and that an appropriate balance between them is necessary. This study finds that these two forms of governance often conflict with one another. We contribute to the research on IT outsourcing governance by opening the black box of the evolutionary process of achieving ambidexterity in this context. Organizations shift their focus between contractual and relational forms of governance in an attempt to develop practices that address conflicts between the two forms. We present the findings from a qualitative study of an organization that outsourced its IT services. Our findings reveal how a balance between contractual and relational governance can be achieved through a process we call the ambidexterity pendulum.
|keyword = IT outsourcing,IT outsourcing governance,organizational ambidexterity,outsourcing,outsourcing governance,pendulum process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Information Artifact in IT Governance: Toward a Theory of Information Governance'''
{{header}}
{{article
|author= Paul P. Tallon,Ronald V. Ramirez,James E. Short,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = In recent years, chief information officers have begun to report exponential increases in the amounts of raw data captured and retained across the organization. Managing extreme amounts of data can be complex and challenging at a time when information is increasingly viewed as a strategic resource. Since the dominant focus of the information technology (IT) governance literature has been on how firms govern physical IT artifacts (hardware, software, networks), the goal of this study is to extend the theory of IT governance by uncovering the structures and practices used to govern information artifacts. Through detailed interviews with 37 executives in 30 organizations across 17 industries, we discover a range of structural, procedural, and relational practices used to govern information within a nomological net that includes the antecedents of these practices and their effects on firm performance. While some antecedents enable the speedy adoption of information governance, others can delay or limit the adoption of information governance practices. Once adopted, however, information governance can help to boost firm performance. By incorporating these results into an extended theory of IT governance, we note how information governance practices can unlock value from the ever-expanding mountains of data currently held within organizations.
|keyword = big data,data growth,information artifact,information governance,information life cycle management,information management,information risk,information value,IT governance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Centralizing Data Management with Considerations of Uncertainty and Information-Based Flexibility'''
{{header}}
{{article
|author= Chander K. Velu,Stuart E. Madnick,Marshall W. Van Alstyne,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = This paper applies the theory of real options to analyze how the value of information-based flexibility should affect the decision to centralize or decentralize data management under low and high uncertainty. This study makes two main contributions. First, we show that in the presence of low uncertainty, centralization of data management decisions creates more total surplus for the firm as the similarity of business units increases. In contrast, in the presence of high uncertainty, centralization creates more total surplus as the dissimilarity of business units increases. The pivoting distinction trades the benefit of reduction of uncertainty from dissimilar businesses for centralization (with cost saving) against the benefit of flexibility from decentralization. Second, the framework helps senior management evaluate the trade-offs in data centralization that drive different business models of the firm. We illustrate the application of these propositions formally using an analytical model and informally using case vignettes and simulation.
|keyword = economics of IS,flexibility and information systems decentralization,real options,uncertainty,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Different Types of Free Trials and Ratings in Sampling of Consumer Software: An Empirical Study'''
{{header}}
{{article
|author= Young-Jin Lee,Yong Tan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Giving away trial software is a common practice for software developers to maximize the exposure of their products to potential consumers and to minimize the consumers' uncertainty about software quality. There are two types of free trials: (1) freeware, which consists of very basic features of focal software without a time lock, and (2) trialware, which has the full functionality of focal software with a time lock. In this paper, we study what factors make some free-trial software attract more potential adopters than others. Our empirical model under the traditional Bass-type diffusion examines the effects of the different types of free-trial software and ratings on consumer software sampling and reveals the dynamics of sampling over time. Using free-trial software downloading data on Download.com, we observe that the consumer software sampling process can be described by the theory of information diffusion. We find that user ratings affect sampling performance positively and that third-party ratings need to be positive to be effective. Finally, our results do not show any discernible differences between freeware and trialware with regard to their impact on sampling performance. This study contributes to the understanding of software free-trial practice from the perspective of consumer sampling growth of different types of free trials. Our findings can help design free-trial strategies to extrapolate the extent of consumer awareness of focal software and effectively convey its quality information to potential customers.
|keyword = freeware,Hausman-Taylor estimation,information diffusion,online user and third-party ratings,software commercialization,software free trials,software sampling,trialware,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Integrating Biosignals into Information Systems: A NeuroIS Tool for Improving Emotion Regulation'''
{{header}}
{{article
|author= Philip J. Astor,Marc T. P. Adam,Petar Jercic,Kristina Schaaff,Christof Weinhardt,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Traders and investors are aware that emotional processes can have material consequences on their financial decision performance. However, typical learning approaches for debiasing fail to overcome emotionally driven financial dispositions, mostly because of subjects' limited capacity for self-monitoring. Our research aims at improving decision makers' performance by (1) boosting their awareness to their emotional state and (2) improving their skills for effective emotion regulation. To that end, we designed and implemented a serious game-based NeuroIS tool that continuously displays the player's individual emotional state, via biofeedback, and adapts the difficulty of the decision environment to this emotional state. The design artifact was then evaluated in two laboratory experiments. Taken together, our study demonstrates how information systems design science research can contribute to improving financial decision making by integrating physiological data into information technology artifacts. Moreover, we provide specific design guidelines for how biofeedback can be integrated into information systems.
|keyword = biofeedback,design science,decision-making processes,emotion regulation,financial decision making,IT artifacts,NeuroIS,serious games,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Prior Reviews on the Subsequent Review Process in Reputation Systems'''
{{header}}
{{article
|author= Xiao (Sean) Ma,Lara Khansa,Yun Deng,Sung S. Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Reputation systems have been recognized as successful online review communities and word-of-mouth channels. Our study draws upon the elaboration likelihood model to analyze the extent that the characteristics of reviewers and their early reviews reduce or worsen the bias of subsequent online reviews. Investigating the sources of this bias and ways to mitigate it is of considerable importance given the previously established significant impact of online reviews on consumers' purchasing decisions and on businesses' profitability. Based on a panel data set of 744 individual consumers collected from Yelp, we used the Markov chain Monte Carlo simulation method to develop and empirically test a system of simultaneous models of consumer review behavior. Our results reveal that male reviewers or those who lack experience, geographic mobility, or social connectedness are more prone to being influenced by prior reviews. We also found that longer and more frequent reviews can reduce online reviews' biases. This paper is among the first to examine the moderating effects of reviewer and review characteristics on the relationship between prior reviews and subsequent reviews. Practically, this study offers businesses effective customer relationship management strategies to improve their reputations and expand their clientele.
|keyword = consumer review,elaboration likelihood model,hierarchical modeling,MCMC simulation,reputation systems,simultaneous equations model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EVALUATING JOURNAL QUALITY AND THE ASSOCIATION FOR INFORMATION SYSTEMS SENIOR SCHOLARS' JOURNAL BASKET VIA BIBLIOMETRIC MEASURES: DO EXPERT JOURNAL ASSESSMENTS ADD VALUE?'''
{{header}}
{{article
|author= Paul Benjamin Lowry,Gregory D. Moody,James Gaskin,Dennis F. Galletta,Sean L. Humpherys,Jordan B. Barlow,David W. Wilson,
|source= MIS QUARTERLY
|year= 2013
|abstract = Information systems journal rankings and ratings help scholars focus their publishing efforts and are widely used surrogates for judging the quality of research. Over the years, numerous approaches have been used to rank IS journals, approaches such as citation metrics, school lists, acceptance rates, and expert assessments. However, the results of these approaches often conflict due to a host of validity concerns. In the current scientometric study, we make significant strides toward correcting for these limitations in the ranking of mainstream IS journals. We compare expert rankings to bibliometric measures such as the ISI Impact Factor T, the h-index, and social network analysis metrics. Among other findings, we conclude that bibliometric measures provide very similar results to expert-based methods in determining a tiered structure of IS journals, thereby suggesting that bibliometrics can be a complete, less expensive, and more efficient substitute for expert assessment. We also find strong support for seven of the eight journals in the Association for Information Systems Senior Scholars' "basket" of journals. A cluster analysis of our results indicates a two-tiered separation in the quality of the highest quality IS journals-with MIS Quarterly, Information Systems Research, and Journal of Management Information Systems belonging, in that order, to the highest A+ tier. Journal quality metrics fit nicely into the sociology of science literature and can be useful in models that attempt to explain how knowledge disseminates through scientific communities.
|keyword = Information systems journal rankings,scientometrics,bibliometrics,journal quality,SenS-6,SenS-8,self-citation,impact factor,h-index,social network analysis,expert opinion,composite ranking or rating,AIS Senior Scholars basket of journals,nomologies for dissemination of scientific knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A LONGITUDINAL STUDY OF HERD BEHAVIOR IN THE ADOPTION AND CONTINUED USE OF TECHNOLOGY'''
{{header}}
{{article
|author= Heshan Sun,
|source= MIS QUARTERLY
|year= 2013
|abstract = Herd literature suggests that people tend to discount their own beliefs and imitate others when making adoption decisions and that the resulting adoption decisions are fragile and can be easily reversed during the post-adoptive stage. This helps explain why the adoption of a number of new technologies. from Amazon's Kindle, to Apple's iPod, iPhone, and iPad, to various types of Web 2. 0 technologies. appears to have adoption patterns similar to those of new fashion trends (i. e., an initial en masse acquisition followed by subsequent abandonment). It is important to understand these phenomena because they are strongly related to the staying power of technology. From a herd behavior perspective, this study proposes two new concepts, namely discounting one's own information and imitating others, to describe herd behavior in technology adoption. A research model is developed to describe the conditions under which herd behavior in technology adoption occurs, how it impacts technology adoption decision making, and how it influences post-adoptive system use. A longitudinal study is conducted to examine the research model. Findings from this research suggest that the discounting of one's own beliefs and the imitating of others when adopting a new technology are provoked primarily by the observation of prior adoptions and perceptions of uncertainty regarding the adoption of new technology. Herd behavior has a significant influence on user technology adoption; however, it does not necessarily lead to the collapse of the user base, as predicted in the herd literature. Instead, imitation can help reduce post-adoption regret and thus serve as a legitimate strategy for choosing a good enough technology, which may or may not be the best option to enhance job performance. People tend to adjust their beliefs when herding and also to revive their discounted initial beliefs to modify their beliefs about the technology at the post-adoptive stage. Findings from this study have significant research and practical implications.
|keyword = Herd behavior,imitating,technology adoption and continued use,longitudinal study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACT OF WIKIPEDIA ON MARKET INFORMATION ENVIRONMENT: EVIDENCE ON MANAGEMENT DISCLOSURE AND INVESTOR REACTION'''
{{header}}
{{article
|author= Sean Xin Xu,Xiaoquan (Michael) Zhang,
|source= MIS QUARTERLY
|year= 2013
|abstract = In this paper, we seek to determine whether a typical social media platform, Wikipedia, improves the information environment for investors in the financial market. Our theoretical lens leads us to expect that information aggregation about public companies on Wikipedia may influence how management's voluntary information disclosure reacts to market uncertainty with respect to investors' information about these companies. Our empirical analysis is based on a unique data set collected from financial records, management disclosure records, news article coverage, and a Wikipedia modification history of public companies. On the supply side of information, we find that information aggregation on Wikipedia can moderate the timing of managers' voluntary disclosure of companies' earnings disappointments, or bad news. On the demand side of information, we find that Wikipedia's information aggregation moderates investors' negative reaction to bad news. Taken together, these findings support the view that Wikipedia improves the information environment in the financial market and underscore the value of information aggregation through the use of information technology.
|keyword = Social media,Wikipedia,information environment,financial market,management disclosure,information aggregation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TALKING ABOUT TECHNOLOGY: THE EMERGENCE OF A NEW ACTOR CATEGORY THROUGH NEW MEDIA'''
{{header}}
{{article
|author= Emmanuelle Vaast,Elizabeth J. Davidson,Thomas Mattson,
|source= MIS QUARTERLY
|year= 2013
|abstract = This paper examines how a new actor category may emerge in a field of discourse through the new media of the Internet. Existing literatures on professional and organizational identity have shown the importance of identity claims and of the tensions surrounding "optimal distinctiveness" for new actors in a field, but have not examined the roles of new media in these processes. The literature on information technology (IT) and identity has highlighted the identity-challenging and identity-enhancing aspects of new IT use for existing actor categories but has not examined the dynamics associated with the emergence of new actor categories. Here, we investigate how a new actor category may emerge through the use of new media as a dynamic interaction of discursive practices, identity claims, and new media use. Drawing on findings from a case study of technology bloggers, we identified discursive practices through which a group of technology bloggers enacted claims of a distinctive identity in the joint construction of their discourse and in response to continuous developments in new media. Emergence of this new category was characterized by ongoing, opposing yet coexisting tendencies toward coalescence, fragmentation, and dispersion. Socio-technical dynamics underlying bloggers' use of new media and the actions of prominent ("A-list") bloggers contributed to these tendencies. We untangle theoretically the identity-enabling and identity-unsettling effects of new media and conceptualize the emergence of a new actor category through new media as an ongoing process in which the category identity may remain fluid, rather than progress to an endpoint.
|keyword = Web 2.0,discursive practices,identity,legitimacy,blogging,socio-technical dynamics,A-listers,coalescence,dispersion,fragmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIFFERENTIAL INFLUENCE OF BLOGS ACROSS DIFFERENT STAGES OF DECISION MAKING: THE CASE OF VENTURE CAPITALISTS'''
{{header}}
{{article
|author= Rohit Aggarwal,Harpreet Singh,
|source= MIS QUARTERLY
|year= 2013
|abstract = In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.
|keyword = UGC,WOM,Blogs,IT ventures,VC funding,venture capital,econometric analysis,multistage decision making,screening stage,choice stage,contract stage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CHANGES IN EMPLOYEES' JOB CHARACTERISTICS DURING AN ENTERPRISE SYSTEM IMPLEMENTATION: A LATENT GROWTH MODELING PERSPECTIVE'''
{{header}}
{{article
|author= Hillol Bala,Viswanath Venkatesh,
|source= MIS QUARTERLY
|year= 2013
|abstract = Enterprise system implementations often create tension in organizations. On the one hand, these systems can provide significant operational and strategic benefits. On the other hand, implementation of these systems is risky and a source of major disruptions. In particular, employees experience significant changes in their work environment during an implementation. Although the relationship between ES implementations and employees' jobs has been noted in prior research, there is limited research on the nature, extent, determinants, and outcomes of changes in employees' job characteristics following an ES implementation. This paper develops and tests a model, termed the job characteristics change model (JCCM), that posits that employees will experience substantial changes in two job characteristics (i.e., job demands and job control) during the shakedown phase (i.e., immediately after the rollout) of an ES implementation. These changes are theorized to be predicted by work process characteristics, namely perceived process complexity, perceived process rigidity, and perceived process radicalness, that in turn will be influenced by technology characteristics (i.e., perceived technology complexity, perceived technology reconfigurability, and perceived technology customization). JCCM further posits that changes in job characteristics will influence employees' job satisfaction. Longitudinal field studies conducted in two organizations (N = 281 and 141 respectively) provided support for the model. The scientific and practical implications of the findings are discussed.
|keyword = Enterprise systems,business process,work process,job characteristics,job demands,job control,job satisfaction,latent growth modeling,process characteristics,technology characteristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ADDRESSING THE PERSONALIZATION-PRIVACY PARADOX: AN EMPIRICAL ASSESSMENT FROM A FIELD EXPERIMENT ON SMARTPHONE USERS'''
{{header}}
{{article
|author= Juliana Sutanto,Elia Palme,Chuan-Hoo Tan,Chee Wei Phang,
|source= MIS QUARTERLY
|year= 2013
|abstract = Privacy has been an enduring concern associated with commercial information technology ( IT) applications, in particular regarding the issue of personalization. IT-enabled personalization, while potentially making the user computing experience more gratifying, often relies heavily on the user's personal information to deliver individualized services, which raises the user's privacy concerns. We term the tension between personalization and privacy, which follows from marketers exploiting consumers' data to offer personalized product information, the personalization-privacy paradox. To better understand this paradox, we build on the theoretical lenses of uses and gratification theory and information boundary theory to conceptualize the extent to which privacy impacts the process and content gratifications derived from personalization, and how an IT solution can be designed to alleviate privacy concerns. Set in the context of personalized advertising applications for smartphones, we propose and prototype an IT solution, referred to as a personalized, privacy-safe application, that retains users' information locally on their smartphones while still providing them with personalized product messages. We validated this solution through a field experiment by benchmarking it against two more conventional applications: a base non-personalized application that broadcasts non-personalized product information to users, and a personalized, non-privacy safe application that transmits user information to a central marketer's server. The results show that (compared to the non-personalized application), while personalized, privacy-safe or not increased application usage (reflecting process gratification), it was only when it was privacy-safe that users saved product messages (reflecting content gratification) more frequently. Follow-up surveys corroborated these nuanced findings and further revealed the users' psychological states, which explained our field experiment results. We found that saving advertisements for content gratification led to a perceived intrusion of information boundary that made users reluctant to do so. Overall our proposed IT solution, which delivers a personalized service but avoids transmitting users' personal information to third parties, reduces users' perceptions that their information boundaries are being intruded upon, thus mitigating the personalization-privacy paradox and increasing both process and content gratification.
|keyword = Personalization-privacy paradox,mobile advertising applications,uses and gratification,information boundary theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN INVESTIGATION OF INFORMATION SYSTEMS USE PATTERNS: TECHNOLOGICAL EVENTS AS TRIGGERS, THE EFFECT OF TIME, AND CONSEQUENCES FOR PERFORMANCE'''
{{header}}
{{article
|author= Ana Ortiz de Guinea,Jane Webster,
|source= MIS QUARTERLY
|year= 2013
|abstract = Information systems use represents one of the core concepts defining the discipline. In this article, we develop a rich conceptualization of IS use patterns as individuals' emotions, cognition, and behaviors while employing an information technology to accomplish a work-related task. By combining two novel perspectives-the affect-object paradigm and automaticity-with coping theory, we theorize how different patterns appear and disappear as a result of different IT events-expected and discrepant-as well as over time, and how these patterns influence short-term performance. In order to test our hypotheses, we conducted two studies, one qualitative and the other quantitative, that combined different methods (e. g., open-ended questions, physiological data, videos, protocol analysis) to study the influence of expected and discrepant events. The synergistic properties of the two studies demonstrate the existence of two IS use patterns, automatic and adjusting. Most interactions are automatic, and adjusting patterns, triggered by discrepant IT events, fade over time and transition into automatic ones. Further, automatic patterns result in enhanced short-term performance, while adjusting ones do not. Our conceptualization of IS use patterns is useful because it addresses important questions (such as why negative IT perceptions persist) and clarifies that it is how (rather than how much) people use IT that is pertinent for performance.
|keyword = Emotion,affect,behavior,cognition,performance,pattern,IS use,usage,heart rate,EKG,physiology,physiological arousal,automaticity,continuance,technological effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INSIDERS' PROTECTION OF ORGANIZATIONAL INFORMATION ASSETS: DEVELOPMENT OF A SYSTEMATICS-BASED TAXONOMY AND THEORY OF DIVERSITY FOR PROTECTION-MOTIVATED BEHAVIORS'''
{{header}}
{{article
|author= Clay Posey,Tom L. Roberts,Paul Benjamin Lowry,Rebecca J. Bennett,James F. Courtney,
|source= MIS QUARTERLY
|year= 2013
|abstract = Protecting information from a variety of security threats is a daunting organizational activity. Organization managers must recognize the roles that organization insiders have in protecting information resources rather than solely relying upon technology to provide this protection. Unfortunately, compared to negative insider behaviors, the extant literature provides sparse coverage of beneficial insider activities. The few beneficial activities in the literature represent only a small portion of the diverse collection of insiders' protective actions. This research focuses on protection-motivated behaviors (PMBs), which are volitional behaviors enacted by organization insiders to protect (1) organizationally relevant information and (2) the computer-based information systems in which the information is stored, collected, disseminated, and/or manipulated from information security threats. Based on systematics, we propose a six-step methodology of qualitative and quantitative approaches to develop a taxonomy and theory of diversity for PMBs. These approaches integrate the classification techniques of multidimensional scaling (MDS), property fitting (ProFit), and cluster analyses. We leverage these techniques to identify and display how insiders collectively classify 67 unique PMBs and their homogeneous classes. Our taxonomy provides researchers and practitioners a comprehensive guide and common nomenclature for PMBs. Our methodology can be similarly used to create other theories of diversity.
|keyword = Protection-motivated behaviors,behavioral information security,systematics,theory of diversity,multidimensional scaling,cluster analysis,taxonomy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTROL BALANCING IN INFORMATION SYSTEMS DEVELOPMENT OFFSHORING PROJECTS'''
{{header}}
{{article
|author= Robert Wayne Gregory,Roman Beck,Mark Keil,
|source= MIS QUARTERLY
|year= 2013
|abstract = While much is known about selecting different types of control that can be exercised in information systems development projects, the control dynamics associated with ISD offshoring projects represent an important gap in our understanding. In this paper, we develop a substantive grounded theory of control balancing that addresses this theoretical gap. Based on a longitudinal case study of an ISD offshoring project in the financial services industry, we introduce a three-dimensional control configuration category that emerged from our data, suggesting that control type is only one dimension on which control configuration decisions need to be made. The other two dimensions that we identified are control degree (tight versus relaxed) and control style (unilateral versus bilateral). Furthermore, we illustrate that control execution during the life cycle of an ISD offshoring project is highly intertwined with the development of client-vendor shared understanding and that each influences the other. Based on these findings, we develop an integrative process model that explains how offshoring project managers make adjustments to the control configuration periodically to allow the ISD offshoring project and relationship to progress, yielding the iterative use of different three-dimensional control configurations that we conceptualize in the paper. Our process model of control balancing may trigger new ways of looking at control phenomena in temporary interfirm organizations such as client-vendor ISD offshoring projects. Implications for research on organizational control and ISD offshoring are discussed. In addition, guidelines for ISD offshoring practitioners are presented.
|keyword = Control balancing,control dynamics,organizational control,information systems development,offshoring projects,outsourcing relationships,longitudinal case study,grounded theory,process model,project management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MEDIA SELECTION AS A STRATEGIC COMPONENT OF COMMUNICATION'''
{{header}}
{{article
|author= Joey F. George,John R. Carlson,Joseph S. Valacich,
|source= MIS QUARTERLY
|year= 2013
|abstract = Why do people select the media they choose for a particular type of communication? The media choice literature has considered myriad contextual factors that influence media choice, from proximity of the communication partners, to the urgency of the situation, to time pressure, and so on. From this body of work, a contingency-based theory of media choice has emerged. An alternative approach is to investigate how communication strategies and media characteristics affect choice. We identified two approaches for investigating these issues: Te'eni's (2001) model of organizational communication and Dennis et al.'s (2008) media synchronicity theory. Using a scenario-based methodology, we asked respondents which medium they would use for a deceptive communication task and why they made that choice. We analyzed the data from the perspective of both the Te'eni and MST frameworks, enabling us to compare the extent to which each was able to explain our respondents' media choices. Both frameworks, at differing levels of communication granularity, suggest that the intent of the communication drives a strategy that ultimately informs media choice. The results suggest that the prior contingency-based explanations of media choice could be improved by not only understanding the intent of the communication, but also the strategy used by an individual to execute this communication. Additionally, we found that the more finely grained view of communication contained in MST explained more of the outcomes and was more parsimonious as well.
|keyword = Computer-mediated communication,deceptive communication,media synchronicity theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFERRING APP DEMAND FROM PUBLICLY AVAILABLE DATA'''
{{header}}
{{article
|author= Rajiv Garg,Rahul Telang,
|source= MIS QUARTERLY
|year= 2013
|abstract = With an abundance of products available online, many online retailers provide sales rankings to make it easier for consumers to find the best-selling products. Successfully implementing product rankings online was done a decade ago by Amazon, and more recently by Apple's App Store. However, neither market provides actual download data, a very useful statistic for both practitioners and researchers. In the past, researchers developed various strategies that allowed them to infer demand from rank data. Almost all of that work is based on an experiment that shifts sales or collaboration with a vendor to get actual sales data. In this research, we present an innovative method to use public data to infer the rank-demand relationship for the paid apps on Apple's iTunes App Store. We find that the top-ranked paid app for iPhone generates 150 times more downloads compared to the paid app ranked at 200. Similarly, the top paid app on iPad generates 120 times more downloads compared to the paid app ranked at 200. We conclude with a discussion on an extension of this framework to the Android platform, in-app purchases, and free apps.
|keyword = Mobile apps,app store,sales-rank calibration,app downloads,pareto distribution,Android,Apple iTunes,in-app purchase,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SENSEMAKING AND SUSTAINABLE PRACTICING: FUNCTIONAL AFFORDANCES OF INFORMATION SYSTEMS IN GREEN TRANSFORMATIONS'''
{{header}}
{{article
|author= Stefan Seidel,Jan Recker,Jan vom Brocke,
|source= MIS QUARTERLY
|year= 2013
|abstract = This paper explores how a world-wide operating software solutions provider implemented environmentally sustainable business practices in response to emerging environmental concerns. Through an interpretive case study, we develop a theoretical framework that identifies four important functional affordances originating in information systems, which are required in environmental sustainability transformations as they create an actionable context in which (1) organizations can engage in a sensemaking process related to understanding emerging environmental requirements, and (2) individuals can implement environmentally sustainable work practices. Through our work, we provide several contributions, including a better understanding of IS-enabled organizational change and the types of functional affordances of information systems that are required in sustainability transformations. We describe implications relating to (1) how information systems can contribute to the creation of environmentally sustainable organizations, (2) the design of information systems to create required functional affordances, (3) the management of sustainability transformations, and (4) the further development of the concept of functional affordances in IS research.
|keyword = Green IS,environmental sustainability,business transformation,case study,socio-technical systems theory,functional affordances,sensemaking,sustainable practicing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ASSESSING THE EFFECTS OF BENEFITS AND INSTITUTIONAL INFLUENCES ON THE CONTINUED USE OF ENVIRONMENTALLY MUNIFICENT BYPASS SYSTEMS IN LONG-HAUL TRUCKING'''
{{header}}
{{article
|author= Kent Marett,Robert F. Otondo,G. Stephen Taylor,
|source= MIS QUARTERLY
|year= 2013
|abstract = Commercial truck driving is an essential part of the national supply chain but one that adversely affects the environment. The purpose of this study is to determine the influence of the potential environmental benefits, among other factors, on continued use of bypass systems that can be discontinued at any time by a driver. The results from our study show that (1) economic benefits and industry pressures positively influence drivers' use of bypass systems but (2) the environmental benefits of the technology do not, even though system vendors and state transportation agencies emphasize these benefits of the technology. Based on these findings, we conclude that sustainable information systems can be a viable option in a business context if usage leads to economic benefits. Our results and conclusions support the U. S. Environmental Protection Agency's differentiation of public policy versus business perspectives on sustainable technology.
|keyword = Sustainable information systems,energy informatics,commercial trucking,intelligent transportation systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MOTIVATING ENERGY-EFFICIENT BEHAVIOR WITH GREEN IS: AN INVESTIGATION OF GOAL SETTING AND THE ROLE OF DEFAULTS'''
{{header}}
{{article
|author= Claire-Michelle Loock,Thorsten Staake,Frederic Thiesse,
|source= MIS QUARTERLY
|year= 2013
|abstract = This study investigates the role of information systems in stimulating energy-efficient behavior in private households. We present the example of Velix, a web portal designed to motivate customers of a utility company to reduce their electricity consumption. In particular, we consider the effectiveness of goal setting functionality and defaults in influencing energy conservation behavior. For this purpose, we use the web portal as a test of the theoretical propositions underlying its design. Based on data collected from a field experiment with 1,791 electricity consumers, we test hypotheses regarding the structural relations between defaults and goals, the impact of defaults and goals on consumption behavior, and the moderating role of feedback on goal choice. Our results confirm the positive impact of goal setting on energy conservation. We show that default goals lead to statistically significant savings by affecting goal choice. However, if the default goals are set too low or too high with respect to a self-set goal, the defaults will detrimentally affect behavior. We also show that feedback on goal attainment moderates the effect of default goals on goal choice. The results extend the knowledge on goal setting and defaults and have implications for the design of effective energy feedback systems. The study's approach, which combines hypothesis-driven work and design-oriented IS research, could serve as a blueprint for further research endeavors of this kind, particularly with regard to feedback systems based on future smart metering infrastructures.
|keyword = Green IS,energy conservation,consumption feedback,goal setting,defaults,field experiment,design research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Too Big to Fail: Large Samples and the p-Value Problem'''
{{header}}
{{article
|author= Mingfeng Lin,Jr. Henry C. Lucas,Galit Shmueli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The Internet has provided IS researchers with the opportunity to conduct studies with extremely large samples, frequently well over 10,000 observations. There are many advantages to large samples, but researchers using statistical inference must be aware of the p-value problem associated with them. In very large samples, p-values go quickly to zero, and solely relying on p-values can lead the researcher to claim support for results of no practical significance. In a survey of large sample IS research, we found that a significant number of papers rely on a low p-value and the sign of a regression coefficient alone to support their hypotheses. This research commentary recommends a series of actions the researcher can take to mitigate the p-value problem in large samples and illustrates them with an example of over 300,000 camera sales on eBay. We believe that addressing the p-value problem will increase the credibility of large sample IS research as well as provide more insights for readers.
|keyword = empirical modeling,practical significance,effect size,p-value,statistical significance,inference,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Blunting Damocles' Sword: A Longitudinal Model of Healthcare IT Impact on Malpractice Insurance Premium and Quality of Patient Care'''
{{header}}
{{article
|author= Nirup M. Menon,Rajiv Kohli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Prior studies on the business value of information technology (IT) mainly focus on the impact of IT investments on productivity and firm profitability. Few have considered its implication on expected and actual product or service quality. This paper fills this gap by investigating the impact of past healthcare IT (HIT) expenditure on the malpractice insurance premium (MIP) and the moderating effect of past HIT expenditure on the relationship between past MIP and current quality of patient care in a longitudinal model. Based on archival panel data on costs, operations, and patient care outcomes of 66 hospitals in the U. S. state of Washington from 1998 to 2007, we find that past HIT expenditure is negatively associated with MIP, supporting our argument that HIT provides value that is anticipated by insurers and is captured by a change in MIP. We find that past HIT is positively associated with quality of patient care. We also find that past MIP is positively associated with quality of patient care, supporting the premise that hospitals respond to MIP by making risk mitigation efforts. However, we find that past HIT moderates this relationship negatively, suggesting a reliance on HIT at the expense of risk mitigation.
|keyword = business value of IT,organizational risk,hospital,dynamic panel model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Knowing It to "Getting It": Envisioning Practices in Computer Games Development'''
{{header}}
{{article
|author= Joe Nandhakumar,Nikiforos S. Panourgias,Harry Scarbrough,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The development of information systems and software applications increasingly needs to deliver culturally rich and affective experiences for user groups. In this paper, we explore how the collaborative practices across different expert groups can enable this experiential dimension of use to be integrated into the development of a software product. In an empirical study of computer games development-an arena in which the novelty and richness of the user experience is central to competitive success-we identify the challenges of conceptualizing and realizing a desired user experience when it cannot be readily specified in an initial design template, nor represented within the expertise of existing groups. Our study develops a theoretical framework to address these challenges. Through this framework, we are able to show how achieving a desired user experience requires developer groups to not only work across the boundaries that arise from specialized expertise, but also across wider fields centred on cultural production and software development, respectively. We find that their ability to do this is supported by distinctive "envisioning practices" that sustain an emerging shared "vision" for each game. The key research contributions that we then make are (a) grounding envisioning practices as a means of theorizing the collaborative practices centred on conceptualizing the user experience; (b) identifying how these practices are interwoven with the "producing practices" of software development, thus enabling collaboration to span expert groups and disparate fields; and (c) theorizing the role of vision as an emerging conceptual boundary object in these practices.
|keyword = collaborative practice,envizioning,interpretive,computer games development,emergence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do Recommender Systems Manipulate Consumer Preferences? A Study of Anchoring Effects'''
{{header}}
{{article
|author= Gediminas Adomavicius,Jesse C. Bockstedt,Shawn P. Curley,Jingjing Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.
|keyword = anchoring effects,behavioral decision making,behavioral economics,electronic commerce,experimental research,preferences,recommender systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology Competencies, Organizational Agility, and Firm Performance: Enabling and Facilitating Roles'''
{{header}}
{{article
|author= Anindita Chakravarty,Rajdeep Grewal,V. Sambamurthy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The hypercompetitive aspects of modern business environments have drawn organizational attention toward agility as a strategic capability. Information technologies are expected to be an important competency in the development of organizational agility. This research proposes two distinct roles to understand how information technology competencies shape organizational agility and firm performance. In their enabling role, IT competencies are expected to directly enhance entrepreneurial and adaptive organizational agility. In their facilitating role, IT competencies should enhance firm performance by helping the implementation of requisite entrepreneurial and adaptive actions. Furthermore, we argue that the effects of the dual roles of IT competencies are moderated by multiple contingencies arising from environmental dynamism and other sources. We test our model and hypotheses through a latent class regression analysis on data from a sample of 109 business-to-business electronic marketplaces. The results provide support for the enabling and facilitating roles of IT competencies. Moreover, we find that these dual effects vary according to environmental dynamism. The results suggest that managers should account for (multiple) contingencies (observed and unobserved) while assessing the effects of IT competencies on organizational agility and firm performance.
|keyword = organizational agility,IT competencies,latent class regression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product-Oriented Web Technologies and Product Returns: An Exploratory Study'''
{{header}}
{{article
|author= Prabuddha De,Yu (Jeffrey) Hu,Mohammad S. Rahman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Internet retailers have been making significant investments in Web technologies, such as zoom, alternative photos, and color swatch, that are capable of providing detailed product-oriented information and, thereby, mitigating the lack of "touch and feel," which, in turn, is expected to lower product returns. However, a clear understanding of the relationship between these technologies and product returns is still lacking. Our study attempts to fill this gap by using several econometric models to explore the said relationship. Our unique and rich data set from a women's clothing company allows us to measure technology usage at the product level for each consumer. The results show that, in this context, zoom usage has a negative coefficient, suggesting that a higher use of the zoom technology is associated with fewer returns. Interestingly, we find that a higher use of alternative photos is associated with more returns and, perhaps more importantly, with lower net sales. Color swatch, on the other hand, does not seem to have any effect on returns. Thus, our findings show that different technologies have different effects on product returns. We provide explanations for these findings based on the extant literature. We also conduct a number of tests to ensure the robustness of the results.
|keyword = online shopping,product returns,type of information,product-oriented technologies,econometric models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Real Options Model for Generalized Meta-Staged Projects-Valuing the Migration to SOA'''
{{header}}
{{article
|author= Suvankar Ghosh,Xiaolin Li,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This paper develops an innovative real options (RO) model for valuing multistage information technology (IT) projects that can be viewed as comprising meta stages. In RO literature, multistage investment programs have been treated as either interproject or intraproject programs, with intraproject programs being evaluated using n-fold Geske compound options and interproject programs valued using the so-called "subsidy-to-exercise price" logic. Our innovative RO model integrates the Geske compound option model with the subsidy-to-exercise price approach to value sequential investment programs that are neither purely interproject nor purely intraproject in nature but are composed of meta-stages. A meta-stage as a whole can be considered an interproject stage resulting in cash flows, but internally it consists of several intraproject stages that do not result in cash flows. We show that a key problem in IT, which is migrating to a Service-Oriented Architecture (SOA) for integrating a firm's many disparate applications, systems, data, and business processes, is best viewed as an investment program comprising meta-stages. Examining SOA migration from an RO lens is particularly apt at this time not only because of the importance of SOA but also because doubts have surfaced about the value of SOA. We illustrate our RO model by applying it to the simulated case of a firm migrating to SOA. We also develop a software tool based on the Mathematica T computational platform so that practitioners can easily apply our innovative options pricing model to determine the true value of SOA in their business contexts.
|keyword = business value of IT,real options,economics of IS,service-oriented architecture (SOA),enterprise systems,analytical modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Outsourcing Contracts and Equity Prices'''
{{header}}
{{article
|author= Deepa Mani,Anitesh Barua,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We investigate the impact of outsourcing on the long-term market performance of the firm. Outsourcing initiatives vary in terms of uncertainty in business requirements, complexity of coordination between the outsourcing firm and provider, and the consequent choice of the governing contract (fixed or variable price). Using theories from institutional economics, strategy, and information systems, we argue that firms pursuing large-scale, fixed price outsourcing, which are characterized by lower business uncertainty and simpler coordination requirements, will realize higher market returns relative to similar firms in the same industry who did not outsource. In contrast, variable price contracts that proxy for higher business uncertainty and coordination complexity may have a higher risk of failure and loss of shareholder value; however, prior outsourcing experience and prior association with the vendor may reduce uncertainty in the outsourcing relationship to help the outsourcing firm better manage challenges associated with complex, variable price engagements. We posit that financial markets are either not privy to or unlikely to accurately interpret such intangible information on the antecedents of outsourcing success during the announcement period. The delay in incorporation of this information in market prices results in positive long-term abnormal returns to fixed price contracts. Variable price contracts characterized by prior association between participant firms and greater outsourcing experience also realize positive long-term abnormal returns. Data on the hundred largest outsourcing initiatives implemented between 1996 and 2005 strongly support our hypotheses. The results imply that firms who retain simple functions and tasks in-house as well as those who outsource complex functions without pertinent experience or association with the vendor experience significant loss of shareholder value.
|keyword = stock return,event study,business value of IT,outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Valuation and Confirmation Bias in Virtual Communities: Evidence from Stock Message Boards'''
{{header}}
{{article
|author= JaeHong Park,Prabhudev Konana,Bin Gu,Alok Kumar,Rajagopal Raghunathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Virtual communities continue to play a greater role in social, political, and economic interactions. However, how users value information from these communities and how that affects their behavior and future expectations is not fully understood. Stock message boards provide an excellent setting to analyze these issues given the large user base and market uncertainty. Using data from 502 investor responses from a field experiment on one of the largest message board operators in South Korea, our analyses revealed that investors exhibit confirmation bias, whereby they preferentially treat messages that support their prior beliefs. This behavior is more pronounced for investors with higher perceived knowledge about the market and higher strength of belief (i.e., sentiment) toward a particular stock. We also find a negative interaction effect between the perceived knowledge and the strength of prior belief on confirmation bias. Those exhibiting confirmation bias are also more overconfident; as a result, they trade more actively and expect higher market returns than is warranted. Collectively, these results suggest that participation in virtual communities may not necessarily lead to superior financial returns.
|keyword = confirmation bias,overconfidence,investment decisions,virtual communities,stock message boards,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Licensing and Competition for Services in Open Source Software'''
{{header}}
{{article
|author= Terrence August,Hyoduk Shin,Tunay I. Tunca,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Open source software is becoming increasingly prominent, and the economic structure of open-source development is changing. In recent years, firms motivated by revenues from software services markets have become the primary contributors to open-source development. In this paper we study the role of services in open source software development and explore the choice between open source and proprietary software. Specifically, our economic model jointly analyzes the investment and pricing decisions of the originators of software and of subsequent open-source contributors. We find that if a contributor is efficient in software development, the originator should adopt an open-source strategy, allowing the contributor to offer higher total quality and capture the higher end of the market while the originator focuses on providing software services to lower end consumers. Conversely, if the contributor is not efficient in development, the originator should adopt a proprietary software development strategy, gaining revenue from software sales and squeezing the contributor out of the services market. In certain cases an increase in originator development efficiency can result in increased contributor profits. Finally, we find that, somewhat counterintuitively, an increase in contributor development efficiency can reduce overall social welfare.
|keyword = analytical modeling,competitive impacts of IS,economics of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Dynamic View of the Impact of Network Structure on Technology Adoption: The Case of OSS Development'''
{{header}}
{{article
|author= Gang Peng,Debabrata Dey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We examine how network centrality and closure, two key aspects of network structure, affect technology adoption. In doing so, we consider the content of potential information flows within the network and argue that the impact of network structure on technology adoption can be better understood by separately examining its impact from two groups of alters-current and potential adopters. We contend that increased network centrality and closure among current adopters contribute positively to adoption, whereas the same among potential adopters has exactly the opposite impact. Accordingly, we propose a dynamic view where the fraction of current adopters in the network positively moderates the impact of network centrality and closure. We empirically test the theory by analyzing the adoption of software version control technology by open source software projects. Our results strongly support the theory.
|keyword = technology adoption,network structure,network centrality,network closure,dynamic view,software version control technology,open source software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Value of Third-Party Assurance Seals in Online Retailing: An Empirical Investigation'''
{{header}}
{{article
|author= Koray Oezpolat,Guodong (Gordon) Gao,Wolfgang Jank,Siva Viswanathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Third-party quality assurance seals have emerged as a prominent mechanism to reduce uncertainty and increase purchase conversion in online markets. However, systematic studies of the effectiveness of these seals are scarce. In this study, we exploit a unique data set of 9,098 shopping sessions at an online retailer's website to empirically measure the value and effectiveness of assurance seals on the likelihood of purchase by shoppers. The data set is collected from a randomized field experiment conducted by a large seal provider, which enables us to infer the causal impacts of the presence of an assurance seal. We find strong evidence that the presence of the assurance seal increases the likelihood of purchase conversion. We discuss the implications of our findings for online retailers, third-party certifiers, policymakers, and researchers.
|keyword = electronic commerce,online certification,online assurance seals,trust seals,information asymmetry,field experiments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Continued Participation in Online Innovation Communities: Does Community Response Matter Equally for Everyone?'''
{{header}}
{{article
|author= Chen Zhang,Jungpil Hahn,Prabuddha De,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In this study, we focus on the factors that influence online innovation community members' continued participation in the context of open source software development ( OSSD) communities. Prior research on continued participation in online communities has primarily focused on social interactions among members and benefits obtained from these interactions. However, members of these communities often play different roles, which have been examined extensively, albeit in a separate stream of research. This study attempts to bridge these two streams of research by investigating the joint influence of community response and members' roles on continued participation. We categorize OSSD community members into users and modifiers and empirically examine the differential effects of community response across these roles. By analyzing a longitudinal data set of activities in the discussion forums of more than 300 OSSD projects, we not only confirm the positive influence of community response on members' continued participation but also find that community response is more influential in driving the continuance behavior of users than that of modifiers. In addition, this research highlights the importance of modifiers, a key subgroup of OSSD participants that has been largely overlooked by prior research.
|keyword = online innovation communities,open source software development (OSSD),continued participation,member roles,community response,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Intellectual Property Rights Enforcement on Open Source Software Project Success'''
{{header}}
{{article
|author= Wen Wen,Chris Forman,Stuart J. H. Graham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We investigate how intellectual property rights (IPR) enforcement against developers and users of open source software (OSS) affects the success of related OSS projects. We hypothesize that when an IPR enforcement action is filed, user interest and developer activity will be negatively affected in two types of related OSS projects-those that display technology overlap with the OSS application in dispute and business projects that are specific to the disputed OSS platform. We examine two widely publicized lawsuits-SCO v. IBM and FireStar/DataTern v. Red Hat-using data from SourceForge. net. Our difference-in-difference estimates show that in the months following the filing of SCO v. IBM, OSS projects that exhibit high technology overlap with the disputed OSS experienced a 15% greater decline in user interest and 45% less developer activity than projects in the control group; OSS projects that are intended for business and specific to the disputed OSS platform had a 34% greater decline in user interest and 86% less developer activity than the control group. We find similar results following the filing of FireStar/DataTern v. Red Hat. Our results are also robust to a variety of robustness checks, including a falsification exercise and subsample analyses.
|keyword = open source software (OSS),OSS success,intellectual property rights (IPR),intellectual property rights enforcement,difference-in-difference estimation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business Value of Information Technology: Testing the Interaction Effect of IT and R&D on Tobin's Q'''
{{header}}
{{article
|author= Indranil Bardhan,Viswanathan Krishnan,Shu Lin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The business case for investing in information technology (IT) has received increasing scrutiny in recent years. We propose that IT investments create additional business value through interactions with other business processes. In this paper, we formalize the interaction effect of IT by focusing on one core function, namely, research and development (R&D). We hypothesize that investments in IT can interact with and complement a firm's R&D investments, enhancing the firm's shareholder value creation potential. We test this by hypothesis by estimating the interaction impact of IT and R&D investments on Tobin's q, a forward-looking measure of firm performance using a recent multiyear, firm-level, archival data set. Our results suggest that the interaction effect of R&D and IT on Tobin's q is positive and significant after controlling for other firm-and industry-specific effects. Our findings provide rigorous empirical support for recent anecdotal evidence in the managerial literature with respect to the manner in which IT is enabling R&D-intensive innovation processes. Our analysis underscores the need for coordinated investments in IT and R&D, and permeating IT capabilities throughout other business processes such as R&D.
|keyword = R&D,IT investments,innovation,firm performance,complementarity,Tobin's q,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DISCOVERING UNOBSERVED HETEROGENEITY IN STRUCTURAL EQUATION MODELS TO AVERT VALIDITY THREATS'''
{{header}}
{{article
|author= Jan-Michael Becker,Arun Rai,Christian M. Ringle,Franziska Voelckner,
|source= MIS QUARTERLY
|year= 2013
|abstract = A large proportion of information systems research is concerned with developing and testing models pertaining to complex cognition, behaviors, and outcomes of individuals, teams, organizations, and other social systems that are involved in the development, implementation, and utilization of information technology. Given the complexity of these social and behavioral phenomena, heterogeneity is likely to exist in the samples used in IS studies. While researchers now routinely address observed heterogeneity by introducing moderators, a priori groupings, and contextual factors in their research models, they have not examined how unobserved heterogeneity may affect their findings. We describe why unobserved heterogeneity threatens different types of validity and use simulations to demonstrate that unobserved heterogeneity biases parameter estimates, thereby leading to Type I and Type II errors. We also review different methods that can be used to uncover unobserved heterogeneity in structural equation models. While methods to uncover unobserved heterogeneity in covariance-based structural equation models (CB-SEM) are relatively advanced, the methods for partial least squares (PLS) path models are limited and have relied on an extension of mixture regression-finite mixture partial least squares (FIMIX-PLS) and distance measure-based methods-that have mismatches with some characteristics of PLS path modeling. We propose a new method-prediction-oriented segmentation (PLS-POS)-to overcome the limitations of FIMIX-PLS and other distance measure-based methods and conduct extensive simulations to evaluate the ability of PLS-POS and FIMIX-PLS to discover unobserved heterogeneity in both structural and measurement models. Our results show that both PLS-POS and FIMIX-PLS perform
|keyword = Unobserved heterogeneity,validity,structural equation modeling,partial least squares,formative measures,prediction-oriented segmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLAINING EMPLOYEE JOB PERFORMANCE: THE ROLE OF ONLINE AND OFFLINE WORKPLACE COMMUNICATION NETWORKS'''
{{header}}
{{article
|author= Xiaojun Zhang,Viswanath Venkatesh,
|source= MIS QUARTERLY
|year= 2013
|abstract = By distinguishing between employees' online and offline workplace communication networks, this paper incorporates technology into social network theory to understand employees' job performance. Specifically, we conceptualize network ties as direct and indirect ties in both online and offline workplace communication networks, thus resulting in four distinct types of ties. We theorize that employees' ties in online and offline workplace communication networks are complementary resources that interact to influence their job performance. We found support for our model in a field study among 104 employees in a large telecommunication company. The paper concludes with theoretical and practical implications.
|keyword = Online networks,offline networks,communication networks,social networks,complementarity,job performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A DRAMATURGICAL MODEL OF THE PRODUCTION OF PERFORMANCE DATA'''
{{header}}
{{article
|author= Joao Vieira da Cunha,
|source= MIS QUARTERLY
|year= 2013
|abstract = The production of performance data in organizations is often described as a functional process that managers enforce on their employees to provide leaders with accurate information about employees' work and their achievements. This study draws on a 15-month ethnography of a desk sales unit to build a dramaturgical model that explains how managers participate in the production of performance data to impress rather than inform leaders. Research on management information systems is reviewed to outline a protective specification of this model where managers participate in the production of performance data to suppress information that threatens the image they present to leaders. Ethnographic data about the production and use of performance records and performance reports in a desk sales unit is examined to induce an exploitive specification of this dramaturgical model. This specification explains how people can take advantage of the opportunities, rather than just avoid the threats that performance data presents for impression management. It also demonstrates how managers can participate in the production of performance data to create an idealized version of their accomplishments and that leaders reify these data by using them in their own attempts at impressing others. By doing so, leaders and managers turn information systems into store windows to show achievement upward instead of transparent windows to monitor compliance downward.
|keyword = Management information systems,production of performance data,performance monitoring,implementation of information technology,ethnography,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHEN DOES TECHNOLOGY USE ENABLE NETWORK CHANGE IN ORGANIZATIONS? A COMPARATIVE STUDY OF FEATURE USE AND SHARED AFFORDANCES'''
{{header}}
{{article
|author= Paul M. Leonardi,
|source= MIS QUARTERLY
|year= 2013
|abstract = The goal of this study is to augment explanations of how newly implemented technologies enable network change within organizations with an understanding of when such change is likely to happen. Drawing on the emerging literature on technology affordances, the paper suggests that informal network change within interdependent organizational groups is unlikely to occur until users converge on a shared appropriation of the new technology's features such that the affordances the technology enables are jointly realized. In making the argument for the importance of shared affordances, this paper suggests that group-level network change has its most profound implications at the organization level when individuals use the same subset of a new information technology's features. To explore this tentative theory, we turn to a comparative, multimethod, longitudinal study of computer-based simulation technology use in automotive engineering. The findings of this explanatory case study show that engineers used the new technology for more than three months, during which time neither group experienced changes to their advice networks. Initially, divergent uses of the technology's features by engineers in both groups precluded them from being able to coordinate their work in ways that allowed them to structure their advice networks differently. Eventually, engineers in only one of the two groups converged on the use of a common set of the technology's features to enact a shared affordance. This convergence was necessary to turn the technology into a resource that could collectively afford group members the ability to compare their simulation outputs with one another and, in so doing, alter the content and structure of the group's advice network. The implications of these findings for the literatures on technology feature use, affordances, social networks, and post-adoption behaviors in organizations are discussed.
|keyword = Technology implementation,organizational change,advice networks,feature use,affordances,frames,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTEGRATING SERVICE QUALITY WITH SYSTEM AND INFORMATION QUALITY: AN EMPIRICAL TEST IN THE E-SERVICE CONTEXT'''
{{header}}
{{article
|author= Jingjun (David) Xu,Izak Benbasat,Ronald T. Cenfetelli,
|source= MIS QUARTERLY
|year= 2013
|abstract = Wixom and Todd (2005) integrated the user satisfaction and the technology acceptance literatures to theorize about and account for the influence of the information technology artifact on usage. Based on Wixom and Todd's integrated model of technology usage, we propose the 3Q model by investigating the role of service quality (SQ), in addition to system quality (SysQ) and information quality (IQ), in website adoption. Attention to SQ is critical, as consumer websites have increasingly become the target of SQ assessment made by consumers, not just traditional SysQ and IQ evaluations. As part of our study, we further theorize and empirically test the relationships among these three types of quality constructs and hypothesize that perceived SysQ influences perceived IQ and perceived SQ, and perceived IQ influences perceived SQ. Our study extends the Wixom and Todd model in the e-service context and is the first of its kind to empirically examine the combined impact of perceived SQ, perceived SysQ, and perceived IQ on usage intention. Our study advances the theoretical understanding of SQ and the relationships among perceptions of SysQ, IQ, and SQ in the e-service context. The results also inform practitioners that high IQ and SysQ can directly or indirectly improve SQ in the e-service context.
|keyword = Service quality (SQ),information quality (IQ),system quality (SysQ),service satisfaction,perceived enjoyment (PE),empirical,e-service,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TECHNOLOGICAL OBJECTS, SOCIAL POSITIONS, AND THE TRANSFORMATIONAL MODEL OF SOCIAL ACTIVITY'''
{{header}}
{{article
|author= Philip Faulkner,Jochen Runde,
|source= MIS QUARTERLY
|year= 2013
|abstract = The transformational model of social activity (TMSA), in many ways the centerpiece of critical realism, has been widely used in areas of information systems research. However, little has been done so far to develop a systematic theory of the nature, position, and identity of technological objects within the context of the TMSA. Our aim in this paper is to fill this gap, paying particular attention to the important category of nonmaterial technological objects that lie at the heart of modern information systems.
|keyword = Organizational theory,sociological theory,society,practice,societal change,position,socio-technical system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CRITICAL REALISM AND AFFORDANCES: THEORIZING IT-ASSOCIATED ORGANIZATIONAL CHANGE PROCESSES'''
{{header}}
{{article
|author= Olga Volkoff,Diane M. Strong,
|source= MIS QUARTERLY
|year= 2013
|abstract = Convincing arguments for using critical realism as an underpinning for theories of IT-associated organizational change have appeared in the Information Systems literature. A central task in developing such theories is to uncover the generative mechanisms by which IT is implicated in organizational change processes, but to do so, we must explain how critical realism's concept of generative mechanisms applies in an IS context. Similarly, convincing arguments have been made for using Gibson's (1986) affordance theory from ecological psychology for developing theories of IT-associated organizational change, but this effort has been hampered due to insufficient attention to the ontological status of affordances. In this paper, we argue that affordances are the generative mechanisms we need to specify and explain how affordances are a specific type of generative mechanism. We use the core principles of critical realism to argue how affordances arise in the real domain from the relation between the complex assemblages of organizations and of IT artifacts, how affordances are actualized over time by organizational actors, and how these actualizations lead to the various effects we observe in the empirical domain. After presenting these arguments, we reanalyze two published cases in the literature, those of ACRO and Autoworks, to illustrate how affordance-based theories informed by critical realism enhance our ability to explain IT-associated organizational change. These examples show how researchers using this approach should proceed, and how managers can use these ideas to diagnose and address IT implementation problems.
|keyword = Affordance,critical realism,generative mechanism,organizational change,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW SHOULD TECHNOLOGY-MEDIATED ORGANIZATIONAL CHANGE BE EXPLAINED? A COMPARISON OF THE CONTRIBUTIONS OF CRITICAL REALISM AND ACTIVITY THEORY'''
{{header}}
{{article
|author= David K. Allen,Andrew Brown,Stan Karanasios,Alistair Norman,
|source= MIS QUARTERLY
|year= 2013
|abstract = In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within "activity systems" and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the "inscription" of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.
|keyword = Critical realism,activity theory,theory,information systems,organization change,evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''METHODOLOGICAL IMPLICATIONS OF CRITICAL REALISM FOR MIXED-METHODS RESEARCH'''
{{header}}
{{article
|author= Markos Zachariadis,Susan Scott,Michael Barrett,
|source= MIS QUARTERLY
|year= 2013
|abstract = Building on recent developments in mixed methods, we discuss the methodological implications of critical realism and explore how these can guide dynamic mixed-methods research design in information systems. Specifically, we examine the core ontological assumptions of CR in order to gain some perspective on key epistemological issues such as causation and validity, and illustrate how these shape our logic of inference in the research process through what is known as retroduction. We demonstrate the value of a CR-led mixed-methods research approach by drawing on a study that examines the impact of ICT adoption in the financial services sector. In doing so, we provide insight into the interplay between qualitative and quantitative methods and the particular value of applying mixed methods guided by CR methodological principles. Our positioning of demi-regularities within the process of retroduction contributes a distinctive development in this regard. We argue that such a research design enables us to better address issues of validity and the development of more robust meta-inferences.
|keyword = IS research,critical realism,retroduction,mixed methods,qualitative and quantitative methods,econometric modeling,qualitative enquiry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE BROADER CONTEXT FOR ICT4D PROJECTS: A MORPHOGENETIC ANALYSIS'''
{{header}}
{{article
|author= James Muranga Njihia,Yasmin Merali,
|source= MIS QUARTERLY
|year= 2013
|abstract = This paper demonstrates the value of Archer's morphogenetic approach (MA) in understanding and explaining the complexity of the broader context within which many developing country information and communication technology (ICT) projects are implemented. It does this by using MA's analytical and explanatory apparatus to examine the evolution of the context of public sector ICT provision in Kenya over the period 1963-2006. In addition to demonstrating the practical value of MA, the paper contributes to the Information Systems literature on ICT for development (ICT4D). The analysis identifies (1) global normative pressures, polity, the national socio-economic base, disruptive technology, and the emergence of multistakeholder networks as key forces in shaping the evolutionary trajectory, (2) the explicit treatment of time and temporality as key for understanding mechanisms underpinning the evolutionary process, and (3) the difficulty of cleanly isolating the implementation of individual public sector ICT projects from the broader context and ICT4D agendas. The discussion elaborates on the features of MA found to be particularly valuable in this study. The paper concludes that explicitly attending to time and temporality, and to the broader context for ICT4D projects, would contribute to the development of more nuanced accounts of such projects and a more emancipatory outlook for ICT4D research.
|keyword = Morphogenetic approach,ICT4D,temporality,networks,emergent social systems,agency and structure,broader context,information systems,development,critical realism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE GENERATIVE MECHANISMS OF DIGITAL INFRASTRUCTURE EVOLUTION'''
{{header}}
{{article
|author= Ola Henfridsson,Bendik Bygstad,
|source= MIS QUARTERLY
|year= 2013
|abstract = The current literature on digital infrastructure offers powerful lenses for conceptualizing the increasingly interconnected information system collectives found in contemporary organizations. However, little attention has been paid to the generative mechanisms of digital infrastructure, that is, the causal powers that explain how and why such infrastructure evolves over time. This is unfortunate, since more knowledge about what drives digital infrastructures would be highly valuable for managers and IT professionals confronted by the complexity of managing them. To this end, this paper adopts a critical realist view for developing a configurational perspective of infrastructure evolution. Our theorizing draws on a multimethod research design comprising an in-depth case study and a case survey. The in-depth case study, conducted at a Scandinavian airline, distinguishes three key mechanisms of digital infrastructure evolution: adoption, innovation, and scaling. The case survey research of 41 cases of digital infrastructure then identifies and analyzes causal paths through which configurations of these mechanisms lead to successful evolution outcomes. The study reported in this paper contributes to the infrastructure literature in two ways. First, we identify three generative mechanisms of digital infrastructure and how they contingently lead to evolution outcomes. Second, we use these mechanisms as a basis for developing a configurational perspective that advances current knowledge about why some digital infrastructures evolve successfully while others do not. In addition, the paper demonstrates and discusses the efficacy of critical realism as a philosophical tradition for developing substantive contributions in the field of information systems.
|keyword = Digital infrastructure,case study,case survey,configuration theory,critical realism,generative mechanism,information infrastructure,multimethod,adoption,innovation,scaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CAUSAL EXPLANATION IN THE COORDINATING PROCESS: A CRITICAL REALIST CASE STUDY OF FEDERATED IT GOVERNANCE STRUCTURES'''
{{header}}
{{article
|author= Clay K. Williams,Elena Karahanna,
|source= MIS QUARTERLY
|year= 2013
|abstract = Large, multi-unit organizations are continually challenged to balance demands for centralization of information technology that lead to cost and service efficiencies through standardization while providing flexibility at the local unit level in order to meet unique business, customer, and service needs. This has led many organizations to adopt hybrid federated information technology governance (ITG) structures to find this balance. This approach to ITG establishes demand for various means to coordinate effectively across the organization to achieve the desired benefits. Past research has focused on the efficacy of various coordination mechanisms (e. g., steering committees, task forces) to coordinate activities related to information technology. However, we lack insights as to how and why these various coordination approaches help organizations achieve desired coordinated outcomes. This research specifically identifies coordinating as a process. Adopting the philosophy of critical realism, we conducted a longitudinal, comparative case study of two coordinating efforts in a federated ITG structure. Through a multifaceted approach to scientific logic employing deductive, inductive, and retroductive elements, we explicate two causal mechanisms, consensus making and unit aligning, which help to explain the coordinating process and the coordination outcomes observed in these efforts. We additionally elaborate the operation of the mechanisms through the typology of macro-micro-macro influences. Further, we demonstrate the value of the causal mechanisms to understanding the coordinating process by highlighting the complementarity in insights relative to the theories of power and politics and of rational choice. The study contributes to our understanding of coordinating as a process and of governance in federated IT organizations. Importantly, our study illustrates the value of applying critical realism to develop causal explanations and generate insights about a phenomenon.
|keyword = Critical realism,causal mechanisms,coordination,coordinating process,consensus-making mechanism,unit-aligning mechanism,federated IT governance,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLAINING BROADBAND ADOPTION IN RURAL AUSTRALIA: MODES OF REFLEXIVITY AND THE MORPHOGENETIC APPROACH'''
{{header}}
{{article
|author= Philip Dobson,Paul Jackson,Denise Gengatharen,
|source= MIS QUARTERLY
|year= 2013
|abstract = Universal fast broadband is currently being implemented by the Australian government. It is the largest single project in Australia's history. Represented as a nation-building exercise by the government and many public and private promoters, it is vilified by others as a massive waste of taxpayers' money. Ultimately the target of successful universal availability will require that metropolitan installations subsidize rural adoption. The take-up of these facilities, particularly in regional and remote areas, constitutes a complex, multi-factorial scenario in which political, personal, and organizational decisions are shaped by physical, cultural, economic, and ideological elements. Critical realism is proposed here as an aid for examining the complex reality of rural adoption for communities and small businesses in the regions. This article highlights the importance of considering individual reflexivity in explaining the adoption decision and potential adoption barriers.
|keyword = Critical realism,broadband adoption,internal conversation theory,morphogenetic approach,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Firm Strategy and the Internet in US Commercial Banking'''
{{header}}
{{article
|author= Kim Huat Goh,Robert J. Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = As information technology (IT) becomes more accessible, sustaining any competitive advantage from it becomes challenging. This has caused some critics to dismiss IT as a less valuable resource. We argue that, in addition to being able to generate strategic advantage, IT should also be viewed as a strategic necessity that prevents competitive disadvantage in rapidly changing business environments. We test a set of hypotheses on strategic advantage and strategic necessity in the context of Internet banking investments for the population of U. S. Federal Deposit Insurance Corporation (FDIC) banks from 2003 to 2005. We seek to understand whether their IT investments were made as a strategic choice or as a result of strategic necessity. Our econometric analysis suggests that IT investments (1) were made to complement firm strategy for strategic advantage as well as due to strategic necessity, and (2) paid off by enhancing firm performance and addressing the issue of strategic necessity. In addition, our analysis reveals the simultaneous relationship between performance and IT investments: high-performing banking firms appear to have been more likely to invest in IT. The econometric analysis methods that we employ made it possible for us to state all of our quantitative findings for the FDIC data to be stated after adjusting for this endogeneity through simultaneity.
|keyword = banking,econometrics,financial services IS and technology,Internet banking,IT investments,strategic advantage,strategic necessity,strategy,transaction costs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Health-Care Security Strategies for Data Protection and Regulatory Compliance'''
{{header}}
{{article
|author= Juhee Kwon,M. Eric Johnson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = This study identifies how security performance and compliance influence each other and how security resources contribute to two security outcomes: data protection and regulatory compliance. Using simultaneous equation models and data from 243 hospitals, we find that the effects of security resources vary for data breaches and perceived compliance and that security operational maturity plays an important role in the outcomes. In operationally mature organizations, breach occurrences hurt compliance, but, surprisingly, compliance does not affect actual security. In operationally immature organizations, breach occurrences do not affect compliance, whereas compliance significantly improves actual security. The results imply that operationally mature organizations are more likely to be motivated by actual security than compliance, whereas operationally immature organizations are more likely to be motivated by compliance than actual security. Our findings provide policy insights on effective security programs in complex health-care environments.
|keyword = compliance,data breach,health care,organizational maturity,security,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Cloud Computing: Should the IT Department Be Organized as a Cost Center or a Profit Center?'''
{{header}}
{{article
|author= Vidyanand Choudhary,Joseph Vithayathil,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = How does the adoption of cloud computing by a firm affect the organizational structure of its information technology (IT) department? To analyze this question, we consider an IT department that procures IT services from a cloud computing vendor and enhances these services for consuming units within the firm. Our model incorporates the competitive environment faced by the cloud vendor, which affects the price of the cloud vendor. We find that when the cloud vendor faces intense competition, the cost-center organizational model is preferred over the profit-center model. Infrastructure services such as basic storage, e-mail, and raw computing face intense competition, and our results suggest that such services be offered as a free corporate resource under the cost-center organizational structure. When the cloud vendor has pricing power, a profit-center organizational structure is likely to be preferred. Our results suggest that highly differentiated services such as cloud-based enterprise-wide enterprise resource planning or business intelligence be offered under the profit-center structure. Finally, the profit-center structure provides greater internal quality enhancement to cloud-based IT services than the cost center.
|keyword = chargeback,cloud,cloud computing,cost center,IaaS,IT governance,PaaS,profit center,SaaS,supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Channel Capabilities, Product Characteristics, and the Impacts of Mobile Channel Introduction'''
{{header}}
{{article
|author= Youngsok Bang,Dong-Joo Lee,Kunsoo Han,Minha Hwang,Jae-Hyeon Ahn,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Drawing on the notion of channel capability, we develop a theoretical framework for understanding the interactions between mobile and traditional online channels for products with different characteristics. Specifically, we identify two channel capabilities-access and search capabilities-that differentiate mobile and online channels, and two product characteristics that are directly related to the channel capabilities-time criticality and information intensity. Based on this framework, we generate a set of predictions on the differential effects of mobile channel introduction across different product categories. We test the predictions by applying a counterfactual analysis based on vector autoregression to a large panel data set from a leading e-market in Korea that covers a 28-month period and contains all of the transactions made through the online and mobile channels before and after the mobile channel introduction. Consistent with our theoretical predictions, our results suggest that the performance impact of the mobile channel depends on the two product characteristics and the resulting product-channel fit. We discuss implications for theory and multichannel strategy.
|keyword = counterfactual analysis,e-commerce,mobile commerce,multichannel strategy,multivariate baseline analysis,substitute and complement,times series,vector autoregression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Competitive Business Impact of Using Telemedicine for the Treatment of Patients with Chronic Conditions'''
{{header}}
{{article
|author= Balaraman Rajan,Abraham Seidman,Earl R. Dorsey,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The use of telemedicine to improve patients' health has been evolving rapidly over the past few years. Initially, our clinical research focus was on the development of effective ways for treating chronically ill patients, mostly those suffering from neurological disorders. While we identified the medical benefits of this information technology, there remains a salient strategic question addressing its competitive impact. In this paper, we analyze the impact of introducing telemedicine on the market share of the specialty hospital deploying this technology and on the competing hospitals in the region. Our analytical results prove that, contrary to earlier expectations, the value of telemedicine relative to in-person visits is not always increasing with the distance of the patient from the hospital. This result explains why patients located far from the specialty hospital may not prefer telemedicine care. We prove that telemedicine, unlike numerous other e-commerce applications, does not lead to the "winner takes all" phenomenon. We found that the advent of telemedicine changes the competitive equilibrium between specialty hospitals and community hospitals. Both hospital types will significantly benefit from delivering complementary care to chronic patients, rather than continuing to compete with each other.
|keyword = chronic conditions,community hospitals,economics of telemedicine,health-care competition,health-care IT,telemedicine,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competing with Piracy: A Multichannel Sequential Search Approach'''
{{header}}
{{article
|author= Xianjun Geng,Young-Jin Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = We consider an online market where consumers may obtain digital goods from two mutually exclusive channels: a legitimate channel consisting of many law-abiding retailers and a piracy channel consisting of many piracy services. We analyze consumer choice, retailer strategy, and piracy control using a sequential-search approach where information acquisition is costly for some consumers (nonshoppers), yet costless for others (shoppers). First, we show that a nonshopper's channel choice is determined by a simple comparison of two reservation prices. Second, we analyze how piracy threats affect in-channel pricing among retailers. If the in-channel competition intensity among retailers is high, piracy does not affect retailer pricing. If the intensity is medium, retailers respond to piracy by giving up some shoppers and, surprisingly, raising prices. If the intensity is low, the legitimate channel loses some shoppers as well as some nonshoppers to the piracy channel. Third, we consider several mechanisms for fighting piracy and analyze their effects on firm profit and consumer surplus. Reducing piracy quality and increasing piracy search costs are both effective in controlling piracy, yet they affect consumer surplus differently. Reducing the number of piracy services is less effective in controlling piracy.
|keyword = channel competition,digital good,digital piracy,price dispersion,search costs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network Structure and Observational Learning: Evidence from a Location-Based Social Network'''
{{header}}
{{article
|author= Zhan Shi,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = In recent years, there has been stellar growth of location-based/enabled social networks in which people can "check in" to physical venues they are visiting and share with friends. In this paper, we hypothesize that the "check-ins" made by friends help users learn the potential payoff of visiting a venue. We argue that this learning-in-a-network process differs from the classic observational learning model in a subtle yet important way: Rather than from anonymous others, the agents learn from their network friends, about whose tastes in experience goods the agents are better informed. The empirical analyses are conducted on a unique data set in which we observe both the explicit interpersonal relationships and their ensuing check-ins. The key result is that the proportion of checked-in friends is not positively associated with the likelihood of a new visit, rejecting the prediction of the conventional observational learning model. Drawing on the literature in sociology and computer science, we show that weighting the friends' check-ins by a parsimonious proximity measure can yield a more intuitive result than the plain proportion does. Repeated check-ins by friends are found to have a pronounced effect. Our empirical result calls for the revisiting of observational learning in a social network setting. It also suggests that practitioners should incorporate network proximity when designing social recommendation products and conducting promotional campaigns in a social network.
|keyword = experience goods,location-based social network,matrix factorization,observational learning,social effect,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Do Consumer Buzz and Traffic in Social Media Marketing Predict the Value of the Firm?'''
{{header}}
{{article
|author= Xueming Luo,Jie Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Consumer buzz in the form of user-generated reviews, recommendations, and blogs signals that consumer attitude and advocacy can influence firm value. Web traffic also affects brand awareness and customer acquisition, and is a predictor of the performance of a firm's stock in the market. The information systems and accounting literature have treated buzz and traffic separately in studying their relationships with firm performance. We consider the interactions between buzz and traffic as well as competitive effects that have been overlooked heretofore. To study the relationship between user-initiated Web activities and firm performance, we collected a unique data set with metrics for consumer buzz, Web traffic, and firm value. We employed a vector autoregression with exogenous variables model that captures the evolution and interdependence between the time series of dependent variables. This model enables us to examine a series of questions that have been raised but not fully explored to date, such as dynamic effects, interaction effects, and market competition effects. Our results support the dynamic relationships of buzz and traffic with firm value as well as the related mediation effects of buzz and traffic. They also reveal significant market competition effects, including effects of both a firm's own and its rivals' buzz and traffic. The findings also provide insights for e-commerce managers regarding Web site design, customer relation management, and how to best respond to competitors' strategic moves.
|keyword = consumer buzz,firm value,online reviews,social media,stock market performance,vector autoregression,Web traffic,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer Learning and Time-Locked Trials of Software Products'''
{{header}}
{{article
|author= Debabrata Dey,Atanu Lahiri,Dengpan Liu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The usefulness of a software product becomes obvious to consumers only after they get to experience it and, upon experiencing it, they may reach different conclusions regarding its true value. We examine the problem of designing free software trials under a general learning function. Our analyses lead to several new findings. We find that a time-locked trial is optimal only when the rate of learning is sufficiently large. It is not optimal in other situations, even when it has an overall positive effect on consumers' valuations. We also find that positive network effects have a minimal impact on this optimality. Interestingly, we find that neither the optimal trial period nor the optimal price is monotonically increasing in the rate of learning. At moderate rates, the software manufacturer pursues a dual strategy of offering a longer trial as well as a lower price. At higher rates of learning, the manufacturer does the opposite. Our results are robust, and incorporating possibilities such as a trial providing a signal of quality or learning being correlated with prior valuation has little impact on their applicability.
|keyword = consumer learning,experience goods,free trial,network effects,signaling,software trial,time-locked trial,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reducing Buyers' Uncertainty About Taste-Related Product Attributes'''
{{header}}
{{article
|author= Panos M. Markopoulos,Eric K. Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = It is becoming increasingly important for firms to know when to take steps to reduce buyers' uncertainty about products and services. This paper focuses on investments that firms can make to reduce buyers' uncertainty about taste-related product attributes. Using an analytical model, we show that firms should disclose more taste-related information when the customer segment they directly target represents a larger share of the overall market. We further show that there are practical ways by which managers can decide if such disclosure investments are financially beneficial to their firms. Specifically, we show that the variance of consumer reviews can guide such decisions. The paper's main contribution to the extant literature is to show that firms must consider the variance, but not the mean, of buyer reviews, to determine the need to invest in reducing consumer uncertainty about taste-related attributes. The papers's findings are managerially important due to the ubiquity of consumer reviews. They are novel because most of the previous literature views the mean of the review as the key indicator. Finally, they are general in their applicability since they are independent of any assumptions about heuristics that buyers may use to ascertain product quality from the reviews of previous buyers.
|keyword = consumer uncertainty reduction,information dissemination,product ratings,product review variance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Examination of the Antecedents and Consequences of Contribution Patterns in Crowd-Funded Markets'''
{{header}}
{{article
|author= Gordon Burtch,Anindya Ghose,Sunil Wattal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Crowd-funded markets have recently emerged as a novel source of capital for entrepreneurs. As the economic potential of these markets is now being realized, they are beginning to go mainstream, a trend reflected by the explicit attention crowdfunding has received in the American Jobs Act as a potential avenue for economic growth, as well as the recent focus that regulators such as the U.S. Securities and Exchange Commission have placed upon it. Although the formulation of regulation and policy surrounding crowd-funded markets is becoming increasingly important, the behavior of crowdfunders, an important aspect that must be considered in this formulation effort, is not yet well understood. A key factor that can influence the behavior of crowd funders is information on prior contribution behavior, including the amount and timing of others' contributions, which is published for general consumption. With that in mind, in this study, we empirically examine social influence in a crowd-funded marketplace for online journalism projects, employing a unique data set that incorporates contribution events and Web traffic statistics for approximately 100 story pitches. This data set allows us to examine both the antecedents and consequences of the contribution process. First, noting that digital journalism is a form of public good, we evaluate the applicability of two competing classes of economic models that explain private contribution toward public goods in the presence of social information: substitution models and reinforcement models. We also propose a new measure that captures both the amount and the timing of others' contribution behavior: contribution frequency (dollars per unit time). We find evidence in support of a substitution model, which suggests a partial crowding-out effect, where contributors may experience a decrease in their marginal utility from making a contribution as it becomes less important to the recipient. Further, we find that the duration of funding and, more importantly, the degree of exposure that a pitch receives over the course of the funding process, are positively associated with readership upon the story's publication. This appears to validate the widely held belief that a key benefit of the crowdfunding model is the potential it offers for awareness and attention-building around causes and ventures. This last aspect is a major contribution of the study, as it demonstrates a clear linkage between marketing effort and the success of crowd-funded projects.
|keyword = economics of IS,electronic commerce,crowdfunding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''All Are Not Equal: An Examination of the Economic Returns to Different Forms of Participation in Open Source Software Communities'''
{{header}}
{{article
|author= Il-Horn Hann,Jeffrey A. Roberts,Sandra A. Slaughter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Open source software (OSS) communities live and die with the continuous contributions of programmers who often participate without direct remuneration. An intriguing question is whether such sustained participation in OSS projects yields economic benefits to the participants. Moreover, as participants engage in OSS projects, they take on different roles and activities in the community. This raises additional questions of whether different forms of participation in OSS communities are associated with different economic rewards and, if so, in which contexts. In this paper, we draw upon theories of signaling and job matching to hypothesize that participants who possess "proof" of their skills in OSS projects are financially rewarded for their activities in the labor market. More specifically, we distinguish between participation in OSS communities that is associated with a signaling value for unobserved productivity characteristics and an additional value that accrues to participants whose OSS roles and activities match those in their paid employment. Following a cohort of OSS programmers over a six-year period, we empirically examine the wages and OSS performance of participants in three of the foremost OSS projects operating within the Apache Software Foundation. Controlling for individual characteristics and other wage-related factors, our findings reveal that credentials earned through a merit-based ranking system are associated with as much as an 18% increase in wages. Moreover, we find that participants who have OSS project management responsibilities receive additional financial rewards if their professional job is in IT management. These findings suggest that rank within an OSS meritocracy is a credible and precise signal of participants' productive capacity and that participants' roles and activities in an OSS community have additional financial value when aligned with their paid employment.
|keyword = open source software,signaling theory,job matching theory,labor economics,software industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Networks, Social Influence, and the Choice Among Competing Innovations: Insights from Open Source Software Licenses'''
{{header}}
{{article
|author= Param Vir Singh,Corey Phelps,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Existing research provides little insight into how social influence affects the adoption and diffusion of competing innovative artifacts and how the experiences of organizational members who have worked with particular innovations in their previous employers affect their current organizations' adoption decision. We adapt and extend the heterogeneous diffusion model from sociology and examine the conditions under which prior adopters of competing open source software (OSS) licenses socially influence how a new OSS project chooses among such licenses and how the experiences of the project manager of a new OSS project with particular licenses affects its susceptibility to this social influence. We test our predictions using a sample of 5,307 open source projects hosted at SourceForge. Our results suggest the most important factor determining a new project's license choice is the type of license chosen by existing projects that are socially closer to it in its inter-project social network. Moreover, we find that prior adopters of a particular license are more infectious in their influence on the license choice of a new project as their size and performance rankings increase. We also find that managers of new projects who have been members of more successful prior OSS projects and who have greater depth and diversity, of experience in the OSS community are less susceptible to social influence. Finally, we find a project manager is more likely to adopt a particular license type when his or her project occupies a similar social role as other projects that have adopted the same license. These results have implications for research on innovation adoption and diffusion, open source software licensing, and the governance of economic exchange.
|keyword = open source software license,social networks,innovation adoption and diffusion,social influence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Analysis of Technical Efficiency: The Role of IT Intensity and Competition'''
{{header}}
{{article
|author= Young Bong Chang,Vijay Gurbaxani,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We analyze the impact of information technology (IT) on the technical efficiency of firms in the context of their observed competitive settings. Because competition can be a driver of efficiency and industries display varying degrees of competitiveness, firm-level efficiency is likely to display considerable heterogeneity. To shed light on these questions, we analyze the economic impact of IT on technical efficiency, a key component of efficiency, in heterogeneous competitive settings. Our study employs a number of econometric techniques, including a stochastic frontier and a generalized method of moments approach, on data from firms in a wide cross-section of industries. We find, after controlling for firm-level heterogeneity and potential endogeneity, that IT is positively associated with gains in technical efficiency but its impact is moderated by the degree of competition. Firms display large variation in their levels of technical efficiency partly because of the heterogeneous market competitiveness conditions they face. In more competitive industries, firms tend to deploy IT more intensively and use it more efficiently. Our study makes a distinct contribution relative to prior studies that have focused on the productivity impacts of IT while assuming perfect competition and not allowing for potential heterogeneity in firm-level efficiency. Overall, our results demonstrate that IT and competition are significant determinants of gains in technical efficiency and provide insight into how competition affects the returns to IT investment.
|keyword = technical efficiency,competition,productivity,economics of IS,business value of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Privacy Concerns and Privacy-Protective Behavior in Synchronous Online Social Interactions'''
{{header}}
{{article
|author= Zhenhui (Jack) Jiang,Cheng Suang Heng,Ben C. F. Choi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Privacy is of prime importance to many individuals when they attempt to develop online social relationships. Nonetheless, it has been observed that individuals' behavior is at times inconsistent with their privacy concerns, e.g., they disclose substantial private information in synchronous online social interactions, even though they are aware of the risks involved. Drawing on the hyperpersonal framework and the privacy calculus perspective, this paper elucidates the interesting roles of privacy concerns and social rewards in synchronous online social Interactions by examining the causes and the behavioral strategies that individuals utilize to protect their privacy. An empirical study involving 251 respondents was conducted in online chat rooms. Our results indicate that individuals utilize both self-disclosure and misrepresentation to protect their privacy and that social rewards help explain why individuals may not behave in accordance with their privacy concerns. In addition, we find that perceived anonymity of others and perceived intrusiveness affect both privacy concerns and social rewards. Our findings also suggest that higher perceived anonymity of self decreases individuals' privacy concerns, and higher perceived media richness increases social rewards. Generally, this study contributes to the information systems literature by integrating the hyperpersonal framework and the privacy calculus perspective to identify antecedents of privacy trade-off and predict individuals' behavior in synchronous online social interactions.
|keyword = synchronous online social interactions,privacy concerns,privacy-protective behavior,social rewards,self-disclosure,misrepresentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Is the Mobile Internet Different? Search Costs and Local Activities'''
{{header}}
{{article
|author= Anindya Ghose,Avi Goldfarb,Sang Pil Han,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We explore how Internet browsing behavior varies between mobile phones and personal computers. Smaller screen sizes on mobile phones increase the cost to the user of browsing for information. In addition, a wider range of offline locations for mobile Internet usage suggests that local activities are particularly important. Using data on user behavior at a (Twitter-like) microblogging service, we exploit exogenous variation in the ranking mechanism of posts to identify the ranking effects. We show that (1) ranking effects are higher on mobile phones suggesting higher search costs: links that appear at the top of the screen are especially likely to be clicked on mobile phones and (2) the benefit of browsing for geographically close matches is higher on mobile phones: stores located in close proximity to a user's home are much more likely to be clicked on mobile phones. Thus, the mobile Internet is somewhat less "Internet-like": search costs are higher and distance matters more. We speculate on how these changes may affect the future direction of Internet commerce.
|keyword = mobile Internet,search costs,ranking effects,cognitive load,recency effects,local interests,microblogging,social media,hierarchical Bayesian methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Use to Effective Use: A Representation Theory Perspective'''
{{header}}
{{article
|author= Andrew Burton-Jones,Camille Grange,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Information systems must be used effectively to obtain maximum benefits from them. However, despite a great deal of research on when and why systems are used, very little research has examined what effective system use involves and what drives it. To move from use to effective use requires understanding an information system's nature and purpose, which in turn requires a theory of information systems. We draw on representation theory, which states that an information system is made up of several structures that serve to represent some part of the world that a user and other stakeholders must understand. From this theory, we derive a high-level framework of how effective use and performance evolve, as well as specific models of the nature and drivers of effective use. The models are designed to explain the effective use of any information system and offer unique insights that would not be offered by traditional views, which tend to consider information systems to be just another tool. We explain how our theory extends existing research, provides a rich platform for research on effective use, and how it contributes back to the theory of information systems from which it was derived.
|keyword = effective system use,performance,goals,representation theory,system structure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Motivational Differences Across Post-Acceptance Information System Usage Behaviors: An Investigation in the Business Intelligence Systems Context'''
{{header}}
{{article
|author= Xixi Li,J. J. Po-An Hsieh,Arun Rai,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We identify two post-acceptance information system (IS) usage behaviors related to how employees leverage implemented systems. Routine use (RTN) refers to employees' using IS in a routine and standardized manner to support their work, and innovative use (INV) describes employees' discovering new ways to use IS to support their work. We use motivation theory as the overarching perspective to explain RTN and INV and appropriate the rich intrinsic motivation (RIM) concept from social psychology to propose a conceptualization of RIM toward IS use, which includes intrinsic motivation toward accomplishment (IMap), intrinsic motivation to know (IMkw), and intrinsic motivation to experience stimulation (IMst). We also consider the influence of perceived usefulness (PU)-a representative surrogate construct of extrinsic motivation toward IS use on RTN and INV. We theorize the relative impacts of the RIM constructs and PU on RTN and INV and the role of personal innovativeness with IT (PIIT) in moderating the RIM constructs' influences on INV. Based on data from 193 employees using a business intelligence system at one of the largest telecom service companies in China, we found (1) PU had a stronger impact on RTN than the RIM constructs, (2) IMkw and IMst each had a stronger impact on INV than either PU or IMap, and (3) PIIT positively moderated the impact of each RIM construct on INV. Our findings provide insights on managing RTN and INV in the post-acceptance stage.
|keyword = post-acceptance stage,post-acceptance behaviors,routine use,innovative use,motivation theory,intrinsic motivation,business intelligence systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''To Personalize or Not to Personalize Online Purchase Interactions: Implications of Self-Selection by Retailers'''
{{header}}
{{article
|author= Sriram Thirumalai,Kingshuk K. Sinha,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Personalization technologies today enable retailers to tailor online purchase interactions to the individual preferences and needs of customers. With personalization being increasingly perceived as a source of competitive advantage, there is a growing trend toward pursuing technology-enabled personalization strategies in online retailing. However, the choice of a retailer whether or not to select into technology-enabled personalization and its implications for customer loyalty are at best ambiguous. This paper is an attempt to resolve this apparent ambiguity. Specifically, the paper conceptualizes retailer selection into technology-enabled personalization strategies relevant to two steps of an online purchase, namely, transaction personalization strategy and decision personalization strategy, based on the operating characteristics of a retailer. The implications of the retailers' self-selection into technology-enabled personalization strategies for customer loyalty are then empirically investigated with data collected from 422 retailers. Further, based on a counterfactual analysis, the paper reveals the implications of making a normatively incorrect decision with respect to personalization strategy. Contrary to popular belief, the results of this study indicate that personalization may not be uniformly beneficial in terms of customer loyalty to all retailers. Although a majority of retailers pursue transaction personalization and realize benefits by way of improved customer loyalty, we find that the choice of a retailer to pursue decision personalization is self-selected and dependent on idiosyncratic characteristics related to its operating context. Retailers that have relatively large-scale operations, provide greater variety and realize higher customer satisfaction with product selection, and that do not necessarily compete on price (i.e., realize lower customer satisfaction with prices relative to competing retailers) are more likely to pursue the decision personalization strategy. Although some retailers pursue decision personalization because they clearly stand to benefit from doing so, other retailers are better off not following suit. Theoretical contributions of the study, managerial implications of the study findings, limitations, and directions for future research are identified.
|keyword = personalization strategy,self-selection,customer loyalty,econometric analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Moving from Access to Use of the Information Infrastructure: A Multilevel Sociotechnical Framework'''
{{header}}
{{article
|author= Pradeep Racherla,Munir Mandviwalla,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Universal access (UA) to the Internet and the associated information infrastructure has become an important economic and societal goal. However, UA initiatives tend to focus on issues such as physical access and geographical ubiquity, and they measure adoption through penetration rates. In this paper, we apply an interpretive case study approach to analyze the Philadelphia wireless initiative to provide insights into the nature of UA and extend this concept to also consider universal use (UU). UU is important because simply providing access does not guarantee use. UU is presented as a conceptual goal that starts with the challenge of physical access, but which necessarily also leads to considerations of use. The results show that the human and technological elements underlying individual access and use are deeply embedded within various institutional elements and collectives that enable but also constrain meaningful use. We integrate our findings into a multilevel framework that shows how access and use are influenced by both micro and macro factors. This framework provides new insights into the study of the information infrastructure, digital divide, and public policy.
|keyword = universal access,universal use,digital divide,information infrastructure,broadband policy, telecommunications policy,sociotechnical systems,multilevel,interpretive case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On Risk Management with Information Flows in Business Processes'''
{{header}}
{{article
|author= Xue Bai,Ramayya Krishnan,Rema Padman,Harry Jiannan Wang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This article investigates the economic consequences of data errors in the information flows associated with business processes. We develop a process modeling-based methodology for managing the risks associated with such data errors. Our method focuses on the topological structure of a process and takes into account its effect on error propagation and risk mitigation using both expected loss and conditional value-at-risk risk measures. Using this method, optimal strategies can be designed for control resource allocation to manage risk in a business process. Our work contributes to the literature on both ex ante risk management-based business process design and ex post risk assessments of existing business processes and control models. This research applies not only to the literature on and practice of process design and risk management but also to business decision support systems in general. An order-fulfillment process of an online pharmacy is used to illustrate the methodology.
|keyword = business process management,control,information flow,expected loss,conditional value at risk,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact and Implications of On-Demand Services on Market Structure'''
{{header}}
{{article
|author= Pei-Yu Chen,Shin-Yi Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This paper considers on-demand services and its impact on market structure, firm profitability, and consumer welfare. The unique properties of on-demand services are the conversion of fixed costs to variable costs, removal of capacity constraint, and fast setup time (which enables quick entry by any firm at any time when there is opportunity), whereas privacy and security concerns and switching costs have been noted as the biggest barriers from adopting on-demand services. With a stylized model capturing these benefits and barriers to using on-demand services, we establish several results. First, we show that conversion of fixed cost to variable cost enables new and small firms to enter existing markets and leads to the creation of new markets. Second, we show that competition and the threat of new entrants can be an important driver of a firm's decision to switch to on-demand services. In addition, a firm's barriers to using on-demand services can influence another firm's entry decision. Third, we show that two identical firms may employ different technologies in equilibrium. Fourth, we show that fast setup time and removal of capacity constraint associated with on-demand services make it impossible for firms to make supranormal return and would lead to a perfect competitive market, even when there is only one firm, under very general conditions. Such a result still holds even when there exists an economy of scale (e.g., quantity discount) from using on-demand services. On the other hand, when there are barriers preventing firms from offering similar products and products are substantially differentiated, on-demand services can amplify this advantage of entry barriers by enabling firms to further increase prices and enhance their profitability. Therefore, contrary to the common belief that offering on-demand services is best for firms offering commodity products, we show on-demand services to be more profitable for firms with differentiated products.
|keyword = on-demand services,utility computing,cloud computing,outsourcing,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ascending Combinatorial Auctions with Allocation Constraints: On Game Theoretical and Computational Properties of Generic Pricing Rules'''
{{header}}
{{article
|author= Ioannis Petrakis,Georg Ziegler,Martin Bichler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Combinatorial auctions are used in a variety of application domains, such as transportation or industrial procurement, using a variety of bidding languages and different allocation constraints. This flexibility in the bidding languages and the allocation constraints is essential in these domains but has not been considered in the theoretical literature so far. In this paper, we analyze different pricing rules for ascending combinatorial auctions that allow for such flexibility: winning levels and deadness levels. We determine the computational complexity of these pricing rules and show that deadness levels actually satisfy an ex post equilibrium, whereas winning levels do not allow for a strong game theoretical solution concept. We investigate the relationship of deadness levels and the simple price update rules used in efficient ascending combinatorial auction formats. We show that ascending combinatorial auctions with deadness level pricing rules maintain a strong game theoretical solution concept and reduce the number of bids and rounds required at the expense of higher computational effort. The calculation of exact deadness levels is a Pi(P)(2)-complete problem. Nevertheless, numerical experiments show that for mid-sized auctions this is a feasible approach. The paper provides a foundation for allocation constraints in combinatorial auctions and a theoretical framework for recent Information Systems contributions in this field.
|keyword = electronic markets and auctions,economics of IS,electronic commerce,decision support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT Implementation Contract Design: Analytical and Experimental Investigation of IT Value, Learning, and Contract Structure'''
{{header}}
{{article
|author= D. J. Wu,Min Ding,Lorin M. Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This article analytically and experimentally investigates how firms can best capture the business value of information technology (IT) investments through IT contract design. Using a small sample of outsourcing contracts for enterprise information technology (EIT) projects in several industries-coupled with reviews of contracts used by a major enterprise software maker-the authors determine the common provisions and structural characteristics of EIT contracts. The authors use these characteristics to develop an analytical model of optimal contract design with principal-agent techniques. The model captures a set of key characteristics of EIT contracts, including a staged, multiperiod project structure; learning; probabilistic binary outcomes; variable fee structures; possibly risk-averse agents; and implementation risks. The model characterizes conditions under which multistage contracts enable clients to create and capture greater project value than single-stage projects, and how project staging enables firms to reduce project risks, capture learning benefits, and increase development effort. Finally, the authors use controlled laboratory experiments to complement their analytical approaches and demonstrate robustness of their key findings.
|keyword = analytical modeling,enterprise systems,economics of IS,management of IS projects,laboratory experiments,business value of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Status Locality on the Web: Implications for Building Focused Collections'''
{{header}}
{{article
|author= Gautam Pant,Padmini Srinivasan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Topical locality on the Web is the notion that pages tend to link to other topically similar pages and that such similarity decays rapidly with link distance. This supports meaningful Web browsing and searching by information consumers. It also allows topical Web crawlers, programs that fetch pages by following hyperlinks, to harvest topical subsets of the Web for applications such as those in vertical search and business intelligence. We show that the Web exhibits another property that we call "status locality." It is based on the notion that pages tend to link to other pages of similar status (importance) and that this status similarity also decays rapidly with link distance. Analogous to topical locality status locality may also be exploited by Web crawlers. Collections built by such crawlers include pages that are both topically relevant and also important. This capability is crucial because of the large numbers of Web pages addressing even niche topics. The challenge in exploiting status locality while crawling is that page importance (or status) is typically recognized through global measures computed by processing link data from billion of pages. In contrast, topical Web crawlers depend on local information based on previously downloaded pages. We solve this problem by using methods developed previously that utilize local characteristics of pages to estimate their global status. This leads to the design of new crawlers, specifically of utility-biased crawlers guided by a Cobb-Douglas utility function. Our crawler experiments show that status and topicality of Web collections present a trade-off. An adaptive version of our utility-biased crawler dynamically modifies output elasticities of topicality and status to create Web collections that maintain high average topicality. This can be done while simultaneously achieving significantly higher average status as compared to several benchmarks including a state-of-the-art topical crawler.
|keyword = status locality,predictive models,topical crawlers,homophily,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interdependencies in IT Infrastructure Services: Analyzing Service Processes for Optimal Incentive Design'''
{{header}}
{{article
|author= Sagnika Sen,T. S. Raghu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Information technology (IT) infrastructure outsourcing arrangements involve multiple services and processes that are interdependent. The interdependencies pose significant challenges in designing appropriate incentives to influence a provider's effort-allocation decisions. By integrating process modeling fundamentals with multitask agency theory, we enumerate the base set of possible interrelationships among different IT service processes and derive corresponding optimal incentives. Our results demonstrate the impacts of risk profile, random noise, value-cost ratio, and process structure on optimal incentive rates. We find that the current practice of treating IT services as essentially independent is optimal only in limited settings where both the service provider and customer are risk neutral. Interestingly, incongruent performance measures require optimal incentive rates to respond in complex ways to the strength of coupling between services and the complementarity and substitutability of services. We also analyze more complex process scenarios using different combinations of the base set. The results demonstrate that, while the findings from the base set largely hold, the value-cost ratio of the services and the performance measure congruity can pose unique challenges in determining incentive rates.
|keyword = IT outsourcing,service level agreements,incentives,agency theory,process interdependence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A View from the Top: Integrated Information Delivery and Effective Information Use from the Senior Executive's Perspective'''
{{header}}
{{article
|author= William J. Kettinger,Chen Zhang,Kuo-Chung Chang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This study frames antecedents of effective information use, outlining a nomological network that firms follow to achieve integrated information delivery and effective information use. Our focus is on senior business executives' assessment of information delivered by their organizations' information systems. We first clarify the definition of information as it relates to information delivery and effective use. Then, drawing from institutional theory and the resource-based view of the firm, we propose a research model consisting of external institutional pressure, internal information systems (IS) resources, integrated information delivery, and effective information use and empirically test it through a field survey of senior business executives and post hoc qualitative analysis. Our findings position information delivery as an important research construct leading to effective information use and value. Our study also highlights the important role of the IS function as a facilitator of effective information use and a nurturer of a strong information culture in organizations. Finally, we offer practical advice on how senior executives assess and improve integrated information delivery and effective use.
|keyword = senior executive,integrated information delivery,effective information use,IS resource-based view,institutional forces,information orientation,information view,information management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Contingency Approach to Investigating the Effects of User-System Interaction Modes of Online Decision Aids'''
{{header}}
{{article
|author= Weiquan Wang,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Interactive online decision aids often employ User-decision aid dialogues as forms of user-system interaction to help construct and elicit users' attribute preferences about a product type. This study extends prior research on online decision aids by investigating the effects of a decision aid's user-system interaction mode (USIM), which can be either user-guided or system-controlled, on users' effort-related (number of iterations of using the aid and perceived cognitive effort expended in using it) and quality-related (perceived quality of the aid and acceptance of the product advice it provides) assessments. A contingency approach with two moderating factors is employed. One factor is the decision strategy (additive-compensatory or elimination) employed by the aid, and the other is the users' product knowledge (high or low). A laboratory experiment was conducted to compare online decision aids with different USIMs. Although the results largely confirm that users assess the user-guided USIM more positively than the system-controlled USIM, the effects of USIM are stronger in two settings: for the elimination-based aid than for the additive-compensatory-based aid and for users with low product knowledge than for those with high product knowledge, especially in terms of effort assessments. This research advances the theoretical understanding of the effects of interaction between two critical components of online decision aids (USIMs and decision strategies) and the moderating role of user characteristics (product knowledge) in affecting users' evaluations. It also provides practitioners with design advice for developing these aids.
|keyword = online decision aid,decision strategy,user-system interaction mode,product knowledge,cognitive effort,system quality,system restrictiveness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''POSITIONING AND PRESENTING DESIGN SCIENCE RESEARCH FOR MAXIMUM IMPACT'''
{{header}}
{{article
|author= Shirley Gregor,Alan R. Hevner,
|source= MIS QUARTERLY
|year= 2013
|abstract = Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the development and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowledge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publication patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications.
|keyword = Design science research (DSR),knowledge,design artifact,knowledge contribution framework,publication schema,information systems,computer science discipline,engineering discipline,DSR theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE AMBIVALENT ONTOLOGY OF DIGITAL ARTIFACTS'''
{{header}}
{{article
|author= Jannis Kallinikos,Aleksi Aaltonen,Attila Marton,
|source= MIS QUARTERLY
|year= 2013
|abstract = Digital artifacts are embedded in wider and constantly shifting ecosystems such that they become increasingly editable, interactive, reprogrammable, and distributable. This state of flux and constant transfiguration renders the value and utility of these artifacts contingent on shifting webs of functional relations with other artifacts across specific contexts and organizations. By the same token, it apportions control over the development and use of these artifacts over a range of dispersed stakeholders and makes their management a complex technical and social undertaking. These ideas are illustrated with reference to (1) provenance and authenticity of digital documents within the overall context of archiving and social memory and ( 2) the content dynamics occasioned by the findability of content mediated by Internet search engines. We conclude that the steady change and transfiguration of digital artifacts signal a shift of epochal dimensions that calls for rethinking some of the inherited wisdom in IS research and practice.
|keyword = Digital artifacts,digital objects,archives,search engines,information platforms and infrastructures,modularity,reflexivity,change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACTFUL RESEARCH ON TRANSFORMATIONAL INFORMATION TECHNOLOGY: AN OPPORTUNITY TO INFORM NEW AUDIENCES'''
{{header}}
{{article
|author= Jr. Henry C. Lucas,Ritu Agarwal,Eric K. Clemons,Omar A. El Sawy,Bruce Weber,
|source= MIS QUARTERLY
|year= 2013
|abstract = Information technology has arguably been one of the most important drivers of economic and social value in the last 50 years, enabling transformational change in virtually every aspect of society. Although the Information Systems community is engaged in significant research on IT, the reach of our findings may be limited. In this commentary, our objective is to focus the IS community's attention on the striking transformations in economic and social systems spawned by IT and to encourage more research that offers useful implications for policy. We present examples of transformations occurring in four distinct sectors of the economy and propose policy-relevant questions that need to be addressed. We urge researchers to write papers based on their findings that inform policy makers, managers, and decision makers about the issues that transformational technologies raise. Finally, we suggest a new outlet to publish these essays on the implications of transformational informational technology.
|keyword = Transformation,strategy,disruptive technology,research policy,academic journals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHEN FILLING THE WAIT MAKES IT FEEL LONGER: A PARADIGM SHIFT PERSPECTIVE FOR MANAGING ONLINE DELAY'''
{{header}}
{{article
|author= Weiyin Hong,Traci J. Hess,Andrew Hardin,
|source= MIS QUARTERLY
|year= 2013
|abstract = As one of the most commonly experienced problems on the Internet, download delay is a significant impediment to the success of e-commerce websites. While some research has examined how such delays can be reduced and how much delay online users will tolerate, little research has taken a theoretically grounded approach to managing perceptions of the wait. Based on time perception theories, we develop a research model of the effects of actual wait time, amount of information, and direction of attention on perceptions of the wait. Two empirical studies were conducted using an experimental travel website to test the proposed hypotheses. The results show that with shorter waits, providing additional visual content, such as a travel picture, may make the wait feel longer. However, with longer waits, additional visual content that distracts the user from the passage of time makes the wait feel shorter and reduces users' negative affect toward the wait. Further, the benefits of providing visual content in longer waits are enhanced as more content is provided. Visual content should also be chosen to distract the user from time and temporal processing, as reminding users of the passage of time can encourage temporal processing and make the wait feel longer, especially in longer waits or when the amount of temporal visual content is high. Our findings extend time perception theories and contribute to the literature by identifying a potential paradigm shift, from the retrospective to the prospective paradigm, when waiting times are prolonged. Post hoc study results confirm the practical contribution of our research, demonstrating that several key findings are counter-intuitive to professional web designers.
|keyword = Online waiting,time perception theory,perceptions of wait,amount of information,visual content,direction of attention,download delay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMMUNITY INTELLIGENCE AND SOCIAL MEDIA SERVICES: A RUMOR THEORETIC ANALYSIS OF TWEETS DURING SOCIAL CRISES'''
{{header}}
{{article
|author= Onook Oh,Manish Agrawal,H. Raghav Rao,
|source= MIS QUARTERLY
|year= 2013
|abstract = Recent extreme events show that Twitter, a micro-blogging service, is emerging as the dominant social reporting tool to spread information on social crises. It is elevating the online public community to the status of first responders who can collectively cope with social crises. However, at the same time, many warnings have been raised about the reliability of community intelligence obtained through social reporting by the amateur online community. Using rumor theory, this paper studies citizen-driven information processing through Twitter services using data from three social crises: the Mumbai terrorist attacks in 2008, the Toyota recall in 2010, and the Seattle cafe shooting incident in 2012. We approach social crises as communal efforts for community intelligence gathering and collective information processing to cope with and adapt to uncertain external situations. We explore two issues: (1) collective social reporting as an information processing mechanism to address crisis problems and gather community intelligence, and (2) the degeneration of social reporting into collective rumor mills. Our analysis reveals that information with no clear source provided was the most important, personal involvement next in importance, and anxiety the least yet still important rumor causing factor on Twitter under social crisis situations.
|keyword = Twitter,social reporting,social information processing,rumor theory,social crisis,extreme events,community intelligence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''KNOWING WHAT A USER LIKES: A DESIGN SCIENCE APPROACH TO INTERFACES THAT AUTOMATICALLY ADAPT TO CULTURE'''
{{header}}
{{article
|author= Katharina Reinecke,Abraham Bernstein,
|source= MIS QUARTERLY
|year= 2013
|abstract = Adapting user interfaces to a user's cultural background can increase satisfaction, revenue, and market share. Conventional approaches to catering for culture are restricted to adaptations for specific countries and modify only a limited number of interface components, such as the language or date and time formats. We argue that a more comprehensive personalization of interfaces to cultural background is needed to appeal to users in expanding markets. This paper introduces a low-cost, yet efficient method to achieve this goal: cultural adaptivity. Culturally adaptive interfaces are able to adapt their look and feel to suit visual preferences. In a design science approach, we have developed a number of artifacts that support cultural adaptivity, including a prototype web application. We evaluate the efficacy of the prototype's automatically generated interfaces by comparing them with the preferred interfaces of 105 Rwandan, Swiss, Thai, and multicultural users. The findings demonstrate the feasibility of providing users with interfaces that correspond to their cultural preferences in a novel yet effective manner.
|keyword = Culture,design science,adaptive systems,personalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF SHAPING ON KNOWLEDGE REUSE FOR ORGANIZATIONAL IMPROVEMENT WITH WIKIS'''
{{header}}
{{article
|author= Ann Majchrzak,Christian Wagner,Dave Yates,
|source= MIS QUARTERLY
|year= 2013
|abstract = In this study, we explore the Wiki affordance of enabling shaping behavior within organizational intranets supported by Wikis. Shaping is the continuous revision of one's own and others' contributions to a Wiki. Shaping promotes knowledge reuse through improved knowledge integration. Recognizing and clarifying the role of shaping allows us to theorize new ways in which knowledge resources affect knowledge reuse. We examine the role of three knowledge resources of a Wiki contributor: knowledge depth, knowledge breadth, and assessment of the level of development of the Wiki community's transactive memory system. We offer preliminary evidence based on a sample of experienced organizational Wiki users that the three different knowledge resources have differential effects on shaping, that these effects differ from the effects on the more common user behavior of simply adding domain knowledge to a Wiki, and that shaping and adding each independently affect contributors' perceptions that their knowledge in the Wiki has been reused for organizational improvement. By empirically distinguishing between the different knowledge antecedents and consequences of shaping and adding, we derive implications for theory and research on knowledge integration and reuse.
|keyword = Wiki,Intranet,knowledge management,KMS,knowledge reuse,shaping,knowledge depth,knowledge breadth,transactive memory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGITAL BUSINESS STRATEGY: TOWARD A NEXT GENERATION OF INSIGHTS'''
{{header}}
{{article
|author= Anandhi Bharadwaj,Omar A. El Sawy,Paul A. Pavlou,N. Venkatraman,
|source= MIS QUARTERLY
|year= 2013
|abstract = Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm's chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy-aligned but essentially always subordinate to business strategy-to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy. We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.
|keyword = Digital business strategy,scope of digital business strategy,scale of digital business strategy,speed of digital business strategy,digital business strategy value creation and capture,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY AND BUSINESS-LEVEL STRATEGY: TOWARD AN INTEGRATED THEORETICAL PERSPECTIVE'''
{{header}}
{{article
|author= Paul L. Drnevich,David C. Croson,
|source= MIS QUARTERLY
|year= 2013
|abstract = Information technology matters to business success because it directly affects the mechanisms through which they create and capture value to earn a profit: IT is thus integral to a firm's business-level strategy. Much of the extant research on the IT/strategy relationship, however, inaccurately frames IT as only a functional-level strategy. This widespread under-appreciation of the business-level role of IT indicates a need for substantial retheorizing of its role in strategy and its complex and interdependent relationship with the mechanisms through which firms generate profit. Using a comprehensive framework of potential profit mechanisms, we argue that while IT activities remain integral to the functional-level strategies of the firm, they also play several significant roles in business strategy, with substantial performance implications. IT affects industry structure and the set of business-level strategic alternatives and value-creation opportunities that a firm may pursue. Along with complementary organizational changes, IT both enhances the firm's current (ordinary) capabilities and enables new (dynamic) capabilities, including the flexibility to focus on rapidly changing opportunities or to abandon losing initiatives while salvaging substantial asset value. Such digitally attributable capabilities also determine how much of this value, once created, can be captured by the firm-and how much will be dissipated through competition or through the power of value chain partners, the governance of which itself depends on IT. We explore these business-level strategic roles of IT and discuss several provocative implications and future research directions in the converging information systems and strategy domains.
|keyword = Management theory,information technology,information systems,competitive advantage,performance,technology management,IT capability,IT strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW A FIRM'S COMPETITIVE ENVIRONMENT AND DIGITAL STRATEGIC POSTURE INFLUENCE DIGITAL BUSINESS STRATEGY'''
{{header}}
{{article
|author= Sunil Mithas,Ali Tafti,Will Mitchell,
|source= MIS QUARTERLY
|year= 2013
|abstract = In this paper, we examine how the competitive industry environment shapes the way that digital strategic posture (defined as a focal firm's degree of engagement in a particular class of digital business practices relative to the industry norm) influences firms' realized digital business strategy. We focus on two forms of digital strategy: general IT investment and IT outsourcing investment. Drawing from prior literature on determinants of IT activity and competitive dynamics, we argue that three elements of the industry environment determine whether digital strategic posture has an increasingly convergent or divergent influence on digital business strategy. By divergent influence, we mean an influence that leads to spending substantially more or less on a particular strategic activity than industry norms. We predict that a digital strategic posture (difference from the industry mean) has an increasingly divergent effect on digital business strategy under higher industry turbulence, while having an increasingly convergent effect on digital business strategy under higher industry concentration and higher industry growth. The study uses archival data for 400 U.S.-based firms from 1999 to 2006. Our findings imply that digital business strategy is not solely a matter of optimizing firm operations internally or of responding to one or two focal competitors, but also arises strikingly from awareness and responsiveness to the digital business competitive environment. Collectively, the findings provide insights on how strategic posture and industry environment influence firms' digital business strategy.
|keyword = Digital business strategy,strategic posture,industry environment,industry turbulence,industry competition,industry growth,IT investments,IT strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DESIGN CAPITAL AND DESIGN MOVES: THE LOGIC OF DIGITAL BUSINESS STRATEGY'''
{{header}}
{{article
|author= C. Jason Woodard,Narayan Ramasubbu,F. Ted Tschang,V. Sambamurthy,
|source= MIS QUARTERLY
|year= 2013
|abstract = As information technology becomes integral to the products and services in a growing range of industries, there has been a corresponding surge of interest in understanding how firms can effectively formulate and execute digital business strategies. This fusion of IT within the business environment gives rise to a strategic tension between investing in digital artifacts for long-term value creation and exploiting them for short-term value appropriation. Further, relentless innovation and competitive pressures dictate that firms continually adapt these artifacts to changing market and technological conditions, but sustained profitability requires scalable architectures that can serve a large customer base and stable interfaces that support integration across a diverse ecosystem of complementary offerings. The study of digital business strategy needs new concepts and methods to examine how these forces are managed in pursuit of competitive advantage. We conceptualize the logic of digital business strategy in terms of two constructs: design capital (i.e., the cumulative stock of designs owned or controlled by a firm) and design moves (i.e., the discrete strategic actions that enlarge, reduce, or modify a firm's stock of designs). We also identify two salient dimensions of design capital, namely, option value and technical debt. Using embedded case studies of four firms, we develop a rich conceptual model and testable propositions to lay out a design-based logic of digital business strategy. This logic highlights the interplay between design moves and design capital in the context of digital business strategy and contributes to a growing body of insights that link the design of digital artifacts to competitive strategy and firm-level performance.
|keyword = Design capital,design moves,digital options,technical debt,IT architecture,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''LEVERAGING DIGITAL TECHNOLOGIES: HOW INFORMATION QUALITY LEADS TO LOCALIZED CAPABILITIES AND CUSTOMER SERVICE PERFORMANCE'''
{{header}}
{{article
|author= Pankaj Setia,Viswanath Venkatesh,Supreet Joglekar,
|source= MIS QUARTERLY
|year= 2013
|abstract = With the growing recognition of the customer's role in service creation and delivery, there is an increased impetus on building customer-centric organizations. Digital technologies play a key role in such organizations. Prior research studying digital business strategies has largely focused on building production-side competencies and there has been little focus on customer-side digital business strategies to leverage these technologies. We propose a theory to understand the effectiveness of a customer-side digital business strategy focused on localized dynamics-here, a firm's customer service units (CSUs). Specifically, we use a capabilities perspective to propose digital design as an antecedent to two customer service capabilities-namely, customer orientation capability and customer response capability-across a firm's CSUs. These two capabilities will help a firm to locally sense and respond to customer needs, respectively. Information quality from the digital design of the CSU is proposed as the antecedent to the two capabilities. Proposed capability-building dynamics are tested using data collected from multiple respondents across 170 branches of a large bank. Findings suggest that the impacts of information quality in capability-building are contingent on the local process characteristics. We offer implications for a firm's customer-side digital business strategy and present new areas for future examination of such strategies.
|keyword = Customer service,capabilities,information quality,process,customer response,customer orientation,business value of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTENT OR COMMUNITY? A DIGITAL BUSINESS STRATEGY FOR CONTENT PROVIDERS IN THE SOCIAL AGE'''
{{header}}
{{article
|author= Gal Oestreicher-Singer,Lior Zalmanson,
|source= MIS QUARTERLY
|year= 2013
|abstract = The content industry has been undergoing a tremendous transformation in the last two decades. We focus in this paper on recent changes in the form of social computing. Although the content industry has implemented social computing to a large extent, it has done so from a techno-centric approach in which social features are viewed as complementary rather than integral to content. This approach does not capitalize on users' social behavior in the website and does not answer the content industry's need to elicit payment from consumers. We suggest that both of these objectives can be achieved by acknowledging the fusion between content and community, making the social experience central to the content website's digital business strategy. We use data from Last.fm, a site offering both music consumption and online community features. The basic use of Last.fm is free, and premium services are provided for a fixed monthly subscription fee. Although the premium services on Last.fm are aimed primarily at improving the content consumption experience, we find that willingness to pay for premium services is strongly associated with the level of community participation of the user. Drawing from the literature on levels of participation in online communities, we show that consumers' willingness to pay increases as they climb the so-called "ladder of participation" on the website. Moreover, we find that willingness to pay is more strongly linked to community participation than to the volume of content consumption. We control for self-selection bias by using propensity score matching. We extend our results by estimating a hazard model to study the effect of community activity on the time between joining the website and the subscription decision. Our results suggest that firms whose digital business models remain viable in a world of "freemium" will be those that take a strategic rather than techno-centric view of social media, that integrate social media into the consumption and purchase experience rather than use it merely as a substitute for offline soft marketing. We provide new evidence of the importance of fusing social computing with content delivery and, in the process, lay a foundation for a broader strategic path for the digital content industry in an age of growing user participation.
|keyword = Premium services,social media,online communities,propensity score matching,UGC,digital business strategy,ladder of participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGITAL BUSINESS STRATEGY AND VALUE CREATION: FRAMING THE DYNAMIC CYCLE OF CONTROL POINTS'''
{{header}}
{{article
|author= Margherita Pagani,
|source= MIS QUARTERLY
|year= 2013
|abstract = Within changing value networks, the profits and competitive advantages of participation reside dynamically at control points that are the positions of greatest value and/or power. The enterprises that hold these positions have a great deal of control over how the network operates, how the benefits are redistributed, and how this influences the execution of a digital business strategy. This article is based on a field study that provides preliminary, yet promising, empirical evidence that sheds light on the dynamic cycle of value creation and value capture points in digitally enabled networks in response to triggers related to technology and business strategy. The context used is that of the European and U. S. broadcasting industry. Specifically, the paper illustrates how incremental innovations may shift value networks from static, vertically integrated networks to more loosely coupled networks, and how cross-boundary industry disruptions may then, in turn, shift those to two-sided markets. Based on the analysis, insights and implications for digital business strategy research and practice are then provided.
|keyword = Control points,incremental innovation,disruptive innovation,digital business strategy,value network,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TRANSPARENCY STRATEGY: COMPETING WITH INFORMATION IN A DIGITAL WORLD'''
{{header}}
{{article
|author= Nelson Granados,Alok Gupta,
|source= MIS QUARTERLY
|year= 2013
|abstract = We contend that in order to compete effectively in a digital business environment, firms should develop a transparency strategy by selectively disclosing information outside the boundaries of the firm. We make the case for transparency strategy by showing why it is relevant in the digital business world, and the consequences of not having such a strategy. We then provide some foundations to develop the strategy and make a call for research.
|keyword = Digital business strategy,electronic markets,transparency strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REVEALING YOUR HAND: CAVEATS IN IMPLEMENTING DIGITAL BUSINESS STRATEGY'''
{{header}}
{{article
|author= Varun Grover,Rajiv Kohli,
|source= MIS QUARTERLY
|year= 2013
|abstract = Digital business strategies (DBS) offer significant opportunities for firms to enhance competitiveness. Unlike the large proprietary systems of the 1980s, today's "micro-applications" allow firms to create and reconfigure digital capabilities to appropriate short-term competitive advantage. In the quest to provide value to customers through digitization, such applications can be efficiently deployed. However, we propose that in the long-term not all digitization is desirable. Indiscriminate digital initiatives through the use of micro-applications by a firm could "reveal its hand" to competitors and erode competitiveness. We propose that a firm's DBS must balance its system-software, process, and information-visibility with the ability to appropriate value from such systems. Through a visibility-value framework, and examples drawn from practice, this article illustrates the tradeoffs involved in making these choices as the firm traverses a dynamic business environment. In doing so, it raises sensitivity to an important caveat in digital environments epitomized by hyper-competition and transparency.
|keyword = Digital business strategy,competitiveness,flexibility,phasing,design,digitization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effect of Social Capital of the Relationship Between the CIO and Top Management Team on Firm Performance'''
{{header}}
{{article
|author= Elena Karahanna,David S. Preston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The paper empirically examines the effects of social capital of the relationship between the chief information officer (CIO) and top management team (TMT) on organizational value creation based on responses from CIOs and matched TMT respondents from 81 hospitals in the United States. Specifically, we theorize how the three dimensions of social capital-structural, cognitive, and relational social capital-facilitate knowledge exchange and combination between the CIO and TMT resulting in the alignment between the organization's information systems (IS) strategy and business strategy. Results show that IS alignment significantly influences the firm's financial performance and mediates the relationship between CIO-TMT social capital and performance. The findings also indicate that cognitive and relational social capital influence information systems strategic alignment but that structural social capital exerts its influence through its effects on cognitive social capital. Recommendations are provided as to how organizations can develop CIO-TMT structural, cognitive, and relational social capital to positively influence firm performance via IS strategic alignment.
|keyword = chief information officer,cognitive social capital,financial performance,information systems,IS leadership,relational social capital,social capital,strategic alignment,structural social capital,top management team,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sustainability of a Firm's Reputation for Information Technology Capability: The Role of Senior IT Executives'''
{{header}}
{{article
|author= Jee-Hae Lim,Theophanis C. Stratopoulos,Tony S. Wirjanto,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = This study investigates the development and sustainability of a firm's information technology (IT) capability reputation from an IT executive's standpoint. Building on institutional theory, we argue that IT executives will try to achieve external legitimacy (i.e., project an image of superior IT capability to external stakeholders) in the hope that the top management team and board members will reciprocate by elevating the internal legitimacy of IT executives. Firms that develop such a culture of reciprocity with their IT executives are more likely to sustain their IT capability reputation. Econometric results based on panel data for 1,326 large U. S. firms from a wide spectrum of industries over a 13-year period (1997-2009) validate these predictions. More specifically, we find that IT executives with greater structural power (e. g., higher job titles) or IT-related expert power (e. g., IT-related education or experience) are more likely to attract public recognition for their firm's IT capability. Firms that build such an IT capability reputation are more likely to promote their IT executives, and IT executives who are promoted are more likely to stay longer with their firms. This continuity in IT strategic leadership is positively associated with the firm's ability to sustain its IT capability reputation. Our findings have important practical implications related to a firm's IT reputation strategy as well as the motivation and career of IT executives. Firms wanting to develop and sustain their IT capability reputation would do well to foster the creation of a cycle of positive reciprocity with their IT executives. IT executives hoping to increase their power within their firm's top management team and improve the legitimacy of the firm's IT organization need to project an image of IT superiority to external stakeholders.
|keyword = external legitimacy,institutional theory,internal legitimacy,IT capability reputation,IT executives,IT strategic leadership,reciprocity,structural power,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology and Productivity in Developed and Developing Countries'''
{{header}}
{{article
|author= Jason Dedrick,Keneth L. Kraemer,Eric Shih,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Previous research has found that information technology (IT) investment is associated with significant productivity gains for developed countries but not for developing countries. Yet developing countries have continued to increase their investment in IT rapidly. Given this apparent disconnect, there is a need for new research to study whether the investment has begun to pay off in greater productivity for developing countries. We analyze new data on IT investment and productivity for 45 countries from 1994 to 2007, and compare the results with earlier research. We find that upper-income developing countries have achieved positive and significant productivity gains from IT investment in the more recent period as they have increased their IT capital stocks and gained experience with the use of IT. We also find that the productivity effects of IT are moderated by country factors, including human resources, openness to foreign investment, and the quality and cost of the telecommunications infrastructure. The academic implication is that the effect of IT on productivity is expanding from the richest countries into a large group of developing countries. The policy implication is that lower-tier developing countries can also expect productivity gains from IT investments, particularly through policies that support IT use, such as greater openness to foreign investment, increased investment in tertiary education, and reduced telecommunications costs.
|keyword = developed countries,developing countries,human resources,IT and productivity,longitudinal analysis,openness to trade and investment,production function,telecommunications cost,telecommunications infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Interdependent Information Security Risks: Cyberinsurance, Managed Security Services, and Risk Pooling Arrangements'''
{{header}}
{{article
|author= Xia Zhao,Ling Xue,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The interdependency of information security risks often induces firms to invest inefficiently in information technology security management. Cyberinsurance has been proposed as a promising solution to help firms optimize security spending. However, cyberinsurance is ineffective in addressing the investment inefficiency caused by risk interdependency. In this paper, we examine two alternative risk management approaches: risk pooling arrangements (RPAs) and managed security services (MSSs). We show that firms can use an RPA as a complement to cyberinsurance to address the overinvestment issue caused by negative externalities of security investments; however, the adoption of an RPA is not incentive-compatible for firms when the security investments generate positive externalities. We then show that the MSS provider serving multiple firms can internalize the externalities of security investments and mitigate the security investment inefficiency. As a result of risk interdependency, collective outsourcing arises as an equilibrium only when the total number of firms is small.
|keyword = cyberinsurance,information security,interdependent risks,managed security services,risk management,risk pooling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Drivers in the Use of Online Whistle-Blowing Reporting Systems'''
{{header}}
{{article
|author= Paul Benjamin Lowry,Gregory D. Moody,Denis F. Galletta,Anthony Vance,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Online whistle-blowing reporting systems (WBRS) have become increasingly prevalent channels for reporting organizational failures. The Sarbanes-Oxley Act and similar international laws now require firms to establish whistle-blowing (WB) procedures and WBRSs, increasing the importance of WB research and applications. Although the literature has addressed conventional WB behavior, it has not explained or measured the use of WBRSs in online contexts that could significantly alter elements of anonymity, trust, and risk for those using such reporting tools. This study proposes the WBRS model (WBRS-M). Using actual working professionals in an online experiment of hypothetical scenarios, we empirically tested the WBRS-M for reporting computer abuse and find that anonymity, trust, and risk are highly salient in the WBRS context. Our findings suggest that we have an improved WB model with increased explanatory power. Organizations can make WB less of a professional taboo by enhancing WBRS users' perceptions of trust and anonymity. We also demonstrate that anonymity means more than the mere lack of identification, which is not as important in this context as other elements of anonymity.
|keyword = anonymity,computer abuse,IT artifacts,organizational failure,organizational governance,risk,trust,whistle-blowing,whistle-blowing reporting systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Influence Tactics in Information System Development Projects: A Control-Loss Perspective'''
{{header}}
{{article
|author= Ravi Narayanaswamy,Varun Grover,Raymond M. Henry,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Information systems development (ISD) projects are prone to high levels of failure. One of the major reasons attributed to these failures is the inability to harmonize values held by a diverse set of participants in an environment that is characterized by uncertainty due to changing requirements. In this paper, we focus on a relational approach to achieve congruence between a project manager and a team member with respect to influence tactics. Constructs of perceptual congruence and communication congruence that reflect a level of agreement and degree of shared understanding between the project manager and team members are described. A congruence model is constructed and tied to an intermediate outcome variable of control loss. One hundred and thirteen dyadic pairs of project managers and team members are surveyed in order to test the model. The results indicate that having strong relational equity and common understanding can minimize control loss. It is important to consider the perspectives of both the project manager and a team member while formulating and assessing monitoring strategies to promote the success of an ISD project. Especially, encouraging team members to discuss disagreements constructively can motivate them to perform better and keep things under control. Finally, it is critical to address the performance problems as they occur rather than wait until the completion of the project.
|keyword = developer relationships,development project,influence tactics,information systems development,leadership exchange,project management,project manager,shared understanding,teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Service-Oriented Methodology for Systems Development'''
{{header}}
{{article
|author= Mark Keith,Haluk Demirkan,Michael Goul,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Despite advances in software development practices, organizations struggle to implement methodologies that match the risk in a project environment with needed coordination capabilities. Plan-driven and agile software development methodologies each have strengths and risks. However, most project environments cannot be classified as entirely "risky" or "stable," suggesting the need for hybrid approaches. We leverage a design science approach to implement a novel hybrid methodology based on concepts from the service-oriented paradigm. We motivate the approach using theory on interdependence and coordination, and design the methodology using theory on modularity and service-dominant logic. We also examine the effects of its adoption at a large electrical power company over a three-year period. The results imply that service-oriented theory should be applied to the human processes involved in systems development in order to achieve better fit between project risk, interdependencies, and the selected methodology(ies) in order to improve overall project performance.
|keyword = design science,modularization,project management,service orientation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Test of Two Models of Value Creation in Virtual Communities'''
{{header}}
{{article
|author= Constance Elise Porter,Sarv Devaraj,Daewon Sun,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Does a firm get any extra value from investing resources in sponsoring its own virtual community above and beyond the value that could be created for the firm, indirectly, via customer-initiated communities? If so, what explains the extra value derived from a firm-sponsored virtual community and how might this understanding inform managers about appropriate strategies for leveraging virtual communities as part of a value-creating strategy for the firm? We test two models of virtual community to help shed light on the answers to these questions. We hypothesize that in customer-initiated virtual communities, three attributes of member-generated information (MGI) drive value, while in firm-sponsored virtual communities, a sponsoring firm's efforts, as well as MGI, drive value. Drawing on information search and processing theories, and developing new measures of three attributes of MGI (consensus, consistency, and distinctiveness), we surveyed 465 consumers across numerous communities. We find that value can emerge via both models, but that in a firm-sponsored model, a sponsor's efforts are more powerful than MGI and have a positive, direct effect on the trust-building process. Our results suggest a continuum of value creation whereby firms extract greater value as they migrate toward the firm-sponsored model.
|keyword = attribution theory,co-creation,online communities,online trust,user-generated content,virtual communities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Risk Mitigation in Supply Chain Digitization: System Modularity and Information Technology Governance'''
{{header}}
{{article
|author= Ling Xue,Cheng Zhang,Hong Ling,Xia Zhao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Firms face significant risk when they adopt digital supply chain systems to transact and coordinate with their partners. Drawn upon modular systems theory, this study proposes that system modularity mitigates the risk of adopting digital supply chain systems and therefore motivates firms to digitize more of their supply chain operations. The study theorizes how the risk-mitigating effect of system modularity can be enhanced by the allocation of decision rights to the IT (information technology) unit. The main logic is that IT managers with more domain IT knowledge can better utilize their knowledge in decision making to achieve effective system modularity. We tested these theoretical propositions using a survey study of Chinese companies and found empirical support. We also found that the allocation of decision rights to the IT unit does not directly mitigate the perceived risk of digital supply chain systems, which highlights the role of decision allocation to the IT unit as a key moderator in risk mitigation. The study generates theoretical and practical implications on how IT governance and system modularity may jointly mitigate risk and foster supply chain digitization.
|keyword = IT governance,modular systems,risk mitigation,supply chain management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Association Between the Disclosure and the Realization of Information Security Risk Factors'''
{{header}}
{{article
|author= Tawei Wang,Karthik N. Kannan,Jackie Rees Ulmer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Firms often disclose information security risk factors in public filings such as 10-K reports. The internal information associated with disclosures may be positive or negative. In this paper, we evaluate how the nature of the disclosed security risk factors, believed to represent the firm's internal information regarding information security, is associated with future breach announcements reported in the media. For this purpose, we build a decision tree model, which classifies the occurrence of future security breaches based on the textual contents of the disclosed security risk factors. The model is able to accurately associate disclosure characteristics with breach announcements about 77% of the time. We further explore the contents of the security risk factors using text-mining techniques to provide a richer interpretation of the results. The results show that the disclosed security risk factors with risk-mitigation themes are less likely to be related to future breach announcements. We also investigate how the market interprets the nature of information security risk factors in annual reports. We find that the market reaction following the security breach announcement is different depending on the nature of the preceding disclosure. Thus, our paper contributes to the literature in information security and sheds light on how market participants can better interpret security risk factors disclosed in financial reports at the time when financial reports are released.
|keyword = information security,information security incident,risk factor,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Breaking the Ice in B2C Relationships: Understanding Pre-Adoption E-Commerce Attraction'''
{{header}}
{{article
|author= Damon E. Campbell,John D. Wells,Joseph S. Valacich,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = This research proposes that the forming of a business-to-consumer (B2C) customer relationship is part of a multiphased technology adoption process where attraction is the first step in this sequence. A conceptual model, called the electronic commerce (e-commerce) attraction model (eCAM), offers a theoretical foundation for guiding two empirical studies (N = 345 and N = 240, respectively) investigating how initial customer perceptions of a website influence attraction toward this website. The results support the eCAM as a new theoretical lens for understanding electronic commerce-based attraction. Comparisons are made between the proposed eCAM and previously established adoption models (i.e., the Technology Acceptance Model and WebQual) as well as the discriminant validity of the constructs in these models. Results demonstrate that the eCAM provides additional insights for understanding how website design influences e-commerce attraction and adoption. The implications of these results for future research and website design are discussed.
|keyword = attraction,competitive impacts of IS,electronic commerce,field experiments,IT adoption,laboratory experiments,questionnaire surveys,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital Divide Initiative Success in Developing Countries: A Longitudinal Field Study in a Village in India'''
{{header}}
{{article
|author= Viswanath Venkatesh,Tracy Ann Sykes,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Digital divide initiatives in developing countries are an important avenue for the socioeconomic advancement of those countries. Yet little research has focused on understanding the success of such initiatives. We develop a model of technology use and economic outcomes of digital divide initiatives in developing countries. We use social networks as the guiding theoretical lens because it is well suited to this context, given the low literacy, high poverty, high collectivism, and an oral tradition of information dissemination in developing countries. We test our model with longitudinal data gathered from 210 families in a rural village in India in the context of a digital divide initiative. As theorized, we found that the social network constructs contributed significantly to the explanation of technology use (R-2 = 0.39). Also as we predicted, technology use partially mediated the effect of social network constructs on economic outcomes (R-2 = 0.47). We discuss implications for theory and practice.
|keyword = Internet kiosk,system use,economic benefits,digital divide,technology adoption,technology diffusion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Governance of Interorganizational Information Systems: A Resource Dependence Perspective'''
{{header}}
{{article
|author= Dipanjan Chatterjee,T. Ravichandran,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In this paper we examine why firms seek to control and own interorganizational information systems, or IOS. Practitioners have largely cited the issues related to control and ownership of IOS, referred to as IOS governance in this paper, as the key reason behind the failure of many IOS initiatives. We distinguish between two IOS governance modes, transactional and financial, and develop a mediated-moderation model to explain the factors influencing the governance choices. In our model, the key motivators of IOS governance are the criticality and the replaceability of the resources that firms procure, which affect IOS governance through their influence on the degree of operational integration existing between partners. We hypothesize that while resource criticality will increase the needs for financial and transactional governance because of its positive impact on operational integration, resource replaceability will negatively influence governance needs because of its negative impact on operational integration. Furthermore, technological uncertainty associated with the resources is argued to negatively impact the extent of IOS governance exercised by firms by weakening the positive impact of resource criticality and strengthening the negative impact of resource replaceability on operational integration respectively. We empirically test our model using data gathered from a survey of 159 United States manufacturing firms. Results show that resource criticality positively affects the extent of financial and transactional IOS governance by increasing the need for operational integration, whereas resource replaceability negatively affects them by reducing the need for operational integration. Furthermore, technological uncertainty creates disincentives for IOS governance primarily by weakening the positive influence of resource criticality on operational integration, while no statistically significant effect of technological uncertainty on the relationship between resource replaceability and operational integration was discerned.
|keyword = interorganizational information systems,IOS governance,inter-firm relationships,procurement systems,resource dependence theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ensuring Employees' IT Compliance: Carrot or Stick?'''
{{header}}
{{article
|author= Huigang Liang,Yajiong Xue,Liansheng Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = With reward (carrot) and punishment (stick) widely applied by organizations to regulate mandatory IT usage, it is imperative to understand how these incentives influence employee compliance behavior. Drawing upon control theory and regulatory focus theory, this study investigates the relationships among regulatory focus, reward, punishment, and compliance behavior in mandatory IT settings. Survey data were collected from 186 employees in companies where enterprise resource planning (ERP) compliance was mandated. Analyses reveal that punishment expectancy is a strong determinant of compliance behavior, whereas the main effect of reward expectancy is not significant. Moreover, the relationship between reward expectancy and compliance behavior is moderated by promotion focus and the relationship between punishment expectancy and compliance behavior is moderated by prevention focus. This study provides an in-depth understanding of reward and punishment in mandatory IT settings and suggests that regulatory focus plays an important role in affecting employees' compliance with organizational controls.
|keyword = organizational control,reward,punishment,regulatory focus,promotion focus,prevention focus,compliance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contracting Information Security in the Presence of Double Moral Hazard'''
{{header}}
{{article
|author= Chul Ho Lee,Xianjun Geng,Srinivasan Raghunathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In information security outsourcing, it is the norm that the outsourcing firms and the outsourcers (commonly called managed security service providers, MSSPs) need to coordinate their efforts for better security. Nevertheless, efforts are often private and thus both firms and MSSPs can suffer from double moral hazard. Furthermore, the double moral hazard problem in security outsourcing is complicated by the existence of strong externality and the multiclient nature of MSSP services. In this prescriptive research, we first show that the prevailing contract structure in security outsourcing, bilateral refund contract, cannot solve double moral hazard. Adding breach-contingent sunk cost or external payment cannot solve double moral hazard either. Furthermore, positive externality can worsen double moral hazard. We then propose a new contract structure termed multilateral contract and show that it can solve double moral hazard and induce first-best efforts from all contractual parties when an MSSP serves two or more client firms, regardless of the externality. Firm-side externality significantly affects how payments flow under a multilateral contract when a security breach happens. When the number of client firms for an MSSP increases, we show that the contingent payments under multilateral contracts for any security breach scenario can be easily calculated using an additive method, and thus are computationally simple to implement.
|keyword = information security outsourcing,managed security service providers,double moral hazard,externality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Diversity in Global, Distributed Collectives: A Study of Open Source Project Success'''
{{header}}
{{article
|author= Sherae Daniel,Ritu Agarwal,Katherine J. Stewart,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Diversity is a defining characteristic of global collectives facilitated by the Internet. Though substantial evidence suggests that diversity has profound implications for a variety of outcomes including performance, member engagement, and withdrawal behavior, the effects of diversity have been predominantly investigated in the context of organizational workgroups or virtual teams. We use a diversity lens to study the success of nontraditional virtual work groups exemplified by open source software (OSS) projects. Building on the diversity literature, we propose that three types of diversity (separation, variety, and disparity) influence two critical outcomes for OSS projects: community engagement and market success. We draw on the OSS literature to further suggest that the effects of diversity on market success are moderated by the application development stage. We instantiate the operational definitions of three forms of diversity to the unique context of open source projects. Using archival data from 357 projects hosted on SourceForge, we find that disparity diversity, reflecting variation in participants' contribution-based reputation, is positively associated with success. The impact of separation diversity, conceptualized as culture and measured as diversity in the spoken language and country of participants, has a negative impact on community engagement but an unexpected positive effect on market success. Variety diversity, reflected in dispersion in project participant roles, positively influences community engagement and market success. The impact of diversity on market success is conditional on the development stage of the project. We discuss how the study's findings advance the literature on antecedents of OSS success, expand our theoretical understanding of diversity, and present the practical implications of the results for managers of distributed collectives.
|keyword = open source software,diversity,global collectives,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Framing Effects of Multipart Pricing on Consumer Purchasing Behavior of Customized Information Good Bundles'''
{{header}}
{{article
|author= Kim Huat Goh,Jesse C. Bockstedt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Applying behavioral economic theories, we hypothesize that consumers have sticky reference prices for individual information goods, but their perceived value for customizable bundle offers can be significantly influenced by the framing of a multipart pricing scheme. The potential impacts of these framing effects are measurable changes in consumer behavior and sales outcomes. We conducted a series of behavioral experiments and a large-scale natural field experiment involving actual purchases of customized information good bundles to confirm and analyze the hypothesized effects. The results demonstrate a consumer's willingness to purchase a customized bundle of information goods, the size of the resulting bundling, and the consumer's perceptions of the transaction are each significantly influenced by the design of the multipart pricing scheme. These results hold even when the final price and size of a customized bundle are the same across differing schemes. We discuss the potential tradeoffs in economic outcomes that result from price framing (e.g., likelihood of sale versus size of purchased bundles) and the implications for information good retailers.
|keyword = behavioral economics,behavioral experiments,bundling,customization,consumer behavior,econometrics,information goods,multipart pricing,natural experiments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Multicommunicating: Juggling Multiple Conversations in the Workplace'''
{{header}}
{{article
|author= Ann-Frances Cameron,Jane Webster,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = As a result of newer communication technologies and an increase in virtual communication, employees often find themselves multicommunicating, or participating in multiple conversations at the same time. This research seeks to explore multicommunicating from the perspective of the person juggling multiple conversations at the same time-the focal individual. To better understand this phenomenon, we extend previous theorizing by including the concepts of the episode initiator (whether the second conversation was focal or partner initiated), the fit of the set of media used in the episode, one process gain (conversation leveraging), and process losses. Employing a series of pilot studies and a main study, the resulting model was analyzed using structural equation modeling, finding overall support for the model. Findings suggest that experienced intensity is an important factor influencing process losses experienced during multicommunicating, whereas episode initiator influences process losses and the process gain. Further, media fit moderates the relationship between intensity and process losses. The importance of multicommunicating in the workplace is discussed, the theoretical and practical contributions of this research are described, and limitations and suggestions for future research are outlined.
|keyword = multicommunicating,polychronic communication,multitasking,dual-task performance,fit,productivity,process losses,intensity,PLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sequential Pricing of Multiple Products: Leveraging Revealed Preferences of Retail Customers Online and with Auto-ID Technologies'''
{{header}}
{{article
|author= John Aloysius,Cary Deck,Amy Farmer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Technological advances enable sellers to price discriminate based on a customer's revealed purchasing intentions. E-tailers can track items in online shopping carts and radio frequency identification tags enable retailers to do the same in brick-and-mortar stores. To leverage this information, it is important to understand how this new visibility impacts pricing and market outcomes. We propose a model in which a seller sets prices for goods A and B, allowing for the possibility of sequentially revising the price for good B if the buyer reveals a preference for good A by making an initial purchase decision. We derive comparative statics results for the prices of products that have superadditive or subadditive values, and also for the associated profits. We also run simulations for a range of distributions of buyer values, to compare sequential pricing with mixed bundling. The results indicate that information technology-enabled sequential pricing can increase profits relative to mixed bundling or pure components pricing for substitute goods due to a reduction of intraseller competition. We also consider the case of goods with positively or negatively correlated values and find that when sellers can condition the second good's price on the buyer's decision to purchase the first good, sequential pricing increases profits when customer's values for the goods are highly positively correlated.
|keyword = electronic commerce,RFID,electronic markets and auctions,online shopping carts,sequential pricing,price discrimination,mixed bundling,complements/substitutes,correlated product values,analytical modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Efficiency with Linear Prices? A Game-Theoretical and Computational Analysis of the Combinatorial Clock Auction'''
{{header}}
{{article
|author= Martin Bichler,Pasha Shabalin,Georg Ziegler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Combinatorial auctions have been suggested as a means to raise efficiency in multi-item negotiations with complementarities among goods because they can be applied in procurement, energy markets, transportation, and the sale of spectrum auctions. The combinatorial clock (CC) auction has become very popular in these markets for its simplicity and for its highly usable price discovery, derived by the use of linear prices. Unfortunately, no equilibrium bidding strategies are known. Given the importance of the CC auction in the field, it is highly desirable to understand whether there are efficient versions of the CC auction providing a strong game theoretical solution concept. So far, equilibrium strategies have only been found for combinatorial auctions with nonlinear and personalized prices for very restricted sets of bidder valuations. We introduce an extension of the CC auction, the CC+ auction, and show that it actually leads to efficient outcomes in an ex post equilibrium for general valuations with only linear ask prices. We also provide a theoretical analysis on the worst case efficiency of the CC auction, which highlights situations in which the CC leads to highly inefficient outcomes. As in other theoretical models of combinatorial auctions, bidders in the field might not be able to follow the equilibrium strategies suggested by the game-theoretical predictions. Therefore, we complement the theoretical findings with results from computational and laboratory experiments using realistic value models. The experimental results illustrate that the CC+ auction can have a significant impact on efficiency compared to the CC auction.
|keyword = electronic market,combinatorial auction,allocative efficiency,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pricing of Wireless Services: Service Pricing vs. Traffic Pricing'''
{{header}}
{{article
|author= Atanu Lahiri,Rajiv M. Dewan,Marshall Freimer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = As the ability to measure technology resource usage gets easier with increased connectivity, the question whether a technology resource should be priced by the amount of the resource used or by the particular use of the resource has become increasingly important. We examine this issue in the context of pricing of wireless services: should the price be based on the service, e.g., voice, multimedia messages, short messages, or should it be based on the traffic generated? Many consumer advocates oppose discriminatory pricing across services believing that it enriches carriers at the expense of consumers. The opposition to discrimination has grown significantly, and it has even prompted the U.S. Congress to question executives of some of the biggest carriers. With this ongoing debate on discrimination in mind, we compare two pricing regimes here. One regime, namely, service pricing, involves pricing different services differently. The other one, namely, traffic pricing, involves pricing the traffic (i.e., bytes) transmitted. We show why the common wisdom, that discriminatory pricing across services increases profits and harms consumers, may not always hold. We also show that such discrimination can increase social welfare.
|keyword = nonlinear pricing,second-degree discrimination,quasi-bundling,telecommunication,net neutrality,dumb pipe,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Fighting Fire with Fire: Commercial Piracy and the Role of File Sharing on Copyright Protection Policy for Digital Goods'''
{{header}}
{{article
|author= Tunay I. Tunca,Qiong Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In recent years, with the emergence and growth of illegal file sharing on the Internet, individual piracy of digital goods, i.e., consumers making illegal copies on their own rather than relying on purchasing copies from commercial pirates, has stirred substantial controversy. Threatened by this growth, the information goods industry took legal action by suing the file sharing peer-to-peer networks and the consumers who illegally share copyrighted material on these networks. In this paper we demonstrate that each one of these two actions aimed to fight individual piracy can backfire by providing strategic disadvantage to legal publishers of information goods. In particular, we show that in the presence of commercial piracy (i) a higher population of consumers who are capable of individual piracy can increase a legal publisher's profits; and (ii) a higher detection and prosecution rate for individual piracy can reduce a legal publisher's profits. Both effects can be observed in markets where commercial piracy is suppressed because the legal publisher can be coerced to take a price cut to minimize the loss of market share. The latter effect can also be observed in markets with active commercial piracy presence because the legal publisher can be forced to raise prices and concede market share to piracy. Our results suggest that information goods producers may be better off by considering their copyright protection policies concerning individual piracy from a more strategic point of view.
|keyword = analytical modeling,competitive impacts of information systems,economics of information systems,strategic management of information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Investigation of the Appropriation of Technology-Mediated Training Methods Incorporating Enactive and Collaborative Learning'''
{{header}}
{{article
|author= Saurabh Gupta,Robert Bostrom,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The growth in the application of information technology to student and employee learning underscores the need to understand the impact of technology-mediated learning (TML) methods. Using previous developed TML models, based on social cognitive theory and adaptive structuration meta-theory, the effectiveness of three training methods were examined in this study: technology mediated (using both vicarious and enactive learning), and collaborative and combined (collaborative plus technology mediated). The study also focused on the learning process. The experimental study results showed a significant positive influence of enactive learning enabled TML and collaborative training on specific training outcomes, and the combined training method shows positive results on all training outcomes. The study results also showed that faithful appropriation of the training methods during the learning process has a moderator effect on training outcomes. The study provides important research implications for theory and practice.
|keyword = technology-mediated learning,appropriation,simulation,e-learning,training,laboratory experiments,longitudinal research,collaborative learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do Large Firms Become Smaller by Using Information Technology?'''
{{header}}
{{article
|author= Kun Shin Im,Varun Grover,James T. C. Teng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The relationship between information technology (IT) and a key organizational design variable, firm size, is an important area of study, particularly given the ongoing transition to an information-based economy. To better understand the more nuanced aspects of the relationship, we formulated a bidirectional and time-lagged model that incorporates different perspectives from organizational theories and transaction cost economics. Our two models-the bidirectional and one-year lagged model and the bidirectional and two-year lagged model-were tested using nine-year panel data on IT spending, IT stock, coordination costs, firm size, and relevant control variables for 277 manufacturing firms. We found a sequential interaction between IT and firm size in both of the two models: as a firm grows in size, its coordination activities increase; the firm then uses more IT to handle the increased activities of coordination; this increased use of IT, in turn, decreases coordination costs, and eventually, the size of the firm decreases. It was also found that the presence of coordination costs is necessary for the sequential interaction between IT and firm size, indicating coordination between and within firms is a major reason for firms to invest in IT and for IT effect to take place on firm size. This study has taken an initial step by attempting to empirically examine dual causality and longitudinal effects between IT and firm size, and to reconcile different theoretical perspectives on the relationship between them. We hope this work can act as a catalyst for developing a better understanding of the complex relationship between IT and organizations, with the ultimate goal of offering robust prescriptions for successful structural change.
|keyword = information systems and organizational change,bidirectional model,time-lagged model,longitudinal research,firm size,coordination costs,production theory,transaction cost economics,information processing perspective,coordination theory,structuration theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Systems Success: The Quest for the Independent Variables'''
{{header}}
{{article
|author= Stacie Petter,William deLone,Ephraim R. McLean,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = In 1992, DeLone and McLean suggested that the dependent variable for information systems (IS) research is IS Success. Their research resulted in the widely cited DeLone and McLean (D&M) IS Success Model, in which System Quality, Information Quality, Use, User Satisfaction, Individual Impact, and Organizational Impact are distinct, but related dimensions of IS success. Since the original IS Success Model was published, research has developed a better understanding of IS success. Meanwhile, comprehensive and integrative research on the variables that influence IS success has been lacking. Therefore, we examine the literature on the independent variables that affect IS success. After examining over 600 articles, we focused our attention on integrating the findings of over 140 studies. In this research, we identify 43 specific variables posited to influence the different dimensions of IS success, and we organize these success factors into five categories based on the Leavitt Diamond of Organizational Change: task characteristics, user characteristics, social characteristics, project characteristics, and organizational characteristics. Next, we identify 15 success factors that have consistently been found to influence IS success: Enjoyment, Trust, User Expectations, Extrinsic Motivation, IT Infrastructure, Task Compatibility, Task Difficulty, Attitudes Toward Technology, Organizational Role, User Involvement, Relationship with Developers, Domain Expert Knowledge, Management Support, Management Processes, and Organizational Competence. Finally, we highlight gaps in our knowledge of success factors and propose a road map for future research.
|keyword = independent variables,IS success,research integration,success determinants,success factors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effect Mechanisms of Perceptual Congruence Between Information Systems Professionals and Users on Satisfaction with Service'''
{{header}}
{{article
|author= Alexander Benlian,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = With the proliferation of available electronic service channels for information systems (IS) users such as mobile or intranet services in companies, service interactions between IS users and IS professionals have become an increasingly important factor for organizational business-IT alignment. Despite the increasing relevance of such interactions, the implications of agreement or disagreement on the fulfillment of critical service quality factors for successful alignment and higher user satisfaction are far from being well understood. While prior research has extensively studied the question of matching different viewpoints on IS service quality in organizations, little or no attention has been paid to the role of perceptual congruence or incongruence in the dyadic relationship between IS professionals and users in forming user satisfaction with the IS function. Drawing on cognitive dissonance theory, prospect theory, and perceptual congruence research, this study examines survey responses from 169 matching pairs of IS professionals and users in different organizations and explains how perceptual fit patterns affect user satisfaction with the IS function. The paper demonstrates that perceptual congruence can, in and of itself, have an impact on user satisfaction, which goes beyond what was found before. Moreover, the results of the study reveal the relevance of nonlinear and asymmetric effect mechanisms arising from perceptual (in)congruence that may affect user satisfaction. This study extends our theoretical understanding of the role of perceptual alignment or misalignment on IS service quality factors in forming user satisfaction and lays the foundation for further study of the interplay between perceptions in the dyadic relationship between IS professionals and IS users. Managers who seek to encourage particular behaviors by the IS professionals or IS users may use the results of this study to reconcile the often troubled business-IT relationship.
|keyword = alignment,IS service quality,perceptual congruence,polynomial modeling,response surface analysis,SERVQUAL,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Strategic Alignment and Misalignment of Knowledge Management Systems: A Social Representation Perspective'''
{{header}}
{{article
|author= Alina Dulipovici,Daniel Robey,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = To derive more business value from existing organizational knowledge, many organizations seek to rely on strategically aligned knowledge management systems (KMS). However, as documented in prior studies, they often underestimate the challenges about social interactions and users' perceptions in response to new information systems. Based on an interpretive case study, this paper examines the implementation of a KMS to show how social representations of four groups of users resulted in the misalignment of the KMS with the organizational strategy. The social representation lens allows us to interpret strategic alignment in terms of dynamic processes of anchoring and objectification that aid individuals and groups to make sense of KMS initiatives. The groups studied developed different cognitive views of the KMS that ultimately led to a strategic misalignment. The key implication is that social interactions within and among groups shape KMS alignment with organizational strategy, thus elucidating the nature of system use.
|keyword = interpretive research,IS alignment,IS implementation,knowledge management system,social representation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Service Expansion of Product Firms in the Information Technology Industry: An Empirical Study'''
{{header}}
{{article
|author= Shu Han,Jason Kuruzovi,T. Ravi Chandran,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The provision of services has become an increasingly important component of the economy of industrialized countries and the revenue stream for many traditional product companies. This is especially true for companies that offer information technology (IT) products. This paper examines factors that are associated with the extent to which IT product companies are able to develop service revenue, which we refer to as service expansion of IT product companies. We identify the characteristics of the product portfolio-specifically, the composition and scope of firm offerings among hardware, application software, and infrastructure software-as key to successful service expansion. We also propose that this relationship is moderated by prior performance of the product business and industry characteristics such as concentration and maturity. Data from IT product vendors spanning five years are used to test the proposed relationships. Overall, this research provides a theoretical foundation for understanding service expansion and diversification in the IT industry as well as practical guidance for IT product companies considering expansion to services.
|keyword = IT services,IT stack,IT vendors,service diversification,software product portfolio,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitator-in-a-Box: Process Support Applications to Help Practitioners Realize the Potential of Collaboration Technology'''
{{header}}
{{article
|author= Robert O. Briggs,Gwendolyn L. Kolfschoten,Gert-Jan De Vrede,Stephan Lukosch,Conan C. Albrecht,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = The potential benefits of collaboration technologies are typically realized only in groups led by collaboration experts. This raises the facilitator-in-the-box challenge: Can collaboration expertise be packaged with collaboration technology in a form that nonexperts can reuse with no training on either tools or techniques? We address that challenge with process support applications (PSAs). We describe a collaboration support system (CSS) that combines a computer-assisted collaboration engineering platform for creating PSAs with a process support system runtime platform for executing PSAs. We show that the CSS meets its design goals: (1) to reduce development cycles for collaboration systems, (2) to allow nonprogrammers to design and develop PSAs, and (3) to package enough expertise in the tools that nonexperts could execute a well-designed collaborative work process without training.
|keyword = collaboration,collaboration engineering,collaboration support system,collaboration technology,computer-assisted collaboration engineering,process support application,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sparking Creativity: Improving Electronic Brainstorming with Individual Cognitive Priming'''
{{header}}
{{article
|author= Alan R. Denis,Randall K. Minas,Akshay P. Bhagwatwar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Much of human behavior involves subconscious cognition that can be manipulated through "priming"-the presentation of a stimulus designed to subconsciously implant a concept in working memory that alters subsequent behavior. Priming is a well-known phenomenon for individual behavior, but we do not know whether priming can be used to influence group behavior. We developed a Web-based computer game that was designed to improve creativity through priming. Participants were exposed to a priming game and then worked as members of a group using electronic brainstorming (EBS) to generate ideas on a creativity task. Our results show that when users played the game, designed to improve performance, their groups generated significantly more ideas that were more creative than when they were exposed to neutral priming. Our findings extend the literature by providing evidence that individual priming substantially affects group idea generation performance. Avenues for future research include designing EBS software that optimizes group ideation through priming, examining the conditions under which priming has the most substantial impact on ideation performance, and examining whether priming can be used to enhance other group processes (e.g., convergence tasks).
|keyword = collaboration,creativity,electronic brainstorming,idea generation,individual cognition,priming,virtual groups,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Emotions and Information Diffusion in Social Media-Sentiment of Microblogs and Sharing Behavior'''
{{header}}
{{article
|author= Stefan Stieglitz,Linh Dang-Xuan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = As a new communication paradigm, social media has promoted information dissemination in social networks. Previous research has identified several content-related features as well as user and network characteristics that may drive information diffusion. However, little research has focused on the relationship between emotions and information diffusion in a social media setting. In this paper, we examine whether sentiment occurring in social media content is associated with a user's information sharing behavior. We carry out our research in the context of political communication on Twitter. Based on two data sets of more than 165,000 tweets in total, we find that emotionally charged Twitter messages tend to be retweeted more often and more quickly compared to neutral ones. As a practical implication, companies should pay more attention to the analysis of sentiment related to their brands and products in social media communication as well as in designing advertising content that triggers emotions.
|keyword = information diffusion,sentiment,social media,Twitter,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Are Users Threatened by Credibility Assessment Systems?'''
{{header}}
{{article
|author= Aaron C. Elkins,Norah E. Dunbar,Bradley Adame,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Despite the improving accuracy of agent-based expert systems, human expert users aided by these systems have not improved their accuracy. Self-affirmation theory suggests that human expert users could be experiencing threat, causing them to act defensively and ignore the system's conflicting recommendations. Previous research has demonstrated that affirming an individual in an unrelated area reduces defensiveness and increases objectivity to conflicting information. Using an affirmation manipulation prior to a credibility assessment task, this study investigated if experts are threatened by counterattitudinal expert system recommendations. For our study, 178 credibility assessment experts from the American Polygraph Association (n = 134) and the European Union's border security agency Frontex (n = 44) interacted with a deception detection expert system to make a deception judgment that was immediately contradicted. Reducing the threat prior to making their judgments did not improve accuracy, but did improve objectivity toward the system. This study demonstrates that human experts are threatened by advanced expert systems that contradict their expertise. As more and more systems increase integration of artificial intelligence and inadvertently assail the expertise and abilities of users, threat and self-evaluative concerns will become an impediment to technology acceptance.
|keyword = credibility assessment systems,deception detection,expert systems,user anxiety,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Accountability to Reduce Access Policy Violations in Information Systems'''
{{header}}
{{article
|author= Anthony Vance,Paul Benjamin Lowry,Denis Eggett,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2013
|abstract = Access policy violations by organizational insiders are a major security concern for organizations because these violations commonly result in fraud, unauthorized disclosure, theft of intellectual property, and other abuses. Given the operational demands of dynamic organizations, current approaches to curbing access policy violations are insufficient. This study presents a new approach for reducing access policy violations, introducing both the theory of accountability and the factorial survey to the information systems field. We identify four system mechanisms that heighten an individual's perception of accountability: identifiability, awareness of logging, awareness of audit, and electronic presence. These accountability mechanisms substantially reduce intentions to commit access policy violations. These results not only point to several avenues for future research on access policy violations but also suggest highly practical design-artifact solutions that can be easily implemented with minimal impact on organizational insiders.
|keyword = access policy violations,accountability,accountability theory,awareness,evaluation,factorial survey method,identifiability,information security,monitoring,social presence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BEYOND DETERRENCE: AN EXPANDED VIEW OF EMPLOYEE COMPUTER ABUSE'''
{{header}}
{{article
|author= Robert Willison,Merrill Warkentin,
|source= MIS QUARTERLY
|year= 2013
|abstract = Recent academic investigations of computer security policy violations have largely focused on non-malicious noncompliance due to poor training, low employee motivation, weak affective commitment, or individual oversight. Established theoretical foundations applied to this domain have related to protection motivation, deterrence, planned behavior, self-efficacy, individual adoption factors, organizational commitment, and other individual cognitive factors. But another class of violation demands greater research emphasis: the intentional commission of computer security policy violation, or insider computer abuse. Whether motivated by greed, disgruntlement, or other psychological processes, this act has the greatest potential for loss and damage to the employer. We argue the focus must include not only the act and its immediate antecedents of intention (to commit computer abuse) and deterrence (of the crime), but also phenomena which temporally precede these areas. Specifically, we assert the need to consider the thought processes of the potential offender and how these are influenced by the organizational context, prior to deterrence. We believe the interplay between thought processes and this context may significantly impact the efficacy of IS security controls, specifically deterrence safeguards. Through this focus, we extend the Straub and Welke (1998) security action cycle framework and propose three areas worthy of empirical investigation-techniques of neutralization (rationalization), expressive/instrumental criminal motivations, and disgruntlement as a result of perceptions of organizational injustice-and propose questions for future research in these areas.
|keyword = Computer abuse,employee computer crime,information systems security,deterrence,neutralization,motivation,disgruntlement,organizational justice,instrumental crimes,expressive crimes,insider,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BRIDGING THE QUALITATIVE-QUANTITATIVE DIVIDE: GUIDELINES FOR CONDUCTING MIXED METHODS RESEARCH IN INFORMATION SYSTEMS'''
{{header}}
{{article
|author= Viswanath Venkatesh,Susan A. Brown,Hillol Bala,
|source= MIS QUARTERLY
|year= 2013
|abstract = Mixed methods research is an approach that combines quantitative and qualitative research methods in the same research inquiry. Such work can help develop rich insights into various phenomena of interest that cannot be fully understood using only a quantitative or a qualitative method. Notwithstanding the benefits and repeated calls for such work, there is a dearth of mixed methods research in information systems. Building on the literature on recent methodological advances in mixed methods research, we develop a set of guidelines for conducting mixed methods research in IS. We particularly elaborate on three important aspects of conducting mixed methods research: (1) appropriateness of a mixed methods approach; (2) development of meta-inferences (i.e., substantive theory) from mixed methods research; and (3) assessment of the quality of meta-inferences (i.e., validation of mixed methods research). The applicability of these guidelines is illustrated using two published IS papers that used mixed methods.
|keyword = Meta-inferences,mixed methods,multimethod,positivist,qualitative,quantitative,research method,research design,validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACT OF INFORMATION FEEDBACK IN CONTINUOUS COMBINATORIAL AUCTIONS: AN EXPERIMENTAL STUDY OF ECONOMIC PERFORMANCE'''
{{header}}
{{article
|author= Gediminas Adomavicius,Shawn P. Curley,Alok Gupta,Pallab Sanyal,
|source= MIS QUARTERLY
|year= 2013
|abstract = Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions-in which bidders can bid on combinations of goods-have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidders can submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free-flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism.
|keyword = Online auctions,continuous combinatorial auctions,experimental economics,information feedback,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT-MEDIATED CUSTOMER SERVICE CONTENT AND DELIVERY IN ELECTRONIC GOVERNMENTS: AN EMPIRICAL INVESTIGATION OF THE ANTECEDENTS OF SERVICE QUALITY'''
{{header}}
{{article
|author= Chee-Wee Tan,Izak Benbasat,Ronald T. Cenfetelli,
|source= MIS QUARTERLY
|year= 2013
|abstract = Despite extensive deliberations in contemporary literature, the design of citizen-centric e-government websites remains an unresolved theoretical and pragmatic conundrum. Operationalizing e-government service quality to investigate and improve the design of e-government websites has been a much sought-after objective. Yet, there is a lack of actionable guidance on how to develop e-government websites that exhibit high levels of service quality. Drawing from marketing literature, we undertake a goal approach to this problem by delineating e-government service quality into aspects of IT-mediated service content and service delivery. Whereas service content describes the functions available on an e-government website that assist citizens in completing their transactional goals, service delivery defines the manner by which these functions are made accessible via the web interface as a delivery channel. We construct and empirically test a research model that depicts a comprehensive collection of web-enabled service content functions and delivery dimensions desirable by citizens. Empirical findings from an online survey of 647 respondents attest to the value of distinguishing between service content functions and delivery dimensions in designing e-government websites. Both service content and delivery are found to be significant contributors to achieving e-government service quality. These IT-mediated service content functions and delivery dimensions represent core areas of e-government website design where the application of technology makes a difference, especially when considered in tandem with the type of transactional activity. A split sample analysis of the data further demonstrates our model's robustness when applied to e-government transactions of varying frequency.
|keyword = Electronic government service quality,IT-mediated service content functions,IT-mediated service delivery dimensions,service content quality,service delivery quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIGITAL GAMES AND BEYOND: WHAT HAPPENS WHEN PLAYERS COMPETE?'''
{{header}}
{{article
|author= De Liu,Xun Li,Radhika Santhanam,
|source= MIS QUARTERLY
|year= 2013
|abstract = Because digital games are fun, engaging, and popular, organizations are attempting to integrate them within organizational activities as serious components, with the anticipation that they can improve employees' motivation and performance. But in order to do so and to obtain the intended outcomes, it is necessary to first obtain an understanding of how different digital game designs impact players' behaviors and emotional responses. Hence, in this study, we address one key element of popular game designs: competition. Using extant research on tournaments and intrinsic motivation, we model competitive games as a skill-based tournament and conduct an experimental study to understand player behaviors and emotional responses under different competition conditions. When players compete with players of similar skill levels, they apply more effort as indicated by more games played and longer duration of play. But when players compete with players of lower skill levels, they report higher levels of enjoyment and lower levels of arousal after game-playing. We discuss the implications for organizations seeking to introduce games premised on competition and provide a framework to guide information system researchers to embark on a study of games.
|keyword = Digital games,intrinsic motivation,experimental study,tournament theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DATA MODEL DEVELOPMENT FOR FIRE RELATED EXTREME EVENTS: AN ACTIVITY THEORY APPROACH'''
{{header}}
{{article
|author= Rui Chen,Raj Sharman,H. Raghav Rao,Shambhu J. Upadhyaya,
|source= MIS QUARTERLY
|year= 2013
|abstract = Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.
|keyword = Data model,extreme events,design science,activity theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXAMINING THE RELATIONAL BENEFITS OF IMPROVED INTERFIRM INFORMATION PROCESSING CAPABILITY IN BUYER-SUPPLIER DYADS'''
{{header}}
{{article
|author= Eric T. G. Wang,Jeffrey C. F. Tai,Varun Grover,
|source= MIS QUARTERLY
|year= 2013
|abstract = Information Systems research has studied how buyers and suppliers can benefit from improved information visibility in supply chains characterized by uncertainty. However, the relation-specific information processing solutions that provide visibility can only be exploited if the two firms engage in sufficient coordination efforts. This work takes a nuanced look at how dyadic benefits are derived in the supply chain. Drawing on the information processing view, resource-based view, and transaction cost theory, this study explicates how buyer performance can result from buyer's use of relation-specific information processing solutions and supplier's relational responses. Two interfirm information processing solutions are proposed and examined: the use of IT-based systems for planning and control, and the use of relational (normative) contracts. Based on a sample of 144 manufacturing firms, eight of the nine proposed research hypotheses receive empirical support using PLS analysis. The findings suggest that as buyers and suppliers utilize the IT and relational solutions, they induce relation-specific responses represented as supplier's business process investments and modification flexibility, which in turn lead to positive buyer outcomes. The results help us gain a more granular understanding on how relation-specific interfirm information processing solutions can lead to performance through enhanced interfirm governance capabilities.
|keyword = IT-enabled planning and control,normative contracts,information processing view,resource-based view,transaction cost theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERNATIONALIZATION STRATEGIES OF CHINESE IT SERVICE SUPPLIERS'''
{{header}}
{{article
|author= Ning Su,
|source= MIS QUARTERLY
|year= 2013
|abstract = With China emerging as a new frontier of global IT outsourcing, many Chinese IT service suppliers are actively expanding in three major markets: Asia, especially Japan, the West, especially the United States, and the Chinese domestic market. Compared to multinational suppliers and established Indian suppliers, Chinese IT service firms are at a relatively early, but rapidly growing stage, which offers a unique opportunity to explore an understudied topic in the information systems literature: internationalization strategies of IT service suppliers from emerging economies. Through a three-part qualitative case study of 13 China-based IT service firms, including almost all of the Chinese suppliers recognized globally, this study elaborates the internationalization behavior and decision rationale of these suppliers. The findings show that these major Chinese suppliers include both firms that incrementally internationalize and firms that are "born global." For both types of firms, the entry and growth in different markets is a highly dynamic activity combining a strategically planned, resource-seeking process and a flexible, opportunistic bricolage process based on existing operation capabilities and client relationships. The suppliers dynamically oscillate between these processes to exploit and create opportunities while expanding in multiple markets.
|keyword = IT outsourcing,supplier's perspective,internationalization,strategy process,China,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERNET PRIVACY CONCERNS: AN INTEGRATED CONCEPTUALIZATION AND FOUR EMPIRICAL STUDIES'''
{{header}}
{{article
|author= Weiyin Hong,James Y. L. Thong,
|source= MIS QUARTERLY
|year= 2013
|abstract = Internet privacy concerns (IPC) is an area of study that is receiving increased attention due to the huge amount of personal information being gathered, stored, transmitted, and published on the Internet. While there is an emerging literature on IPC, there is limited agreement about its conceptualization in terms of its key dimensions and its factor structure. Based on the multidimensional developmental theory and a review of the prior literature, we identify alternative conceptualizations of IPC. We examine the various conceptualizations of IPC with four online surveys involving nearly 4,000 Internet users. As a baseline, study 1 compares the integrated conceptualization of IPC to two existing conceptualizations in the literature. While the results provide support for the integrated conceptualization, the second-order factor model does not outperform the correlated first-order factor model. Study 2 replicates the study on a different sample and confirms the results of study 1. We also investigate whether the prior results are affected by the different perspectives adopted in the wording of items in the original instruments. In study 3, we find that focusing on one's concern for website behavior (rather than one's expectation of website behavior) and adopting a consistent perspective in the wording of the items help to improve the validity of the factor structure. We then examine the hypothesized third-order conceptualizations of IPC through a number of alternative higher-order models. The empirical results confirm that, in general, the third-order conceptualizations of IPC outperform their lower-order alternatives. In addition, the conceptualization of IPC that has the best fit with the data contains a third-order general IPC factor, two second-order factors of interaction management and information management, and six first-order factors (i.e., collection, secondary usage, errors, improper access, control, and awareness). Study 4 cross-validates the results with another data set and examines IPC within the context of a nomological network. The results confirm that the third-order conceptualization of IPC has nomological validity, and it is a significant determinant of both trusting beliefs and risk beliefs. Our research helps to resolve inconsistencies in the key underlying dimensions of IPC, the factor structure of IPC, and the wording of the original items in prior instruments of IPC. Finally, we discuss the implications of this research.
|keyword = Internet privacy concerns,information privacy concerns,online privacy,multidimensional development theory,higher-order factors,confirmatory factor analysis,LISREL,nomological validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SOCIAL INFLUENCE AND KNOWLEDGE MANAGEMENT SYSTEMS USE: EVIDENCE FROM PANEL DATA'''
{{header}}
{{article
|author= Yinglei Wang,Darren B. Meister,Peter H. Gray,
|source= MIS QUARTERLY
|year= 2013
|abstract = Theory suggests that coworkers may influence individuals' technology use behaviors, but there is limited research in the technology diffusion literature that explicates how such social influence processes operate after initial adoption. We investigate how two key social influence mechanisms (identification and internalization) may explain the growth over time in individuals' use of knowledge management systems (KMS)-a technology that because of its publicly visible use provides a rich context for investigating social influence. We test our hypotheses using longitudinal KMS usage data on over 80,000 employees of a management consulting firm. Our approach infers the presence of identification and internalization from associations between actual system use behaviors by a focal individual and prior system use by a range of reference groups. Evidence of these kinds of associations between system use behaviors helps construct a more complete picture of social influence mechanisms, and is to our knowledge novel to the technology diffusion literature. Our results confirm the utility of this approach for understanding social influence effects and reveal a fine-grained pattern of influence across different social groups: we found strong support for bottom-up social influence across hierarchical levels, limited support for peer-level influence within levels, and no support for top-down influence.
|keyword = Information technology diffusion,social influence,knowledge management,knowledge management systems,longitudinal research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY OUTSOURCING AND NON-IT OPERATING COSTS: AN EMPIRICAL INVESTIGATION'''
{{header}}
{{article
|author= Kunsoo Han,Sunil Mithas,
|source= MIS QUARTERLY
|year= 2013
|abstract = Does information technology outsourcing reduce non-IT operating costs? This study examines this question and also asks whether internal IT investments moderate the relationship between IT outsourcing and non-IT operating costs. Using a panel data set of approximately 300 U. S. firms from 1999 to 2003, we find that IT outsourcing has a significant negative association with firms' non-IT operating costs. However, this finding does not imply that firms should completely outsource their entire IT function. Our results suggest that firms benefit more in terms of reduction in non-IT operating costs when they also have higher levels of complementary investments in internal IT, especially IT labor. Investments in internal IT systems can make business processes more amenable to outsourcing, and complementary investments in internal IT staff can facilitate monitoring of vendor performance and coordination with vendors. We discuss the implications of these findings for further research and for practice.
|keyword = IT outsourcing,information technology,IT expenditures,IT impacts,IT services,IT labor,IT human capital,non-IT operating costs,business value of IT,IT governance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When Social Media Can Be Bad for You: Community Feedback Stifles Consumer Creativity and Reduces Satisfaction with Self-Designed Products'''
{{header}}
{{article
|author= Christian Hildebrand,Gerald Haeubl,Andreas Herrmann,Jan R. Landwehr,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Enabling consumers to self-design unique products that match their idiosyncratic preferences is the key value driver of modern mass customization systems. These systems are increasingly becoming "social," allowing for consumer-to-consumer interactions such as commenting on each other's self-designed products. The present research examines how receiving others' feedback on initial product configurations affects consumers' ultimate product designs and their satisfaction with these self-designed products. Evidence from a field study in a European car manufacturer's brand community and from two follow-up experiments reveals that receiving feedback from other community members on initial self-designs leads to less unique final self-designs, lower satisfaction with self-designed products, lower product usage frequency, and lower monetary product valuations. We provide evidence that the negative influence of feedback on consumers' satisfaction with self-designed products is mediated by an increase in decision uncertainty and perceived process complexity. The implications of socially enriched mass customization systems for both consumer welfare and seller profitability are discussed.
|keyword = mass customization systems,user self-design,product configurators,consumer decision making,social influence,field study,experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Network Effects on Productivity and Job Security: Evidence from the Adoption of a Social Networking Tool'''
{{header}}
{{article
|author= Lynn Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = By studying the change in employees' network positions before and after the introduction of a social networking tool, I find that information=rich networks (low in cohesion and rich in structural holes), enabled by social media, have a positive effect on various work outcomes. Contrary to the notion that network positions are difficult to alter, I show that social media can induce a change in network structure, one from which individuals can derive economic benefits. In addition, I consider two intermediate mechanisms by which an informationrich network is theorized to improve work performance-information diversity and social communication-and quantify their effects on productivity and job security. Analysis shows that productivity, as measured by billable revenue, is more associated with information diversity than with social communication. However, the opposite is true for job security. Social communication is more correlated with reduced layoff risks than with information diversity. This, in turn, suggests that information-rich networks enabled through the use of social media can drive both work performance and job security, but that there is a trade-off between engaging in social communication and gathering diverse information.
|keyword = social media,social network,productivity,job security,information diversity,social communication,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Active Social Media Management: The Case of Health Care'''
{{header}}
{{article
|author= Amalia R. Miller,Catherine Tucker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Given the demand for authentic personal interactions over social media, it is unclear how much firms should actively manage their social media presence. We study this question empirically in a health care setting. We show that active social media management drives more user-generated content. However, we find that this is due to an incremental increase in user postings from an organization's employees rather than from its clients. This result holds when we explore exogenous variation in social media policies, employees, and clients that are explained by medical marketing laws, medical malpractice laws, and distortions in Medicare incentives. Further examination suggests that content being generated mainly by employees can be avoided if a firm's postings are entirely client focused. However, most firm postings seem not to be specifically targeted to clients' interests, instead highlighting more general observations or achievements of the firm itself. We show that untargeted postings like these provoke activity by employees rather than clients. This may not be a bad thing because employee-generated content may help with employee motivation, recruitment, or retention, but it does suggest that social media should not be funded or managed exclusively as a marketing function of the firm.
|keyword = business value of IT,computer-mediated communication and collaboration,social media,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Ties and User Content Generation: Evidence from Flickr'''
{{header}}
{{article
|author= Xiaohua Zeng,Liyuan Wei,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = The content created by the users of social networking sites has reached such high levels of quality and variety that it is comparable to that produced by professional agencies. Therefore, understanding what types of content users generate and the underlying motivational factors is vital to the success of the sites. The extant research on content generation has primarily focused on the amount of content and on how to encourage participation in content creation, and less attention has been paid to the content itself and how social relations affect the types of content that users upload. This study aims to empirically document the relationship between social ties and the similarities between the types of content that people create online. We collected a large data set from the photo-hosting website Flickr detailing the users' social relations over time in conjunction with their photo-uploading behavior. We found that around the time of the formation of a social tie, members of dyads began to upload more similar photos than they did before that time. After a social tie was formed, this similarity evolved in different ways in different subgroups of dyads. Whereas the similarity between photos uploaded by dyads experiencing notably different popularity levels on the site continued to grow, the dyads of users with similar levels of popularity gradually began to upload less similar photos. In cultural production, individuals appear to present themselves as unique; this feature is more salient when the social contacts are similar in popularity status. Photo-shooting behaviors have been found to exhibit the same patterns. Furthermore, we show that the most divergent uploading behavior is observed when a high-popularity user initiates a tie with a user with lower popularity. We use social psychological motivations to explain these results.
|keyword = user-generated content,social networks,computer-mediated communication and collaboration,within-subjects design,photography,distinctiveness,tags,Flickr,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Media Brand Community and Consumer Behavior: Quantifying the Relative Impact of User- and Marketer-Generated Content'''
{{header}}
{{article
|author= Khim-Yong Goh,Cheng-Suang Heng,Zhijie Lin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Despite the popular use of social media by consumers and marketers, empirical research investigating their economic values still lags. In this study, we integrate qualitative user-marketer interaction content data from a fan page brand community on Facebook and consumer transactions data to assemble a unique data set at the individual consumer level. We then quantify the impact of community contents from consumers (usergenerated content, i.e., UGC) and marketers (marketer-generated content, i.e., MGC) on consumers' apparel purchase expenditures. A content analysis method was used to construct measures to capture the informative and persuasive nature of UGC and MGC while distinguishing between directed and undirected communication modes in the brand community. In our empirical analysis, we exploit differences across consumers' fan page joining decision and across timing differences in fan page joining dates for our model estimation and identification strategies. Importantly, we also control for potential self-selection biases and relevant factors such as pricing, promotion, social network attributes, consumer demographics, and unobserved heterogeneity. Our findings show that engagement in social media brand communities leads to a positive increase in purchase expenditures. Additional examinations of UGC and MGC impacts show evidence of social media contents affecting consumer purchase behavior through embedded information and persuasion. We also uncover the different roles played by UGC and MGC, which vary by the type of directed or undirected communication modes by consumers and the marketer. Specifically, the elasticities of demand with respect to UGC information richness are 0.006 (directed communication) and 3.140 (undirected communication), whereas those for MGC information richness are insignificant. Moreover, the UGC valence elasticity of demand is 0.180 (undirected communication), whereas that for MGC valence is 0.004 (directed communication). Overall, UGC exhibits a stronger impact than MGC on consumer purchase behavior. Our findings provide various implications for academic research and practice.
|keyword = social media,brand community,consumer behavior,user-generated content,marketer-generated content,communication mode,text mining,econometric modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effect of Customers' Social Media Participation on Customer Visit Frequency and Profitability: An Empirical Investigation'''
{{header}}
{{article
|author= Rishika Rishika,Ashish Kumar,Ramkumar Janakiraman,Ram Bezawada,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In this study we examine the effect of customers' participation in a firm's social media efforts on the intensity of the relationship between the firm and its customers as captured by customers' visit frequency. We further hypothesize and test for the moderating roles of social media activity and customer characteristics on the link between social media participation and the intensity of customer-firm relationship. Importantly, we also quantify the impact of social media participation on customer profitability. We assemble a novel data set that combines customers' social media participation data with individual customer level transaction data. To account for endogeneity that could arise because of customer self-selection, we utilize the propensity score matching technique in combination with difference in differences analysis. Our results suggest that customer participation in a firm's social media efforts leads to an increase in the frequency of customer visits. We find that this participation effect is greater when there are high levels of activity in the social media site and for customers who exhibit a strong patronage with the firm, buy premium products, and exhibit lower levels of buying focus and deal sensitivity. We find that the above set of results holds for customer profitability as well. We discuss theoretical implications of our results and offer prescriptions for managers on how to engage customers via social media. Our study emphasizes the need for managers to integrate knowledge from customers' transactional relationship with their social media participation to better serve customers and create sustainable business value.
|keyword = social media marketing,social media participation,customer-firm relationship,shopping visit,frequency,customer profitability,propensity score matching,quasi-experiment,difference-in-differences,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting Adoption Probabilities in Social Networks'''
{{header}}
{{article
|author= Xiao Fang,Paul Jen-Hwa Hu,Zhepeng (Lionel) Li,Weiyu Tsai,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = In a social network, adoption probability refers to the probability that a social entity will adopt a product, service, or opinion in the foreseeable future. Such probabilities are central to fundamental issues in social network analysis, including the influence maximization problem. In practice, adoption probabilities have significant implications for applications ranging from social network-based target marketing to political campaigns, yet predicting adoption probabilities has not received sufficient research attention. Building on relevant social network theories, we identify and operationalize key factors that affect adoption decisions: social influence, structural equivalence, entity similarity, and confounding factors. We then develop the locally weighted expectation-maximization method for Naive Bayesian learning to predict adoption probabilities on the basis of these factors. The principal challenge addressed in this study is how to predict adoption probabilities in the presence of confounding factors that are generally unobserved. Using data from two large-scale social networks, we demonstrate the effectiveness of the proposed method. The empirical results also suggest that cascade methods primarily using social influence to predict adoption probabilities offer limited predictive power and that confounding factors are critical to adoption probability predictions.
|keyword = adoption probability,social network,Bayesian learning,social influence,structural equivalence,entity similarity,confounding factor,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Media and Firm Equity Value'''
{{header}}
{{article
|author= Xueming Luo,Jie Zhang,Wenjing Duan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Companies have increasingly advocated social media technologies to transform businesses and improve organizational performance. This study scrutinizes the predictive relationships between social media and firm equity value, the relative effects of social media metrics compared with conventional online behavioral metrics, and the dynamics of these relationships. The results derived from vector autoregressive models suggest that social media-based metrics (Web blogs and consumer ratings) are significant leading indicators of firm equity value. Interestingly, conventional online behavioral metrics (Google searches and Web traffic) are found to have a significant yet substantially weaker predictive relationship with firm equity value than social media metrics. We also find that social media has a faster predictive value, i.e., shorter "wear-in" time, than conventional online media. These findings are robust to a consistent set of volume-based measures (total blog posts, rating volume, total page views, and search intensity). Collectively, this study proffers new insights for senior executives with respect to firm equity valuations and the transformative power of social media.
|keyword = social media,word of mouth,online reviews,Web blogs,vector autoregression,firm equity value,stock market performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Engineering Optimal Network Effects via Social Media Features and Seeding in Markets for Digital Goods and Services'''
{{header}}
{{article
|author= Yifan Dou,Marius F. Niculescu,D. J. Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = Firms nowadays are increasingly proactive in trying to strategically capitalize on consumer networks and social interactions. In this paper, we complement an emerging body of research on the engineering of word-of-mouth effects by exploring a different angle through which firms can strategically exploit the value-generation potential of the user network. Namely, we consider how software firms should optimize the strength of network effects at utility level by adjusting the level of embedded social media features in tandem with the right market seeding and pricing strategies in the presence of seeding disutility. We explore two opposing seeding cost models where seeding-induced disutility can be either positively or negatively correlated with customer type. We consider both complete and incomplete information scenarios for the firm. Under complete information, we uncover a complementarity relationship between seeding and building social media features that holds for both disutility models. When the cost of any of these actions increases, rather than compensating by a stronger action on the other dimension to restore the overall level of network effects, the firm will actually scale back on the other initiative as well. Under incomplete information, this complementarity holds when seeding disutility is negatively correlated with customer type but may not always hold in the other disutility model, potentially leading to fundamentally different optimal strategies. We also discuss how our insights apply to asymmetric networks.
|keyword = social commerce and social media,network effects,social interaction,seeding,adoption process,digital goods and services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Rewarding User Engagement: The Case of Facebook Apps'''
{{header}}
{{article
|author= Jorg Claussen,Tobias Kretschmer,Philip Mayrhofer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2013
|abstract = We study the market for apps on Facebook, the dominant social networking platform, and make use of a rule change by Facebook by which highly engaging apps were rewarded with further opportunities to engage users. The rule change led to new applications with significantly higher user ratings being developed. Moreover, user ratings became more important drivers of app success. Other drivers of app success are also affected by the rule change; sheer network size became a less important driver for app success, update frequency benefitted apps more in staying successful, and active users of Facebook apps declined less rapidly with age. Our results show that social media channels do not necessarily have to be managed through hard exclusion of participants but can also be steered through "softer" changes in reward and incentive systems.
|keyword = app markets,social media,platform management,Facebook,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Everybody Needs Somebody: The Influence of Team Network Structure on Information Technology Use'''
{{header}}
{{article
|author= Masimo Magni,Corey M. Angst,Ritu Agarwal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Team network structure has been shown to be an important determinant of both team and individual performance outcomes, yet few studies have investigated the relationship between team network structure and technology usage behaviors. Drawing from social network and technology use literature, we examine how the structure of a team's advice-seeking network affects individual use of a newly implemented information technology. We develop cross-level hypotheses related to the effects of the structure of mutually interconnected ties within the team (i.e., internal closure) as well as the structure of nonredundant ties outside the team boundaries (i.e., external bridging). The hypotheses are tested in a field study of 265 employees working in 44 teams in a large financial services institution. Results show that internal closure has a U-shaped effect on individual use such that individual usage of the system is higher when the number of internaladvice-seeking ties within the team is low or high, suggesting that medium levels of internal closure are the least desirable network configurations because in such instances teams neither realize the benefits of high closure information sharing nor are they able to avoid in-group biases associated with low closure conditions. Our results also reveal that in addition to having a direct positive effect on individual use, external bridging interacts with internal closure in a complex manner. The U-shaped effect of closure is dominant when bridging is high but assumes an inverted U-shaped pattern when bridging is low. Several implications for managers follow from these findings. First, in order to increase usage of technology, in teams characterized by low internal closure, managers should encourage the development of ties across team boundaries. Second, managers should maximize within-team interconnections in order to facilitate the circulation of external knowledge within team boundaries. Finally, managers should be aware that maximizing internal closure by facilitating interconnections among team members could be dangerous if not accompanied by mechanisms for external bridging.
|keyword = advice-seeking network,external bridging,integration perspective,internal closure,social categorization theory,technology use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward an Integrated Model of Group Development: Disruption of Routines by Technology-Induced Change'''
{{header}}
{{article
|author= Monica J. Garfield,Alan R. Denis,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Current research argues that the most prominent models of group development (the linear stage model and the punctuated equilibrium model) are simply different lenses for studying the same phenomenon. We argue that the two models are distinct (groups do not simultaneously follow both models) and that the key to understanding their use lies in routines. We studied six newly formed groups whose members came from the same organization that worked on similar projects over a seven-week period. Three groups worked nonmediated and three groups used a collaboration technology that was new to them. The three nonmediated groups followed the punctuated equilibrium model and the three collaboration technology groups followed the stage model. We argue that groups that enact the shared routines common in their organizations will experience a different group development path than those groups whose shared routines are disrupted and which must adapt to a new technology. When group members enact shared routines (which they may share due to having a common organizational culture), they can quickly begin work, and group development follows the punctuated equilibrium model. When groups cannot enact shared routines, they must first negotiate how they will work before work can begin, so group development follows the stage model. Thus, the introduction of new collaboration technology (or any new technology or work process) influences how group development occurs.
|keyword = case study,collaboration technology,field experiment,group development models,mixed methods,routines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Corporate Wikis: The Effects of Owners' Motivation and Behavior on Group Members' Engagement'''
{{header}}
{{article
|author= Ofer Arazy,Ian R. Gellatly,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Originally designed as a tool to alleviate bottlenecks associated with knowledge management, the suitability of wikis for corporate settings has been questioned given the inherent tensions between wiki affordances and the realities of organizational life. Drawing on regulatory focus theory and social cognitive theory, we developed and tested a model of the motivational dynamics underlying corporate wikis. We examined leaders (owners) and users of 187 wiki-based projects within a large multinational firm. Our findings revealed two countervailing motivational forces, one oriented toward accomplishment and achievement (promotion focus) and one oriented toward safety and security (prevention focus), that not only predicted owners' participation but also the overall level of engagement within the wiki groups. Our primary contribution is in showing that, notwithstanding the potential benefits to users, wikis can trigger risk-avoidance motives that potentially impede engagement. Practically, our findings call for an alignment between organizational procedures surrounding wiki deployment and the technology's affordances.
|keyword = knowledge management (KM),knowledge management systems (KMS),knowledge sharing,motivation,owner,regulatory focus theory,social cognitive theory,wiki,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Security Outsourcing with System Interdependency and Mandatory Security Requirement'''
{{header}}
{{article
|author= Kai-Lung Hui,Wendy Hui,Wei T. Yue,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = The rapid growth of computer networks has led to a proliferation of information security standards. To meet these security standards, some organizations outsource security protection to a managed security service provider (MSSP). However, this may give rise to system interdependency risks. This paper analyzes how such system interdependency risks interact with a mandatory security requirement to affect the equilibrium behaviors of an MSSP and its clients. We show that a mandatory security requirement will increase the MSSP's effort and motivate it to serve more clients. Although more clients can benefit from the MSSP's protection, they are also subjected to greater system interdependency risks. Social welfare will decrease if the mandatory security requirement is high, and imposing verifiability may exacerbate social welfare losses. Our results imply that recent initiatives such as issuing certification to enforce computer security protection, or encouraging auditing of managed security services, may not be advisable.
|keyword = information security,information security outsourcing,interdependency risks,mandatory security requirement,security compliance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizations' Information Security Policy Compliance: Stick or Carrot Approach?'''
{{header}}
{{article
|author= Yan Chen,K. (Ram) Ramamurthy,Kuang-Wei Wen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Companies' information security efforts are often threatened by employee negligence and insider breach. To deal with these insider issues, this study draws on the compliance theory and the general deterrence theory to propose a research model in which the relations among coercive control, which has been advocated by scholars and widely practiced by companies; remunerative control, which is generally missing in both research and practice; and certainty of control are studied. A Web-based field experiment involving real-world employees in their natural settings was used to empirically test the model. While lending further support to the general deterrence theory, our findings highlight that reward enforcement, a remunerative control mechanism in the information systems security context, could be an alternative for organizations where sanctions do not successfully prevent violation. The significant interactions between punishment and reward found in the study further indicate a need for a more comprehensive enforcement system that should include a reward enforcement scheme through which the organizational moral standards and values are established or reemphasized. The findings of this study can potentially be used to guide the design of more effective security enforcement systems that encompass remunerative control mechanisms.
|keyword = coercive control,compliance theory,general deterrence theory,information security policy,punishment,remunerative control,reward,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competitive Target Advertising and Consumer Data Sharing'''
{{header}}
{{article
|author= Xia Zhao,Ling Xue,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Advances in information technologies enable firms to collect detailed consumer data and target individual consumers with tailored ads. Consumer data are among the most valuable assets that firms own. An interesting phenomenon is that competing firms often trade their consumer data with each other. Based on a common-value all-pay auction framework, this paper studies the advertising competition between two firms that target the same consumer but are asymmetrically informed about the consumer value. We characterize firms' equilibrium competition strategies. The results show that better consumer information does not help the better-informed firm save the advertising expenditure but does enable it to reap a higher expected profit in competition. Sharing individual-level consumer data may soften the competition even though firms compete head-to-head for the same consumer. We also find that the better-informed firm may sell its data to its competitor but never voluntarily shares it with its competitor.
|keyword = advertising,all-pay auction,common-value auction,information asymmetry,information sharing,target marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alternate Strategies for a Win-Win Seeking Agent in Agent-Human Negotiations'''
{{header}}
{{article
|author= Yinping Yang,Sharad Singhal,Yunjie (Calvin) Xu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = With the growth of e-commerce and e-markets, there is an increasing potential for the use of software agents to negotiate business tasks with human negotiators. Guided by design science methodology, this research prescribes and validates a win-win seeking negotiation agent using strategies of "simultaneous-equivalent offers" and "delayed acceptance" and compares their effects against the use of conventional sequential-single offer and immediate acceptance strategies. To evaluate the alternate strategies, a negotiation agent system was implemented and an experiment was conducted in which 110 agent-human dyads negotiated over a four-issue online purchase task. Our results indicate that the proposed agent strategies can enhance the economic performance of the negotiated outcome (counterpart agreement ratio, individual utility, joint utility, and the distance to Pareto-efficient frontier) and maintain the human counterparts' positive perceptions toward the outcome and the agent. The findings confirm the efficacy of the proposed design and showcase an innovative system to facilitate e-commerce transactions.
|keyword = agent-human negotiation,delayed acceptance,design science,electronic markets,negotiation agent,simultaneous-equivalent offers,win-win negotiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Inducing Intrinsic Motivation to Explore the Enterprise System: The Supremacy of Organizational Levers'''
{{header}}
{{article
|author= Weiling Ke,Chuan-Ho Tan,Choon-Ling Sia,Kwok-Kee Wei,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = The adoption of an organization-wide system, such as an enterprise system (ES), has often been mandated by organizational management, which may not necessarily motivate users to proactively explore the system's features and subsequently apply pertinent features that best support their job tasks. Anchoring on self-determination theory, this research investigates the antecedents and consequences of users' intrinsic motivation to explore ES features. We propose two organizational levers (i.e., autonomous job design and socialization tactics) that the management could exercise to trigger intrinsic motivation, thereby leading to improved ES feature exploration. Intrinsic motivation is manifested by hedonic motivation and normative motivation, whereas ES feature exploration is conceptualized as a dual-dimensional outcome reflected by cognitive behavior (exploratory usage) and positive affect (exploration satisfaction). Through a two-stage survey of 127 organizational users in China, we find general support for our research model. We further observe significant moderating effects of prevention focus on the association between organizational levers and intrinsic motivations. Beyond demonstrating how organizational users respond to different organizational levers, this research examines a broader, enduring challenge, which is to determine how organizational users can be induced to be intrinsically inspired to innovatively harness implemented information systems.
|keyword = enterprise system,exploration usage,intrinsic motivation,organizational levers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Postadoptive Behaviors in Information Systems Use: A Longitudinal Analysis of System Use Problems in the Business Intelligence Context'''
{{header}}
{{article
|author= Xuefei (Nancy) Deng,Lei Chi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = For an organization to gain maximum benefits from a new information system (IS), individual users in the organization must use it effectively and extensively. To do so, users need to overcome many problems associated with their system use in order to integrate the new IS into their work routines. Much remains to be learned about the types of problems that users encounter in using the new system, in particular, the root causes of system use problems and how they relate to and co-evolve with the problems over time. In this study, we seek to develop a comprehensive and dynamic view of system use problems in organizations. Using a combined method of revealed causal mapping and in-depth network analysis, we analyze nine-month archival data on user-reported problems with a new business intelligence application in a large organization. Our data analysis revealed seven emergent constructs of system use problems and causes, including reporting, data, workflow, role authorization, users' lack of knowledge, system error, and user-system interaction. The seven constructs were found to interact differentially across two usage phases (initial versus continued) and between two types of users (regular versus power user). This study contributes to advancing our theoretical understanding of postadoptive IS use by focusing on its problematic aspect. This study also suggests useful methods for organizations to effectively monitor users' system use problems over time and thus guides organizations to effectively target mechanisms to promote the use of new technologies.
|keyword = business intelligence,IS use,postadoptive behavior,revealed causal mapping,social network analysis,system use problem,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating the Value of Sociomaterialism in Conceptualizing IT Capability of a Firm'''
{{header}}
{{article
|author= Gimun Kim,Bongsik Shin,Ohbyung Kwon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Sociomateriality (or sociomaterialism) allows us to approach the information technology (IT) capability research from an angle that has been rarely visited by information systems scholars. While relevant studies presume that humans and materials are distinct and largely independent, sociomateriality emphasizes agency that represents the relational, emergent, and shifting capacity realized through the association of actors (both humans and materials). The objective of this paper is to explore the value of conducting IT capability research through the theoretical lens of sociomaterialism. For this, we expand the imbrication metaphor introduced in an early study to explain the formation and advancement of a firm's IT capability from the sociomaterial perspective. Then, the key building blocks of IT capability of an organization are conceptualized based on the combination of existing studies and the expanded imbrication metaphor. Lastly, the effectiveness of formulating IT capability as a third-order construct that substantiates the entanglement concept of sociomaterialism is examined in comparison with that of traditional modeling approaches. We confirm the value of sociomaterialism in conceptualizing IT capability and subsequently in unraveling the true contribution of IT capability toward strengthening business performance. The findings also have practical implications in which IT capability is a function of IT management capability as well as IT personnel capability and IT infrastructure capability.
|keyword = imbrication metaphor,IT capability,sociomaterialism,sociomateriality,third-order factor,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Generalizability of Information Systems Research Using Student Subjects-A Reflection on Our Practices and Recommendations for Future Research'''
{{header}}
{{article
|author= Deborah Compeau,Barbara Marcolin,Helen Kelley,Chris Higgins,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Information systems researchers, like those in many other disciplines in the social sciences, have debated the value and appropriateness of using students as research subjects. This debate appears in several articles that have been published on the subject as well as in the review process. In this latter arena, however, the debate has become increasingly like a script-the actors (authors and reviewers) simply read their parts of the script; some avoid the underlying issues whereas others cursorily address generalizability without real consideration of those issues. As a result, despite the extent of debate, we seem no closer to a resolution. Authors who use student subjects rely on their scripted arguments to justify the use of student subjects and do not always consider whether those arguments are valid. But reviewers who oppose the use of student subjects are equally culpable. They, too, rely on scripted arguments to criticize work using student subjects, and do not always consider whether those arguments are salient to the particular study. By presenting and reviewing one version of this script in the context of theoretical discussions of generalizability, we hope to demonstrate its limitations so that we can move beyond these scripted arguments into a more meaningful discussion. To do this, We review empirical studies from the period 1990-2010 to examine the extent to which student subjects are being used in the field and to critically assess the discussions within the field about the use of student samples. We conclude by presenting recommendations for authors and reviewers, for determining whether the use of students is appropriate in a particular context, and for presenting and discussing work that uses student subjects.
|keyword = external validity,student samples,review process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Advancing Public Trust Relationships in Electronic Government: The Singapore E-Filing Journey'''
{{header}}
{{article
|author= Eric T. K. Lim,Chee-Wee Tan,Dianne Cyr,Shan L. Pan,Bo Xiao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = E-governments have become an increasingly integral part of the virtual economic landscape. However, e-government systems have been plagued by an unsatisfactory, or even a decreasing, level of trust among citizen users. The political exclusivity and longstanding bureaucracy of governmental institutions have amplified the level of difficulty in gaining citizens' acceptance of e-government systems. Through the synthesis of trust-building processes with trust relational forms, we construct a multidimensional, integrated analytical framework to guide our investigation of how e-government systems can be structured to restore trust in citizen-government relationships. Specifically, the analytical framework identifies trust-building strategies (calculative-based, prediction-based, intentionality-based, capability-based, and transference-based trust) to be enacted for restoring public trust via e-government systems. Applying the analytical framework to the case of Singapore's Electronic Tax-Filing (E-Filing) system, we advance an e-government developmental model that yields both developmental prescriptions and technological specifications for the realization of these trust-building strategies. Further, we highlight the impact of sociopolitical climates on the speed of e-government maturity.
|keyword = e-government,public trust,calculative-based trust,prediction-based trust,intentionality-based trust,capability-based trust,transference-based trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Adoption and Impacts of Interorganizational Business Process Standards: Role of Partnering Synergy'''
{{header}}
{{article
|author= Viswanath Venkatesh,Hillol Bala,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Notwithstanding potential benefits, such as quality of interorganizational relationships and operational and strategic gains, adoption of information technology (IT)-enabled interorganizational business process standards (IBPS) is still limited. Given that these standards are designed for interorganizational business processes, we suggest that adoption of these standards depends not only on the factors pertinent to a focal firm but also on factors that represent synergies between a focal firm and its trading partners. In this paper, building on the technological, organizational, and environmental (TOE) framework and interorganizational theories, we propose a model that postulates that a set of TOE factors will have synergistic effects (i.e., interactions between a focal firm's and its partner's factors) on IBPS adoption. We tested our model in a study of 248 firms (124 dyads) in the high-tech industry implementing Rosetta Net-based IBPS and found that three TOE factors (i.e., process compatibility, standards uncertainty, and technology readiness) had synergistic effects and two factors (i.e., expected benefits and relational trust) had direct effects on IBPS adoption. We also found that IBPS adoption led to greater relationship quality (i.e., partnering satisfaction) and operational efficiency (i.e., cycle time). Further, we found that IBPS adoption mediated the effect of TOE factors on partnering satisfaction and cycle time.
|keyword = interorganizational relationships,business process,process standards,process compatibility,standards uncertainty,cycle time,relationship quality,partnering synergy,synergistic effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Corporate IT Standardization: Product Compatibility, Exclusive Purchase Commitment, and Competition Effects'''
{{header}}
{{article
|author= Xinxin Li,Yuxin Chen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = When companies purchase information technology (IT) products for their employees, departments, or divisions, whether to standardize on one product or to allow the users to make their own choices is an important decision for IT managers to make. By consolidating demand and committing to buy from a single seller, standardization ensures product compatibility within the corporation and has a potential to induce intense price competition among sellers, but this potential is subject to whether competing products are compatible and the relative competitive advantages of the sellers. This paper studies when it is optimal for an employer to commit to exclusive purchase from a single seller to enforce standardization and sellers' incentives to invest in mutual compatibility. Our results suggest that the employer is more likely to make such a commitment when the competing products are compatible, less vertically differentiated, and/or more horizontally differentiated. We also find that the sellers agree to cooperate and invest in mutual compatibility only when the gap between their competitive advantages is moderate, but the availability of third party converters that enable partial compatibility can induce more collaboration among the sellers.
|keyword = corporate IT standardization,product compatibility,network effects,exclusive purchase commitment,competition effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Synergy and Its Limits in Managing Information Technology Professionals'''
{{header}}
{{article
|author= Thomas W. Ferratt,Jayesh Prasad,Harvey G. Enns,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We examine the effects of human resource management (HRM) practices (e.g., career development, social support, compensation, and security) on information technology (IT) professionals' job search behavior. Job search is a relatively novel dependent variable in studies of voluntary withdrawal behavior in general and for IT professionals in particular. From a universalistic perspective, FIRM practices individually and in combination exhibit independently additive effects on job search behavior. Our study contrasts this perspective with configurational theory, hypothesizing that proposed ideal-type configurations of HRM practices have synergistic effects on job search behavior. We contribute to the IT and broader HRM literature by theoretically explicating and empirically demonstrating with IT professionals the power of configurational theory to explain the relationship between HRM practices and job search behavior. Our empirical results show that two configurations of HRM practices-Human Capital Focused (HCF) and Task Focused (TF), which are high and low on all HRM practices, respectively-exhibit a synergistic relationship with the job search behavior of IT professionals. HCF has lower job search behavior than would be expected based on the independently additive effects of the HRM practices, whereas TF has correspondingly higher job search behavior. Our results also show that less than perfect horizontal fit detracts from the synergy of these extreme configurations. Just as importantly, several other nonextreme configurations of HRM practices exhibit independently additive effects for the HRM practices but not synergy, suggesting that synergy is limited to extreme configurations. We also discuss a number of implications for research and practice.
|keyword = synergy,configurations,information technology professionals,management of IT resources,human resource practices,staffing,strategic human resource management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User Satisfaction with Information Technology Service Delivery: A Social Capital Perspective'''
{{header}}
{{article
|author= Yongqiang Sun,Yulin Fang,Kai H. Lim,Detmar Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Existing research has long considered service quality as a primary determinant of user satisfaction with information technology (IT) service delivery. In response to the knowledge-intensive and collaborative nature of IT service delivery in the contemporary business context, we advance the theoretical understanding of user satisfaction by re-conceptualizing IT service delivery as a bilateral, relational process between the IT staff and users. Based on this reconceptualization, we draw on social capital theory to examine the antecedents of user satisfaction with IT service delivery. Specifically, we posit that two major dimensions of social capital, i.e., cognitive capital and relational capital, not only positively affect user satisfaction but also strengthen the established relationship between service quality and user satisfaction. Furthermore, we propose that the effect of the other dimension of social capital-structural capital-on user satisfaction is fully mediated through cognitive capital and relational capital. A field study of 159 users in four financial companies provides general empirical support for our hypotheses. Theoretical and practical implications of these findings are discussed.
|keyword = IT service,social capital,service quality,user satisfaction,survey,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ushering Buyers into Electronic Channels: An Empirical Analysis'''
{{header}}
{{article
|author= Nishtha Langer,Chris Forman,Sunder Kekre,Baohong Sun,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Despite many success stories, B2B e-commerce penetration remains low. Many firms introduce electronic channels in addition to their traditional sales channels but find that buyer usage of the e-channel over time does not keep up with initial expectations. Firms must understand the underlying factors that drive channel usage and how these factors change over time and across buyers. Using panel data pertaining to the purchase histories of 683 buyers over a 43-month period, we estimate a dynamic discrete choice model in a B2B setting that (i) recognizes how price, channel inertia, and inventory change over time; (ii) allows buyers to dynamically trade off these factors when making e-channel adoption decisions; and (iii) takes into account buyer heterogeneity. We find that channel usage is both heterogeneous and dynamic across buyers. Our findings reveal the dynamic tradeoff between channel inertia and the adverse price effect, which interact in opposing directions as the e-channel grows more popular over time: price increases resulting from more bids deter buyers, whereas channel inertia built from sampling experience helps retain repeat buyers for the new channel. Second, we find that the buyers' size and diversity influence purchase decisions, and the e-channel appears more attractive to small and/or diversified buyers. Based on our analysis, we postulate that the seller's allocation decisions of products across channels, if not aligned with buyer behavior, can alienate some buyers. Based on the parameter estimates from the buyer response model, we propose an improved channel allocation that enables firms to selectively attract more buyers to the e-channel and improve revenues. Channel acceptance increases as a result of smart allocation when firms understand and account for individual buyers' channel usage behavior.
|keyword = electronic markets,channel choice,buyer heterogeneity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What Motivates People to Purchase Digital Items on Virtual Community Websites? The Desire for Online Self-Presentation'''
{{header}}
{{article
|author= Hee-Woong Kim,Hock Chuan Chan,Atreyi Kankanhalli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The sale of digital items, such as avatars and decorative objects, is becoming an important source of revenue for virtual community (VC) websites. However, some websites are unable to leverage this source of revenue, and there is a corresponding lack of understanding about what motivates people to purchase digital items in VCs. To explain the phenomenon, we develop a model based on the theory of self-presentation. The model proposes that the desire for online self-presentation is a key driver for such purchases. We also hypothesize that the social influence factors of online self-presentation norms and VC involvement as well as personal control in the form of online presentation self-efficacy are antecedents of the desire for online self-presentation. The model was validated by using survey data collected from Cyworld (N = 217) and Habbo (N = 197), two online social network communities that have been pioneers in the sale of digital items. This work contributes to our understanding of the purchase of digital items by extending the theory of self-presentation and adds to the broader line of research on online identity. It also lends insights into how VC providers can tap this source of revenue.
|keyword = digital item purchase,virtual community,desire for online self-presentation,VC norms,VC involvement,online presentation self-efficacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Two Worlds of Trust for Potential E-Commerce Users: Humans as Cognitive Misers'''
{{header}}
{{article
|author= Ben Q. Liu,Dale L. Goodhue,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = In this paper we consider the impact of trust on a new visitor's intention to revisit a website, but instead of using the typical expectancy-value theories as our conceptual basis, we look at the issue from the perspective of cognitive complexity and "humans as cognitive misers." Starting with the suggestion that it is cognitively taxing to distrust, we propose that in order to conserve on cognitive resources, once a new visitor has convinced him or herself that a website is "trustworthy enough," that user will drop trustworthiness from their concerns and only consider other characteristics of the website (e.g., task-technology fit, aesthetic appeal, etc.) in determining their revisit intention. This leads to what we call a "trust tipping point" and two different worlds of trust. Above the tipping point revisit intention is constructed in one way, and below the trust tipping point it is constructed in a quite different way. This perspective results in very different recommendations for website designers as to the likely payoff from improving task-technology fit, aesthetic appeal, or trustworthiness, depending upon where their existing website stands relative to the trust tipping point. To test our hypotheses we used data from 314 student website users, and expanded a technique called piecewise regression (Neter et al. Applied Linear Statistical Models, 4th ed.) to allow us to analyze data as two different linear surfaces, joined at the tipping point. We found good support for our assertions that users operate differently above and below a trust tipping point.
|keyword = accessibility-diagnosticity,bounded rationality,humans as cognitive misers,piecewise regression,task-technology fit,trust,web aesthetics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Real-Time Tactical and Strategic Sales Management for Intelligent Agents Guided by Economic Regimes'''
{{header}}
{{article
|author= Wolfgang Ketter,John Collins,Maria Gini,Alok Gupta,Paul Schrater,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Many enterprises that participate in-dynamic markets need to make product pricing and inventory resource utilization decisions in real time. We describe a family of statistical models that addresses these needs by combining characterization of the economic environment with the ability to predict future economic conditions to make tactical (short-term) decisions, such as product pricing, and strategic (long-term) decisions, such as level of finished goods inventories. Our models characterize economic conditions, called economic regimes, in the form of recurrent statistical patterns that have clear qualitative interpretations. We show how these models can be used to predict prices, price trends, and the probability of receiving a customer order at a given price. These "regime" models are developed using statistical analysis of historical data and are used in real time to characterize observed market conditions and predict the evolution of market conditions over multiple time scales. We evaluate our models using a testbed derived from the Trading Agent Competition for Supply Chain Management, a supply chain environment characterized by competitive procurement, sales markets, and dynamic pricing. We show how regime models can be used to inform both short-term pricing decisions and long-term resource allocation decisions. Results show that our method outperforms more traditional short- and long-term predictive modeling approaches.
|keyword = enabling technologies,agent-mediated electronic commerce,dynamic pricing,price forecasting,economic regimes,supply chain,dynamic markets,trading agent competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of the Presence of Organic Listing in Search Advertising'''
{{header}}
{{article
|author= Lizhen Xu,Jianqing Chen,Andrew Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper analyzes how the presence of organic listing as a competing information source affects advertisers' sponsored bidding and the equilibrium outcomes in search advertising. We consider a game-theoretic model in which two firms bid for sponsored advertising slots provided by a monopolistic search engine and then compete for consumers in price in the product market. Firms are asymmetrically differentiated in market preference and are given different exposure in organic listing aligned with their market appeal. We identify two aspects of a firm's sponsored bidding incentive, namely,. the promotive and the preventive incentives. The presence of organic listing alters firms' sponsored bidding incentives such that the stronger firm has primarily preventive incentive, whereas the weaker has mainly promotive incentive. We show that the preventive incentive decreases and the promotive incentive increases as the difference in firms' market appeal decreases, and as a result, even the weaker firm may outbid the stronger competitor under such a co-listing setting. We further examine how the presence of organic listing affects the equilibrium outcomes by comparing it with a benchmark case in which there is only a sponsored list. We show that the differentiated exposure in the organic list gives the weaker advertiser chances to win a better sponsored position, which improves the overall information structure the search engine provides. As a result, the equilibrium social welfare, sales diversity, and consumer surplus increase. Although the presence of the free exposure from the organic list may reduce advertisers' sponsored bidding incentive per se, the overall effect benefits the search engine's growth in the long run.
|keyword = organic listing,sponsored bidding,search advertising,information structure,asymmetric differentiation,price competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network Neutrality and Congestion Sensitive Content Providers: Implications for Content Variety, Broadband Investment, and Regulation'''
{{header}}
{{article
|author= Jan Kraemer,Lukas Wiewiorra,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We study departures from network neutrality through implementing a quality of service tiering regime in which an Internet service provider charges for prioritization on a nondiscriminatory basis. We find that quality of service tiering may be more efficient in the short run because it better allocates the existing network capacity and in the long run because it provides higher investment incentives due to the increased demand for priority services by the entry of new congestion sensitive content providers. Which network regime is the most efficient depends on the distribution of congestion sensitivity among content providers, but a guideline is that the regime that provides higher incentives for infrastructure investments is more efficient in the long run.
|keyword = telecommunications,net neutrality,quality of service,content variety,investment,regulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Underlying Consumer Heterogeneity in Markets for Subscription-Based IT Services with Network Effects'''
{{header}}
{{article
|author= Marius F. Niculescu,Hyoduk Shin,Seungjin Whang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = In this paper we explore the underlying consumer heterogeneity in competitive markets for subscription-based information technology services that exhibit network effects. Insights into consumer heterogeneity with respect to a given service are paramount in forecasting future subscriptions, understanding the impact of price and information dissemination on market penetration growth, and predicting the adoption path for complementary products that target the same customers as the original service. Employing a continuous-time utility model, we capture the behavior of a continuum of consumers who are differentiated by their intrinsic valuations from using the service. We study service subscription patterns under both perfect and imperfect information dissemination. In each case, we first specify the conditions under which consumer rational behavior supported by the utility model can explain a general observed adoption path, and if so, we explicitly derive the analytical closed-form expression for the consumer valuation distribution. We further explore the impact of awareness and distribution skewness on adoption. In particular, we highlight the practical forecasting importance of understanding the information dissemination process in the market as observed past adoption may be explained by several distinct awareness and heterogeneity scenarios that may lead to divergent adoption paths in the future. Moreover, we show that in the later part of the service lifecycle the subscription decision for new customers can be driven predominantly by information dissemination instead of further price markdowns. We also extend our results to time-varying consumer valuation scenarios. Furthermore, based on our framework, we advance a set of heuristic methods to be applied to discrete-time real industry data for estimation and forecasting purposes. In an empirical exercise, we apply our methodology to the Japanese mobile voice services market and provide relevant managerial insights from the analysis.
|keyword = subscription-based IT services,consumer utility models,consumer information awareness,network effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Individual Self-Protection, Industry Self-Regulation, and Government Regulation on Privacy Concerns: A Study of Location-Based Services'''
{{header}}
{{article
|author= Heng Xu,Hock-Hai Teo,Bernard C. Y. Tan,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This study seeks to clarify the nature of control in the context of information privacy to generate insights into the effects of different privacy assurance approaches on context-specific concerns for information privacy. We theorize that such effects are exhibited through mediation by perceived control over personal information and develop arguments in support of the interaction effects involving different privacy assurance approaches (individual self-protection, industry self-regulation, and government legislation). We test the research model in the context of location-based services using data obtained from 178 individuals in Singapore. In general, the results support our core assertion that perceived control over personal information is a key factor affecting context-specific concerns for information privacy. In addition to enhancing our theoretical understanding of the link between control and privacy concerns, these findings have important implications for service providers and consumers as well as for regulatory bodies and technology developers.
|keyword = privacy,context-specific concerns for information privacy,psychological control,control agency,individual self-protection,industry self-regulation,and government regulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Analyzing Pricing Strategies for Online Services with Network Effects'''
{{header}}
{{article
|author= Min-Seok Pang,Hila Etzion,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = In this study, we model firms that sell a product and a complementary online service, where only the latter displays positive network effects. That is, the value each consumer derives from the service increases with the total number of consumers that subscribe to the service. In addition, the service is valuable only to consumers who buy the product. We consider two pricing strategies: (1) bundle pricing, in which the firm charges a single price for the product and the service, and (2) separate pricing, in which the firm sets the prices of the product and the service separately, and consumers self-select whether to buy both or only the product. We show that in contrast to the common result in the bundling literature, often the monopolist chooses not to offer the bundle (he either sells the service separately or not at all) although bundling would increase both consumer surplus and social welfare. Thus, underprovision of the service can be the market outcome. We also demonstrate that network effects may cause the underprovision of the service.
|keyword = bundling,network effects,price discrimination,online services,online game industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BIDDING BEHAVIOR EVOLUTION IN SEQUENTIAL AUCTIONS: CHARACTERIZATION AND ANALYSIS'''
{{header}}
{{article
|author= Paulo B. Goes,Gilbert G. Karuga,Arvind K. Tripathi,
|source= MIS QUARTERLY
|year= 2012
|abstract = Retailers are increasingly exploiting sequential online auctions as an effective and low cost distribution channel for disposing large quantities of inventory. In such auction environments, bidders have the opportunity of participating in many auctions to learn and choose the bidding strategy that best fits their preferences. Previous studies have mostly focused on identifying bidding strategies in single, isolated online auctions. Using a large data set collected from sequential online auctions, we first characterize bidding strategies in this interesting online environment and then develop an empirical model to explain bidders' adoption of different strategies. We also examine how bidders change their strategies over time. Our findings challenge the general belief that bidders employ their strategies regardless of experience or their specific demand. We. find that bidders' demand, participation experience, and auction design parameters affect their choice of bidding strategies. Bidders with unit demand are likely to choose early bidding strategies, while those with multiple unit demand adopt late bidding strategies. Auction design parameters that affect bidders' perception of demand and supply trends affect bidders' choice of bidding strategies. As bidders gain experience within a sequence of auctions, they start choosing late bidding strategies. Our findings help auctioneers to design auction sequences that maximize their objectives.
|keyword = Sequential online auctions,bidding behavior,bidding strategies,auction design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY OUTSOURCING, KNOWLEDGE TRANSFER, AND FIRM PRODUCTIVITY: AN EMPIRICAL ANALYSIS'''
{{header}}
{{article
|author= Young Bong Chang,Vijay Gurbaxani,
|source= MIS QUARTERLY
|year= 2012
|abstract = Firms are increasingly sourcing internal information systems functions,from external service providers. However, there is limited empirical evidence of the economic impact of this delivery option and, more specifically, of the productivity gains accruing to firms that have outsourced. Moreover, there is little evidence of the role and contributions of the individual mechanisms by which service providers create value for client firms. We are particularly interested in whether client firms benefit from the accumulated knowledge held by information technology (IT) service firms. In this paper, we examine the impact of IT outsourcing on the productivity of firms that choose this mode of services delivery focusing, on the role of IT-related knowledge. Since firms self-select into their optimal sourcing mode, we use a variety of econometric techniques including propensity score-based matching and switching regression to control for potential bias arising from endogenously determined sourcing modes. We demonstrate that IT outsourcing does lead to productivity gains for firms that select this mode of service delivery. Our results also suggest that IT-related knowledge held by IT services vendors enables these productivity gains, the magnitude of which is moderated by a firm's IT intensity. Moreover, the value of outsourcing to a client firm increases with its propensity for outsourcing, which in turn depends on firm-specific attributes including efficiency level, financial leverage, and variability in business conditions. Our analyses also show that firms that outsource have been able to achieve additional productivity gains from contracting out compared with their counterfactuals.
|keyword = IT outsourcing,productivity,knowledge transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACT OF USER SATISFACTION WITH MANDATED CRM USE ON EMPLOYEE SERVICE QUALITY'''
{{header}}
{{article
|author= J. J. Po-An Hsieh,Arun Rai,Stacie Petter,Ting Zhang,
|source= MIS QUARTERLY
|year= 2012
|abstract = An increasing number of organizations are now implementing customer relationship management (CRM) systems to support front-line employees' service tasks. With the belief that CRM can enhance employees' service quality, management often mandates employees to use the implemented CRM. However, challenges emerge if/when employees are dissatisfied with using the system. To understand the role of front-line employee users' satisfaction with their mandated use of CRM in determining their service quality, we conducted afield study in one of the largest telecommunications service organizations in China and gathered time-lagged data from self-reported employee surveys, as well as from the firm's archival data sources. Our results suggest that employees' overall user satisfaction (UserSat) with their mandated use of CRM has a positive impact on employee service quality (ESQ) above and beyond the expected positive impacts that job dedication (JD) and embodied service knowledge (ESK) have on ESQ. Interestingly, the positive effect of UserSat on ESQ is comparable to the positive effects of JD and ESK, respectively, on ESQ. Importantly, UserSat and ESK have a substitutive effect on ESQ, suggesting that the impact of UserSat on ESQ is stronger/weaker for employees with lower/higher levels of ESK. Finally, ESQ predicts customer satisfaction with customer service employees (CSWCSE); ESQ also fully mediates the impacts of UserSat and ESK, and partially mediates the impact of JD, on CSWCSE. The results of this study emphasize the importance of user satisfaction in determining employees' task outcomes when use of an information system is mandated.
|keyword = User satisfaction,mandatory use,customer relationship management systems,employee service quality,job dedication,embodied service knowledge,task performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIALECTICS OF COLLECTIVE MINDING: CONTRADICTORY APPROPRIATIONS OF INFORMATION TECHNOLOGY IN A HIGH-RISK PROJECT'''
{{header}}
{{article
|author= Jessica Luo Carlo,Kalle Lyytinen,Jr. Richard J. Boland,
|source= MIS QUARTERLY
|year= 2012
|abstract = In unpredictable and unforgiving environments, organizations need to act with care and reliability, often referred to as collective mindfulness. We present a theory-generating, interpretative field study of a highly complex and successful building project by architect Frank O. Gehry. We argue that what has been labeled collective mindfulness is only possible through a dialectic process of collective minding, in which organizational actors simultaneously exhibit elements of being mindful and mindless. Our analysis reveals that collective minding emerges from struggling with contradictions in the five elements of mindfulness. We argue that when actors struggle with these dialectic tensions, the same information technology capabilities are enacted as multiple, contradictory technologies-in-practice. Implications for the further study of collective minding and the appropriation of IT capabilities are discussed.
|keyword = Collective minding,collective mindfulness,high-reliability organizations (HROs),complex socio-technical systems,dialectics,IT affordances,IT capabilities,technology-in-practice,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GROWTH AND SUSTAINABILITY OF MANAGED SECURITY SERVICES NETWORKS: AN ECONOMIC PERSPECTIVE'''
{{header}}
{{article
|author= Alok Gupta,Dmitry Zhdanov,
|source= MIS QUARTERLY
|year= 2012
|abstract = Managed security service provider (MSSP) networks are a form of collaboration where several firms share resources such as diagnostics, prevention tools, and policies to provide security for their computer networks. While the decision to outsource the security operations of an organization may seem counterintuitive, there are potential benefits from joining an MSSP network that include pooling of risk and access to more security-enabling resources and expertise. We examine structural results explaining the reasons firms join an MSSP network, and characterize the growth of MSSP network size under different forms of ownership (monopoly versus consortium). We find that the need for an initial investment in MSSP networks (which is necessary to overcome the stalling effect) only affects the optimal network size for a consortium but has no impact on the optimal network size for a profit-maximizing monopolist. Our results provide an explanation why the majority of the MSSPs are for-profit entities and consortium-based MSSPs are less common. Such a market structure can be attributed to the potential for larger size by the for-profit MSSP owner combined with beneficial pricing structure and a lack of growth uncertainty for the early clients.
|keyword = Information security,managed security services,outsourcing,network effects,network growth,network ownership structure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECTIVENESS OF ONLINE SHOPPING CHARACTERISTICS AND WELL-DESIGNED WEBSITES ON SATISFACTION'''
{{header}}
{{article
|author= Jifeng Luo,Sulin Ba,Han Zhang,
|source= MIS QUARTERLY
|year= 2012
|abstract = Electronic commerce has grown rapidly in recent years. However, surveys of online customers continue to indicate that many remain unsatisfied with their online purchase experiences. Clearly, more research is needed to better understand what affects customers' evaluations of their online experiences. Through a large dataset gathered from two online websites, this study investigates the importance of product uncertainty and retailer visibility in customers' online purchase decisions, as well as the mitigating effects of retailer characteristics. We find that high product uncertainty and low retailer visibility have a negative impact on customer satisfaction. However, a retailer's service quality, website design, and pricing play important roles in mitigating the negative impact of high product uncertainty and low retailer visibility. Specifically, service quality can mitigate the negative impacts of low retailer visibility and high product uncertainty in online markets. Website design, on the other hand, helps to reduce the impact of product uncertainty when experience goods are involved.
|keyword = Product uncertainty,retailer visibility,service quality,website design,customer satisfaction,search goods,experience goods,archival data,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DOES INFORMATION TECHNOLOGY INVESTMENT INFLUENCE A FIRM'S MARKET VALUE? A CASE OF NON-PUBLICLY TRADED HEALTHCARE FIRMS'''
{{header}}
{{article
|author= Rajiv Kohli,Sarv Devaraj,Terence T. Ow,
|source= MIS QUARTERLY
|year= 2012
|abstract = Managers make informed information technology investment decisions when they are able to quantify how IT contributes to firm performance. While financial accounting measures inform IT's influence on retrospective firm performance, senior managers expect evidence of how IT influences prospective measures such as the firm's market value. We examine the efficacy of IT's influence on firm value combined with measures of financial performance for non-publicly traded (NPT) hospitals that lack conventional market-based measures. We gathered actual sale transactions for NPT hospitals in the United States to derive the q ratio, a measure of market value. Our findings indicate that the influence of IT investment on the firm is more pronounced and statistically significant on firm value than exclusively on the accounting performance measures. Specifically, we find that the impact of IT investment is not significant on return on assets (ROA) and operating income for the same set of hospitals. This research note contributes to research and practice by demonstrating that the overall impact of IT is better understood when accounting measures are complemented with the firm's market value. Such market valuation is also critical in merger and acquisition decisions, an activity that is likely to accelerate in the healthcare industry. Our findings provide hospitals, as well as other NPT firms, with insights into the impact of IT investment and a pragmatic approach to demonstrating IT's contribution to firm value.
|keyword = IT payoff,firm valuation,non-publicly traded hospitals,NPT,health care,firm performance,market value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUSINESS INTELLIGENCE AND ANALYTICS: FROM BIG DATA TO BIG IMPACT'''
{{header}}
{{article
|author= Hsinchun Chen,Roger H. L. Chiang,Veda C. Storey,
|source= MIS QUARTERLY
|year= 2012
|abstract = Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.
|keyword = Business intelligence and analytics,big data analytics,Web 2.0,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUSINESS INTELLIGENCE IN BLOGS: UNDERSTANDING CONSUMER INTERACTIONS AND COMMUNITIES'''
{{header}}
{{article
|author= Michael Chau,Jennifer Xu,
|source= MIS QUARTERLY
|year= 2012
|abstract = The increasing popularity of Web 2.0 has led to exponential growth of user-generated content in both volume and significance. One important type of user-generated content is the blog. Blogs encompass useful information (e.g., insightful product reviews and information-rich consumer communities) that could potentially be a gold mine for business intelligence, bringing great opportunities for both academic research and business applications. However, performing business intelligence on blogs is quite challenging because of the vast amount of information and the lack of commonly adopted methodology for effectively collecting and analyzing such information. In this paper, we propose a framework fir gathering business intelligence from blogs by automatically collecting and analyzing blog contents and bloggers' interaction networks. Through a system developed using the framework, we conducted two case studies with one case focusing on a consumer product and the other on a company. Our case studies demonstrate how to use the framework and appropriate techniques to effectively collect, extract, and analyze blogs related to the topics of interest, reveal novel patterns in the blogger interactions and communities, and answer important business intelligence questions in the domains. The framework is sufficiently generic and can be applied to any topics of interest, organizations, and products. Future academic research and business applications related to the topics examined in the two cases can also be built using the findings of this study.
|keyword = Business intelligence,Web mining,blog mining,social networks,design science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A SOCIAL NETWORK-BASED INFERENCE MODEL FOR VALIDATING CUSTOMER PROFILE DATA'''
{{header}}
{{article
|author= Sung-Hyuk Park,Soon-Young Huh,Wonseok Oh,Sang Pil Han,
|source= MIS QUARTERLY
|year= 2012
|abstract = Drawing from the social and relational perspectives, this study offers an innovative conceptualization and operational approach regarding the validation of self:reported customer demographic data, which has become an essential corporate asset for harnessing business intelligence. Specifically, based on social network and homophily paradigms in which individuals have a natural tendency to associate and interact frequently with others with similar characteristics, we constructed a relational inference model to determine the accuracy of self-administered consumer profiles. In addition, to flirt her enhance the reliability of our model's prediction capability, we employed the entropy mechanism that minimizes potential biases that may arise from a simple probabilistic approach. To empirically validate the accuracy of our inference framework, we obtained and analyzed over 20 million actual call transactions supplied by one of the largest global telecommunication service providers. The results suggest that our social network-based inference model consistently outperforms other competing mechanisms (e.g., weighted average and simple relational classifier) regardless of the criteria choice (e.g., number of call receivers, call duration, and call frequency), with an accuracy rate of approximately 93 percent. Finally, to confirm the generalizability of our findings, we conducted simulation experiments to validate the robustness of the results in response to variations in parameter values and increases in potential noise in the data. We discuss several implications related to business intelligence for both research and practice, and offer new directions for future studies.
|keyword = Customer profile,data quality,business intelligence,inference model,social network,query processing system,simulation experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WEB 2.0 ENVIRONMENTAL SCANNING AND ADAPTIVE DECISION SUPPORT FOR BUSINESS MERGERS AND ACQUISITIONS'''
{{header}}
{{article
|author= Raymond Y. K. Lau,Stephen S. Y. Liao,K. F. Wong,Dickson K. W. Chiu,
|source= MIS QUARTERLY
|year= 2012
|abstract = Globalization has triggered a rapid increase in cross-border mergers and acquisitions (M&As). However, research shows that only 17 percent of cross-border M&As create shareholder value. One of the main reasons for this poor track record is top management's lack of attention to nonfinancial aspects (e.g., sociocultural aspects) of M&As. With the rapid growth of Web 2.0 applications, online environmental scanning provides top executives with unprecedented opportunities to tap into collective web intelligence to develop better insights about the sociocultural and political-economic factors that cross-border M&As face. Grounded in Porter's five forces model, one major contribution of our research is the design of a novel due diligence scorecard model that leverages collective web intelligence to enhance M&A decision making. Another important contribution of our work is the design and development of an adaptive business intelligence (BI) 2.0 system underpinned by an evolutionary learning approach, domain-specific sentiment analysis, and business relation mining to operationalize the aforementioned scorecard model for adaptive M&A decision support. With Chinese companies' cross-border M&As as the business context, our experimental results confirm that the proposed adaptive BI 2.0 system can significantly aid decision makers under different M&A scenarios. The managerial implication of our findings is that firms can apply the proposed BI 2.0 technology to enhance their strategic decision making, particularly when making cross-border investments in targeted markets for which private information may not be readily available.
|keyword = Domain-specific sentiment analysis,business relation mining,statistical learning,evolutionary learning,business intelligence,Web 2.0,mergers and acquisitions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NETWORK-BASED MODELING AND ANALYSIS OF SYSTEMIC RISK IN BANKING SYSTEMS'''
{{header}}
{{article
|author= Daning Hu,J. Leon Zhao,Zhimin Hua,Michael C. S. Wong,
|source= MIS QUARTERLY
|year= 2012
|abstract = In the wake of the 2008 financial tsunami, existing methods and tools for managing financial risk have been criticized for weaknesses in monitoring and alleviating risks at the systemic level. A 2009 article in Nature suggested new approaches to modeling economic meltdowns are needed to prevent future financial crises. However, existing studies have not focused on analysis of systemic risk at the individual bank level in a banking network, which is essential for monitoring and mitigating contagious bank failures. To this end, we develop a network approach to risk management (NARM) for modeling and analyzing systemic risk in banking systems. NARM views banks as a network linked through financial relationships. It incorporates network and financial principles into a business intelligence (BI) algorithm to analyze systemic risk attributed to each individual bank via simulations based on real-world data from the Federal Deposit Insurance Corporation. Our research demonstrates the feasibility of modeling and analyzing systemic risk at the individual bank level in a banking network using a BI-based approach. In terms of business impact, NARM offers a new means for predicting contagious bank failures and determining capital injection priorities in the wake of financial crises. Our simulation study shows that under significant market shocks, the interbank payment relationship becomes more influential than the correlated bank portfolio relationship in determining an individual bank's survival. These insights should help financial regulators devise more effective policies and mechanisms to prevent the collapse of a banking system. Further, NARM and the simulation procedure driven by real-world data proposed in this study have instructional value to similar research areas such as bank stress testing, where time series data and business networks may be studied.
|keyword = Systemic risk,contagious bank failures,business intelligence,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''METAFRAUD: A META-LEARNING FRAMEWORK FOR DETECTING FINANCIAL FRAUD'''
{{header}}
{{article
|author= Ahmed Abbasi,Conan Albrecht,Anthony Vance,James Hansen,
|source= MIS QUARTERLY
|year= 2012
|abstract = Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.
|keyword = Fraud detection,financial statement fraud,feature construction,meta-learning,business intelligence,design science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A HIDDEN MARKOV MODEL FOR COLLABORATIVE FILTERING'''
{{header}}
{{article
|author= Nachiketa Sahoo,Param Vir Singh,Tridas Mukhopadhyay,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this paper, we present a method to make personalized recommendations when user preferences change over time. Most of the works in the recommender systems literature have been developed under the assumption that user preference has a static pattern. However, this is a strong assumption especially when the user is observed over a long period of time. With the help of a data set on employees' blog reading behavior, we show that users' product selection behaviors change overtime. We propose a hidden Markov model to correctly interpret the users' product selection behaviors and make personalized recommendations. The user preference is modeled as a hidden Markov sequence. A variable number of product selections of different types by each user in each time period requires a novel observation model. We propose a negative binomial mixture of multinomial to model such observations. This allows us to identify stable global preferences of users and to track individual users through these preferences. We evaluate our model using three real-world data sets with different characteristics. They include data on employee blog reading behavior inside a firm, users' movie rating behavior at Netflix, and users' music listening behavior collected through last.fm. We compare the recommendation performance of the proposed model with that of a number of a filtering algorithms and a recently proposed temporal link prediction algorithm. We find that the proposed HMM-based collaborative filter performs as well as the best among the alternative algorithms when the data is sparse or static. However, it outperforms the existing algorithms when the data is less sparse and the user preference is changing. We further examine the performances of the algorithms using simulated data with different characteristics and highlight the scenarios where it is beneficial to use a dynamic model to generate product recommendation.
|keyword = Recommender systems,collaborative filtering,changing preference,dynamic models,latent class model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network Positions and Contributions to Online Public Goods: The Case of Chinese Wikipedia'''
{{header}}
{{article
|author= Xiaoquan (Michael) Zhang,Chong (Alex) Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = We study the effect of collaboration network structure on the contribution behavior of participating editors in Wikipedia. Collaboration in Wikipedia is organized around articles, and any two editors co-editing an article have a collaborative relationship. Based on the economic theories about network games and social role theory, we propose that an editor's position in the collaboration network influences the editor's decisions about her total contribution as well as the allocation of her efforts. By leveraging panel data collected from the Chinese language version of Wikipedia and a natural experiment resulting from blocking it in mainland China, we find strong support for the proposed effect of network position on contribution behavior. Our analysis further reveals that different aspects of an individual's network position have distinct implications. This research enhances our understanding about how collaboration network structure shapes individual behavior in online mass collaboration platforms.
|keyword = effort allocation,mass collaboration,natural experiment,network centrality,online public goods,Wikipedia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Content Contribution for Revenue Sharing and Reputation in Social Media: A Dynamic Structural Model'''
{{header}}
{{article
|author= Qian Tang,Bin Gu,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = This study examines the incentives for content contribution in social media. We propose that exposure and reputation are the major incentives for contributors. Besides, as more and more social media Web sites offer advertising-revenue sharing with some of their contributors, shared revenue provides an extra incentive for contributors who have joined revenue-sharing programs. We develop a dynamic structural model to identify a contributor's underlying utility function from observed contribution behavior. We recognize the dynamic nature of the content-contribution decision-that contributors are forward-looking, anticipating how their decisions affect future rewards. Using data collected from YouTube, we show that content contribution is driven by a contributor's desire for exposure, revenue sharing, and reputation and that the contributor makes decisions dynamically.
|keyword = content contribution,contribution motivation,dynamic structural model,reputation,revenue sharing,social media,YouTube,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Hacker Behavior, Network Effects, and the Security Software Market'''
{{header}}
{{article
|author= Debabrata Dey,Atanu Lahiri,Guoying Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = The market for security software has witnessed an unprecedented growth in recent years. A closer examination of this market reveals certain idiosyncrasies that are not observed in a traditional market. For example, it is a highly competitive market with over 80 vendors. Yet the market coverage is relatively low. Prior research has not attempted to explain what makes this market so different. In this paper, we develop an economic model to find possible answers to this question. Our model uses existing classification of different types of attacks and models their resulting network effects. We find that the negative network effect from indirect attacks, which is further enhanced by value-based targeted attacks, provides a possible explanation for the unique structure of this market. Overall, our results highlight the unique nature of the security software market, furnish rigorous arguments for several counterintuitive observations in the real world, and provide managerial insights for vendors on market competition.
|keyword = market structure,mass attacks,negative network effect,network effect,oligopoly,pricing,security software,strategic hacker,targeted attacks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Co-opetition Between Differentiated Platforms in Two-Sided Markets'''
{{header}}
{{article
|author= Ravi Mantena,Rajib L. Saha,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Technology is an important factor underlying the value propositions of intermediary platforms in two-sided markets. Here, we address two key questions related to the effect of technology in platform markets. First, how does technology asymmetry affect competition between platforms? Second, how does it affect the incentives for platforms to collaborate? Using a game-theoretic model of a two-sided market where technology strongly influences network value, we show that small asymmetries in platform technologies can translate into large differences in their profitability. We find that technology improvements by the inferior platform do not significantly increase its profits, but can reduce opportunities for fruitful cooperation, since collaboration is less likely in markets with closely matched competitors. We also show that collaboration is most profitable when it takes the form of direct network interconnection. Interestingly, collaboration may provide incentives for a dominant platform to accommodate entry, where it would not otherwise do so.
|keyword = competitive strategy,co-opetition,game theory,network sharing,platform interconnections,technology platforms,two-sided markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Theory and Analysis of Company-Sponsored Value Co-Creation'''
{{header}}
{{article
|author= Li Chen,James R. Marsden,Zhongju Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = In today's dynamic business environment, companies are under tremendous pressure to become more innovative and maintain a steady stream of ideas that can lead to new and improved products and services. Companies have begun to explore the possibility of capturing consumers' "collective intelligence" by establishing firm-sponsored online brainstorming sites where individuals can share their ideas and offer comments on the ideas contributed by others. We term these sites "Company-Sponsored Online Co-Creation Brainstorming" (COCB). The value of this open and voluntary co-creation depends largely on members' contribution levels, the quality of the contributions, and sustained participation. In this paper, utilizing Zwass's taxonomy of co-creation value as a base, we structure a taxonomic framework of COCBs and an accompanying basic model of COCBs. We then present a series of hypotheses concerning the relationships between the model's various factors and specific COCB activities. We validate the model using empirical data collected over two and a half years, starting from the initiation of a pioneering company-sponsored online brainstorming site. Our analyses demonstrate that the level of peer feedback and the responsiveness (speed) of sponsor company feedback have significant influences on both members' contribution levels and duration of active participation. The sponsoring company's feedback, however, seems to influence only the quality of member's contribution level. On the practical side, the outcomes suggest that sponsoring companies should develop efficient processes for reviewing and responding to submitted ideas. Regarding theory, our findings provide an initial piece of contextualized research that offers implications for theory building in the COCB context, most notably the identification of key relationships between feedback (both peer and company) and participant activity levels and duration of participation.
|keyword = brainstorming,co-creation,contribution quality,sustained participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Data-Driven Approach to Measure Web Site Navigability'''
{{header}}
{{article
|author= Xiao Fang,Paul Jen-Hwa Hu,Michael Chau,Han-fen Hu,Zhuo Yang,Olivia R. Liu Sheng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Web site navigability refers to the degree to which a visitor can follow a Web site's hyperlink structure to successfully find information with efficiency and ease. In this study, we take a data-driven approach to measure Web site navigability using Web data readily available in organizations. Guided by information foraging and information-processing theories, we identify fundamental navigability dimensions that should be emphasized in metric development. Accordingly, we propose three data-driven metrics-namely, power, efficiency, and directness-that consider Web structure, usage, and content data to measure a Web site's navigability. We also develop a Web mining-based method that processes Web data to enable the calculation of the proposed metrics. We further implement a prototype system based on the Web mining-based method and use it to assess the navigability of two sizable, real-world Web sites with the metrics. To examine the analysis results by the metrics, we perform an evaluation study that involves these two sites and 248 voluntary participants. The evaluation results show that user performance and assessments are consistent with the analysis results revealed by our metrics. Our study demonstrates the viability and practical value of data-driven metrics for measuring Web site navigability, which can be used for evaluative, diagnostic, or predictive purposes.
|keyword = data-driven navigability metrics,Web metrics,Web mining,Web site navigability,Web site navigation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Hybrid Relational-Contractual Governance for Business Process Outsourcing'''
{{header}}
{{article
|author= Arun Rai,Mark Keil,Rob Hornyak,Kim Wuellenweber,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = We examined 335 business process outsourcing (BPO) ventures to understand the effect of contractual and relational governance factors on BPO satisfaction from the client's perspective. While both contractual and relational factors explain significant variance in BPO satisfaction, relational factors dominate. By examining interactions between key contractual and relational mechanisms, we found that elements of the two governance approaches operate as substitutes with respect to BPO satisfaction. Specifically, the relational mechanism, trust, was found to substitute for contractually specified activity expectations, goal expectations, and contractual flexibility. Similarly, the relational mechanism, information exchange, was found to substitute for contractually specified activity expectations and goal expectations. Finally, the relational mechanism, conflict resolution, was found to substitute for contractually specified goal expectations. Our results can be applied to more effectively realize controls in outsourcing contexts and to design governance systems that integrate contractual and relational governance mechanisms based on the characteristics of client-vendor relationships.
|keyword = business process outsourcing,controls,formal contract,hybrid governance,relational governance,services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dual Role of IT-Assisted Communication in Patient Care: A Validated Structure-Process-Outcome Framework'''
{{header}}
{{article
|author= Corey M. Angst,Sarv Devaraj,John D'Arcy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Despite the fact that about 90 percent of information transactions in hospitals are communications between patients, doctors, nurses, and other staff, little research has addressed the role that information technology (IT) plays in improving the efficiency and effectiveness of these communications-based transactions. Addressing this research gap is important considering that a substantial number of adverse hospital events stem from communication failures. Furthermore, effective communication is a major driver of patient satisfaction in hospitals. Using a structure-process-outcome (SPO) framework and guided by the strategic role of IT literature, we develop a model that includes "structure," operationalized as organizational characteristics and two different categories of IT; " process," two different communication-based processes; and " outcomes," quantified as case-mix adjusted mortality, patient loyalty, and patient ratings. Specifically, we hypothesize that a subset of clinical IT (cardiology IT) will affect technical protocols of patient care, which in turn affects mortality, while administrative IT will affect interpersonal patient care, which relates to patient loyalty and ratings. Thus, IT can serve as a double-edged sword affecting both technical and interpersonal processes of care, but possibly independently and differentially. We test our hypotheses on 2,179 hospitals using data collected and matched from three different sources. Our findings suggest that different types of IT differentially affect hospital processes and these same processes influence performance metrics such as mortality and patient satisfaction. For example, cardiology IT has a greater effect on objective patient health status through improvements in the technical protocols of care. Surprisingly, administrative IT was shown to adversely affect interpersonal care processes. It could be true that the IT is intrusive and interferes in the doctor-patient relationship; however, a post hoc analysis suggests the possibility of curvilinear impacts. Thus, managers should recognize that over- and underinvestment in IT can potentially have negative effects on performance and these results vary by IT type. Both technical and interpersonal processes yielded significant relationships to their respective outcomes and some cross-outcome effects were found, further suggesting that the mediating role of processes is an important link between IT and value.
|keyword = business value of IT,health information technology,operational IT,strategic IT,structure-process-outcome,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Contingencies Associated with the Early Adoption of Customer-Facing Web Portals'''
{{header}}
{{article
|author= Aaron Baird,Michael F. Furukawa,T. S. Raghu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Web-based portals extend many convenient and collaborative capabilities to consumers and are being adopted by small firms with ever greater frequency, especially in the context of health care. The early adoption of patient portals by ambulatory-care clinics (outpatient health providers) presents a unique opportunity to more fully understand the characteristics of supply-side adopters in a context where firms (ambulatory-care clinics) are extending digital services to consumers (patients). Using diffusion of innovations literature and contingency theory as the theoretical base, we expand upon the firm characteristics traditionally considered to be predictors of adoption (e.g., firm size, slack resources, competition, capabilities, and management support) and examine how demand contingencies, service contingencies, and learning externality contingencies affect patient portal adoption by ambulatory-care clinics in the United States. We find that early adopters often have a structure in place that provides support for innovations (e.g., part of integrated delivery systems), as would be predicted by diffusion of innovation theory. We also find, though, that service contingencies associated with continuity of care, learning externality contingencies associated with local influences, and select demand contingencies associated with the local market significantly influence patient portal adoption decisions. Our findings suggest that the adoption and diffusion of patient portals may be affected by more than traditionally considered "dominant" firm characteristics and provide insights into how such customer-facing systems may be affected by contingent factors.
|keyword = adoption of innovations,bivariate probit with sample selection,demand contingencies,factors of adoption,learning externality contingencies,patient portals,service contingencies,Web portals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Resource Structuring or Capability Building? An Empirical Study of the Business Value of Information Technology'''
{{header}}
{{article
|author= Nianxin Wang,Huigang Liang,Weijun Zhong,Yajiong Xue,Jinghua Xiao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = This paper examines two ways to create business value of information technology (BVIT): resource structuring and capability building. We develop a research model positing that IT resources and IT capabilities enhance a firm's performance by providing support to its competitive strategies and core competencies, and the strengths of these supports vary in accord with environmental dynamism. The model is empirically tested using data collected from 296 firms in China. It is found that IT resources generate more business effects in stable environments than in dynamic environments, while IT capabilities generate more business effects in dynamic environments than in stable environments. The results suggest that the BVIT creation mechanism in stable environments is primarily resource structuring while the mechanism in dynamic environments is primarily capability building.
|keyword = business value of IT,capability building,competitive strategy,core competence,environmental dynamism,resource structuring,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Productivity of Information Technology Investments: New Evidence from IT Labor Data'''
{{header}}
{{article
|author= Prasanna Tambe,Lorin M. Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper uses newly collected panel data that allow for significant improvements in the measurement and modeling of information technology (IT) productivity to address some longstanding empirical limitations in the IT business value literature. First, we show that using generalized method of moments-based estimators to account for the endogeneity of IT spending produces coefficient estimates that are only about 10% lower than unadjusted estimates, suggesting that the effects of endogeneity on IT productivity estimates may be relatively small. Second, analysis of the expanded panel suggests that (a) IT returns are substantially lower in midsize firms than in Fortune 500 firms; (b) they materialize more slowly in large firms-in midsize firms, unlike in larger firms, the short-run contribution of IT to output is similar to the long-run output contribution; and (c) the measured marginal product of IT spending is higher from 2000 to 2006 than in any previous period, suggesting that firms, and especially large firms, have been continuing to develop new, valuable IT-enabled business process innovations. Furthermore, we show that the productivity of TT investments is higher in manufacturing sectors and that our productivity results are robust to controls for IT labor quality and outsourcing levels.
|keyword = business value of IT,economics of IS,econometrics,productivity,IT labor,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Analysis of the Contractual and Information Structures of Business Process Outsourcing Relationships'''
{{header}}
{{article
|author= Deepa Mani,Anitesh Barua,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The emergence of information-intensive business process outsourcing (BPO) relationships calls for the study of exchange performance beyond traditional considerations of the contractual structure that facilitates cooperative intent to include the information structure that facilitates the mutual exchange of information to enact cooperative intent and coordinate actions between the user firm and the service provider. Yet, there has been little analysis of the drivers and performance effects of the information structure of BPO relationships, including its linkages to the underlying contractual structure. This study integrates perspectives in neo-institutional economics and information processing to develop and test the theoretical argument that the extent of use and performance effects of the information structure of the BPO relationship are greater in time and materials BPO contracts than in fixed-price BPO contracts. Survey data on 134 BPO relationships provide empirical support for our hypotheses. The synergistic impact of incentives and information on BPO performance emphasizes that their joint assessment is necessary to enhance the explanatory power of extant theories of organization. This result also has implications for achieving maximum benefits from complex BPO arrangements that are more likely to be characterized by time and material contracts.
|keyword = BPO,outsourcing,governance,contractual structure,information structure,coordination,information processing,performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Postrelease Testing and Software Release Policy for Enterprise-Level Systems'''
{{header}}
{{article
|author= Zhengrui Jiang,Sumit Sarkar,Varghese S. Jacob,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = prior work on software release policy implicitly assumes that testing stops at the time of software release. In this research, we propose an alternative release policy for custom-built enterprise-level software projects that allows testing to continue for an additional period after the software product is released. Our analytical results show that the software release policy with postrelease testing has several important advantages over the policy without postrelease testing. First, the total expected cost is lower. Second, even though the optimal time to release the software is shortened, the reliability of the software is improved throughout its lifecycle. Third, although the expected number of undetected bugs is higher at the time of release, the expected number of software failures in the field is reduced. We also analyze the impact of market uncertainty on the release policy and find that all our prior findings remain valid. Finally, we examine a comprehensive scenario where in addition to uncertain market opportunity cost, testing resources allocated to the focal project can change before the end of testing. Interestingly, the software should be released earlier when testing resources are to be reduced after release.
|keyword = software reliability,market opportunity cost,market uncertainty,learning,Bayes risk principle,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Social Network Structure on Enterprise Systems Success: A Longitudinal Multilevel Analysis'''
{{header}}
{{article
|author= Sharath Sasidharan,Radhika Santhanam,Daniel J. Brass,Vallabh Sambamurthy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The implementation of enterprise systems has yielded mixed and unpredictable outcomes in organizations. Although the focus of prior research has been on training and individual self-efficacy as important enablers, we examine the roles that the social network structures of employees, and the organizational units where they work, play in influencing the postimplementation success. Data were gathered across several units within. a large organization: immediately after the implementation, six months after the implementation, and one year after the implementation. Social network analysis was used to understand the effects of network structures, and hierarchical linear modeling was used to capture the multilevel effects at unit and individual levels. At the unit level of analysis, we found that centralized structures inhibit implementation success. At the individual level of analysis, employees with high in-degree and betweenness centrality reported high task impact and information quality. We also found a cross-level effect such that central employees in centralized units reported implementation success. This suggests that individual-level success can occur even within a unit structure that is detrimental to unit-level success. Our research has significant implications for the implementation of enterprise systems in large organizations.
|keyword = enterprise systems,postimplementation,information exchange,learning,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What's in a "Name"? Impact of Use of Customer Information in E-Mail Advertisements'''
{{header}}
{{article
|author= Sunil Wattal,Rahul Telang,Tridas Mukhopadhyay,Peter Boatwright,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = In this study, we examine how consumers respond to firms' use of two types of information for personalization: product preferences and name. We collect a unique data set of over 10 million e-mail advertisements sent by a website to over 600,000 customers who could buy the advertised products from the online merchant. We estimate a two-stage hierarchical model using Bayesian analysis to account for observable and unobservable consumer heterogeneity. Our analysis suggests several interesting results regarding consumers' responses to firms' use of information. When firms use product-based personalization (where the use of information is not explicitly mentioned), consumers respond positively. On the other hand, consumers respond negatively when firms are explicit in their use of personally identifiable information (i.e., a personalized greeting). We also find that negative responses to personalized greetings are moderated by consumers' familiarity with firms. The main contribution of this study is that it not only indicates the economic benefits of personalization in e-mails but also highlights consumers' concerns over the use of information in personalization.
|keyword = personalization,privacy,information use,hierarchical Bayesian model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Business Intelligence to Competitive Intelligence: Inferring Competitive Measures Using Augmented Site-Centric Data'''
{{header}}
{{article
|author= Zhiqiang (Eric) Zheng,Peter Fader,Balaji Padmanabhan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Managers routinely seek to understand firm performance relative to the competitors. Recently, competitive intelligence (CI) has emerged as an important area within business intelligence (BI) where the emphasis is on understanding and measuring a firm's external competitive environment. A requirement of such systems is the availability of the rich data about a firm's competitors, which is typically hard to acquire. This paper proposes a method to incorporate competitive intelligence in BI systems by using less granular and aggregate data, which is usually easier to acquire. We motivate, develop, and validate an approach to infer key competitive Measures about customer activities without requiring detailed cross-firm data. Instead, our method derives these competitive measures for online firms from simple "site-centric" data that are commonly available, augmented with aggregate data summaries that may be obtained from syndicated data providers. Based on data provided by comScore Networks, we show empirically that our method performs well in inferring several key diagnostic competitive measures-the penetration, market share, and the share of wallet-for various online retailers.
|keyword = business intelligence,competitive intelligence,competitive measures,probability models,NBD/Dirichlet,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does the Web Reduce Customer Service Cost? Empirical Evidence from a Call Center'''
{{header}}
{{article
|author= Anuj Kumar,Rahul Telang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Firms are investing millions to deploy Web-based self-services at their call centers. The rationale for such investment is that the firm's cost of interacting with its customers through the Web-based channel is an order of magnitude cheaper than the assisted channels such as telephony. We conduct a field study at the call center of a prominent U.S. health insurance firm to examine this cost-saving rationale of the Web-based self-service channel. On the one hand, the Web channel may substitute for the telephony channel in some cases. On the other hand, the Web also exposes customers to a vast amount of information about their health policy, claims, and coverage; this information can create uncertainty leading to customers seeking more information and hence making more telephone calls. We designed a quasi-natural experiment in our field setting and used difference-in-difference specifications to show that the Web-based self-service usage leads to a 14% increase in telephone calls. We conduct several robustness checks to show that our specifications are robust to any potential selection of customers in the Web-based self-service usage. We further find that the impact of Web portal usage is moderated by the Web portal characteristics. We find that if the information is unambiguous and easily retrievable on the Web, calls for such information decline by 29%. However, for ambiguous information, the calls increase substantially. Our research provides insights into the challenges and opportunities of self-service technologies design.
|keyword = self-service,call center,customer support,Web portal,multichannel service management,health insurance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic Health Records Assimilation and Physician Identity Evolution: An Identity Theory Perspective'''
{{header}}
{{article
|author= Abhay Nath Mishra,Catherine Anderson,Corey M. Angst,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = With the lack of timely and relevant patient information at the point of care increasingly being linked to adverse medical outcomes, effective management and exchange of patient data has emerged as a strategic imperative for the healthcare industry. Healthcare informaticians have suggested that electronic health record systems (EHRS) can facilitate information sharing within and between healthcare stakeholders such as physician practices, hospitals, insurance companies, and laboratories. We examine the assimilation of EHRS in physician practices through a novel and understudied theoretical lens of physicians' identities. Physician practices and the physicians that lead them occupy a central position in the healthcare value chain and possess a number of unique characteristics that differentiate them from other institutional contexts, including a strong sense of affiliation with other physicians, potent professional identities, and a desire for autonomy. We investigate two salient physician identities, those of careprovider and physician community, grounded in the roles physicians play and the groups with which they affiliate. We argue that these identities and their evolution, triggered by EHRS, manifest as both identity reinforcement and deterioration, and are important drivers of EHRS assimilation. We use survey data from 206 physician practices, spread across the United States, to test our theoretical model. Results suggest that physician community identity reinforcement and physician community identity deterioration directly influence the assimilation of EHRS. We further find that the effects of careprovider identity reinforcement and careprovider identity deterioration on EHRS assimilation are moderated by governmental influence. Theoretical and pragmatic implications of the findings are discussed.
|keyword = assimilation,careprovider identity,EHR,electronic health records,health informatics,health IT,identity deterioration,identity reinforcement,identity theory,physician community identity,physician practices,professional identity,role identity,self-categorization theory,social identity,social identity theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design Principles of Integrated Information Platform for Emergency Responses: The Case of 2008 Beijing Olympic Games'''
{{header}}
{{article
|author= Lili Yang,Guofeng Su,Hongyong Yuan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper investigates the challenges faced in designing an integrated information platform for emergency response management and uses the Beijing Olympic Games as a case study. The research methods are grounded in action research, participatory design, and situation-awareness oriented design. The completion of a more than two-year industrial secondment and six-month field studies ensured that a full understanding of user requirements had been obtained. A service-centered architecture was proposed to satisfy these user requirements. The proposed architecture consists mainly of information gathering, database management, and decision support services. The decision support services include situational overview, instant risk assessment, emergency response preplan, and disaster development prediction. Abstracting from the experience obtained while building this system, we outline a set of design principles in the general domain of information systems (IS) development for emergency management. These design principles form a contribution to the information systems literature because they provide guidance to developers who are aiming to support emergency response and the development of such systems that have not yet been adequately met by any existing types of IS. We are proud that the information platform developed was deployed in the real world and used in the 2008 Beijing Olympic Games.
|keyword = emergency response,fire safety,Olympic games,situation-awareness oriented design,participatory design,action research,integrated information platform,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''In Search of Efficient Flexibility: Effects of Software Component Granularity on Development Effort, Defects, and Customization Effort'''
{{header}}
{{article
|author= Ramanath Subramanyam,Narayan Ramasubbu,M. S. Krishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Simultaneously achieving efficiency and flexibility in enterprise software production has been a considerable challenge for firms. Newer software development paradigms such as component-based and model-driven development attempt to overcome this challenge by emphasizing modular design of complex systems. However, there is a paucity of rigorous empirical research on the use of such software methodologies and the associated extent to which trade-offs between efficiency and flexibility can be influenced. Addressing this gap, we investigate the performance outcomes of a model-driven, component-based software development methodology using data collected from an enterprise software development firm that deployed such a methodology for its product development processes. Examining the design, development, and implementation of 92 business software components of the firm's enterprise resource planning product, we discuss how the design of software components, specifically component granularity, affects development efficiency (development effort and defects) and flexibility (customization effort). Our results suggest that (a) components that are coarse grained are associated with higher flexibility (lower customization effort) but are also associated with lower development efficiency (more development effort and defects), and (b) defect proneness of a component plays a mediating role on the relationship between component granularity and flexibility. These findings present strong evidence for the existence of trade-offs between efficiency and flexibility in mass-customized software product life cycles. They establish component granularity as a key design dimension that needs to be managed judiciously to enable potential trade-off shifting mechanisms through the use of software methodologies that emphasize modular design approaches.
|keyword = modular design,model-driven development,component-based software development,efficiency,flexibility,complexity,component granularity,software engineering,project performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pricing Models for Online Advertising: CPM vs. CPC'''
{{header}}
{{article
|author= Kursad Asdemir,Nanda Kumar,Varghese S. Jacob,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Online advertising has transformed the advertising industry with its measurability and accountability. Online software and services supported by online advertising is becoming a reality as evidenced by the success of Google and its initiatives. Therefore, the choice of a pricing model for advertising becomes a critical issue for these firms. We present a formal model of pricing models in online advertising using the principal-agent framework to study the two most popular pricing models: input-based cost per thousand impressions (CPM) and performance-based cost per click-through (CPC). We identify four important factors that affect the preference of CPM to the CPC model, and vice versa. In particular, we highlight the interplay between uncertainty in the decision environment, value of advertising, cost of mistargeting advertisements, and alignment of incentives. These factors shed light on the preferred online-advertising pricing model for publishers and advertisers under different market conditions.
|keyword = online advertising,cost per impression (CPM),cost per click (CPC),pricing models,asymmetric information,delegation,principal agent model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Computational Analysis of Bundle Trading Markets Design for Distributed Resource Allocation'''
{{header}}
{{article
|author= Zhiling Guo,Gary J. Koehler,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Online auction markets play increasingly important roles for resource allocations in distributed systems. This paper builds upon a market-based framework presented by Guo et al. (Guo, Z., G. J. Koehler, A. B. Whinston. 2007. A market-based optimization algorithm for distributed systems. Management Sci. 53(8) 1345-1458), where a distributed system optimization problem is solved by self-interested agents iteratively trading bundled resources in a double auction market run by a dealer. We extend this approach to a dynamic, asynchronous Internet market environment and investigate how various market design factors including dealer inventory policies, market communication patterns, and agent learning strategies affect the computational market efficiency, market liquidity, and implementation. We prove finite convergence to an optimal solution under these various schemes, where individual rational and budget-balanced trading leads to an efficient auction outcome. Empirical investigations further show that the algorithmic implementation is robust to a number of dealer and agent manipulations and scalable to larger sizes and more complicated bundle trading markets. Interestingly, we find that, though both asynchronous communication and asymmetric market information negatively affect the speed of market convergence and lead to more agent welfare loss, agents' ability to predict market prices has a positive effect on both. Contrary to conventional wisdom that a dealer's intertemporal liquidity provisions improve market performance, we find that the dealer's active market intervention may not be desirable in a simple market trading environment where an inherent market liquidity effect dominates, especially when the dealer owns a significant amount of resources. Different from the traditional market insight, our trading data suggest that high trading volume does not correlate to low price volatility and quicker price discovery.
|keyword = electronic markets and auctions,electronic commerce,resource allocation,computational experiment,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information, Technology, and Information Worker Productivity'''
{{header}}
{{article
|author= Sinan Aral,Erik Brynjolfsson,Marshall Van Alstyne,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We econometrically evaluate information worker productivity at a midsize executive recruiting firm and assess whether the knowledge that workers accessed through their electronic communication networks enabled them to multitask more productively. We estimate dynamic panel data models of multitasking, knowledge networks, and productivity using several types of micro-level data: (a) direct observation of more than 125,000 email messages over a period of 10 months; (b) detailed accounting data on individuals' project output and team membership for more than 1,300 projects spanning five years; and (c) survey and interview data about the same workers' IT skills, IT use, and information sharing. We find that (1) more multitasking is associated with more project output, but diminishing marginal returns, and (2) recruiters whose network contacts have heterogeneous knowledge an even distribution of expertise over many project types are less productive on average but more productive when juggling diverse multitasking portfolios. These results show how multitasking affects productivity and how knowledge networks, enabled by IT, can improve worker performance. The methods developed can be replicated in other settings, opening new frontiers for research on social networks and IT value.
|keyword = social networks,productivity,information worker,IT,multitasking,dynamic panel data,system GMM,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of IT-Related Spillovers on Long-Run Productivity: An Empirical Analysis'''
{{header}}
{{article
|author= Young Bong Chang,Vijay Gurbaxani,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper examines the effects of IT-related spillovers on firm-level productivity improvements over a long-term horizon. In contrast, prior research has largely focused on the direct and contemporaneous impacts of IT investments. As a result, we do not fully understand how IT investments are associated with ongoing productivity improvements in future periods and how spillovers influence these gains. In this paper, we examine whether firms receive incremental benefits from IT-related spillovers and whether these spillovers lead to more persistent returns. We focus on the spillovers that accrue to firms from their interindustry transactions, especially the IT services industry. We model and estimate the impact of spillovers on long-run productivity using firm-level data from the manufacturing, transportation, trade, and services sectors. We find that spillover impacts are highly significant, but that the magnitude and persistence of the impacts vary. Firms with high IT intensity receive greater spillover benefits from the IT services industry. Moreover, these benefits are sustained over a long-term horizon. However, the impact of IT-related spillovers does not persist in low IT intensity firms regardless of the source. Overall, our results shed light on the existence and sources of IT-related spillovers and on their important role in shaping the long-run returns to IT investment. Our results also help explain the findings of excess returns to IT investment in the IT productivity literature.
|keyword = long-run productivity,business value of IT,economics of IS,spillovers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Learning Curve of IT Knowledge Workers in a Computing Call Center'''
{{header}}
{{article
|author= Youngsoo Kim,Ramayya Krishnan,Linda Argote,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We analyze learning and knowledge transfer in a computing call center. The information technology (IT) technical services provided by call centers are characterized by constant changes in relevant knowledge and a wide variety of support requests. Under this IT problem-solving context, we analyze the learning curve relationship between problem-solving experience and performance enhancement. Based on data collected from a university computing call center consisting of different types of consultants, our empirical findings indicate that (a) the learning effect-as measured by the reduction of average resolution time-occurs with experience, (b) knowledge transfer within a group occurs among lower-level consultants utilizing application-level knowledge (as opposed to technical-level knowledge), and (c) knowledge transfers across IT problem types. These estimates of learning and knowledge transfer contribute to the development of an empirically grounded understanding of IT knowledge workers' learning behavior. The results also have implications for operational decisions about the staffing and problem-solving strategy of call centers.
|keyword = computing call center,learning curves,knowledge transfer,IT problem type,knowledge classification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Advertising Strategies in Electronic Retailing: A Differential Games Approach'''
{{header}}
{{article
|author= Dengpan Liu,Subodha Kumar,Vijay S. Mookerjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We consider advertising problems under an information technology (IT) capacity constraint encountered by electronic retailers in a duopolistic setting. There is a considerable amount of literature on advertising games between firms, yet introducing an IT capacity constraint fundamentally changes this problem. In the presence of information processing constraints, although advertising may still cause a customer to switch, it may not result in a sale, i.e., the customer may be lost by both firms. This situation could occur when customers have a limited tolerance for processing delays and leave the website of a firm because of slow response. In such situations, attracting more traffic to a firm's site (by increasing advertising expenditure) may not generate enough additional revenue to warrant this expenditure. We use a differential game formulation to obtain closed-form solutions for the advertising effort over time in the presence of IT capacity constraints. Based on these solutions, we present several useful managerial insights.
|keyword = IT capacity,advertising,optimal control theory,differential game,reneging,Nash equilibrium,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Institutional Influences on Information Systems Security Innovations'''
{{header}}
{{article
|author= Carol Hsu,Jae-Nam Lee,Detmar W. Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This research investigates information security management as an administrative innovation. Although a number of institutional theories deal with information systems (IS) innovation in organizations, most of these institutional-centered frameworks overlook external economic efficiency and internal organizational capability in the presence of pressures of institutional conformity. Using Korea as the institutional setting, our research model posits that economic-based consideration will moderate the institutional conformity pressure on information security adoption while organization capability will influence the institutional confirmation of information security assimilation. The model is empirically tested using two-stage survey data from a field study of 140 organizations in Korea. The results indicate that in addition to institutional influences, our six proposed economic-based and organizational capability moderating variables all have significant influences on the degree of the adoption and assimilation of information security management. We conclude with implications for research in the area of organizational theory and the information security management literature, and for practices regarding how managers can factor into their information security planning the key implementation variables discovered in this study. The robust setting of the study in Korean firms allows us to generalize the theory to a new context and across cultures.
|keyword = administrative innovation,information security management,institutional theories,adoption and assimilation,economic,organizational,IT capability factors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Boundaries of Trust and Risk: The Quadratic Moderating Role of Institutional Structures'''
{{header}}
{{article
|author= David Gefen,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = A prevalent assumption in the literature is that trust and risk are always relevant in online marketplaces, and that there is always a need to build trust and reduce risk irrespective of context. Challenging this assumption, this study seeks to identify the boundaries of the effects of trust and risk on transaction activity in the context of institutional structures in online marketplaces. The perceived effectiveness of institutional structures (PETS), defined as the extent buyers believe that appropriate conditions are in place to facilitate transactions with sellers, sets the boundaries of trust and risk by moderating their effects on transaction activity in a quadratic (inverted-U) fashion. Specifically, at the lower boundary condition of PETS (among buyers who believe institutional structures are ineffective), the high situational uncertainty they perceive should make these buyers unwilling to become vulnerable to sellers, thus rendering trust and risk immaterial to their decision making. Trust and risk should also be immaterial at the higher boundary condition of PEIS (among buyers who believe institutional structures are very effective), because the insufficient situational uncertainty makes trust and risk irrelevant to these buyers' decision making because of a lack of vulnerability. Only between these two boundary conditions (among buyers who perceive moderate levels of PEIS), and thus a moderate degree of situational uncertainty and vulnerability in the marketplace, should trust and risk have a significant effect on transaction activity. Data from 398 buyers on eBay's and Amazon's online marketplaces support the quadratic moderating role of PETS on the effect of risk on transaction activity, but not on the effect of trust. Theoretical and practical implications on specifying the boundaries of the effects of trust and risk and understanding the direct and moderating role of institutional structures are discussed.
|keyword = online marketplaces,trust,risk,institutional structures,quadratic moderating effects,polynomial regression,response surface methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Content Provision Strategies in the Presence of Content Piracy'''
{{header}}
{{article
|author= Monica Johar,Nanda Kumar,Vijay Mookeijee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We consider a publisher that earns advertising revenue while providing content to serve a heterogeneous population of consumers. The consumers derive benefit from consuming content but suffer from delivery delays. A publisher's content provision strategy comprises two decisions: (a) the content quality (affecting consumption benefit) and (b) the content distribution delay (affecting consumption cost). The focus here is on how a publisher should choose the content provision strategy in the presence of a content pirate such as a peer-to-peer (P2P) network. Our study sheds light on how a publisher could leverage a pirate's presence to increase profits, even though the pirate essentially encroaches on the demand for the publisher's content. We find that a publisher should sometimes decrease the delivery speed but increase quality in the presence of a pirate (a quality focused strategy). At other times, a distribution focused strategy is better; namely, increase delivery speed, but lower quality. In most cases, however, we show that the publisher should improve at least one dimension of content provision (quality or delay) in the presence of a pirate.
|keyword = content provision and distribution,delivery delay,content piracy,P2P networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Putting Money Where the Mouths Are: The Relation Between Venture Financing and Electronic Word-of-Mouth'''
{{header}}
{{article
|author= Rohit Aggarwal,Ram Gopal,Alok Gupta,Harpreet Singh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = External financing is critical to ventures that do not have a revenue source but need to recruit employees, develop products, pay suppliers, and market their products/services. There is an increasing belief among entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM make it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed-upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures along with other sources including VentureXpert, surveys, Google Blogsearch, Lexis-Nexis, and Archive.org. The key findings of our econometric analysis are that the impact of negative eWOM is greater than is the impact of positive eWOM and that the effect of eWOM on financing decreases with the progress through the financing stages. We also find that the eWOM of popular bloggers helps ventures in getting higher funding amounts and valuations. The empirical model used in this work accounts for inherent selection biases of entrepreneurs and venture capitalists, and we conduct numerous robustness checks for potential issues of endo-geneity, selection bias, nonlinearities, and popularity cutoff for blogs. The findings have important implications for entrepreneurs and suggest ways by which entrepreneurs can take advantage of eWOM.
|keyword = electronic word-of-mouth,blogs,venture funding,VC funding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Cross-Purposes of Cross-Posting: Boundary Reshaping Behavior in Online Discussion Communities'''
{{header}}
{{article
|author= Brian S. Butler,Xiaoqing Wang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Increasingly, online discussion communities are used to support activities ranging from software development to political campaigns. An important feature of an online discussion community is its content boundaries, which are individual perceptions of what materials and discussions are part of the community and what are not, and how that community is related to others within a larger system. Yet in spite of its importance, many community infrastructures allow individual participants to reshape content boundaries by simultaneously associating their contributions with multiple online discussion communities. This reshaping behavior is a controversial aspect of the creation and management of many types of online discussion communities. On one hand, many communities explicitly discourage boundary reshaping behaviors in their frequently asked questions or terms-of-use document. On the other hand, community infrastructures continue to allow such reshaping behaviors. To explain this controversy, we theorize how the extent of boundary reshaping in an online discussion community has simultaneously positive and negative effects on its member dynamics and responsiveness. We test predictions about the conflicting effects of reshaping behaviors with 60 months of longitudinal data from 140 USENET newsgroups, focusing on cross-posting activities as a form of reshaping behavior. Empirical results are consistent with the proposed hypotheses that reshaping behaviors within a discussion community affect member dynamics and community responsiveness in both positive and negative ways. Taken together, the findings highlight the boundary-related design challenges faced by managers seeking to support ongoing activity within online discussion communities.
|keyword = online communities,virtual communities,boundaries,social computing,design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Real Options to Investigate the Market Value of Virtual World Businesses'''
{{header}}
{{article
|author= Sung-Byung Yang,Jee-Hae Lim,Wonseok Oh,Artimesh Animesh,Alain Pinsonneault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Virtual worlds are relatively nascent IT platforms with the potential to radically transform business processes and generate significant payoffs. However, in striving to achieve specific outcomes, firms may incur significant risks. Although many companies claim to have attained substantial benefits from their virtual world initiatives, many others have recently scaled down or even abandoned their experimental virtual world projects. This paper assesses the value proposition of virtual world initiatives from the real options perspective. Specifically, we argue that virtual worlds act as a firm's growth option, and we adopt the lens of real options to evaluate the value of this emerging and uncertain technological platform. We employ the event study method to assess the stock market's perception of the future revenue streams of 261 virtual world initiatives announced between 2006 and 2008. Our results indicate that, overall, the market reacts positively to virtual world initiatives. Our findings also show that investors' reactions to virtual world initiatives are contingent on four key characteristics of virtual world initiatives: interpretive flexibility (i.e., technologies that allow managers to experiment), divisibility (i.e., ability to incrementally implement the technology), strategic importance (i.e., an initiative that affects a process of strategic importance to the firm), and exploitable absorptive capacity (i.e., ability to exploit the knowledge acquired through the initiative). We discuss the key implications for real-world practitioners and suggest directions for future research.
|keyword = virtual world investments,value creation,real options,strategic importance,divisibility,exploitative absorptive capacity,event study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Performance-Based Advertising: Advertising as Signals of Product Quality'''
{{header}}
{{article
|author= Juan Feng,Jinhong Xie,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Performance-based advertising is becoming increasingly popular in the online advertising industry, where advertisers pay the publisher only when the advertisement generates an "action" (e.g., a click-through or a purchase). This paper illustrates that adopting this emerging advertising scheme has profound impacts on one fundamental function of advertising-signaling product quality. We identify several important dimensions that affect the signaling function of performance-based advertising relative to its traditional counterpart (impression-based advertising). These include: (1) information-total advertising expenditure is determined after the demand is realized, so it is unobservable to consumers when making purchase decisions; (2) ad performance-the measured "performance" (e.g., recorded click-throughs) includes actions generated by first-time buyers (i.e., advertising performance) and actions generated by repeat buyers (i.e., product performance), which increases the cost of signaling through advertising; (3) demand uncertainty-the merchant pays only when a response to the advertisement is generated, which reduces the merchant's advertising uncertainty. We build a model of performance-based advertising by explicitly incorporating these factors, and we derive the conditions under which switching to performance-based advertising will (a) disable or strengthen the signaling function of advertising, (b) help or hurt the merchant, and (c) lead to a higher or lower advertising expenditure.
|keyword = performance-based pricing,advertising,signaling,sponsored search,quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do Electronic Linkages Reduce the Bullwhip Effect? An Empirical Analysis of the US Manufacturing Supply Chains'''
{{header}}
{{article
|author= Yuliang Yao,Kevin Xiaoguo Zhu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The bullwhip effect is a major source of supply chain inefficiency. Whereas prior literature has identified a number of potential contributing factors and recommended such remedies as information sharing enabled by information technology (IT) or electronic linkage (EL), few studies have provided empirical support. We use industry-level data to examine whether EL use with buyer and supplier industries helps reduce the bullwhip effect as measured by inventory demand variance ratio. Our major findings are that (1) EL use with supplier industries reduces the bullwhip effect, whereas (2), surprisingly, EL use with buyer industries increases it, but (3) this adverse effect tends to be mitigated by IT use. These findings point to the possible asymmetric effects of EL use in supply chains and provide a different perspective to the existing conclusions in the literature that EL use improves performance. Combining the above results, we have learned that the use of EL tends to behave differently depending on whether it is used upstream or downstream in the supply chain. This also sheds light on the conditions under which such investment may be more (or less) beneficial.
|keyword = supply chain management,bullwhip effect,information technology,electronic markets,empirical operations,econometrics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Music Blogging, Online Sampling, and the Long Tail'''
{{header}}
{{article
|author= Sanjeev Dewan,Jui Ramaprasad,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Online social media such as blogs are transforming how consumers make consumption decisions, and the music industry is at the forefront of this revolution. Based on data from a leading music blog aggregator, we analyze the relationship between music blogging and full-track sampling, drawing on theories of online social interaction. Our results suggest that intensity of music sampling is positively associated with the popularity of a blog among previous consumers and that this association is stronger in the tail than in the body of music sales distribution. At the same time, the incremental effect of music popularity on sampling is also stronger in the tail relative to the body. in the last part of the paper, we discuss the implications of our results for music sales and potential long-tailing of music sampling and sales. Put together, our analysis sheds new light on how social media are reshaping music sharing and consumption.
|keyword = blogs,social interactions,observational learning,word of mouth,long tail,music industry,social media,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Cost Impact of Spam Filters: Measuring the Effect of Information System Technologies in Organizations'''
{{header}}
{{article
|author= Marco Caliendo,Michel Clement,Dominik Papies,Sabine Scheel-Kopeinig,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Dealing with spam is very costly, and many organizations have tried to reduce spam-related costs by installing spam filters. Relying on modern econometric methods to reduce the selection bias of installing a spam filter, we use a unique data setting implemented at a German university to measure the costs associated with spam and the costs savings of spam filters. Our methodological framework accounts for effect heterogeneity and can be easily used to estimate the effect of other IS technologies implemented in organizations. The majority of costs stem from the time that employees spend identifying and deleting spam, amounting to an average of approximately five minutes per employee per day. Our analysis, which accounts for selection bias, finds that the installation of a spam filter reduces these costs by roughly one third. Failing to account for the selection bias would lead to a result that suggests that installing a spam filter does not reduce working time losses. However, cost savings only occur when the spam burden is high, indicating that spam filters do not necessarily reduce costs and are therefore no universal remedy. The analysis further shows that spam filters alone are a countermeasure against spam that exhibits only limited effectiveness because they only reduce costs by one third.
|keyword = spam,spam filter,selection bias,propensity score matching,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ON THE USE OF NEUROPHYSIOLOGICAL TOOLS IN IS RESEARCH: DEVELOPING A RESEARCH AGENDA FOR NEUROIS'''
{{header}}
{{article
|author= Angelika Dimoka,Rajiv D. Banker,Izak Benbasat,Fred D. Davis,Alan R. Dennis,David Gefen,Alok Gupta,Anja Lschebeck,Peter H. Kenning,Paul A. Pavlou,Gernot Mueller-Putz,Rene Riedl,Jan vom Brocke,Bernd Weber,
|source= MIS QUARTERLY
|year= 2012
|abstract = This article discusses the role of commonly used neurophysiological tools such as psychophysiological tools (e.g., EKG, eye tracking) and neuroimaging tools (e.g., fMRI, EEG) in Information Systems research. There is heated interest now in the social sciences in capturing presumably objective data directly from the human body, and this interest in neurophysiological tools has also been gaining momentum in IS research (termed NeuroIS). This article first reviews commonly used neurophysiological tools with regard to their major strengths and weaknesses. It then discusses several promising application areas and research questions where IS researchers can benefit from the use of neurophysiological data. The proposed research topics are presented within three thematic areas: (1) development and use of systems, (2) IS strategy and business outcomes, and (3) group work and decision support. The article concludes with recommendations on how to use neurophysiological tools in IS research along with a set of practical suggestions for developing a research agenda for NeuroIS and establishing NeuroIS as a viable subfield in the IS literature.
|keyword = NeuroIS,neuroscience,neurophysiological tools,psychophysiological tools,neuroimaging,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPARING PLS TO REGRESSION AND LISREL: A RESPONSE TO MARCOULIDES, CHIN, AND SAUNDERS'''
{{header}}
{{article
|author= Dale L. Goodhue,William Lewis,Ron Thompson,
|source= MIS QUARTERLY
|year= 2012
|abstract = In the Foreword to an NHS Quarterly Special Issue on PLS, the senior editors for the special issue noted that they rejected a number of papers because the authors attempted comparisons between results from PLS, multiple regression, and structural equation modeling (Marcoulides et al. 2009). They raised several issues they argued had to be taken into account to have legitimate comparison studies, supporting their position primarily by citing three authors: Dijkstra (1983), McDonald(1996), and Schneeweiss (1993). As researchers interested in conducting comparison studies, we read the Foreword carefully, but found it did not provide clear guidance on how to conduct "legitimate" comparisons. Nor did our reading of Dijksta, McDonald, and Schneeweiss raise any red flags about dangers in this kind of comparison research. We were concerned that instead of helping researchers to successfully engage in comparison research, the Foreword might end up discouraging that type of work, and might even be used incorrectly to reject legitimate comparison studies. This Issues and Opinions piece addresses the question of why one might conduct comparison studies, and gives an overview of the process of comparison research with a focus on what is required to make those comparisons legitimate. In addition, we explicitly address the issues raised by Marcoulides et al., to explore where they might (or might not) come into play when conducting or evaluating this type of study.
|keyword = Comparing statistical techniques,partial least squares,structural equation modeling,regression,Monte Carlo simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GENERALIZATION AND INDUCTION: MISCONCEPTIONS, CLARIFICATIONS, AND A CLASSIFICATION OF INDUCTION'''
{{header}}
{{article
|author= Eric W. K. Tsang,John N. Williams,
|source= MIS QUARTERLY
|year= 2012
|abstract = In "Generalizing Generalizability in Information Systems Research," Lee and Baskerville (2003) try to clarify generalization and classify it into four types. Unfortunately, their account is problematic. We propose repairs. Central among these is our balance-of-evidence argument that we should adopt the view that Hume's problem of induction has a solution, even if we do not know what it is. We build upon this by proposing an alternative classification of induction. There are five types of generalization: (1) theoretical, (2) within-population, (3) cross-population, (4) contextual, and (5) temporal, with theoretical generalization being across the empirical and theoretical levels and the rest within the empirical level. Our classification also includes two kinds of inductive reasoning that do not belong to the domain of generalization. We then discuss the implications of our classification for information systems research.
|keyword = Research methodology,generalization,generalizability,induction,deduction,statistical generalization,statistical syllogism,inductive analogy,Hume's problem of induction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONCEPTUALIZING GENERALIZABILITY: NEW CONTRIBUTIONS AND A REPLY'''
{{header}}
{{article
|author= Allen S. Lee,Richard L. Baskerville,
|source= MIS QUARTERLY
|year= 2012
|abstract = Tsang and Williams offer some good and provocative ideas in their critique of our earlier article on generalizing and generalizability. In this essay we will advance some new ideas by building on those collected in both Tsang and Williams and our original article (Lee and Baskerville 2003). Because IS is a pluralist scientific discipline, one in which both qualitative and quantitative (and both interpretive and positivist) research approaches are valued, "generalize" is unlikely to be a viable term or concept if only one IS research paradigm may lay claim to it and excludes others from using it. Both papers agree on this point, but approach the problem differently. Where we originally generalized generalizability by offering new language, Tsang and Williams conceptualize generalizability by framing it more closely to its older, more statistically oriented form. We agree about the importance of induction and about the classification or taxonomy of different types of induction. We build further in this essay, advancing the ethical questions raised by generalization: A formulation of judgment calls that need to be made when generalizing a theory to a new setting. We further demonstrate how the process of generalizing may actually proceed, based on the common ground between Tsang and Williams and our original article.
|keyword = Research approach,philosophical approach,philosophy,reference theory,type of theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ROLES OF THEORY IN CANONICAL ACTION RESEARCH'''
{{header}}
{{article
|author= Robert M. Davison,Maris G. Martinsons,Carol X. J. Ou,
|source= MIS QUARTERLY
|year= 2012
|abstract = Canonical action research (CAR) aims to address real-world problems and improve organizational performance by combining scholarly observations with practical interventions. However, efforts to conduct CAR have revealed challenges that reflect a significant research-practice gap. We examine these challenges by revisiting the process, principles, and criteria of CAR developed earlier. The specific roles of two different types of theory in the cyclical action research process are considered. A project undertaken in two public relations firms illustrates how our methodological revision improves the rigor and quality of CAR. This article contributes both a significantly enhanced action research method, with detailed guidelines and suggestions that emphasize the roles of focal and instrumental theories, and an emerging theory of knowledge sharing that incorporates key elements of Chinese management and culture.
|keyword = Canonical action research,instrumental theory,focal theory,principles and criteria,knowledge management,knowledge sharing,culture,organizational change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PRINCIPLES FOR CONDUCTING CRITICAL REALIST CASE STUDY RESEARCH IN INFORMATION SYSTEMS'''
{{header}}
{{article
|author= Jr. Donald Wynn,Clay K. Williams,
|source= MIS QUARTERLY
|year= 2012
|abstract = Critical realism is emerging as a viable philosophical paradigm for conducting social science research, and has been proposed as an alternative to the more prevalent paradigms of positivism and interpretivism. Few papers, however, have offered clear guidance for applying this philosophy to actual research methodologies. Under critical realism, a causal explanation for a given phenomenon is inferred by explicitly identifying the means by which structural entities and contextual conditions interact to generate a given set of events. Consistent with this view of causality, we propose a set of methodological principles for conducting and evaluating critical realism-based explanatory case study research within the information systems field. The principles are derived directly from the ontological and epistemological assumptions of critical realism. We demonstrate the utility of each of the principles through examples drawn from existing critical realist case studies. The article concludes by discussing the implications of critical realism based research for IS research and practice.
|keyword = Critical realism,case study research,methodology,philosophy,causal explanation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW TO CONDUCT A FUNCTIONAL MAGNETIC RESONANCE (FMRI) STUDY IN SOCIAL SCIENCE RESEARCH'''
{{header}}
{{article
|author= Angelika Dimoka,
|source= MIS QUARTERLY
|year= 2012
|abstract = This research essay outlines a set of guidelines for conducting functional Magnetic Resonance Imaging (fMRI) studies in social science research in general and also, accordingly, in Information Systems research. Given the increased interest in using neuroimaging tools across the social sciences, this study aims at specifying the key steps needed to conduct an fMRI study while ensuring that enough detail is provided to evaluate the methods and results. The outline of an fMRI study consists of four key steps: (1) formulating the research question, (2) designing the fMRI protocol, (3) analyzing fMRI data, and (4) interpreting and reporting fMRI results. These steps are described with an illustrative example of a published fMRI study on trust and distrust in this journal (Dimoka 2010). The paper contributes to the methodological literature by (1) providing a set of guidelines for designing and conducting fMRI studies, (2) specifying methodological details that should be included in fMRI studies in academic venues, and (3) illustrating these practices with an exemplar fMRI study. Future directions for conducting high-quality fMRI studies in the social sciences are discussed.
|keyword = fMRI,decision neuroscience,neuroIS,brain imaging,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUILDING MEMBER ATTACHMENT IN ONLINE COMMUNITIES: APPLYING THEORIES OF GROUP IDENTITY AND INTERPERSONAL BONDS'''
{{header}}
{{article
|author= Yuqing Ren,F. Maxwell Harper,Sara Drenner,Loren Terveen,Sara Kiesler,John Riedl,Robert E. Kraut,
|source= MIS QUARTERLY
|year= 2012
|abstract = Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants' visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group's activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.
|keyword = Online community,group identity,interpersonal bonds,attachment,participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A KNOWLEDGE-BASED MODEL OF RADICAL INNOVATION IN SMALL SOFTWARE FIRMS'''
{{header}}
{{article
|author= Jessica Luo Carlo,Kalle Lyytinen,Gregory M. Rose,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this paper, we adopt the lens of absorptive capacity (ACAP), defined by two dimensions the-knowledge base (consisting of knowledge diversity, depth, and linkages) and routines (consisting of sensing and experimentation)-to explain how a software firm's knowledge endowments influence its level of radical information technology innovation during a technological breakthrough. We distinguish three types of IT innovations-base, processes, and service innovation-that form an innovation ecology. We posit that (I) ACAP is a relational construct where the impact of the knowledge base is mediated by routines; (2) IT innovations are either externally adopted or internally generated; and (3) knowledge antecedents associated with different types of innovations differ. We hypothesize a three-step, mediated path (knowledge base -> sensing -> experimentation -> innovation) for external innovation adoption, and a two-step path (knowledge diversity/depth -> experimentation -> innovation)for internal innovation creation to explain the software firm's level of radical innovation across three IT innovation types. We validate the model through a cross-sector study that examined how 121 small software firms innovated with Internet computing. We confirm the mediated nature of ACAP for external base innovations, which are driven by all three knowledge-based factors as follows: (1) knowledge depth (direct positive effect); (2) knowledge diversity (mediated three-step path), (3) knowledge linkages (mediated three step path). Process innovations are externally driven by a three-step mediated path fir knowledge linkages, as well as being directly affected by knowledge diversity, but negatively and directly impeded by knowledge depth. Service innovations are not driven by any mediated influence of ACAP, but driven directly by knowledge diversity. At the same time, both service and process innovations are strongly influenced by prior IT innovations: base and/or service. Several directions for future studies of radical IT innovation are proposed.
|keyword = Absorptive capacity,knowledge base models,routines,organization knowledge base,IT innovation,innovation ecology,Internet computing,mediation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY IMPLEMENTERS' RESPONSES TO USER RESISTANCE: NATURE AND EFFECTS'''
{{header}}
{{article
|author= Suzanne Rivard,Liette Lapointe,
|source= MIS QUARTERLY
|year= 2012
|abstract = User resistance has long been acknowledged as a critical issue during information technology implementation. Resistance can be functional when it signals the existence of problems with the IT or with its effects; it will be dysfunctional when it leads to organizational disruption. Notwithstanding the nature of resistance, the implementers-business managers, functional managers, or IT professionals-have to address it. Although the literature recognizes the importance of user resistance, it has paid little attention to implementers' responses-and their effect-when resistance occurs. Our study focuses on this phenomenon, and addresses two questions: What are implementers' responses to user resistance? What are the effects of these responses on user resistance? To answer these questions, we conducted a case survey, which combines the richness of case studies with the benefits of analyzing large quantities of data. Our case database includes 89 cases with a total of 137 episodes of resistance. In response to our first research question, we propose a taxonomy that includes four categories of implementers' responses to user resistance: inaction, acknowledgment, rectification, and dissuasion. To answer our second question, we adopted a set-theoretic analysis approach, which we enriched with content analysis of the cases. Based on these analyses, we offer a theoretical explanation of how implementers' responses may affect the antecedents that earlier research found to be associated with user resistance behaviors.
|keyword = User resistance,information technology implementation,implementers' response,theory building,case survey,set-theoretic analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STANDARDS COMPETITION IN THE PRESENCE OF DIGITAL CONVERSION TECHNOLOGY: AN EMPIRICAL ANALYSIS OF THE FLASH MEMORY CARD MARKET'''
{{header}}
{{article
|author= Charles Zhechao Liu,Chris F. Kemerer,Sandra A. Slaughter,Michael D. Smith,
|source= MIS QUARTERLY
|year= 2012
|abstract = Both theoretical and empirical evidence suggest that, in many markets with standards competition, network effects make the strong grow stronger and can "tip" the market toward a single, winner-take-all standard. We hypothesize, however, that low cost digital conversion technologies, which facilitate easy compatibility across competing standards, may reduce the strength of these network effects. We empirically test our hypotheses in the context of the digital flash memory card market. We first test for the presence of network effects in this market and find that network effects, as measured here, are associated with a significant positive price premium for leading flash memory card formats. We then find that the availability of digital converters reduces the price premium of the leading flash card formats and reduces the overall concentration in the flash memory market. Thus, our results suggest that, in the presence of low cost conversion technologies and digital content, the probability of market dominance can be lessened to the point where multiple, otherwise incompatible, standards are viable. Our conclusion that the presence of converters weakens network effects implies that producers of non-dominant digital goods standards benefit from the provision of conversion technology. Our analysis thus aids managers seeking to understand the impact of converters on market outcomes, and contributes to the existing literature on network effects by providing new insights into how conversion technologies can affect pricing strategies in these increasingly important digital settings.
|keyword = Network effects,network externalities,standards competition,conversion technologies,flash memory,digital goods,market competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A RESEARCH NOTE ON REPRESENTING PART-WHOLE RELATIONS IN CONCEPTUAL MODELING'''
{{header}}
{{article
|author= Gove N. Allen,Salvatore T. March,
|source= MIS QUARTERLY
|year= 2012
|abstract = Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article "Representing Part-Whole Relations in Conceptual Modeling: An Empirical Evaluation" (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying developing, and evaluating appropriate foundations for the discipline of conceptual modeling.
|keyword = Conceptual modeling,empirical research,ontology,information systems development,composition,UML,entity-relationship model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE HOLE IN THE WHOLE: A RESPONSE TO ALLEN AND MARCH'''
{{header}}
{{article
|author= Graeme Shanks,Ron Weber,
|source= MIS QUARTERLY
|year= 2012
|abstract = Allen and March provide a critique of one of our papers in which we argue composites should be represented as entities/objects in a conceptual model rather than relationships/associations (Shanks et al. 2008). They contend we have addressed a non-issue. Furthermore, they argue our theoretical rationale and empirical evidence have flaws. In this paper, we provide a response to their arguments. We show that the issue we address is substantive. We show, also, that our theoretical analysis and empirical results are robust. We find, instead, that Allen and March's theoretical arguments and empirical evidence have flaws.
|keyword = Conceptual modeling,empirical research,ontology,information systems development,aggregation,composition,UML,entity-relationship model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DOES PLS HAVE ADVANTAGES FOR SMALL SAMPLE SIZE OR NON-NORMAL DATA?'''
{{header}}
{{article
|author= Dale L. Goodhue,William Lewis,Ron Thompson,
|source= MIS QUARTERLY
|year= 2012
|abstract = There is a pervasive belief in the MIS research community that PLS has advantages over other techniques when analyzing small sample sizes or data with non-normal distributions. Based on these beliefs, major MIS journals have published studies using PLS with sample sizes that would be deemed unacceptably small if used with other statistical techniques. We used Monte Carlo simulation more extensively than previous research to evaluate PLS, multiple regression, and LISREL in terms of accuracy and statistical power under varying conditions of sample size, normality of the data, number of indicators per construct, reliability of the indicators, and complexity of the research model. We found that PLS performed as effectively as the other techniques in detecting actual paths, and not falsely detecting non-existent paths. However, because PLS (like regression) apparently does not compensate for measurement error, PLS and regression were consistently less accurate than LISREL. When used with small sample sizes, PLS, like the other techniques, suffers from increased standard deviations, decreased statistical power,and reduced accuracy. All three techniques were remarkably robust against moderate departures from normality, and equally so. In total, we found that the similarities in results across the three techniques were much stronger than the differences.
|keyword = Partial least squares,PLS,regression,structural equation modeling,statistical power,small sample size,non-normal distributions,Monte Carlo simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ASSESSING COMMON METHOD BIAS: PROBLEMS WITH THE ULMC TECHNIQUE'''
{{header}}
{{article
|author= Wynne W. Chin,Jason Bennett Thatcher,Ryan T. Wright,
|source= MIS QUARTERLY
|year= 2012
|abstract = Recent work, in journals such as MIS Quarterly and Management Science, has highlighted the importance of evaluating the influence of common method bias (CMB) on the results of statistical analysis. In this research note, we assess the utility of the unmeasured latent method construct (ULMC) approach in partial least squares (PLS), introduced by Liang et al. (2007). Such an assessment of the ULMC approach is important, because it has been employed in 76 studies since it appeared in MIS Quarterly in early 2007. Using data generated via Monte Carlo simulations, we use PLS structural equation modeling (SEM) to demonstrate that the ULMC approach of Liang et al. is neither able to detect, nor control for, common method bias. Method estimates using this approach resulted in negligible estimates, regardless of whether there were some, large, or no method bias introduced in the simulated data. Our study contributes to the IS and research methods literature by illustrating that, and explaining why the ULMC approach does not accurately detect common method bias in PLS. Further, our results build on prior work done using covariance-based SEM questioning the usefulness of the ULMC technique for detecting CMB.
|keyword = Common method bias,unmeasured latent method construct,partial least squares,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Novelty-Knowledge Alignment: A Theory of Design Convergence in Systems Development'''
{{header}}
{{article
|author= Amrit Tiwana,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Recent research emphasizing the need for more business knowledge in information technology (IT) units and more technical knowledge in line functions largely overlooks the question of when maintaining either form of such "peripheral" knowledge-a costly endeavor-is valuable. Further application and process novelty are increasingly unavoidable in systems development projects but remain largely overlooked in theory. It is plausible that one type of peripheral knowledge is valuable under one type of novelty but not the other. I develop the idea that discriminating alignment between project novelty and peripheral knowledge is needed for them to enhance systems development performance. Thus, the valuable type of peripheral knowledge depends on whether a project involves novelty in the project concept or in its development processes. Further, we lack an explanation for how such discriminating alignment translates into improved project performance. I develop and test a middle-range theory built around two ideas to address these gaps. First, alignment between project novelty and peripheral knowledge must be discriminating to enhance systems development performance. Second, such discriminating alignment accelerates design convergence, which in turn enhances systems development performance. Tests using data from 159 projects support the proposed ideas. The primary contribution of this paper is therefore explaining when and how alignment between project novelty and peripheral knowledge in IT and client departments enhances systems development performance. The key implication is that greater application domain knowledge in the IT unit (technical knowledge in the client department) enhances performance in projects involving greater application novelty (process novelty).
|keyword = design convergence,iteration,novelty,oscillations,peripheral knowledge,systems development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effect of an Initial Budget and Schedule Goal on Software Project Escalation'''
{{header}}
{{article
|author= Jong Seok Lee,Mark Keil,Vijay Kasi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Software project escalation is a costly problem that leads to significant financial losses. Prior research suggests that setting a publicly announced limit on resources can make individuals less willing to escalate their commitment to a failing course of action. However, the relationship between initial budget and schedule goals and software project escalation remains unexplored. Drawing on goal setting theory as well as sunk cost and mental budgeting perspectives, we explore the effect of goal difficulty and goal specificity on software project escalation. The findings from a laboratory experiment with 349 information technology professionals suggest that both very difficult and very specific goals for budget and schedule can limit software project escalation. Further, the level of commitment to a budget and schedule goal directly affects software project escalation and also interacts with goal difficulty and goal specificity to affect software project escalation. This study makes a theoretical contribution to the existing body of knowledge on software project management by establishing a connection between goal setting theory and software project escalation. The study also contributes to practice by highlighting the potential negative consequences that can result from the nature of initial budget and schedule goals that are established at the outset of a project.
|keyword = escalation of commitment,goal setting theory,mental budgeting,project estimation,software project escalation,software project management,sunk cost,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What's the Weather Like? The Effect of Team Learning Climate, Empowerment Climate, and Gender on Individuals' Technology Exploration and Use'''
{{header}}
{{article
|author= Likoebe M. Maruping,Massimo Magni,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Given the pervasive use of teams in organizations coupled with high levels of investment in collaboration technology, there is increasing interest in identifying factors that affect the exploration and use of a broader scope of system features so that firms can benefit from the use of such technology. Prior research has called for a deeper understanding of how managers can encourage greater innovation with technology in the workplace. Drawing on the team climate and technology use literatures, we identify team learning climate and team empowerment climate as key factors that affect employees' propensity to explore a new system's features. We develop and test our multilevel model on team climate, team technology exploration, and team technology use in a field study involving 268 employees embedded in 56 work teams. Three main findings come out of this research. First, the results reveal that the two types of team climate differ in their cross-level effects on individual intention to explore, such that team learning climate promotes greater intention to explore, whereas team empowerment climate reduces employees' intention to explore the technology. In addition, we find that team learning climate and team empowerment climate interact in shaping individual intention to explore, such that the presence of a strong learning climate is more effective in promoting intention to explore when teams also have a strong empowerment climate. Second, the findings show that men and women are affected differently by team climate. We find that for men, team empowerment climate has no influence on intention to explore, whereas for women there is a significant negative cross-level effect. Finally, we find that intention to explore has a positive effect on usage scope, suggesting an important link between team climate, individual cognition, and the scope of features used by employees in team settings. Taken together, the model and results highlight the important role of team climate and gender and the interplay between them as drivers of technology feature exploration. Our findings, especially those related to team empowerment climate, are counterintuitive when compared to prior literature and offer useful insights for managers. On the one hand, managers should consider leveraging team learning climate to intrinsically stimulate employees to engage in exploration of technology. On the other hand, managers should be cautious and guard against saddling employees with too many additional responsibilities during the stages of exploration and experimentation with system features. It is possible that through an expanded set of responsibilities and expectations fostered by team empowerment climate, employees may be experiencing work overload, thus reducing their likelihood of exploring a broader set of technology features. Managers should be especially attentive to this based on the gender composition of their teams.
|keyword = collaboration technology,intention to explore,multilevel research,postadoption use,team climate,team technology use,usage scope,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contract Performance in Offshore Systems Development: Role of Control Mechanisms'''
{{header}}
{{article
|author= Shirish C. Srivastava,Thompson S. H. Teo,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Although control theory has often been invoked to explain the coordination between client and vendor for information systems development (ISD), insights into its moderating effects for explicating ISD contract performance, especially in the offshore context, is rather limited. Such insights would en able better understanding of variables that have complementary or substitutive effects on performance. Further, the control literature talks about different control modes (e.g., formal and informal control modes classified as behavior, outcome, clan, and self-control modes) without adequately distinguishing among the different control mechanisms enacting each of the control modes. In this research, by explicitly classifying the distinctions that exist within each of the control modes, we uncover the key role played by mechanistic governance in outsourced ISD. Grounding our arguments in the information requirement for performance evaluation, the study theorizes the moderating influence of mechanistic governance on the relationships of contract specificity and relational governance with ISD quality and cost performance. We test the theorized model in a field study comprising 160 offshore ISD projects executed by Indian vendors. Our results establish the significant complementary role of mechanistic governance on the relationships of contract specificity with both cost and quality performance variables. Further, mechanistic governance substitutes the impact of relational governance on cost performance. Thus, the study theoretically as well as empirically establishes the need for conceptualizing mechanistic governance as a viable and significant governance mechanism for offshore ISD contracts. The study also teases out the distinctions between the two prime contract types in vogue for managing offshore ISD contracts, namely, fixed price and time and materials contracts. The study thus contributes not only to control theory but also to the stream of literature examining offshore ISD contracts. Further, the study provides insights to managers on having well-specified contracts and acknowledging the role of mechanistic governance for better performance.
|keyword = contract performance,control mechanisms,control modes,control theory,interaction effects,offshoring,outsourcing,project governance,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Information Technology Investments on Downside Risk of the Firm: Alternative Measurement of the Business Value of IT'''
{{header}}
{{article
|author= Samual Otim,Kevin E. Dow,Varun Grover,Jeffrey A. Wong,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = We examine the effect that investments in information technology (IT) have on downside risk profiles of companies that made public announcements of their investments in technology. Given the limitations of financial and decision theory perspectives on risk, we adopt the strategic management perspective that stresses downside risk as an important alternative measure of firm performance. We examine whether different types of IT investments have a differential impact on firm downside risk. Drawing on the resource-based view of the firm and the real options perspective, we find evidence that IT investments and their timing influence organizational downside risk. Transformational and informational IT investments lead to a reduction in downside risk only if they lead to strategic IT investments in the industry. For competitive necessities such as IT investments that automate business functions, a reduction in downside risk is realized by investing in parity with industry participants. Our study contributes to the literature by offering an alternative perspective on the benefits of IT investments, particularly where no apparent incremental financial results may be evident. It also generates insights on IT investment strategies that may help firms keep up with or stay ahead of the competition.
|keyword = downside risk,IT investment,IT strategic role,real options perspective,resource-based view of the firm,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Vertical Differentiation and a Comparison of Online Advertising Models'''
{{header}}
{{article
|author= Mei Lin,Xuqing Ke,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Designing business models that take into consideration the role of advertising support is critical to the success of online services. In this paper, we address the challenges of these business model strategies and compare different ad revenue models. We use game theory to model vertical differentiation in both monopoly and duopoly settings, in which online service providers may offer an ad-free service, an ad-supported service, or a combination of these services. Offering both ad-free and ad-supported services is the optimal strategy for a monopolist because ad revenues compensate for the cannibalistic effect of vertical differentiation. In a duopoly equilibrium, exactly one firm offers both services when the ad revenue rate is sufficiently high. Furthermore, we find that a higher ad revenue rate may lead to lower service prices. Consistently across both monopoly and duopoly settings, such price reductions are more severe in the cost-per-thousand-impressions model than in the cost-per-click model. Our findings emphasize the role of advertising revenues in vertical differentiation and offer strategic guidance for monetizing online services.
|keyword = ad-supported business models,e-commerce,economic analysis,game theory,online advertising,vertical differentiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Attracted to or Locked In? Predicting Continuance Intention in Social Virtual World Services'''
{{header}}
{{article
|author= Zhongyun (Phil) Zhou,Yulin Fang,Douglas R. Vogel,Xiao-Ling Jin,Xi Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Internet-based social virtual world (SVW) services have aroused extensive interest among academicians and practitioners. The success of SVW services depends heavily on customers' continuance usage, a topic not yet adequately investigated in information systems research. It is unclear to what extent, and how, the existing theories can be extended to explain the continuance usage of such services. In consideration of the distinctive features of these services, this study adapts the dedication-constraint framework of commitment and develops a model of SVW continuance, which is assessed empirically using data collected from 438 experienced users of Second Life, a typical SVW service. Results indicate that SVW customers' continuance intention is jointly determined by two mechanisms: affective commitment (being attracted to) and calculative commitment (being locked in), with the former playing a more central role. Perceived utilitarian value, hedonic value, and relational capital promote affective commitment directly and indirectly through satisfaction, while service-specific investments in personalization and relational capital increase calculative commitment. Theoretical and practical implications and future research directions are subsequently discussed.
|keyword = commitment,continuance intention,dedication-constraint dual model,Second Life,social virtual world services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Self-Regulated Learning Processes on E-Learning Outcomes in Organizational Settings'''
{{header}}
{{article
|author= Zeying Wan,Deborah Compeau,Nicole Haggerty,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = This paper focuses on employees' e-learning processes during online job training. A new categorization of self-regulated learning strategies, that is, personal versus social learning strategies, is proposed, and measurement scales are developed. The new measures were tested using data collected from employees in a large company. Our approach provides context-relevant insights into online training providers and employees themselves. The results suggest that learners adopt different self-regulated learning strategies resulting in different e-learning outcomes. Furthermore, the use of self-regulated learning strategies is influenced by individual factors such as virtual competence and goal orientation, and job and contextual factors such as intellectual demand and cooperative norms. The findings can (1) help e-learners obtain better learning outcomes through their active use of varied learning strategies, (2) provide useful information for organizations that are currently using or plan to use e-learning for training, and (3) inform software designers to integrate self-regulated learning strategy support in e-learning system design and development.
|keyword = e-learning,job training,learning outcomes,learning processes,self-regulated learning strategies,social cognitive theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cost-Sensitive Learning via Priority Sampling to Improve the Return on Marketing and CRM Investment'''
{{header}}
{{article
|author= Geng Cui,Man Leung Wong,Xiang Wan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Because of the unbalanced class and skewed profit distribution in customer purchase data, the unknown and variant costs of false negative errors are a common problem for predicting the high-value customers in marketing operations. Incorporating cost-sensitive learning into forecasting models can improve the return on investment under resource constraint. This study proposes a cost-sensitive learning algorithm via priority sampling that gives greater weight to the high-value customers. We apply the method to three data sets and compare its performance with that of competing solutions. The results suggest that priority sampling compares favorably with the alternative methods in augmenting profitability. The learning algorithm can be implemented in decision support systems to assist marketing operations and to strengthen the strategic competitiveness of organizations.
|keyword = cost-sensitive learning,customer relationship management,direct marketing,forecasting,priority sampling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Are New IT-Enabled Investment Opportunities Diminishing for Firms?'''
{{header}}
{{article
|author= Brian L. Dos Santos,Zhiqiang (Eric) Zheng,Vijay S. Mookerjee,Hongyu Chen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Today, few firms could survive for very long without their computer systems. IT has permeated every corner of firms. Firms have reached the current state in their use of IT because IT has provided myriad opportunities for firms to improve performance and, firms have availed themselves of these opportunities. Some have argued, however, that the opportunities for firms to improve their performance through new uses of IT have been declining. Are the opportunities to use TT to improve firm performance diminishing? We sought to answer this question. In this study, we develop a theory and explain the logic behind our empirical analysis; an analysis that employs a different type of event study. Using the volatility of firms' stock prices to news signaling a change in economic conditions, we compare the stock price behavior of firms in the IT industry to firms in the utility and transportation and freight industries. Our analysis of the IT industry as a whole indicates that the opportunities for firms to use TT to improve their performance are not diminishing. However, there are sectors within the TT industry that no longer provide value-enhancing opportunities for firms. We also find that IT products that provided opportunities for firms to create value at one point in time, later become necessities for staying in business. Our results support the key assumption in our work.
|keyword = information technology industry,business value of IT,event study,stock price volatility,financial market evaluation,IT and firm performance, macroeconomic news,IT value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Blog, Blogger, and the Firm: Can Negative Employee Posts Lead to Positive Outcomes?'''
{{header}}
{{article
|author= Rohit Aggarwal,Ram Gopal,Ramesh Sankaranarayanan,Param Vir Singh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Consumer-generated media, particularly blogs, can help companies increase the visibility of their products without spending millions of dollars in advertising. Although a number of companies realize the potential of blogs and encourage their employees to blog, a good chunk of them are skeptical about losing control over this new media. Companies fear that employees may write negative things about them and that this may bring significant reputation loss. Overall, companies show mixed response toward negative posts on employee blogs some companies show complete aversion; others allow some negative posts. Such mixed reactions toward negative posts motivated us to probe for any positive aspects of negative posts. In particular, we investigate the relationship between negative posts and readership of an employee blog. In contrast to the popular perception, our results reveal a potential positive aspect of negative posts. Our analysis suggests that negative posts act as catalyst and can exponentially increase the readership of employee blogs, suggesting that companies should permit employees to make negative posts. Because employees typically write few negative posts and largely write positive posts, the increase in readership of employee blogs generally should be enough to offset the negative effect of few negative posts. Therefore, not restraining negative posts to increase readership should be a good strategy. This raises a logical question: what should a firm's policy be regarding employee blogging? For exposition, we suggest an analytical framework using our empirical model.
|keyword = blog,employee blogs,bloggers,attribution theory,nonlinear models,negative posts,influence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ambidexterity in Agile Distributed Development: An Empirical Investigation'''
{{header}}
{{article
|author= Balasubramaniam Ramesh,Kannan Mohan,Lan Cao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Distributed software development has become a common reality with the advent of off-shore development and the need to be close to markets. Also, the dynamic nature of the environment in which businesses operate suggests the use of agile development methods. Whereas distributed software development requires the use of formal processes advocated by plan-driven approaches, rapidly changing environments are appropriate candidates for the use of agile development methods. This tension in agile distributed development poses conflicting demands between alignment and adaptability in the software development process. We conducted a multisite case study of three projects that use agile distributed development to examine how these organizations developed contextual ambidexterity the ability to pursue conflicting demands simultaneously. Our findings, presented as a conceptual framework, indicate that conflicting demands between alignment and adaptability posed by agile distributed development can be addressed by a set of balanced practices that shape performance management and social context two important antecedents of contextual ambidexterity.
|keyword = agile development,ambidexterity,distributed development,qualitative case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relative Industry Concentration and Customer-Driven IT Spillovers'''
{{header}}
{{article
|author= Zhuo (June) Cheng,Barrie R. Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We examine how one industry's productivity is affected by the IT capital of its customers and how this effect depends on industries' relative concentration. These customer-driven IT spillovers result from customers' IT investments in various information systems that reduce transaction costs through information sharing and coordination and lead to more efficient production and logistics upstream. The magnitude of IT spillovers depends on relative industry concentration because customers in more concentrated industries relative to those of their suppliers are better able to retain the benefits from their IT investments. We model customer-driven effects based on production theory and empirically test the model using two industry-level data sets covering different and overlapping time periods (1987-1999 and 1998-2005), different scopes of the economy (manufacturing only versus all industries), and different levels of industry aggregation. We find that, given an increase in a downstream industry's IT capital, there is a significant increase in downstream industry output as well as significant increases in upstream industry output. Moreover, the magnitude of IT spillovers is related to relative industry concentration: A 1% decrease in a customer's relative industry concentration increases spillovers by roughly 1%. Thus, further increases in IT capital can be justified along the supply chain, and an industry's relative concentration which can reflect market power in part determines the distribution of productivity benefits.
|keyword = business value of IT,IT-enabled supply chains,economics of IS,spillovers,production function framework,input-output tables,industry concentration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cooperative Cashing? An Economic Analysis of Document Duplication in Cooperative Web Caching'''
{{header}}
{{article
|author= Kartik Hosanagar,Yong Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Cooperative caching is a popular mechanism to allow an array of distributed caches to cooperate and serve each others' Web requests. Controlling duplication of documents across cooperating caches is a challenging problem faced by cache managers. In this paper, we study the economics of document duplication in strategic and nonstrategic settings. We have three primary findings. First, we find that the optimum level of duplication at a cache is nondecreasing in intercache latency, cache size, and extent of request locality. Second, in situations in which cache peering spans organizations, we find that the interaction between caches is a game of strategic substitutes wherein a cache employs lesser resources towards eliminating duplicate documents when the other caches employs more resources towards eliminating duplicate documents at that cache. Thus, a significant challenge will be to simultaneously induce multiple caches to contribute more resources towards reducing duplicate documents in the system. Finally, centralized decision making, which as expected provides improvements in average latency over a decentralized setup, can entail highly asymmetric duplication levels at the caches. This in turn can benefit one set of users at the expense of the other, and thus will be challenging to implement.
|keyword = Web caching,cooperative caching,duplication in caching,analytical modeling,incentive-centered design,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Institutional Contradictions and Loose Coupling: Postimplementation of NASA's Enterprise Information System'''
{{header}}
{{article
|author= Nicholas Berente,Youngjin Yoo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Through a grounded analysis of the National Aeronautics and Space Administration (NASA's) enterprise information system (IS) implementation in the months immediately following the go-live, we show how NASA can be characterized as an institutionally plural organization, rife with diverse institutional logics, some consistent and some contradictory to each other. The enterprise system is introduced in accordance with the logic of managerial rationalism, but some of the institutional logics that organizational actors draw upon and reproduce contradict the logic of managerial rationalism in certain situations. In these situations, organizational actors loosely couple elements of their practices from the practices implied by the enterprise system, thus satisfying the demands associated with both institutional fields. We identify four generalizable forms of loose coupling that result from these institutional contradictions: temporal, material, procedural, and interpretive, and discuss their effects on both the system implementation and local practices. Further, we show how, through the use of institutional logics, researchers can identify fundamental institutional contradictions that explain regularities in the situated responses to enterprise system implementations regularities that are consistently identified in the literature across a variety of organizational contexts.
|keyword = institutional pluralism,institutional contradiction,institutional logic,institutional theory,loose coupling,loosely coupled,enterprise systems,ERP,NASA,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modeling Supply-Side Dynamics of IT Components, Products, and Infrastructure: An Empirical Analysis Using Vector Autoregression'''
{{header}}
{{article
|author= Gediminas Adomavicius,Jesse Bockstedt,Alok Gupta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Prior IS research on technological change has focused primarily on organizational information systems and technology innovation; however, there is a growing need to understand the dynamics of supply-side forces in the introduction of new technologies. In this paper we investigate how the interdependencies among information technology components, products, and infrastructure affect the release of new technologies. Going beyond the ad hoc heuristic approaches applied in previous studies, we empirically validate the existence of several patterns of supply-side technology relationships in the context of wireless networking. We use vector autoregression (VAR) to model the comovements of new component, product, and infrastructure introductions and provide evidence of strong Granger-causal interdependencies. We also demonstrate that substantial improvements in forecasting can be gained by incorporating these cross-level effects into models of technological change. This paper provides some of the first research that empirically demonstrates these cross-level effects and also provides an exposition of VAR methodology for both analysis and forecasting in IS research.
|keyword = information systems and technology trends,supply-side forces,technological change,technology ecosystems,technology forecasting,time series analysis,vector autoregression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Performance Implications of CRM Technology Use: A Multilevel Field Study of Business Customers and Their Providers in the Telecommunications Industry'''
{{header}}
{{article
|author= Alex R. Zablah,Danny N. Bellenger,Detmar W. Straub,Wesley J. Johnston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Extant research is equivocal about the organizational performance effects of customer relationship management (CRM) technology use, with some studies reporting positive effects and other studies reporting no effects at all. The present research effort posits that these mixed findings may potentially be explained by two factors: (1) CRM technology use may have different effects on different customers, and (2) different CRM tools may have different performance consequences. This study investigates this possibility by building on relationship marketing and management theory to propose and test a model of the customer- and firm-level consequences of the organizational use of CRM interaction support and customer prioritization tools. The results of data analysis of 295 customer firms nested within 10 provider firms reveal that firm use of CRM interaction support tools is positively related to customers' relationship perceptions, regardless of customer account size. In contrast, the data indicate that use of CRM prioritization tools appears to have positive effects on a firm's larger customers and negative effects on smaller customers. The results also suggest that when considered at an aggregate level, customer perceptions of the exchange relationship are predictive of organizational performance and that the association between these two variables is significant for larger customer accounts but insignificant for smaller accounts. Overall, the study's results help explain some of the inconsistent findings reported in the literature regarding the performance implications of CRM technology use and suggest that use of the technology may serve to enhance organizational performance, at least over the short term.
|keyword = customer relationship management,CRM,CRM technology,relationship investment,relationship marketing and management,multilevel modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reputation and Uncertainty in Online Markets: An Experimental Study'''
{{header}}
{{article
|author= Sarah C. Rice,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper employs a modified investment game to study how online reputation ratings are assigned, and I thus how electronic reputations are formed in transactions where buyers and sellers interact anonymously. Of particular interest are the important questions of how online reputations evolve and how specific reputation information is interpreted by market participants. We vary the level of uncertainty in the transaction environment, and measure the effects of this manipulation on buyers' trust and their subsequent rating behaviors. We distinguish between a reputation mechanism and specific reputation information, finding the former has an association with the overall decision of whether to transact in the marketplace, while the latter shows significance in purchase decisions regarding specific sellers. We also find that aggregate reputation information is weighted differently than singular reputation information. Finally, we show that when reputations are increasingly noisy, buyers are less likely to react negatively to poor ratings and are more likely to give sellers the benefit of the doubt when seemingly uncooperative outcomes occur.
|keyword = reputation systems,online markets,experimental economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Data Quality Risk in Accounting Information Systems'''
{{header}}
{{article
|author= Xue Bai,Manuel Nunez,Jayant R. Kalagnanam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The quality of data contained in accounting information systems has a significant impact on both internal business decision making and external regulatory compliance. Although a considerable body of literature exists on the issue of data quality, there has been little research done at the task level of a business process to develop effective control strategies to mitigate data quality risks. In this paper, we present a methodology for managing the risks associated with the quality of data in accounting information systems. This methodology first models the error evolution process in transactional data flow as a dynamical process; it then finds optimal control policies at the task level to mitigate the data quality-related risks using a Markov decision process model with risk constraints. The proposed Markov decision methodology facilitates the modeling of multiple dimensions of error dependence, captures the correlated impact among control procedures, and identifies an optimal control policy. A revenue realization process of an international production company is used to illustrate this methodology.
|keyword = data quality,risk,audit,control,accounting information systems,constrained Markov decision processes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Expectation Confirmation in Technology Use'''
{{header}}
{{article
|author= Susan A. Brown,Viswanath Venkatesh,Sandeep Goyal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = We propose a model to study expectation confirmation in information systems. The proposed model is based on the assimilation-contrast model and prospect theory, and suggests that both are needed to account for the magnitude and direction of the deviations between experiences and expectations. Using the technology acceptance model's (TAM) primary construct namely, perceived usefulness expectations and experiences were conceptualized and operationalized to test our model. Data were collected in a field study from 1,113 participants at two points in time. Using polynomial modeling and response surface analysis, we demonstrated that our model offers a good explanation of the relationship among information systems expectations, experiences, and use. We discuss theoretical and practical implications.
|keyword = technology acceptance,TAM,cognitive dissonance theory,polynomial modeling,response surface analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal Software Free Trial Strategy: The Impact of Network Externalities and Consumer Uncertainty'''
{{header}}
{{article
|author= Hsing Kenneth Cheng,Yipeng Liu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Many software firms offer a fully functional version of their products free of charge, for a limited trial period, to ease consumers' uncertainty about the functionalities of their products and to help the diffusion of their new software. This paper examines the trade-off between the effects of reduced uncertainty and demand cannibalization, uncovers the condition under which software firms should introduce the time-locked free trial software, and finds the optimal free trial time. As software firms have the option of providing free trial software with full functionalities but a limited trial time or limited functionalities for an unlimited trial time, we develop a unified framework to provide useful guidelines for deciding which free trial strategy is preferred in the presence of network externalities and consumer uncertainty.
|keyword = software free trial,time-locked free trial,demo software,experience goods,network effect,product trial,product sampling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SOA Performance Enhancement Through XML Fragment Caching'''
{{header}}
{{article
|author= Anindya Datta,Kaushik Dutta,Qianhui Liang,Debra VanderMeer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Organizations are increasingly choosing to implement service-oriented architectures to integrate distributed, loosely coupled applications. These architectures are implemented as services, which typically use XML-based messaging to communicate between service consumers and service providers across enterprise networks. We propose a scheme for caching fragments of service response messages to improve performance and service quality in service-oriented architectures. In our fragment caching scheme, we decompose responses into smaller fragments such that reusable components can be identified and cached in the XML routers of an XML overlay network within an enterprise network. Such caching mitigates processing requirements on providers and moves content closer to users, thus reducing bandwidth requirements on the network as well as improving Service times. We describe the system architecture and caching algorithm details for our caching scheme, develop an analysis of the expected benefits of our scheme, and present the results of both simulation and case study-based experiments to show the validity and performance improvements provided by our caching scheme. Our simulation experimental results show an up to 60% reduction in bandwidth consumption and up to 50% response time improvement. Further, our case study experiments demonstrate that when there is no resource bottleneck, the cache-enabled case reduces average response times by 40%-50% and increases throughput by 150% compared to the no-cache and full message caching cases. In experiments contrasting fragment caching and full message caching, we found that full message caching provides benefits when the number of possible unique responses is low while the benefits of fragment caching increase as the number of possible unique responses increases. These experimental results clearly demonstrate the benefits of our approach.
|keyword = caching,XML,SOA,service-oriented architecture,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Lock-In Strategy in Software Competition: Open-Source Software vs. Proprietary Software'''
{{header}}
{{article
|author= Kevin Xiaoguo Zhu,Zach Zhizhong Zhou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Open-source software poses a serious challenge to proprietary software vendors. "Lock in customers" seems a tempting strategy for proprietary software vendors, who attempt to lock in customers by creating switching costs. This paper examines whether such a lock-in strategy will indeed benefit proprietary software vendors facing competition from open-source software, who can credibly commit future prices. Developing a two-period duopoly model in which software products are differentiated and customers are heterogeneous, we find that the lock-in strategy is actually counterproductive in competing against open-source software. In fact, giving customers the freedom of choice may end up benefiting the proprietary software vendor. In terms of the broader effect, we find that lock-in reduces overall social welfare, but certain customers may actually be better off with it. Finally, we show that the lock-in strategy works differently for different types of customers in the software market (i.e., foresighted versus myopic customers). This suggests that customer behavior could significantly alter the equilibrium strategy of software vendors.
|keyword = software,competition,lock-in,open-source software,proprietary software,game theory,switching cost,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trust Is in the Eye of the Beholder: A Vignette Study of Postevent Behavioral Controls' Effects on Individual Trust in Virtual Teams'''
{{header}}
{{article
|author= Alan R. Dennis,Jr. Lionel P. Robert,Aaron M. Curtis,Stacy T. Kowalczyk,Bryan K. Hasty,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Research in face-to-face teams shows conflicting results about the impact of behavioral controls on trust; some research shows that controls increase the salience of good behavior, which increases trust while other research shows that controls increase the salience of poor behavior that decreases trust. The only study in virtual teams, which examined poorly functioning teams, found that controls increased the salience of poor behavior, which decreased trust. We argue that in virtual teams behavioral controls amplify the salience of all behaviors (positive and negative) and that an individual's selective perception bias influences how these behaviors are interpreted. Thus the link from behavioral controls to trust is more complex than first thought. We conducted a 2 x 2 experiment, varying the use of behavioral controls (controls, no controls) and individual team member behaviors (reneging behaviors designed to reduce trust beliefs and fulfilling behaviors designed to increase trust beliefs). We found that behavioral controls did amplify the salience of all behaviors; however, contrary to what we expected, this actually weakened the impact of reneging and fulfilling behaviors on trust. We believe that completing a formal evaluation increased empathy and the awareness of context in which the behaviors occurred and thus mitigated extreme perceptions. We also found that behavioral controls increased the selective perception bias which induced participants to see the behaviors their disposition to trust expected rather than the behaviors that actually occurred.
|keyword = virtual teams,trust,controls,disposition to trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Generating Shareable Statistical Databases for Business Value: Multiple Imputation with Multimodal Perturbation'''
{{header}}
{{article
|author= Nigel Melville,Michael McQuaid,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Business organizations are generating growing volumes of data about their employees, customers, and suppliers. Much of these data cannot be exploited for business value due to privacy and confidentiality concerns. National statistical agencies share sensitive data collected from individuals and businesses by modifying the data so individuals and firms cannot be identified but statistical utility is preserved. We build on this literature to develop a hybrid approach to data masking for business organizations. We demonstrate the validity of the hybrid approach, which we call multiple imputation with multimodal perturbation (MIMP), using Monte Carlo simulation and illustrate its application in a specific business context. Results of our analysis open new areas of research for information systems scholarship and new potential revenue sources for business organizations.
|keyword = Bayesian bootstrap,business value of information technology,confidentiality,data masking,data safety,data security,decision support systems,disclosure risk,Monte Carlo simulation,multimodal perturbation,multiple imputation,privacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Price Dispersion: A Game-Theoretic Perspective and Empirical Evidence'''
{{header}}
{{article
|author= Sulin Ba,Jan Stallaert,Zhongju Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The existence and persistence of price dispersion for identical products in online markets have been well-documented in the literature. Possible explanations of this price dispersion, derived mainly using hedonic price models, have seen only modest success. In this paper, we propose a competitive model based on online retailers' differentiation mainly in service provided and recognition enjoyed to explain price dispersion. Our exploratory empirical analyses, using cross-sectional data, demonstrate that the competitive model provides a better explanation of the association between prices and online retailers' service and recognition levels. In addition, our competitive model is able to explain observations that are seemingly inconsistent with the hedonic model such as the negative association between service and price. This paper contributes to the literature on price dispersion by offering a differentiation model that provides a good fit with data and by proposing a theory that explains previous counterintuitive observations of prices. Our model also helps an e-tailer to choose a desirable position in the competitive market.
|keyword = online price dispersion,vertical differentiation,e-service,service quality,brand recognition,competitive strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STYLE COMPOSITION IN ACTION RESEARCH PUBLICATION'''
{{header}}
{{article
|author= Lars Mathiassen,Mike Chiasson,Matt Germonprez,
|source= MIS QUARTERLY
|year= 2012
|abstract = Examining action research publications in leading Information Systems journals as a particular genre of research communication, we develop the notion of style composition to understand how authors structure their arguments for a research contribution. We define style composition as the activity through which authors select, emphasize, and present elements of their research to establish premises, develop inferences, and present contributions in publications. Drawing on this general notion, we identify a set of styles that is characteristic of how IS action researchers compose their argument. Premise styles relate to the dual goals of action research through practical or theoretical positioning of the argument; inference styles combine insights from the problem-solving and the research cycles through inductive or deductive reasoning; and contribution styles focus on different types of contributions experience report, field study, theoretical development, problem-solving method, and research method. Based on the considered sample, we analyze the styles adopted in selected publications and show that authors have favored certain styles while leaving others underexplored; further, we reveal important strengths and weaknesses in the composition of styles within the IS discipline. Based on these insights, we discuss how action research practices and writing can be improved, as well as how to further develop style compositions to support the publication of engaged scholarship research.
|keyword = Action research,research methodology,style composition,journal publication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CAN ONLINE WAIT BE MANAGED? THE EFFECTS OF FILLER INTERFACES AND PRESENTATION MODES ON PERCEIVED WAITING TIME ONLINE'''
{{header}}
{{article
|author= Younghwa Lee,Andrew N. K. Chen,Virginia Ilie,
|source= MIS QUARTERLY
|year= 2012
|abstract = Long waits online undermine users' evaluations of Web sites and their providers, triggering abandonment behaviors. Yet e-business researchers and practitioners have not perfected mechanisms to respond to online wait issues. A filler interface that runs during the wait for search results may influence online users perceived waiting time (PWT); however, no scientific investigation has attempted to design effective filler interfaces for managing online waits. By adopting resource allocation theory, cognitive absorption theory, and human computer interaction (HCI) theories (competition for attention, visual search, and motion effect), we design diverse filler interlaces and investigate their effects on antecedents of PWT. The proposed research model considers cognitive absorption factors such as temporal dissociation, focused immersion, and heightened enjoyment as antecedents of PWT, which in turn triggers three outcomes: affective appraisals, cognitive appraisals, and Web site use intention. A multistage, multimethod approach is used to test the research hypotheses. In the first stage, we compare a filler interface condition with a no-filler interface condition, and find the superiority of a filler interface with respect to inducing focused immersion and temporal dissociation. In the second stage, we conduct two controlled experiments to examine whether filler interfaces with various designs (varying the presence and relevance of image. text, and image motion) distinctly influence antecedents of PWT and confirm their distinctive effects on focused immersion, temporal dissociation, and heightened enjoyment. In addition, by conducting a structural equation modeling analysis, we find that our research model explains 51 percent, 51 percent, 44 percent, and 45 percent of the variance in PWT, affective appraisals, cognitive appraisals, and Web site use intention respectively. Theoretical and practical implications of these findings are provided.
|keyword = Filler interface,interface design,online wait management,perceived waiting time,cognitive absorption,motion effect,competition for attention,visual search,resource allocation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ON PRODUCT UNCERTAINTY IN ONLINE MARKETS: THEORY AND EVIDENCE'''
{{header}}
{{article
|author= Angelika Dimoka,Yili Hong,Paul A. Pavlou,
|source= MIS QUARTERLY
|year= 2012
|abstract = Online markets pose a difficulty for evaluating products, particularly experience goods, such as used cars, that cannot be easily described online. This exacerbates product uncertainty, the buyer's difficulty in evaluating product characteristics, and predicting how a product will perform in the future. However, the IS literature has focused on seller uncertainty and ignored product uncertainty. To address this void, this study conceptualizes product uncertainty and examines its effects and antecedents in online markets for used cars (eBay Motors). Extending the information asymmetry literature from the seller to the product, we first theorize the nature and dimensions (description and performance) of product uncertainty. Second, we propose product uncertainty to be distinct from,,yet shaped by, seller uncertainty. Third, we conjecture product uncertainty to negatively affect price premiums in online markets beyond seller uncertainty. Fourth, based on the information signaling literature, we describe how information signals (diagnostic product descriptions and third-party product assurances) reduce product uncertainty. The structural model is validated by a unique dataset comprised of secondary transaction data from used cars on eBay Motors matched with primary data from 331 buyers who bid on these used cars. The results distinguish between product and seller uncertainty, show that product uncertainty has a stronger effect on price premiums than seller uncertainty, and identify the most influential information signals that reduce product uncertainty. The study's implications for the emerging role of product uncertainty in online markets are discussed.
|keyword = Product uncertainty,information signals,price premiums,online auction markets,eBay Motors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE CAREER PATHS LESS (OR MORE) TRAVELED: A SEQUENCE ANALYSIS OF IT CAREER HISTORIES, MOBILITY PATTERNS, AND CAREER SUCCESS'''
{{header}}
{{article
|author= Damien Joseph,Wai Fong Boh,Soon Ang,Sandra A. Slaughter,
|source= MIS QUARTERLY
|year= 2012
|abstract = This paper examines the objective career histories, mobility patterns, and career success of 500 individuals drawn from the National Longitudinal Survey of Youth (NLSY79), who had worked in the information technology workforce. Sequence analysis of career histories shows that careers of the IT workforce are more diverse than the traditional view of a dual IT career path (technical versus managerial). This study reveals a new career typology comprising three broad, distinct paths: IT careers; professional labor market (PLM) careers; and secondary labor market (SLM) careers. Of the 500 individuals in the IT workforce, 173 individuals pursued IT careers while the remaining 327 individuals left IT for other high-status non-IT professional jobs in PLM or lower-status, non-ITjobs in SLM careers. Findings from this study contribute to refining the concept of "boundaryless" careers. By tracing the diverse trajectories of career mobility, we enrich our understanding of how individuals construct boundaryless careers that span not only organizational hut also occupational boundaries. Career success did not differ in terms of average pay for individuals in IT and PLM careers. By contrast, individuals in SLM careers attained the lowest pay. We conclude this study with implications for future research and for the management of IT professionals' careers.
|keyword = Management of IT human resources,longitudinal,careers,sequence analysis,IT profession boundaryless,mobility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''UNDERSTANDING USER REVISIONS WHEN USING INFORMATION SYSTEM FEATURES: ADAPTIVE SYSTEM USE AND TRIGGERS'''
{{header}}
{{article
|author= Heshan Sun,
|source= MIS QUARTERLY
|year= 2012
|abstract = Post-adoptive system use is often characterized by cycles of adaptation, in which people actively revise how they use information systems. This paper investigates how and why individual users revise their system use at the feature level. A new concept, adaptive system use (ASU), is conceptualized as a user's revisions of which and how system features are used. This research identifies four specific ASU behaviors that collectively describe how people revise their use of system features. A model of ASU is developed based on Louis and Sutton's (1991) research on how people switch to active thinking from automatic thinking. The model specifies three antecedents of ASU (novel situations, discrepancies, and deliberate initiatives) and two moderators (personal innovativeness in IT and facilitating conditions). An empirical study of 253 Microsoft Office users largely supported the research model. The findings suggest that triggers-including novel situations, discrepancies, and deliberate initiatives are a significant impetus to ASU. This research also confirms moderating effects of personal innovativeness in IT: The findings also show the relationships among triggers: in addition to their direct impact on ASU, novel situations and deliberate initiatives exert their influence on ASU indirectly by giving rise to discrepancies in system use. Moreover, a cluster analysis identifies three heterogeneous triggering conditions and reveals that people engage in different ASU behaviors under different triggering conditions.
|keyword = Post-adoptive system use,adaptive system use,triggers,features in use,formative factor,personal innovativeness in IT,facilitating conditions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A COST-BASED DATABASE REQUEST DISTRIBUTION TECHNIQUE FOR ONLINE E-COMMERCE APPLICATIONS'''
{{header}}
{{article
|author= Debra VanderMeer,Kaushik Dutta,Anindya Datta,
|source= MIS QUARTERLY
|year= 2012
|abstract = E-commerce is growing to represent an increasing share of overall sales revenue, and online sales are expected to continue growing for the foreseeable future. This growth translates into increased activity on the supporting infrastructure, leading to a corresponding need to scale the infrastructure. This is difficult in an era of shrinking budgets and increasing functional requirements. Increasingly, IT managers are turning to virtualized cloud providers, drawn by the pay-for-use business model. As cloud computing becomes more popular, it is important jar data center managers to accomplish more with fewer dollars (i.e., to increase the utilization of existing resources). Advanced request distribution techniques can help ensure both high utilization and smart request distribution, where requests are sent to the service resources best able to handle them. While such request distribution techniques have been applied to the web and application layers of the traditional online application architecture, request distribution techniques for the data layer have focused primarily on online transaction processing scenarios. However, online applications often have a significant read-intensive workload, where read operations constitute a significant percentage of workloads (up to 95 percent or higher). In this paper, we propose a cost-based database request distribution (C-DBRD) strategy, a policy to distribute requests, across a cluster of commercial, off-the-shelf databases, and discuss its implementation. We first develop the intuition behind our approach, and describe a high-level architecture for database request distribution. We then develop a theoretical model for database load computation, which we use to design a method for database request distribution and build a software implementation. Finally, following a design science methodology, we evaluate our artifact:: through experimental evaluation. Our experiments, in the lab and in production-scale systems, show significant improvement of database layer resource utilization, demonstrating up to a 45 percent improvement over existing request distribution techniques.
|keyword = Database clusters,request distribution,task allocation,design research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EFFICIENCY OR INNOVATION: HOW DO INDUSTRY ENVIRONMENTS MODERATE THE EFFECTS OF FIRMS' IT ASSET PORTFOLIOS?'''
{{header}}
{{article
|author= Ling Xue,Gautam Ray,Vallabh Sambamurthy,
|source= MIS QUARTERLY
|year= 2012
|abstract = Firms invest in a variety of information technologies and seek to align their IT asset portfolios with two key performance outcomes: efficiency and innovation. Existing research makes the universalistic assumption that both outcomes will always be realized through firms' IT asset portfolios. There has been limited research on the conditions under which firms' IT asset portfolios should be oriented more toward efficiency or innovation. Here, we argue that the nature of the industry where a firm competes will have a significant moderating effect on the link between firms' IT asset portfolios and efficiency or innovation outcomes. Using panel data that covers a wide range of industry environments, we find that at lower levels of dynamism, munificence, and complexity. IT asset portfolios are associated with a greater increase in efficiency. In contrast, in environments with higher levels of complexity, IT asset portfolios are associated with a greater increase in innovation (i.e., development of new products and processes, and exploration of growth opportunities). These results provide insights about how firms could realize strategic alignment by tailoring their IT asset portfolios toward an efficiency or innovation focus.
|keyword = Efficiency,innovation,exploitation,exploration,IT asset portfolio,IT value,competitive environment,dynamism,munificence,complexity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD A NEW THEORY OF THE CONTRIBUTION OF THE IT FUNCTION IN ORGANIZATIONS'''
{{header}}
{{article
|author= Manon G. Guillemette,Guy Pare,
|source= MIS QUARTERLY
|year= 2012
|abstract = Because changes in organizations and information technology environments are enduring, the alignment of the IT function with business objectives must not only be understood, but constantly renewed and adjusted. This is amply reflected in recent surveys of CIOs, which consistently suggest that the notion of alignment is a top challenge and management priority. Many CIOs face a double challenge when addressing the issue of alignment: they must first clarify, top management's expectations and assumptions about IT, which may be contradictory, and then understand their implications for how the IT department should be managed (i.e., translate the function's strategic mission into an IT management model that adds value to the organization). The characterization of the IT function has constituted a central and growing subject of research in the information systems field. Although the extant literature has much to teach us, knowledge in this area is nevertheless fragmented and has not been properly integrated. In response to these limitations, this study proposes and tests a new theory of the contribution of the IT/function. Specifically, our objective is to offer an explanation of the contribution of the IT function in organizations with a typology of ideal profiles. A field study was conducted in 24 large Canadian companies in order to validate a set of research propositions. Our results first suggest that there are five distinct "ideal" IT management profiles in organizations and each of these profiles tends to focus on specific sources of value. Next, we observed that IT functions that are close to the ideal of any given profile seem to be outperforming those with hybrid profiles. Finally, our findings provide a compelling explanation as to how ideal IT management profiles are adopted in organizations. The article concludes with a discussion of the theoretical and practical implications of the proposed theory.
|keyword = IT function,IT management profile,IT contribution,CIO,theory building,typological theory,field study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ASYMMETRIC BENEFITS OF RELATIONAL FLEXIBILITY: EVIDENCE FROM SOFTWARE DEVELOPMENT OUTSOURCING'''
{{header}}
{{article
|author= Anandasivam Gopal,Balaji R. Koka,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this paper, the interacting effect of formal contracts and relational governance on vendor profitability and quality in the software outsourcing industry are examined. We focus on a critical manifestation of relational governance the presence of relational flexibility in the exchange relationship and argue that the enacted observation of relational flexibility is driven by perceptions of exchange hazards. In a departure from extant literature, however, we propose that the benefits accruing from it are asymmetric and depend on how the exchange risks are apportioned by the formal contract. Formally, we hypothesize that relational flexibility provides greater benefits to an exchange partner that faces the greater proportion of risk in a project, induced through the contract. In addition, we hypothesize that these benefits manifest on the performance dimensions that are of importance to the risk-exposed partner. We test our hypotheses on 105 software projects completed by a software outsourcing vendor for multiple clients. The results show that relational,flexibility positively affects profitability in only fixed price contracts, where the vendor faces greater risk, while positively affecting quality only in time and materials contracts, where the client is at greater risk. We thus provide evidence for the asymmetric benefits from relational governance, thereby arguing for a more contingent and limited view of the value of relational governance, based on risk-exposure, rather than the more expansive view prevalent in the literature contending that relational governance provides benefits for all parties to an exchange. We conclude with a discussion of the research and managerial implications of our findings.
|keyword = Relational governance,relational flexibility,formal contracts,software development,outsourcing,exchange hazards,regression analysis,quality,profitability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ENACTING CLAN CONTROL IN COMPLEX IT PROJECTS: A SOCIAL CAPITAL PERSPECTIVE'''
{{header}}
{{article
|author= Cecil Eng Huang Chua,Wee-Kiat Lim,Christina Soh,Siew Kien Sia,
|source= MIS QUARTERLY
|year= 2012
|abstract = The information technology project control literature has documented that clan control is often essential in complex multistakeholder projects for project success. However, instituting clan control in such conditions is challenging as people come to a project with diverse skills and backgrounds. There is often insufficient time for clan control to develop naturally This paper investigates the question, "How can clan control be enacted in complex IT projects?" Recognizing social capital as a resource, we conceptualize a clan as a group with strong social capital (i.e., where its members have developed their structural, cognitive, and relational ties to the point that they share common values and beliefs and are committed to a set of peer norms). We theorize that the enactment of clan control is a dual process of (1) building the clan by developing its social capital dimensions (structural, cognitive, and relational ties) or reappropriating social capital from elsewhere and (2) leveraging the clan by reinforcing project-facilitating shared values, beliefs, and norms, and inhibiting those that impede the achievement of project goals. We explore how clan control was enacted in a large IT project at a major logistics organization in which clan control was quickly instituted to avoid an impending project failure. Our research contributes to theory in three ways: (1) we reconcile the two differing views of clan control into a single framework, (2) we explain the role of controllers in enacting clan control, and (3) we clarify, how formal control can be employed to develop clan control.
|keyword = Behavioral control theory,clan control,formal control,project management,project control,IT projects,social capital,enterprise systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IS EMPLOYEE ATTITUDES AND PERCEPTIONS AT VARYING LEVELS OF SOFTWARE PROCESS MATURITY'''
{{header}}
{{article
|author= Janet K. Ply,Jo Ellen Moore,Clay K. Williams,Jason Bennett Thatcher,
|source= MIS QUARTERLY
|year= 2012
|abstract = Taking a control theory view of software process innovation, we tested prevalent beliefs regarding software process maturity and Information Systems employee attitudes and perceptions by surveying 736 IS professionals in 10 organizations at varying levels of the CMM (capability maturity model). Although anecdotal reports and the scant empirical studies to date suggest job attitudes and perceptions are more positive for employees in organizations at higher levels of software process maturity, we found evidence of a more complex picture. While our data supported expectations that role conflict and perceived work overload were lower for IS professionals in organizations at a level of maturity where software process behavioral controls are implemented, other results were not fully in line with prevalent beliefs. Most notably, IS workers reported significantly lower professional efficacy and lower job satisfaction in organizations at CMM Level 3, where behavioral controls are the dominant form of formal control, than in organizations at Level I, which is relatively free of formal controls. Some anticipated positive attitudes and perceptions surfaced in organizations at the highest rungs of software process maturity (CMM Levels 4/5), where the established behavioral controls are supplemented by substantial outcome controls, as IS professionals reported lower role ambiguity and higher job satisfaction than did their counterparts in organizations at CMM Level 3.
|keyword = Software process improvement,CMM,job satisfaction,role conflict,role ambiguity,work overload,cynicism,professional efficacy,control theory,IS professionals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Two Rule-Based Natural Language Strategies for Requirements Discovery and Classification in Open Source Software Development Projects'''
{{header}}
{{article
|author= Radu E. Vlas,William N. Robinson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Open source projects do have requirements; they are, however, mostly informal text descriptions found in requests, forums, and other correspondence. Understanding such requirements provides insight into the nature of open source projects. Unfortunately, manual analysis of natural language requirements is time-consuming, and for large projects, error prone. Automated analysis of natural language requirements, even partial, will be of great benefit. Toward that end, we describe the design and validation of an automated natural language requirements classifier for open source projects. We compare two strategies for recognizing requirements in open forums of software features. Our results suggest that classifying text at the forum post-aggregation and sentence aggregation levels may be effective. Our results suggest that it can reduce the effort required to analyze requirements of open source projects.
|keyword = natural language processing,open source,requirements classification,requirements discovery,software requirements,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Supporting Agile Organizations with a Decision Guidance Query Language'''
{{header}}
{{article
|author= Alexander Brodsky,Nathan E. Egge,X. Sean Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Decision optimization is widely used in many decision guidance and support systems (DGSS) to support business decisions such as procurement, scheduling, and planning. In spite of rapid changes in users' requirements, the implementation of DGSS is typically rigid, expensive, and not easily extensible, which is in stark contrast to the agile implementation of management information systems (MIS) based on the database management systems (DBMS) and SQL technologies. This paper focuses on the Decision Guidance Query Language (DGQL) designed to (re-)use SQL programs for decision optimization with the goals of making DGSS implementation agile and intuitive and leveraging existing investment in SQL-implemented MIS. The paper addresses two related technical issues with DGQL: (1) how to annotate existing queries to precisely express the optimization semantics, and (2) how to translate the annotated queries into equivalent mathematical programming formulations that can be solved efficiently.
|keyword = agile organizations,decision guidance,decision support,optimization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Technology Support for Organizational Transactive Memory: Requirements, Application, and Customization'''
{{header}}
{{article
|author= Dorit Nevo,Izak Benbasat,Yair Wand,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Transactive memory is an effective mechanism for locating and coordinating expertise in small groups and has been shown to hold numerous benefits for groups and organizations. To extend transactive memory beyond the scope of small groups, researchers have proposed the use of information technology (IT). This paper provides an integrated discussion of our knowledge from three studies concerning IT support in transactive memory in organizations. Focusing on meta-memory, which is at the heart of transactive memory systems, we examine what meta-memory is maintained by members of transactive memory systems, whether providing this meta-memory in a technology-mediated environment can lead to transactive memory development, whether IT can realistically provide this meta-memory, and whether different requirements exist for different users and in different stages of transactive memory development. We discuss the implications of these studies to both research and practice.
|keyword = expertise location,meta-memory,social media,transactive memory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Theory-Informed Design and Evaluation of an Advanced Search and Knowledge Mapping System in Nanotechnology'''
{{header}}
{{article
|author= Yan Dang,Yulei Zhang,Hsinchun Chen,Susan A. Brown,Paul Jen-Hwa Hu,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Effective search support is an important tool for helping individuals deal with the problem of information overload. This is particularly true in the field of nanotechnology, where information from patents, grants, and research papers is growing rapidly. Guided by cognitive fit and cognitive load theories, we develop an advanced Web-based system, Nano Mapper, to support users' search and analysis of nanotechnology developments. We perform controlled experiments to evaluate the functions of Nano Mapper. We examine users' search effectiveness, efficiency, and evaluations of system usefulness, ease of use, and satisfaction. Our results demonstrate that Nano Mapper enables more effective and efficient searching, and users consider it to be more useful and easier to use than the benchmark systems. Users are also more satisfied with Nano Mapper and have higher intention to use it in the future. User evaluations of the analysis functions are equally positive.
|keyword = cognitive fit,cognitive load,information system evaluation,knowledge mapping,searching,visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitation Roles and Responsibilities for Sustained Collaboration Support in Organizations'''
{{header}}
{{article
|author= Gwendolyn L. Kolfschoten,Fred Niederman,Robert O. Briggs,Gert-Jan De Vreede,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Research shows that under certain conditions, groups using collaboration technologies such as group support systems (GSS) can gain substantial improvements in the effectiveness and efficiency of their work processes. GSS, however, have been slow to develop self-sustaining communities of users in the workplace. Organizations that use collaboration technology may require two kinds of support: process support and technology support. Both types of support involve (1) design tasks (e.g., designing a work process and designing the technology to support the process), (2) application tasks (to apply the process and to use the technology), and (3) management tasks (to monitor and control the process and to oversee the maintenance of the technology). This paper explores how these tasks and associated roles can be anchored in organizations, and the relationship of task allocation patterns to the sustained use of collaboration technology in organizations.
|keyword = collaboration engineering,collaboration support,facilitation,group support systems,group work,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Going Concerns: The Governance of Interorganizational Coordination Hubs'''
{{header}}
{{article
|author= M. Lynne Markus,Quang Neo Bui,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Business-to-business interactions are increasingly conducted through interorganizational coordination hubs, in which standardized information technology-based platforms provide data and business process interoperability for interactions among the organizations in particular industrial communities. Because the governance of interorganizational arrangements is believed to affect their efficiency and effectiveness, this paper explores how and why interorganizational coordination hubs are governed. Analysis of relevant prior theory and case examples shows that coordination hub governance is designed to balance the sometimes conflicting needs for capital to invest in new technology, for participation of industry members, and for the protection of data resources. Findings suggest that the governance of interorganizational coordination hubs is not the starkly categorical choice between collective (member-owned) and investor-owned forms as suggested by prior theory. Instead, many hybrid arrangements are observed in the five examined cases. Future theoretical development and empirical research are needed to understand the increasingly important phenomenon of coordination hub governance.
|keyword = case study,coordination hubs,corporate governance,interorganizational relationships,IT investment,IT standards,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effect of Information Feedback on the Outcomes and Dynamics of Multisourcing Multiattribute Procurement Auctions'''
{{header}}
{{article
|author= Gediminas Adomavicius,Alok Gupta,Pallab Sanyal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Electronic auctions are increasingly being used to facilitate the procurement of goods and services in organizations. Multiattribute auctions, which allow bids on multiple dimensions of the product and not just price, are information technology-enabled sourcing mechanisms that can increase the efficiency of procurement for configurable goods and services compared to price-only auctions. Given the strategic nature of procurement auctions, the amount of information concerning the buyer's preferences that is disclosed to the suppliers has implications on the profits of the buyer and the suppliers and, consequently, on the long-term relationship between them. This study explores novel feedback schemes for multisourcing multiattribute auctions that require limited exchange of strategic information between the buyer and the suppliers. To study the impact of feedback on the outcomes and dynamics of the auctions, we conduct laboratory experiments wherein we analyze bidder behavior and economic outcomes under three different treatment conditions with different types of information feedback. Our results indicate that, in contrast to winner-take-all multiattribute auctions, multisourcing multiattribute auctions, with potentially multiple winners, allow bidders (i.e., suppliers) to extract more profit when greater transparency in terms of provisional allocations and prices is provided. We develop several insights for mechanism designers toward developing sustainable procurement auctions that efficiently allocate multiple units of an asset with multiple negotiable attributes among multiple suppliers.
|keyword = bidder behavior,experimental economics,information feedback,multiattribute auction,procurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging Information Technology Infrastructure to Facilitate a Firm's Customer Agility and Competitive Activity: An Empirical Investigation'''
{{header}}
{{article
|author= Nicholas Roberts,Varun Grover,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = This paper investigates how information technology (IT) facilitates a firm's customer agility and, in turn, competitive activity. Customer agility captures the extent to which a firm is able to sense and respond quickly to customer-based opportunities for innovation and competitive action. Drawing from the dynamic capability and IT business value research streams, we propose that IT plays an important role in facilitating a "knowledge creating" synergy derived from the interaction between a firm's Web-based customer infrastructure and its analytical ability. This will enhance the firm's ability to sense customer-based opportunities. IT also plays an important role in "process enhancing" synergy obtained from the interaction between a firm's coordination efforts and its level of information systems integration, which facilitates the firm's ability to respond to those opportunities. We also leverage the competitive dynamics and strategic alignment literature to propose that the alignment between customer-sensing capability and customer-responding capability will impact the firm's competitive activity. We test our model with a two-stage research design in which we survey marketing executives of high-tech firms. Our results show that a Web-based customer infrastructure facilitates a firm's customer-sensing capability; furthermore, analytical ability positively moderates this relationship. We also find that internal systems integration positively moderates the relationship between interfunctional coordination and a firm's customer-responding capability. Finally, our results show that agility alignment affects the efficacy of a firm's competitive actions. In particular, action efficacy is higher when sensing and responding capabilities are both high.
|keyword = alignment,competitive dynamics,customer agility,dynamic capability,IT infrastructure,IT value,open innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Methodology Mashups: An Exploration of Processes Used to Maintain Software'''
{{header}}
{{article
|author= Dana Edberg,Polina Ivanova,William Kuechler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = The majority of studies of software development processes explore initial development rather than ongoing software maintenance, yet the majority of the systems development budget in many organizations is devoted to maintenance. Software maintenance projects differ significantly from original development projects, indicating a need for more research specifically concerning maintenance processes. This study uses a grounded theory research method to explore how information technology professionals define and select a methodology to maintain existing software. We found that in-use maintenance methodologies are composed of components from multiple formal methodologies. We developed a factor model describing how these components are chosen. The findings contribute to a better understanding of how standard methodologies are applied in software practice and the critical factors used by professionals when choosing an appropriate methodology for software maintenance activities. This research underscores the need for incorporating the full software life cycle in information systems development research and education.
|keyword = grounded theory,maintenance methodology,software development,software maintenance,software process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Patch Release Behaviors of Software Vendors in Response to Vulnerabilities: An Empirical Analysis'''
{{header}}
{{article
|author= Orcun Temizkan,Ram L. Kumar,SungJune Park,Chandrasekar Subramaniam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2012
|abstract = Software vulnerabilities have become a serious concern because unpatched software runs the risk of being exploited by hackers. There is a need for software vendors to make software patches available in a timely manner for vulnerabilities in their products. We develop a survival analysis model of software vendors' patch release behavior and test it using a data set compiled from the National Vulnerability Database, United States Computer Emergency Readiness Team, and vendor Web sites. This model helps to understand how factors specific to vulnerabilities, patches, software vendors, and software affect the patch release behavior of software vendors based on their cost structure. This study also analyzes the impact of the presence of multiple vendors and type of vendor on the patch release behavior of software vendors. Our results indicate that vulnerabilities with high confidentiality impact or high integrity impact are patched faster than vulnerabilities with high availability impact. Interesting differences in the patch release behavior of software vendors based on software type (new release versus update) and type of vendor (open source versus proprietary) are found. Our results illustrate that when there are legislative pressures, vendors react faster in patching vulnerabilities. Thus, appropriate regulations can be an important policy tool to influence vendor behavior toward socially desirable security outcomes.
|keyword = patch quality,patch release time,patch types,software vendor types,software vulnerability characteristics,survival analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Moving Beyond the Single Site Implementation Study: How (and Why) We Should Study the Biography of Packaged Enterprise Solutions'''
{{header}}
{{article
|author= Robin Williams,Neil Pollock,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The single site implementation study is an invaluable tool for studying the large-scale enterprise solution. Together with constructivist frameworks and ethnographic approaches it has allowed the development of rich local pictures of the immediate and adaptive response by user organizations to the take-up of what are, today, often generic packaged systems. However, to view the packaged enterprise solution principally at the place where the user encounters it also has limitations. It produces somewhat partial understandings of these complex artifacts. In particular, it downplays important influences from other sites and time frames. This paper argues that if we are to understand the full implications of enterprise solutions for organizations then we should study their "biography." This idea points to how the career of workplace technology is often played out over multiple time frames and settings. To understand its shaping therefore requires scholars to go beyond the study of technology at a single locale or moment and, rather, attempt to follow it through space and time. The paper develops two ideas to aid this kind of study. We discuss better spatial metaphors that might help us explore the hybrid and extended spaces in which packaged software systems develop and evolve. We also review improved temporal understandings that may capture the multiple contemporary and historical time frames at play. The paper concludes by discussing some possible research directions that a focus on the biography of a technology might allow.
|keyword = implementation,biography,ethnography,enterprise resource planning,sociology,actor network theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Networks and the Diffusion of User-Generated Content: Evidence from YouTube'''
{{header}}
{{article
|author= Anjana Susarla,Jeong-Ha Oh,Yong Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper is motivated by the success of YouTube, which is attractive to content creators as well as corporations for its potential to rapidly disseminate digital content. The networked structure of interactions on YouTube and the tremendous variation in the success of videos posted online lends itself to an inquiry of the role of social influence. Using a unique data set of video information and user information collected from YouTube, we find that social interactions are influential not only in determining which videos become successful but also on the magnitude of that impact. We also find evidence for a number of mechanisms by which social influence is transmitted, such as (i) a preference for conformity and homophily and (ii) the role of social networks in guiding opinion formation and directing product search and discovery. Econometrically, the problem in identifying social influence is that individuals' choices depend in great part upon the choices of other individuals, referred to as the reflection problem. Another problem in identification is to distinguish between social contagion and user heterogeneity in the diffusion process. Our results are in sharp contrast to earlier models of diffusion, such as the Bass model, that do not distinguish between different social processes that are responsible for the process of diffusion. Our results are robust to potential self-selection according to user tastes, temporal heterogeneity and the reflection problem. Implications for researchers and managers are discussed.
|keyword = diffusion,user-generated content,YouTube,social networks,reflection problem,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology and Intangible Output: The Impact of IT Investment on Innovation Productivity'''
{{header}}
{{article
|author= Landon Kleis,Paul Chwelos,Ronald V. Ramirez,Iain Cockburn,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Prior research concerning IT business value has established a link between firm-level IT investment and tangible returns such as output productivity. Research also suggests that IT is vital to intermediate processes such as those that produce intangible output. Among these, the use of IT in innovation and knowledge creation processes is perhaps the most critical to a firm's long-term success. However, little is known about the relationship between IT, knowledge creation, and innovation output. In this study, we contribute to the literature by comprehensively examining the contribution of IT to innovation production across multiple contexts using a quality-based measure of innovation output. Analyzing annual information from 1987 to 1997 for a panel of large U. S. manufacturing firms, we find that a 10% increase in IT input is associated with a 1.7% increase in innovation output for a given level of innovation-related spending. This relationship between IT, research and development (R&D), and innovation production is robust across multiple econometric methodologies and is found to be particularly strong in the mid to late 1990s, a period of rapid technological innovation. Our results also demonstrate the importance of IT in creating value at an intermediate stage of production, in this case, through improved innovation productivity. However, R&D and its related intangible factors (skill, knowledge, etc.) appear to play a more crucial role in the creation of breakthrough innovations.
|keyword = information technology,productivity,knowledge production function,innovation,patents,research and development,IT business value,breakthrough innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competitive Behavior-Based Price Discrimination for Software Upgrades'''
{{header}}
{{article
|author= Amit Mehra,Ram Bala,Ramesh Sankaranarayanan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The introduction of product upgrades in a competitive environment is commonly observed in the software industry. When introducing a new product, a software vendor may employ behavior-based price discrimination (BBPD) by offering a discount over its market price to entice existing customers of the competitor. This type of pricing is referred to as competitive upgrade discount pricing and is possible because the vendor can use proof of purchase of a competitor's product as credible evidence to offer the discount. At the same time, the competitor may offer a discount to its own previous customers in order to induce them to buy its upgrade. We formulate a game-theoretic model involving an incumbent and entrant where both firms can offer discounts to existing customers of the incumbent. Although several equilibrium possibilities exist, we establish that an equilibrium with competitive upgrade discount pricing is observed only for a unique market structure and a corresponding unique set of prices. In this equilibrium, instead of leveraging its first mover advantage, the incumbent cedes market share to the entrant. Furthermore, the profits of both the incumbent and the entrant reduce with switching costs. This implies that the use of BBPD has product design implications because firms may influence the switching costs between their products by making appropriate compatibility decisions. In addition, lower switching costs result in reduced consumer surplus. Hence, a social planner may want to increase switching costs. The resulting policy implications are different from those prevalent in other industries such as mobile telecommunications where the regulators reduced switching costs by enforcing number portability.
|keyword = behavior-based pricing,software upgrades,competitive strategy,switching costs,forward-looking customers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Information Revelation Policies Under Cost Uncertainty'''
{{header}}
{{article
|author= Karthik N. Kannan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The paper presents insights regarding the key learning-related factors a buyer should consider when deciding the extent to which information about bids is revealed in a procurement auction context. It offers the insights by analyzing the following two first-price sealed-bid policies in a private-value sequential auction with no winner dropouts: (i) IIS, where only the winner's bid is revealed, and (ii) CIS, where all bids are revealed. Our analysis identifies two important learning effects-the extraction and the deception effects-as having significant welfare implications. Both these effects arise because of a bidder's desire to gain an informational advantage relative to his competitors, but their manifestations are different. The extraction effect occurs because of a bidder's incentive to learn about his competitors, and the deception effect is a consequence of the incentive to prevent an opponent from gaining the information. Both effects lead to higher bid prices, and either may be dominant from a procurer surplus standpoint. With the deception effect, social welfare can decrease even when the number of suppliers increases, a result that is counterintuitive. The paper also discusses how insights regarding the learning effects might apply to other policies.
|keyword = information revelation,electronic markets,economics of information systems,perfect Bayesian Nash equilibrium,auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Switching Costs, Network Effects, and Competition in the European Mobile Telecommunications Industry'''
{{header}}
{{article
|author= Lucio Fuentelsaz,Juan Pablo Maicas,Yolanda Polo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This paper empirically analyzes the joint effect of switching costs and network effects in determining the level of competition in the European mobile communications industry. Theoretical reasoning argues that switching costs and network effects may confer some market power that firms can strategically exploit to reduce competition and thus increase profits. Theoretical predictions are completely confirmed by the empirical evidence and important asymmetries between the market structures in the different European countries can be observed. These asymmetries are clearly related to the levels of switching costs and network effects-the greater their importance, the lower the rivalry in the market. This suggests that the recent efforts of policymakers to reduce the negative consequences of switching costs and network effects have not been successful enough and these efforts must be strengthened, at least in several countries.
|keyword = network effects,switching costs,mobile telecommunications industry,competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modeling Spatial and Temporal Set-Based Constraints During Conceptual Database Design'''
{{header}}
{{article
|author= Faiz Currim,Sudha Ram,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = From a database perspective, business constraints provide an accurate picture of the real world being modeled and help enforce data integrity. Typically, rules are gathered during requirements analysis and embedded in code during the implementation phase. We propose that the rules be explicitly modeled during conceptual design, and develop a framework for understanding and classifying spatiotemporal set-based (cardinality) constraints and an associated syntax. The constraint semantics are formally specified using first-order logic. Modeling rules in conceptual design ensures they are visible to designers and users and not buried in application code. The rules can then be semiautomatically translated into logical design triggers yielding productivity gains. Following the principles of design science research, we evaluate the framework's expressiveness and utility with a case study.
|keyword = data modeling,conceptual design,spatiotemporal cardinality constraints,data management spatiotemporal databases,design science research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT Outsourcing Contracts and Performance Measurement'''
{{header}}
{{article
|author= David Fitoussi,Vijay Gurbaxani,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Companies that outsource information technology (IT) services usually focus on achieving multiple objectives. Correspondingly, outsourcing contracts typically specify a variety of metrics to measure and reward (or penalize) vendor performance. The specific types of performance metrics included in a contract strongly affect its incentive content and ultimately its outcome. One specific challenge is the measurement of performance when an outsourcing arrangement has a mix of objectives, some that are highly measurable and others that are not. Recent advances in contract theory suggest that the design of incentives for a given objective is affected by the characteristics of other objectives. However, there is little empirical work that demonstrates how relevant these "multitask" concerns are in real-world contracts. We apply contract theory to examine how objectives and incentives are related in IT outsourcing contracts that include multiple objectives with varying measurement costs. In our context, contracts generally share the objective of reducing IT costs but vary in the importance of increasing IT quality. We establish empirical results about performance measurement in IT outsourcing contracts that are consistent with recent theoretical propositions. We find that the use of strong direct incentives for a given measurable objective is negatively correlated with the presence of less measurable objectives in the contract. We show that outsourcing contracts that emphasize goals with high measurement costs employ more performance metrics than initiatives whose objectives have a lower measurement cost profile. Surprisingly, as the number of performance metrics increases, satisfactory outcomes decrease, which we explain within a multitask theory framework. Overall, our results provide empirical support for multitask principal-agent theory and important guidance in designing outsourcing contracts for complex IT services.
|keyword = IT outsourcing,multitask,contract theory,performance measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Peripheral Developers Contribute to Open-Source Software Development'''
{{header}}
{{article
|author= Pankaj Setia,Balaji Rajagopalan,Vallabh Sambamurthy,Roger Calantone,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Open-source software development is the next stage in the evolution of product development, particularly software products. Compared with the prevailing proprietary approaches, open-source software products are developed by co-opting external developers and prospective users. Although a core group of developers might still play a key role in the initial design and development, a notable aspect of the open-source software paradigm is the role of peripheral developers in the enhancement and popularization of the product. Peripheral developers are not formal members of the core development team. They voluntarily contribute their time and creative talent in improving the quality of the product or in popularizing the product through word-of-mouth advocacy. As volunteers, they are not subject to the traditional hierarchical controls, nor are they contractually obligated. Peripheral developers represent a novel and unique aspect of open-source software development, and there is a greater interest in tapping their potential. However, there has been limited evidence about how and when their participation has beneficial impacts. We examine how peripheral developers contribute to product quality and diffusion by utilizing longitudinal data on 147 open-source software products. Hierarchical linear modeling analysis indicates that peripheral developers make significant contributions to product quality and diffusion, especially on projects that are in the more mature stages of product development.
|keyword = open source,diffusion,quality,new product development,adoption,software development,peripheral developers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online and Offline Demand and Price Elasticities: Evidence from the Air Travel Industry'''
{{header}}
{{article
|author= Nelson Granados,Alok Gupta,Robert J. Kauffman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The Internet has brought consumers increased access to information to make purchase decisions. One of the expected consequences is an increase in the price elasticity of demand, or the percent change in demand caused by a percent change in price, because consumers are better able to compare offerings from multiple suppliers. In this paper, we analyze the impact of the Internet on demand, by comparing the demand functions in the Internet and traditional air travel channels. We use a data set that contains information for millions of records of airline ticket sales in both online and offline channels. The results suggest that consumer demand in the Internet channel is more price elastic for both transparent and opaque online travel agencies (OTAs), in part, because of more leisure travelers self-selecting the online channel, relative to business travelers. Yet, after controlling for this channel self-selection effect, we still find differences in price elasticity across channels. We find that the opaque OTAs are more price elastic than the transparent OTAs, which suggests that product information can mitigate the price pressures that arise from Internet-enabled price comparisons. We discuss the broader implications for multichannel pricing strategy and for the transparency-based design of online selling mechanisms.
|keyword = air travel industry,economics of information systems,electronic markets,market transparency,mechanism design,multichannel strategy,price elasticity,online travel agencies,self-selection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of External Word-of-Mouth Sources on Retailer Sales of High-Involvement Products'''
{{header}}
{{article
|author= Bin Gu,Jaehong Park,Prabhudev Konana,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Online word-of-mouth (WOM) such as consumer opinions, user experiences, and product reviews has become a major information source in consumer purchase decisions. Prior research on online WOM effect has focused mostly on low-involvement products such as books or CDs. For these products, retailer-hosted (internal) WOM is shown to influence sales overwhelmingly. Numerous surveys, however, suggest consumers often conduct pre-purchase searches for high-involvement products (e. g., digital cameras) and visit external WOM websites during the search process. In this study, we analyze the relative impact of external and internal WOMs on retailer sales for high-involvement products using a panel of sales and WOM data for 148 digital cameras from Amazon.com and three external WOM websites (Cnet, DpReview, and Epinions) over a four-month period. The results suggest that a retailer's internal WOM has a limited influence on its sales of high-involvement products, while external WOM sources have a significant impact on the retailer's sales. The findings imply that external WOM sources play an important role in the information search process.
|keyword = user generated content,word-of-mouth,consumer information search,product involvement,electronic commerce,digital cameras,Amazon,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Users' Switching Costs: Their Nature and Formation'''
{{header}}
{{article
|author= Soumya Ray,Sung S. Kim,James G. Morris,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = The highly competitive and rapidly changing market for online services is becoming increasingly effective at locking users in through the coercive effects of switching costs. Although the information systems field increasingly recognizes that switching costs plays a big part in enforcing loyalty, little is known about what factors users regard as switching costs or why they perceive these costs. Consequently, it is hard for online services to know what lock-in strategies to use and when to apply them. We address this problem by first developing a theory-driven structure of online users' perceived switching costs that distinguishes between vendor-related and user-related factors. We then propose that important antecedent influences on switching costs from economic value, technical self-efficacy, and past investments are more complex and intertwined than previously thought. We empirically validated the proposed model using data collected from home users of Internet service providers. Our findings demonstrate that an online service's economic value more heavily influences users' perceptions of vendor-related switching costs than does technical self-efficacy. However, users' technical abilities outweigh economic value in influencing user-related switching costs. Furthermore, although we confirmed the commonly held notion that deeply invested users are generally more vulnerable to lock-in, we also found that this relationship is contingent on users' technical abilities. Finally, we found that our multidimensional measure of switching costs is a valid predictor of user loyalty and is more powerful than previous global measures. Overall, this study uncovered a finer network of switching-cost production than had been previously established and suggests a new approach to modeling and exploiting online users' perceived switching costs.
|keyword = switching costs,online consumer behavior,survey data,partial least squares,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Multilevel Analysis of the Effect of Group Appropriation on Collaborative Technologies Use and Performance'''
{{header}}
{{article
|author= Sora Kang,Kai H. Lim,Min Soo Kim,Hee-Dong Yang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = This study develops a comprehensive model to predict and explain the use of collaborative technologies (CT) and the task performance of individual users as a result of using CT. The integrated model attempts to capture how the individual user's extent of use of CT is a function of both the technical features and the structures embedded within or created by the interactions among the technology, group, and organization. The model developed is tested using data collected from a national bank with 279 members working in 40 different workgroups. A hierarchical linear model (HLM) is used to test the hypotheses generated from the model. Results show that our integrated model provides a more complete explanation of the use of CT and task performance beyond those of the individual-level factors. The study is an early effort to develop an integrated theory to provide comprehensive insight into individual use of CT in a group or organizational context.
|keyword = IT diffusion and adoption,IT use,questionnaire surveys,adaptive structuration theory,collaborative technologies,consensus of appropriation,faithfulness of appropriation,social comparison theory,multilevel analysis,hierarchical linear model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Halo Effect in Multicomponent Ratings and Its Implications for Recommender Systems: The Case of Yahoo! Movies'''
{{header}}
{{article
|author= Nachiketa Sahoo,Ramayya Krishnan,George Duncan,Jamie Callan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find personalized recommendations for each user. Traditionally they have been designed to work with one-dimensional ratings. With interest growing in recommendations based on multiple aspects of items, we present an algorithm for using multicomponent rating data. The presented mixture model-based algorithm uses the component rating dependency structure discovered by a structure learning algorithm. The structure is supported by the psychometric literature on the halo effect. This algorithm is compared with a set of model-based and instance-based algorithms for single-component ratings and their variations for multicomponent ratings. We evaluate the algorithms using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations. However, we find that the choice of algorithm depends on the sparsity of the training data. It also depends on whether the task of the algorithm is to accurately predict ratings or to retrieve relevant items. In our experiments a model-based multicomponent rating algorithm is able to better retrieve items when training data are sparse. However, if the training data are not sparse, or if we are trying to predict the rating values accurately, then the instance-based multicomponent rating collaborative filtering algorithms perform better. Beyond generating recommendations we show that the proposed model can fill in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multicomponent ratings.
|keyword = collaborative filtering,multicomponent rating,halo effect,Bayesian network,mixture model,expectation maximization,recommender system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''To Continue Sharing or Not to Continue Sharing? An Empirical Analysis of User Decision in Peer-to-Peer Sharing Networks'''
{{header}}
{{article
|author= Mu Xia,Yun Huang,Wenjing Duan,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Peer-to-peer sharing networks have seen explosive growth recently. In these networks, sharing files is completely voluntary, and there is no financial reward for users to contribute. However, many users continue to share despite the massive free-riding by others. Using a large-scale data set of individual activities in a peer-to-peer music-sharing network, we seek to understand users' continued-sharing behavior as a private contribution to a public good. We find that the more benefit users "get from" the network, in the form of downloads, browses, and searches, the more likely they are to continue sharing. Also, the more value users "give to" the network, in the form of downloads by other users and recognition by the network, the more likely they are to continue sharing. Moreover, our findings suggest that, overall, "getting from" is a stronger force for the continued-sharing decision than "giving to."
|keyword = peer-to-peer networks,music sharing,IRC, voluntary contribution,sharer,free rider,public good,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Codiffusion of Wireless Voice and Data Services: An Empirical Analysis of the Japanese Mobile Telecommunications Market'''
{{header}}
{{article
|author= Marius F. Niculescu,Seungjin Whang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2012
|abstract = Wireless telecommunications have become over time a ubiquitous tool that not only sustains our increasing need for flexibility and efficiency, but also provides new ways to access and experience both utilitarian and hedonic information goods and services. This paper explores the parallel market evolution of the two main categories of wireless services-voice and data-in leading technology markets, inspecting the differences and complex interactions between the associated adoption processes. We propose a model that addresses specific individual characteristics of these two services and the stand-alone/add-on relationship between them. In particular, we acknowledge the distinction between the nonoverlapping classes of basic consumers, who only subscribe to voice plans, and sophisticated consumers, who adopt both services. We also account for the fact that, unlike voice services, data services rapidly evolved over time due to factors such as interface improvement, gradual technological advances in data transmission speed and security, and the increase in volume and diversity of the content and services ported to mobile Internet. Moreover, we consider the time gap between the market introduction of these services and allow for different corresponding consumer learning curves. We test our model on the Japanese wireless market. The empirical analysis reveals several interesting results. In addition to an expected one-way effect of voice on data adoption at the market potential level, we do find two-way codiffusion effects at the speed of adoption level. We also observe that basic consumers impact the adoption of wireless voice services in a stronger way compared to sophisticated consumers. This, in turn, leads to a decreasing average marginal network effect of voice subscribers on the adoption of wireless voice services. Furthermore, we find that the willingness of voice consumers to consider adopting data services is positively related to both time and penetration of 3G-capable handsets among voice subscribers.
|keyword = wireless telecommunication markets,mobile Internet,stand-alone and add-on services,network and imitation effects,codiffusion of contingent IT products and services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ENDS OF INFORMATION SYSTEMS RESEARCH: A PRAGMATIC FRAMEWORK'''
{{header}}
{{article
|author= Panos Constantinides,Mike W. Chiasson,Lucas D. Introna,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this paper, we argue that any effort to understand the state of the Information Systems field has to view IS research as a series of normative choices and value judgments about the ends of research. To assist a systematic questioning of the various ends of IS research, we propose a pragmatic framework that explores the choices IS researchers make around theories and methodologies, ethical methods of conduct, desirable outcomes, and the long-term impact of the research beyond a single site and topic area. We illustrate our framework by considering and questioning the explicit and implicit choices of topics, design and execution, and the representation of knowledge in experimental research-research often considered to be largely beyond value judgments and power relations. We conclude with the implications of our pragmatic framework by proposing practical questions for all IS researchers to consider in making choices about relevant topics, design and execution, and representation of findings in their research.
|keyword = IS research practice,rigor and relevance,pragmatism,ethics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SHACKLED TO THE STATUS QUO: THE INHIBITING EFFECTS OF INCUMBENT SYSTEM HABIT, SWITCHING COSTS, AND INERTIA ON NEW SYSTEM ACCEPTANCE'''
{{header}}
{{article
|author= Greta L. Polites,Elena Karahanna,
|source= MIS QUARTERLY
|year= 2012
|abstract = Given that adoption of a new system often implies Ally or partly replacing an incumbent system, resistance is often manifested as failure of a user to switch from an incumbent technology to a newly introduced one. Thus, a potential source of resistance to adopting a new system lies in the use of an incumbent system. Using the status quo bias and habit literatures as theoretical lenses, the study explains how use of an incumbent system negatively impacts new system perceptions and usage intentions. We argue that habitual use of an incumbent system, rationalization due to perceived transition costs, and psychological commitment due to perceived sunk costs all encourage development of inertia. Inertia in turn filly mediates the impact of these incumbent system constructs on constructs related to acceptance of the new system via psychological commitment based on cognitive consistency and by increasing the importance of normative pressures. Specifically, we hypothesize that inertia leads to decreased perceptions of the ease of use and relative advantage of a newly introduced system and has a negative impact on intentions to use the new system, above and beyond its impact through perceptions. Finally, we hypothesize that inertia moderates the relationship between subjective norm and intention, such that normative pressures to use a new system become more important in the presence of inertia. Empirical results largely support the hypothesized relationships showing the inhibiting effect of incumbent-system habit, transition and sunk costs, and inertia on acceptance of a new system. Our study thus extends theoretical understanding of the role of incumbent system constructs such as habit and inertia in technology acceptance, and lays the foundations for further study of the interplay between perceptions and cognition with respect to the incumbent system and those with respect to a new system.
|keyword = IS habit,technology acceptance,inhibitors,inertia,switching costs,status quo bias,subjective norm,sunk costs,transition costs,incumbent system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ARE MARKETS FOR VULNERABILITIES EFFECTIVE?'''
{{header}}
{{article
|author= Sam Ransbotham,Sabyaschi Mitra,Jon Ramsey,
|source= MIS QUARTERLY
|year= 2012
|abstract = Current reward structures in security vulnerability disclosure may be skewed toward benefiting nefarious usage of vulnerability information rather than responsible disclosure. Recently suggested market-based mechanisms offer incentives to responsible security researchers for discovering and reporting vulnerabilities. However, concerns exist that any benefits gained through increased incentives for responsible discovely may be lost through information leakage. Using perspectives drawn from the diffusion of innovations literature, we examine the effectiveness of market-based vulnerability disclosure mechanisms. Empirical examination of two years of security alert data finds that market-based disclosure restricts the diffusion of vulnerability exploitations, reduces the risk of exploitation, and decreases the volume of exploitation attempts.
|keyword = Information security,vulnerability disclosure,information technology policy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''RECOMMENDATION NETWORKS AND THE LONG TAIL OF ELECTRONIC COMMERCE'''
{{header}}
{{article
|author= Gal Oestreicher-Singer,Arun Sundararajan,
|source= MIS QUARTERLY
|year= 2012
|abstract = It has been conjectured that the peer-based recommendations associated with electronic commerce lead to a redistribution of demand from popular products or "blockbusters" to less popular or "niche" products, and that electronic markets will therefore be characterized by a "long tail" of demand and revenue. We test this conjecture using the revenue distributions of books in over 200 distinct categories on Amazon. corn and detailed daily snapshots of co-purchase recommendation networks in which the products of these categories are situated. We measure how much a product is influenced by its position in this hyperlinked network of recommendations using a variant of Google's PageRank measure of centrality. We then associate the average influence of the network on each category with the inequality in the distribution of its demand and revenue, quantifying this inequality using the Gini coefficient derived from the category's Lorenz curve. We establish that categories whose products are influenced more by the recommendation network have significantly flatter demand and revenue distributions, even after controlling for variation in average category demand, category size, and price differentials. Our empirical findings indicate that doubling the average network influence on a category is associated with an average increase of about 50 percent in the relative revenue for the least popular 20 percent of products, and with an average reduction of about 15 percent in the relative revenue for the most popular 20 percent of products. We also show that this effect is enhanced by higher assortative mixing and lower clustering in the network, and is greater in categories whose products are more evenly influenced by recommendations. The direction of these results persists over time, across both demand and revenue distributions, and across both daily and weekly demand aggregations. Our work illustrates how the microscopic economic data revealed by online networks can be used to define and answer new kinds of research questions, offers a fresh perspective on the influence of networked IT artifacts on business outcomes, and provides novel empirical evidence about the impact of visible recommendations on the long tail of electronic commerce.
|keyword = Networks,social networks,electronic commerce,recommender systems,Gini coefficient,long tail,influence,social media,Web 2.0,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF ANALYST-INDUCED MISINFORMATION ON THE REQUIREMENTS ELICITATION PROCESS'''
{{header}}
{{article
|author= Radha Appan,Glenn J. Browne,
|source= MIS QUARTERLY
|year= 2012
|abstract = Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstances users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (I) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users' experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.
|keyword = Information requirements determination,misinformation effect,user-analyst communication,user participation,elicitation techniques,systems development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HUMAN CAPITAL DEVELOPMENT FOR PROGRAMMERS USING OPEN SOURCE SOFTWARE'''
{{header}}
{{article
|author= Amit Mehra,Vijay Mookerjee,
|source= MIS QUARTERLY
|year= 2012
|abstract = A firm can upgrade relevant skills of its programmers by ensuring their participation in carefully chosen open source projects. Highly skilled programmers are more valuable for the firm but participating in open source projects reduces the time they spend doing the firm's projects. This tradeoff determines the optimal extent of programmer participation in open source for the firm. The extent of open source participation may also be influenced by the minimum compensation that must be paid to hire a programmer in the labor market. This is because providing better skills is a way of compensating the programmers by improving their future market value. Hence the firm may want to increase open source participation to keep direct wage payments in check. We develop an analytical model based on optimal control theory to characterize the employment contract that features the best mix of open source participation and wage payments. We also find that the firm benefits more from the presence of open source in a tight labor market (i.e., when programmers have good options besides the employment offered by the firm). On the other hand, programmers are compensated better in the presence of open source opportunities when they have few outside options. This benefit is more for less skilled programmers.
|keyword = Human capital,open source software,employment contracts,training,skill development incentives,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REVISITING BIAS DUE TO CONSTRUCT MISSPECIFICATION: DIFFERENT RESULTS FROM CONSIDERING COEFFICIENTS IN STANDARDIZED FORM'''
{{header}}
{{article
|author= Miguel I. Aguirre-Urreta,George M. Marakas,
|source= MIS QUARTERLY
|year= 2012
|abstract = Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.
|keyword = Construct specification,formative,reflective,simulations,standardized coefficients,unstandardized coefficients,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE CRITICAL IMPORTANCE OF CONSTRUCT MEASUREMENT SPECIFICATION: A RESPONSE TO AGUIRRE-URRETA AND MARAKAS'''
{{header}}
{{article
|author= Stacie Petter,Arun Rai,Detmar Straub,
|source= MIS QUARTERLY
|year= 2012
|abstract = Aguirre-Urreta and Marakas (A&M) suggest in their simulation "Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form," that, like Jarvis et al. (2003), MacKenzie et al. (2005), and Petter et al. (2007) before them, bias does occur when formative constructs are misspecified as reflective. But A&M argue that the level of bias in prior simulation studies has been exaggerated. They parameterize their simulation models using standardized coefficients in contrast to Jarvis et al., MacKenzie et al., and Petter et al., who parameterize their simulation models using unstandardized coefficients. Thus, across these four simulation studies, biases in parameter estimates are likely to result in misspecified measurement models (i.e., using either unstandardized or standardized coefficients); yet, the biases are greater in magnitude when unstandardized coefficients are used to parameterize the misspecified model. We believe that regardless of the extent of the bias, it is critically important for researchers to achieve correspondence between the measurement specification and the conceptual meaning of the construct so as to not alter the theoretical meaning of the construct at the operational layer of the model. Such alignment between theory and measurement will safeguard against threats to construct and statistical conclusion validity.
|keyword = Formative measurement,construct misspecification,standardized coefficients,unstandardized coefficients,simulation,construct validity,statistical conclusion validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONSUMER ACCEPTANCE AND USE OF INFORMATION TECHNOLOGY: EXTENDING THE UNIFIED THEORY OF ACCEPTANCE AND USE OF TECHNOLOGY'''
{{header}}
{{article
|author= Viswanath Venkatesh,James Y. L. Thong,Xin Xu,
|source= MIS QUARTERLY
|year= 2012
|abstract = This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences-namely, age, gender, and experience-are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTA UT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.
|keyword = Unified theory of acceptance and use of technology (UTAUT),UTAUT2,habit,hedonic motivation,price value,mobile Internet,consumer,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE CONSEQUENCES OF INFORMATION TECHNOLOGY CONTROL WEAKNESSES ON MANAGEMENT INFORMATION SYSTEMS: THE CASE OF SARBANES-OXLEY INTERNAL CONTROL REPORTS'''
{{header}}
{{article
|author= Chan Li,Gary F. Peters,Vernon J. Richardson,Marcia Weidenmier Watson,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this article, the association between the strength of information technology controls over management information systems and the subsequent forecasting ability of the information produced by those systems is investigated. The Sarbanes-Oxley Act of 2002 highlights the importance of information system controls by requiring management and auditors to report on the effectiveness of internal controls over the financial reporting component of the firm's management information systems. We hypothesize and find evidence that management forecasts are less accurate for firms with information technology material weaknesses in their financial reporting system than the forecasts for firms that do not have information technology material weaknesses. In addition, we examine three dimensions of information technology material weaknesses: data processing integrity, system access and security, and system structure and usage. We find that the association with forecast accuracy appears to be strongest for IT control weaknesses most directly related to data processing integrity. Our results support the contention that information technology controls, as apart of the management information system, affect the quality of the information produced by the system. We discuss the complementary nature of our findings to the information and systems quality literature.
|keyword = Sarbanes-Oxley,internal controls,information quality,management forecast,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGY AND FIRM PROFITABILITY: MECHANISMS AND EMPIRICAL EVIDENCE'''
{{header}}
{{article
|author= Sunil Mithas,Ali Tafti,Indranil Bardhan,Jie Mein Goh,
|source= MIS QUARTERLY
|year= 2012
|abstract = Do information technology investments improve firm profitability? Ifs, is this effect because such investments help improve sales, or is it because they help reduce overall operating expenses? How does the effect of IT on profitability compare with that of advertising and of research and development? These are important questions because investments in IT constitute a large part of firms discretionary expenditures, and managers need to understand the likely impacts and mechanisms to justify and realize value from their IT and related resource allocation processes. The empirical evidence in this paper, derived using archival data from 199810 2003 for more than 400 global firms, suggests that IT has a positive impact on profitability. Importantly, the effect of IT investments on sales and profitability is higher than that of other discretionary investments, such as advertising and R&D. A significant portion of the impact of IT on firm profitability is accounted for by IT-enabled revenue growth, but there is no evidence for the effect of IT on profitability through operating cost reduction. Taken together, these findings suggest that firms have had greater success in achieving higher profitability through IT-enabled revenue growth than through IT-enabled cost reduction. They also provide important implications for managers to make allocations among discretionary expenditures such as IT, advertising, and R&D. With regard to IT expenditures, the results imply that firms should accord higher priority to IT projects that have revenue growth potential over those that focus mainly on cost savings.
|keyword = Information technology,profitability,revenue growth,cost reduction,firm performance,discretionary expenditures,advertising,research and development,resource-based view,profitability paradox,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COCREATING IT VALUE: NEW CAPABILITIES AND METRICS FOR MULTIFIRM ENVIRONMENTS'''
{{header}}
{{article
|author= Varun Grover,Rajiv Kohli,
|source= MIS QUARTERLY
|year= 2012
|abstract = Most research on IT value has been from the vantage point of a single firm. Multifirm studies have largely been dyadic and emphasize transaction costs over cocreation of value. Contemporary environments involve IT investments being made by multiple companies in cooperative, platform-based, and relational arrangements where the objective is to cocreate value. If IT serves as a tool, an output. or is instrumental in generating this cocreated value, then it falls within the cocreation domain of this special issue. In this introductory article, we frame the discussion of cocreating IT value through four layers of relational arrangement between firms, describe the papers in the special issue with respect to this framework, and briefly describe an agenda for research in this important area.
|keyword = Information technology value,cocreation of value,multifirm environments,IT-based relationship value,IT-based platforms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERFIRM IT CAPABILITY PROFILES AND COMMUNICATIONS FOR COCREATING RELATIONAL VALUE: EVIDENCE FROM THE LOGISTICS INDUSTRY'''
{{header}}
{{article
|author= Arun Rai,Paul A. Pavlou,Ghiyoung Im,Steve Du,
|source= MIS QUARTERLY
|year= 2012
|abstract = This study seeks to identify the means by which information technology helps cocreate relational value in the context of interfirm relationships in the logistics industry-a large and information-intensive industry. We identify a set of IT functionalities-single-location shipping, multilocation shipping, supply chain visibility, and financial settlement-that can be used to manage the flows of physical goods, information, and finances across locations in interfirm logistics processes. Progressively more advanced sets of IT functionalities, when implemented and used in the interfirm relationship to execute logistics processes, are proposed to form four distinct IT capability profiles of increased sophistication. Interfirm IT capability profiles of higher sophistication are proposed to help cocreate greater relational value by facilitating the flows of physical goods, information, and finances across locations in the interfirm logistics process. Besides their direct role in helping cocreate relational value, these interfirm IT capability profiles are proposed to further enhance relational value cocreation when complemented by interfirm communications for business development and IT development. Our empirical study was situated in one of the world's largest logistics suppliers and over 2,000 of its interfirm relationships with buyers across industries. Integrated data from four archival sources on the IT functionalities implemented and used in interfirm logistics relationships, interfirm communications, relational value (share of wallet and loyalty), and multiple control variables were collected. The results show that the proposed interfirm IT capability profiles and interfirm communications have both a direct and an interaction effect on relational value. Implications for cocreating relational value in interfirm relationships with the aid of IT are discussed.
|keyword = Relational value,relational view,share of wallet,interfirm relationships,IT capability profiles,IT functionalities,contact streams,interfirm communications,complementarities,logistics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COCREATION OF VALUE IN A PLATFORM ECOSYSTEM: THE CASE OF ENTERPRISE SOFTWARE'''
{{header}}
{{article
|author= Marco Ceccagnoli,Chris Forman,Peng Huang,D. J. Wu,
|source= MIS QUARTERLY
|year= 2012
|abstract = It has been argued that platform technology owners cocreate business value with other firms in their platform ecosystems by encouraging complementary invention and exploiting indirect network effects. In this study, we examine whether participation in an ecosystem partnership improves the business performance of small independent software vendors (ISVs) in the enterprise software industry and how appropriability mechanisms influence the benefits of partnership. By analyzing the partnering activities and performance indicators of a sample of 1,210 small ISVs over the period 1996-2004, we find that joining a major platform owner's platform ecosystem is associated with an increase in sales and a greater likelihood of issuing an initial public offering (IPO). In addition, we show that these impacts are greater when ISVs have greater intellectual property rights or stronger downstream capabilities. This research highlights the value of inter-operability between software products, and stresses that value cocreation and appropriation are not mutually exclusive strategies in interfirm collaboration.
|keyword = Platform ecosystem,partnership,business value,sales,IPO,intellectual property rights,downstream capabilities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''VALUE COCREATION AND WEALTH SPILLOVER IN OPEN INNOVATION ALLIANCES'''
{{header}}
{{article
|author= Kunsoo Han,Wonseok Oh,Kun Shin Im,Ray M. Chang,Hyelim Oh,Alain Pinsonneault,
|source= MIS QUARTERLY
|year= 2012
|abstract = In this study, we investigate the economic and strategic value of open innovation alliances (OIAs), in which collaborators and competitors integrate in the pursuit of the codevelopment of technological innovations. Given that OIAs differ substantially from traditional, closed alliances in many aspects, including their strategic scope and scale, governing mechanisms, and member composition, it is important to understand and assess the potential value inherent in these new modes of collaboration. Furthermore, OIAs evolve over time as the participating members are free to enter and leave at will. Therefore, we also examine the on-going value creation and wealth spillover that result from changes in membership. Moreover, we investigate how a firm's participation in an IT-based open alliance alters the market value of its rivals operating within the same marketplace. To gain additional insight into the factors that moderate the market valuation of OIA participation, several contextual factors, including the degree of partner heterogeneity, innovation type, and degree of openness of the OIAs are used to account for variability in abnormal returns. Based on 194 observations, we found that allying firms realize significant positive abnormal returns when their entry into an OIA is made public. The results also suggest that substantial excessive returns accrue to the allying firms with the belated entry of a market leader firm. Furthermore, we discovered that a firm's entry into an OIA increases, rather than decreases, the market valuation of its rivals. Interestingly, an incumbent rival that did not participate in the alliance appears to gain greater "free-riding" benefits from the OIA, as compared to peer rivals. Innovation type and openness were significantly associated with the amount of abnormal returns accruing to allying firms, while no significance was found for partner heterogeneity. Finally, we conclude with a discussion of the implications of our findings for research and practice with respect to value cocreation in multifirm environments.
|keyword = Open innovation,open innovation alliances,value cocreation,wealth spillover,event study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLORING VALUE COCREATION IN RELATIONSHIPS BETWEEN AN ERP VENDOR AND ITS PARTNERS: A REVELATORY CASE STUDY'''
{{header}}
{{article
|author= Suprateek Sarker,Saonee Sarker,Arvin Sahaym,Niels Bjorn-Andersen,
|source= MIS QUARTERLY
|year= 2012
|abstract = Contemporary business organizations are increasingly turning their attention to jointly creating value with a variety of stakeholders. such as individual customers and other business organizations. However, a review of the literature reveals that very few studies have systematically examined value cocreation within business-to-business (B2B) contexts. Using a revelatory case study of the relationship between an ERP vendor with a global reputation and its partners, and informed by the resource-based view of the firm and related theoretical perspectives, we develop an understanding of value cocreation in B2B alliances associated with selling, extending, and implementing packaged software, specifically ERP systems. Our study reveals that there are different mechanisms underlying value cocreation within B2B alliances, and also points to several categories of contingency Actors that influence these mechanisms. In addition to providing insights about the phenomenon of cocreation itself the study contributes to the stream of packaged software literature, where the implications of value cocreation in alliances between packaged software vendors and their partners for the client organizations have not been sufficiently explored.
|keyword = Value creation,cocreation,business-to-business alliance,ERP systems,SME market,vendor-partner relationship,information technology characteristics,case study,interpretive study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Value Chain Linkages and the Spillover Effects of Strategic Information Technology Alignment: A Process-Level View'''
{{header}}
{{article
|author= Paul P. Tallon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = The alignment of information technology (IT) and business strategy is a perennial challenge for corporate executives. While earlier studies confirm the value of alignment, there is still some question as to how alignment creates value and the level at which value is created. In this research, we use a series of theoretical arguments based on the interconnected structure of the value chain to consider the extended effects of alignment at the process level. Since processes are often linked to create a complex chain of activities, the absence or presence of alignment in any process could have implications for business performance elsewhere in the value chain. Minimally aligned processes can not only disrupt performance within the focal process, but their effects may also be felt further downstream in the form of bottlenecks and a diminution in the business value of IT. Using a simplified form of the value chain and data from matched surveys of business and IT executives at 317 U. S. and EU firms, we examine how the effects of alignment on a given process spill over into processes further downstream, creating higher IT business value in those downstream processes. We also show that these spillover effects continue along the length of the value chain and do not diminish based on distance from the focal process. Our results reinforce the call for firms to improve the fit between business and IT strategy by showing how efforts to improve alignment in a given process can deliver a stream of benefits along the value chain. This research provides a fresh perspective on the value of alignment, facilitating a deeper understanding and appreciation of the link between strategic IT alignment and firm performance.
|keyword = IT business value,process bottlenecks,profile deviation,spillover effects,strategic IT alignment,value chain,value disciplines,value flows,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Path Dependence of Dynamic Information Technology Capability: An Empirical Investigation'''
{{header}}
{{article
|author= Jee-Hae Lim,Theophanis C. Stratopoulos,Tony S. Wirjanto,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Organizations seek to differentiate themselves in the marketplace by deploying information technology (IT) to develop dynamic IT capabilities and resist competitors' attempts to imitate or improve these capabilities. While this strategy has been justified on the grounds that dynamic IT capabilities are durably heterogeneous, there does not seem to be empirical evidence supporting or refuting this assumption. This study empirically validates the assumption of durable heterogeneity of dynamic organizational IT capability (ITC) due to path dependence. We capture ITC heterogeneity by introducing a framework in which firms try to achieve ITC leadership in their industry and we propose that durable ITC heterogeneity can be attributed to path dependence, and hence, it can be tested using Heckman's true state dependence of ITC leadership status. Using random and fixed effect dynamic logit models, we investigate true state dependence of ITC leadership on a sample of large U. S. firms. The results, which are robust to alternative sample, dependent, and control variable specifications, show that achieving ITC leadership is a true state-dependent process, suggesting durable heterogeneity of ITC due to path dependence. The study contributes to the dynamic capabilities literature and has important managerial implications. The proposed framework for conceptualizing durable resource heterogeneity due to path dependence is general and versatile, thus providing a foundation for future research on dynamic capabilities. The findings provide empirical evidence to confirm that ITC is durably heterogeneous and should be managed as a potential source of competitive advantage.
|keyword = dynamic organizational IT capability,dynamic random effects,fixed effects logit models,IT business value,path dependence,true state dependence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Service Quality in Software-as-a-Service: Developing the SaaS-Qual Measure and Examining Its Role in Usage Continuance'''
{{header}}
{{article
|author= Alexander Benlian,Marios Koufaris,Thomas Hess,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Despite the need to better understand how customers of software-as-a-service (SaaS) solutions perceive the quality of these software services and how these perceptions influence SaaS adoption and use, there is no extant measure that comprehensively captures service quality evaluations in SaaS. Based on previous SERVQUAL and SaaS literature, field interviews and focus groups, a card-sorting exercise, and two surveys of SaaS using companies, we develop, refine, and test SaaS-Qual, a zones-of-tolerance (ZOT)-based service quality measurement instrument specifically for SaaS solutions. Besides validating already established service quality dimensions (i.e., rapport, responsiveness, reliability, and features), we identify two new factors (i.e., security and flexibility) that are essential for the evaluation of service quality of SaaS solutions. SaaS-Qual demonstrates strong psychometric properties and shows high nomological validity within a framework that predicts the continued use of SaaS solutions by existing customers. In addition to developing a validated instrument that provides a fine-grained measurement of SaaS service quality, we also enrich existing research models on information systems continuance. Moreover, the SaaS-Qual instrument can be used as a diagnostic tool by SaaS providers and users alike to spot strengths and weaknesses in the service delivery of SaaS solutions.
|keyword = IS continuance,SaaS-Qual,service quality,SERVQUAL,software-as-a-service,zones of tolerance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Benchmarking Model for Management of Knowledge-Intensive Service Delivery Networks'''
{{header}}
{{article
|author= Su Dong,Monica S. Johar,Ram L. Kumar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Effective management of information technology (IT) and IT-enabled services is becoming increasingly important due to the growing complexity of their context. These services are often delivered by employees who work at widely dispersed locations and interact with each other to constitute knowledge-intensive service delivery networks (KISDNs). This paper contributes to the effective design and management of KISDNs by presenting a mixed-integer programming model that integrates disparate streams of research. This model facilitates analysis and managerial benchmarking of KISDN performance. It captures how the performance of such networks depends on the interaction between workflow decisions, structure of information flow networks (IFNs), and knowledge management decisions. We propose that knowledge about IFNs and worker competence can be effectively used to make workflow decisions. Our results, based on the study of different IFN archetypes, illustrate practices for effective management of KISDNs. Managers can enhance business value by recognizing existing IFNs, increasing randomness in IFNs, nurturing weak or performative ties depending on the archetype, assigning tasks based on effective worker competence, and selectively delaying assignment of tasks to workers. In addition, our results illustrate the impact of training and network density on KISDN performance.
|keyword = benchmarking,IT services,knowledge-based services,knowledge management,OR models,service delivery,service science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Value of Information Integration to Supply Chain Management: Roles of Internal and External Contingencies'''
{{header}}
{{article
|author= Christina W. Y. Wong,Kee-Hung Lai,T. C. E. Cheng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = While integrating information flows between internal organizational functions and across partner firms is widely acknowledged as a contributor to organizational competitiveness, there is little empirical research on the effects of situational factors on the success of information integration. Based on contingency theory, we address the following question: Under what circumstances does information integration contribute to better performance outcomes in supply chain management (SCM)? Our results provide a contingency perspective of information integration, which highlights that the performance outcomes of information integration are contingent on both external environmental conditions and internal operational characteristics. We find that information integration improves firms' ability to perform, particularly when they operate under favorable environmental conditions-a highly munificent and a less uncertain environment-and when they offer durable and complex products. Our findings advance contingency research on the performance outcomes of information integration for SCM. Our study provides managers with empirical insights on the effects of information integration on the cost and customer-oriented operational performance of SCM under favorable and unfavorable environmental conditions.
|keyword = business environment,information integration,IT-enabled supply chain,IT value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Individual, Contextual, and Social Factors on Perceived Behavioral Control of Information Technology: A Field Theory Approach'''
{{header}}
{{article
|author= Christophe Elie-Dit-Cosaque,Jesie Pallud,Michel Kalika,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Organizations are increasingly concerned about ensuring that workers have sufficient sense of control over the information technology (IT) that they use. However, we know little about the antecedents of the end user's perceived behavioral control (PBC) with respect to IT. Drawing on Kurt Lewin's field theory, the present study responds to this concern by formulating and testing a model whereby individual, contextual, and social forces influence PBC directly and indirectly via computer anxiety. In order to test the model, a survey was conducted in France with IT end users enrolled in professional training programs. The results show that increasing autonomy, offering appropriate managerial support, reducing work overload, and perceived innovativeness with IT can together reduce computer anxiety and increase PBC. These findings emphasize the forces that managers can manipulate in order to foster users' feelings of control with respect to IT in the workplace. Following this, the paper makes three main contributions to research. First, it increases our knowledge of the nomological net surrounding PBC by shedding light on the joint influences of internal, external, and social forces on this variable. Second, it reveals the role of computer anxiety, emphasizing that it is an important conduit through which these forces influence workers' PBC. Third, the paper shows how Lewin's field theory can help to create richer and less fragmented models in order to capture more fully the determinants of IT adoption and adaptation. The practical implications regarding the actions that managers can take in order to increase workers' PBC are discussed.
|keyword = autonomy,computer anxiety,control over IT,demand-control model,field theory,managerial support,perceived behavioral control,personal innovativeness with IT,work environment,work overload,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Seller Strategies for Differentiation in Highly Competitive Online Auction Markets'''
{{header}}
{{article
|author= Jese Bockstedt,Kim Huat Goh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We explore the issue of seller differentiation in competitive auction environments, where most sellers have a high percentage of positive feedback. Analyzing a set of eBay auction listings for identical products, we find evidence that the use of visibility-enhancing and quality-signaling discretionary auction attributes affects auction outcomes throughout the auction process (i.e., listing views, bids, and price premiums). We also find strong evidence that the number of reputable sellers in an auction marketplace moderates the effects of these discretionary attributes on auction outcomes. Specifically, as auction environments become more competitive, these attributes become more effective tools for differentiation, whereas seller feedback scores become less effective. Furthermore, sellers appear to select their strategies for employing these discretionary attributes based on both their prior experience and the number of experienced reputable sellers in the market. These findings suggest that in addition to relying on feedback scores, online sellers must take a more strategic approach in the selection of discretionary attributes in their auction listings.
|keyword = differentiation,e-auctions,online auctions,seller differentiation,signaling theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Usability Design and Psychological Ownership of a Virtual World'''
{{header}}
{{article
|author= Younghwa Lee,Andrew N. K. Chen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Virtual worlds, immersive three-dimensional virtual spaces where users interact with projected identities of other users (avatars) and objects, are becoming increasingly popular and continue to grow as highly interactive, collaborative, and commercial cyberspaces. However, extant research in this context has not paid much attention to usability design of a virtual world and corresponding effects on users' psychological desire to own and control the space and objects within it and subsequent behavior intention. In this study, we apply concepts of Web site usability and psychological ownership to develop a model that illustrates the relationships between seven usability factors (legibility, firmness, coherence, variety, mystery, classic, and expressive visual aesthetics), four antecedents of psychological ownership (cognitive appraisals, perceived control, affective appraisals, and self-investment), psychological ownership, and use intention. A cross-sectional study with 239 Second Life users was conducted. The results demonstrate that designing a usable virtual world that induces strong psychological ownership is crucial to attract users to spend more time, participate in more activities, and revisit the virtual world. This is an important finding for forward-looking e-business managers looking to invest their limited resources in designing a usable virtual world. In addition, by using our model and corresponding survey items, designers can benchmark and evaluate the usability of their current virtual worlds, compare the results to the designs of competitors, and upgrade the offerings of virtual worlds, as needed, by allocating available resources to the most influential design factors to suit their specific needs.
|keyword = architectural quality model,human-computer interaction,landscape preference model,psychological ownership,usability,virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Factors Affecting Bloggers' Knowledge Sharing: An Investigation Across Gender'''
{{header}}
{{article
|author= Sangmi Chai,Sanjukta Das,H. Raghav Rao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Blogs have emerged as an innovative tool for sharing information and knowledge, and they command significant interest from information technology (IT) users as well as providers. Our study establishes a research framework to provide an understanding of the factors affecting knowledge sharing among bloggers in online social networks. The research results indicate that bloggers' trust, strength of social ties, and reciprocity all have a positive effect on their knowledge-sharing behavior. Further, the impact of each factor on such behavior varies by gender. Our results provide evidence that offline expected social norms tend to persist in the online blogosphere and that gender differences need to be considered as a significant factor in understanding the IT usage behavior in the context of social capital theory. For IT managers and blog service providers, our results also highlight the importance of being gender aware in an effort to elicit participation from all constituent members for the successful adoption and usage of blogs as a knowledge-sharing mechanism.
|keyword = bloggers,blogs,gender,information privacy,knowledge sharing,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Same Coin, Different Sides: Differential Impact of Social Learning on Two Facets of Music Piracy'''
{{header}}
{{article
|author= Jingguo Wang,Zhiyong Yang,Sudip Bhattacharjee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We demonstrate that two intertwined activities of music piracy, unauthorized obtaining and unauthorized sharing, are differentially influenced by the same social learning environment. We develop a structural model and test it using survey data from a prime demographic set of respondents who engage in music piracy. Considering behavioral heterogeneity, we employ a factor mixture modeling technique to classify respondents into different groups that highlight distinct patterns of social learning influences. We find that the differential effects of social learning factors on obtaining and sharing persist across these groups. We further utilize demographic variables to profile members in each group for segmentation insights. From a theoretical perspective, our findings advance the understanding of music piracy and suggest the importance of separating obtaining from sharing activities when studying piracy. From a managerial perspective, our research provides new avenues for managers and policymakers to design targeted incentives to curtail music piracy.
|keyword = intellectual property infringement,latent class analysis,music piracy,partial least squares regression,social learning theory,unauthorized obtaining,unauthorized sharing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NeuroIS: The Potential of Cognitive Neuroscience for Information Systems Research'''
{{header}}
{{article
|author= Angelika Dimoka,Paul A. Pavlou,Fred D. Davis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This paper introduces the idea of drawing upon the cognitive neuroscience literature to inform IS research (herein termed "NeuroIS"). Recent advances in cognitive neuroscience are uncovering the neural bases of cognitive, emotional, and social processes, and they offer new insights into the complex interplay between IT and information processing, decision making, and behavior among people, organizations, and markets. The paper reviews the emerging cognitive neuroscience literature to propose a set of seven opportunities that IS researchers can use to inform IS phenomena, namely (1) localizing the neural correlates of IS constructs, (2) capturing hidden mental processes, (3) complementing existing sources of IS data with brain data, (4) identifying antecedents of IS constructs, (5) testing consequences of IS constructs, (6) inferring the temporal ordering among IS constructs, and (7) challenging assumptions and enhancing IS theories. The paper proposes a framework for exploring the potential of cognitive neuroscience for IS research and offers examples of potentially fertile intersections of cognitive neuroscience and IS research in the domains of design science and human-computer interaction. This is followed by an example NeuroIS study in the context of e-commerce adoption using fMRI, which spawns interesting new insights. The challenges of using functional neuroimaging tools are also discussed. The paper concludes that there is considerable potential for using cognitive neuroscience theories and functional brain imaging tools in IS research to enhance IS theories.
|keyword = cognitive neuroscience,functional brain imaging,NeuroIS,neuroeconomics,neuromarketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cross-Business Information Technology Integration and Acquirer Value Creation in Corporate Mergers and Acquisitions'''
{{header}}
{{article
|author= Hueseyin Tanriverdi,Vahap Buelent Uysal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This study develops and tests the idea that the cross-business information technology integration (CBITI) capability of an acquirer creates significant value for shareholders of the acquirer in mergers and acquisitions (M&A). In M&A, integrating the IT systems and IT management processes of acquirer and target could generate benefits such as (a) the consolidation of IT resources and the reduction of overall IT costs of the combined firm, (b) the development of an IT-based coordination mechanism and the realization of cross-firm business synergies, (c) the minimization of potential disruptions to business operations, and (d) greater ability to comply with relevant laws and regulations and the reduction of regulatory compliance costs. We test these ideas in a sample of 141 acquisitions conducted by 86 Fortune 1000 firms. In the short run, acquirers that have high levels of CBITI capabilities receive positive and significant cumulative abnormal returns to their M&A announcements. Announcement period returns indicate that the capital markets value CBITI similarly in same-industry and different-industry acquisitions. In the long run, acquirers with high levels of CBITI capabilities obtain significantly higher abnormal operating performance. They create significantly greater value in complementary acquisitions from different industries than in related acquisitions from the same industry. The findings have important implications for M&A research and practice.
|keyword = corporate mergers and acquisitions,cross-business IT integration,short-run abnormal stock returns,long-run abnormal operating performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pricing Digital Goods: Discontinuous Costs and Shared Infrastructure'''
{{header}}
{{article
|author= Ke-Wei Huang,Arun Sundararajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = In this paper, we analyze a model of usage pricing for digital products with discontinuous supply functions. This model characterizes a number of information technology-based products and services for which variable increases in demand are fulfilled by the addition of blocks of computing or network infrastructure. Such goods are often modeled as information goods with zero variable costs; in fact, the actual cost structure resembles a mixture of zero marginal costs and positive periodic fixed costs. This paper discusses the properties of a general solution for the optimal nonlinear pricing of such digital goods. We show that the discontinuous cost structure can be accrued as a virtual constant variable cost. This paper applies the general solution to solve two related extensions by first investigating the optimal technology capacity planning when the cost function is both discontinuous and declining over time, and then characterizing the optimal costing for the discontinuous supply when it is shared by several business profit centers. Our findings suggest that the widely adopted full cost recovery policies are typically suboptimal.
|keyword = pricing digital goods,nonlinear pricing,infrastructure cost,IT chargeback,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Usercentric Operational Decision Making in Distributed Information Retrieval'''
{{header}}
{{article
|author= Kartik Hosanagar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Information specialists in enterprises regularly use distributed information retrieval (DIR) systems that query a large number of information retrieval (IR) systems, merge the retrieved results, and display them to users. There can be considerable heterogeneity in the quality of results returned by different IR servers. Further, because different servers handle collections of different sizes and have different processing and bandwidth capacities, there can be considerable heterogeneity in their response times. The broker in the DIR system has to decide which servers to query, how long to wait for responses, and which retrieved results to display based on the benefits and costs imposed on users. The benefit of querying more servers and waiting longer is the ability to retrieve more documents. The costs may be in the form of access fees charged by IR servers or user's cost associated with waiting for the servers to respond. We formulate the broker's decision problem as a stochastic mixed-integer program and present analytical solutions for the problem. Using data gathered from FedStats-a system that queries IR engines of several U.S. federal agencies-we demonstrate that the technique can significantly increase the utility from DIR systems. Finally, simulations suggest that the technique can be applied to solve the broker's decision problem under more complex decision environments.
|keyword = distributed information retrieval (IR),personalization,utility theory,optimal operational decisions,source selection,query termination,stochastic modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Risk Management and Optimal Pricing in Online Storage Grids'''
{{header}}
{{article
|author= Sanjukta Das,Anna Ye Du,Ram Gopal,R. Ramesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Online storage service providers grant a way for companies to avoid spending resources on maintaining their own in-house storage infrastructure and thereby allowing them to focus on their core business activities. These providers, however, follow a fixed, posted pricing strategy that charges the same price in each time period and thus bear all the risk arising out of demand uncertainties faced by their client companies. We examine the effects of providing a spot market with dynamic prices and forward contracts to hedge against future revenue uncertainty. We derive revenue-maximizing spot and forward prices for a single seller facing a known set of buyers. We perform a simulation study using publicly available traffic data regarding Amazon S3 clients from Alexa.com to validate our analytical results. Our field study supports our analysis and indicates that spot markets alone can enhance revenues to Amazon, but this comes at the cost of increased risks due to the increased market share in the spot markets. Furthermore, adding a forward contract feature to the spot markets can reduce risks while still providing the benefits of enhanced revenues. Although the buyers incur an increase in costs in the spot market, adding a forward contract does not cause any additional cost increase while transferring the risk to the buyers. Thus, storage grid providers can greatly benefit by applying a forward contract alongside the spot market.
|keyword = online storage,grid computing,forward contracts,market mechanism design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Protecting Privacy Against Record Linkage Disclosure: A Bounded Swapping Approach for Numeric Data'''
{{header}}
{{article
|author= Xiao-Bai Li,Sumit Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Record linkage techniques have been widely used in areas such as antiterrorism, crime analysis, epidemiologic research, and database marketing. On the other hand, such techniques are also being increasingly used for identity matching that leads to the disclosure of private information. These techniques can be used to effectively reidentify records even in deidentified data. Consequently, the use of such techniques can lead to individual privacy being severely eroded. Our study addresses this important issue and provides a solution to resolve the conflict between privacy protection and data utility. We propose a data-masking method for protecting private information against record linkage disclosure that preserves the statistical properties of the data for legitimate analysis. Our method recursively partitions a data set into smaller subsets such that data records within each subset are more homogeneous after each partition. The partition is made orthogonal to the maximum variance dimension represented by the first principal component in each partitioned set. The attribute values of a record in a subset are then masked using a double-bounded swapping method. The proposed method, which we call multivariate swapping trees, is nonparametric in nature and does not require any assumptions about statistical distributions of the original data. Experiments conducted on real-world data sets demonstrate that the proposed approach significantly outperforms existing methods in terms of both preventing identity disclosure and preserving data quality.
|keyword = privacy,record linkage,data partitioning,data swapping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Hidden Markov Model of Developer Learning Dynamics in Open Source Software Projects'''
{{header}}
{{article
|author= Param Vir Singh,Yong Tan,Nara Youn,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This study develops a stochastic model to capture developer learning dynamics in open source software projects (OSS). A hidden Markov model (HMM) is proposed that allows us to investigate (1) the extent to which individuals learn from their own experience and from interactions with peers, (2) whether an individual's ability to learn from these activities varies as she evolves/learns over time, and (3) to what extent individual learning persists over time. We calibrate the model based on six years of detailed data collected from 251 developers working on 25 OSS projects hosted at Sourceforge. Using the HMM, three latent learning states (high, medium, and low) are identified, and the marginal impact of learning activities on moving the developer between these states is estimated. Our findings reveal different patterns of learning in different learning states. Learning from peers appears to be the most important source of learning for developers across the three states. Developers in the medium learning state benefit the most through discussions that they initiate. On the other hand, developers in the low and the high states benefit the most by participating in discussions started by others. While in the low state, developers depend entirely upon their peers to learn, whereas in the medium or high state, they can also draw upon their own experiences. Explanations for these varying impacts of learning activities on the transitions of developers between the three learning states are provided. The HMM is shown to outperform the classical learning curve model. The HMM modeling of this study contributes to the development of a theoretically grounded understanding of learning behavior of individuals. Such a theory and associated findings have important managerial and operational implications for devising interventions to promote learning in a variety of settings.
|keyword = hidden Markov model,learning curve,productivity,learning by doing,learning from peers,open source software,dynamic models,structural models,regime switching models,behavior dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Identifying and Testing the Inhibitors of Technology Usage Intentions'''
{{header}}
{{article
|author= Ronald T. Cenfetelli,Andrew Schwarz,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = An important area of information systems (IS) research has been the identification of the individual-level beliefs that enable technology acceptance such as the usefulness, reliability, and flexibility of a system. This study posits the existence of additional beliefs that inhibit usage intentions and thus foster technology rejection rather than acceptance. We theorize that these inhibitors are more than just the antipoles of enablers (e.g., the opposite of usefulness or reliability) and so are distinct constructs worthy of their own investigation. Inhibitors are proposed to have effects on usage intentions beyond that of enablers as well as effects on enablers themselves. We report on a series of empirical studies designed to test the existence and effects of inhibitors. A candidate set of six inhibitors is shown to be distinct from enablers. These inhibitors are subsequently tested in a field study of 387 individuals nested within 32 different websites. Effects at both individual and website unit levels of analysis are tested using multilevel modeling. We find that inhibitors have negative effects on usage intentions, as well as on enablers, and these effects vary contingent upon individual or website unit levels of analysis. The overall results support the existence and importance of inhibitors in explaining individual intent to use-or not use-technology.
|keyword = usage intentions,inhibitors,nonacceptance,technology rejection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Returns to Information Technology Outsourcing'''
{{header}}
{{article
|author= Kunsoo Han,Robert J. Kauffman,Barrie R. Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This study extends existing information technology (IT) productivity research by evaluating the contributions of spending in IT outsourcing using a production function framework and an economywide panel data set from 60 industries in the United States over the period from 1998 to 2006. Our results demonstrate that IT outsourcing has made a positive and economically meaningful contribution to industry output and labor productivity. It has not only helped industries produce more output, but it has also made their labor more productive. Moreover, our analysis of split data samples reveals systematic differences between high and low IT intensity industries in terms of the degree and impact of IT outsourcing. Our results indicate that high IT intensity industries use more IT outsourcing as a percentage of their output, but less as a percentage of their own IT capital, and they achieve higher returns from IT outsourcing. This finding suggests that to gain greater value from IT outsourcing, firms need to develop IT capabilities by intensively investing in IT themselves. By comparing the results from subperiods and analyzing a separate data set for the earlier period of 1987-1999, we conclude that the value of IT outsourcing has been stable from 1998 to 2006 and consistent over the past two decades. The high returns we find for IT outsourcing also suggest that firms may be underinvesting in IT outsourcing.
|keyword = economic analysis,industry analysis,information technology,IT impacts,IT intensity,IT outsourcing,output elasticity,production function,production theory,productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Community Commitment on Participation in Online Communities'''
{{header}}
{{article
|author= Patrick J. Bateman,Peter H. Gray,Brian S. Butler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Online discussion communities have become a widely used medium for interaction, enabling conversations across a broad range of topics and contexts. Their success, however, depends on participants' willingness to invest their time and attention in the absence of formal role and control structures. Why, then, would individuals choose to return repeatedly to a particular community and engage in the various behaviors that are necessary to keep conversation within the community going? Some studies of online communities argue that individuals are driven by self-interest, while others emphasize more altruistic motivations. To get beyond these inconsistent explanations, we offer a model that brings dissimilar rationales into a single conceptual framework and shows the validity of each rationale in explaining different online behaviors. Drawing on typologies of organizational commitment, we argue that members may have psychological bonds to a particular online community based on (a) need, (b) affect, and/or (c) obligation. We develop hypotheses that explain how each form of commitment to a community affects the likelihood that a member will engage in particular behaviors (reading threads, posting replies, moderating the discussion). Our results indicate that each form of community commitment has a unique impact on each behavior, with need-based commitment predicting thread reading, affect-based commitment predicting reply posting and moderating behaviors, and obligation-based commitment predicting only moderating behavior. Researchers seeking to understand how discussion-based communities function will benefit from this more precise theorizing of how each form of member commitment relates to different kinds of online behaviors. Community managers who seek to encourage particular behaviors may use our results to target the underlying form of commitment most likely to encourage the activities they wish to promote.
|keyword = online communities,virtual communities,discussion groups,commitment,online behavior,Web 2.0,social media,social technologies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge Exploration and Exploitation: The Impacts of Psychological Climate and Knowledge Management System Access'''
{{header}}
{{article
|author= Alexandra Durcikova,Kelly J. Fadel,Brian S. Butler,Dennis F. Galletta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Firms need to balance efficiency gains obtained through exploiting existing knowledge assets with long-term competitive viability achieved through exploring new knowledge resources. Because the use of knowledge management systems (KMSs) continues to expand, understanding how these systems affect exploration and exploitation practices at the individual level is important to advance both knowledge management theory and practice. This study reports the results of a multi-industry survey investigating how psychological climate and KMS access influence solution reuse (exploitation) and solution innovation (exploration) in the context of technical support work. Our results show that KMS access does not directly determine solution innovation or solution reuse. Instead, KMS access strengthens the positive relationship between a climate for innovation and solution innovation and reverses the positive relationship between a climate for autonomy and solution innovation. The implications for knowledge management research and practice are discussed.
|keyword = knowledge management systems,exploration,exploitation,psychological climate,technical support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Trade-off Difficulty Caused by Preference Elicitation Methods on User Acceptance of Recommendation Agents Across Loss and Gain Conditions'''
{{header}}
{{article
|author= Young Eun Lee,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Prior studies on product recommendation agents (RAs) have been based on the effort-accuracy perspective in which the amount of effort required to make a decision and the accuracy of such decisions are two dominant antecedents of user acceptance of RAs. The current study extends the effort-accuracy perspective by considering trade-off difficulty, a type of negative emotion that arises when attainment of one's goals is blocked by the attainment of other goals; consequently, one must make trade-offs among the conflicting goals. Many product purchase choices for which RAs are used require users to make trade-offs among conflicting product attributes. A key feature of RAs, the preference elicitation method (PEM), often compels users to make explicit trade-offs. We examine whether an RA's PEM generates trade-off difficulty, which, in turn, affects users' evaluations (i.e., perceived amount of effort and perceived accuracy of recommendations) and the resultant acceptance of the RA. Trade-off difficulty influences users' evaluations of an RA via perceived control over execution of the RA PEM. In addition, the decision context in which users employ a PEM moderates the degree to which that PEM generates trade-off difficulty. Specifically, a PEM generates a greater degree of trade-off difficulty in a choice context that leads to a loss than in a choice context that leads to a gain. Consequently, users exert more effort to cope with trade-off difficulty in a loss condition. Because users voluntarily spend more effort, the negative influence of perceived effort on users' acceptance of an RA-which is supported in prior studies-decreases in a loss condition. A laboratory experiment was conducted using two between-subject factors: two RAs, one that employed a trade-off-compelling PEM and the other a trade-off-hiding PEM, and two decision contexts, one of which was a loss condition and the other a gain condition. The results supported all of the hypotheses.
|keyword = product recommendation agent,effort-accuracy framework,decision context,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NETWORK EFFECTS: THE INFLUENCE OF STRUCTURAL CAPITAL ON OPEN SOURCE PROJECT SUCCESS'''
{{header}}
{{article
|author= Param Vir Singh,Yong Tan,Vijay Mookerjee,
|source= MIS QUARTERLY
|year= 2011
|abstract = What determines the success of open source projects? In this study, we investigate the impact of network social capital on open source project success. We define network social capital as the benefits open source developers secure from their membership in developer collaboration networks. We focus on one specific type of success as measured by the rate of knowledge creation in an open source project. Specific hypotheses are developed and tested using a longitudinal panel of 2,378 projects hosted at Source Forge. We find that network social capital is not equally accessible to or appropriated by all projects. Our main results are as follows. First, projects with greater internal cohesion (that is, cohesion among the project members) are more successful. Second, external cohesion (that is, cohesion among the external contacts of a project) has an inverse U-shaped relationship with the project's success; moderate levels of external cohesion are best for a project's success rather than very low or very high levels. Third, the technological diversity of the external network of a project also has the greatest benefit when it is neither too low nor too high. Fourth, the number of direct and indirect external contacts positively affects a project's success such that the effect of the number of direct contacts is moderated by the number of indirect contacts. These results are robust to several control variables and alternate model specifications. Several theoretical and managerial implications are provided.
|keyword = Social networks,open source software development,cohesion,project success,team composition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TECHNOSTRESS: TECHNOLOGICAL ANTECEDENTS AND IMPLICATIONS'''
{{header}}
{{article
|author= Ramakrishna Ayyagari,Varun Grover,Russell Purvis,
|source= MIS QUARTERLY
|year= 2011
|abstract = With the proliferation and ubiquity of information and communication technologies (ICTs), it is becoming imperative for individuals to constantly engage with these technologies in order to get work accomplished. Academic literature, popular press, and anecdotal evidence suggest that ICTs are responsible for increased stress levels in individuals (known as technostress). However, despite the influence of stress on health costs and productivity, it is not very clear which characteristics of ICTs create stress. We draw from IS and stress research to build and test a model of technostress. The person-environment fit model is used as a theoretical lens. The research model proposes that certain technology characteristics-like usability (usefulness, complexity, and reliability), intrusiveness (presenteeism, anonymity), and dynamism (pace of change)-are related to stressors (work overload, role ambiguity, invasion of privacy, work home conflict, and job insecurity). Field data from 661 working professionals was obtained and analyzed. The results clearly suggest the prevalence of technostress and the hypotheses from the model are generally supported. Work overload and role ambiguity are found to be the two most dominant stressors, whereas intrusive technology characteristics are found to be the dominant predictors of stressors. The results open up new avenues for research by highlighting the incidence of technostress in organizations and possible interventions to alleviate it.
|keyword = Technostress,ICTs,information and communication technologies,stress,technology characteristics,strain,stressors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE VALUE OF IT-ENABLED RETAILER LEARNING: PERSONALIZED PRODUCT RECOMMENDATIONS AND CUSTOMER STORE LOYALTY IN ELECTRONIC MARKETS'''
{{header}}
{{article
|author= Tongxiao (Catherine) Zhang,Ritu Agarwal,Jr. Henry C. Lucas,
|source= MIS QUARTERLY
|year= 2011
|abstract = Recent research has acknowledged the key role of information technology in helping build stronger and more enduring customer relationships. Personalized product recommendations (PPRs) adapted to individual customers' preferences and tastes are one IT-enabled strategy that has been widely adopted by online retailers to enhance customers' shopping experience. Although many online retailers have implemented PPRs on their electronic storefronts to improve customer retention, empirical evidence for the effects of PPRs on retention is sparse, and the limited anecdotal evidence is contradictory. We draw upon the household production function model in the consumer economics literature to develop a theoretical framework that explains the mechanisms through which PPRs influence customer store loyalty in electronic markets. We suggest that retailer learning that occurs as a result of customer knowledge obtained to enable personalization influences the efficiency of the online product brokering activity. Data collected from a two-phase lab experiment with 253 student subjects where the quality of PPRs was manipulated are used to empirically test the predictions of the theoretical model. Empirical analyses of the data indicate that retailer learning reflected in higher quality PPRs is associated with lower product screening cost, but higher product evaluation cost. We further find that higher quality PPRs are associated with greater value derived by consumers from the online product brokering activity in terms of higher decision making quality, which is positively associated with repurchase intention. The paper presents the implications, limitations, and contributions of this study along with areas for future research.
|keyword = Personalized product recommendations,recommender systems,household production function,retailer learning,laboratory experiment,online product brokering,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GUIDELINES FOR DESIGNING VISUAL ONTOLOGIES TO SUPPORT KNOWLEDGE IDENTIFICATION'''
{{header}}
{{article
|author= Palash Bera,Andrew Burton-Jones,Yair Wand,
|source= MIS QUARTERLY
|year= 2011
|abstract = Organizations often provide workers with knowledge management systems to help them obtain knowledge they need. A significant constraint on the effectiveness of such systems is that they assume workers know what knowledge they need (they know what they don't know) when, in fact, they often do not know what knowledge they need (they don't know what they don't know). A way to overcome this problem is to use visual ontologies to help users learn relevant concepts and relationships in the knowledge domain, enabling them to search the knowledge base in a more educated manner. However, no guidelines exist for designing such ontologies. To fill this gap, we draw on theories of philosophical ontology and cognition to propose guidelines for designing visual ontologies for knowledge identification. We conducted three experiments to compare the effectiveness of guided ontologies, visual ontologies that followed our guidelines, to unguided ontologies, visual ontologies that violated our guidelines. We found that subjects performed considerably better with the guided ontologies, and that subjects could perceive the benefits of using guided ontologies, at least in some circumstances. On the basis of these results, we conclude that the way visual ontologies are presented makes a difference in knowledge identification and that theories of philosophical ontology and cognition can guide the construction of more effective visual representations. Furthermore, we propose that the principles we used to create the guided visual ontologies can be generalized for other cases where visual models are used to inform users about application domains.
|keyword = Knowledge work,knowledge identification,visual ontologies,knowledge management system,ontology,cognition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MULTILEVEL MODEL FOR MEASURING FIT BETWEEN A FIRM'S COMPETITIVE STRATEGIES AND INFORMATION SYSTEMS CAPABILITIES'''
{{header}}
{{article
|author= Tim S. McLaren,Milena M. Head,Yufei Yuan,Yolande E. Chan,
|source= MIS QUARTERLY
|year= 2011
|abstract = To compete in a highly dynamic marketplace, firms must frequently adapt and align their competitive strategies and information systems. The dominant literature on the strategic fit of a firm's information systems focuses primarily on high-level measures of the strategic fit of a firm's overall IS portfolio and the impact of fit on business performance. This paper addresses the need for a more fine-grained approach for assessing the specific areas of misfit between a firm's competitive strategies and IS capabilities. We describe the design and evaluation of a multilevel strategic fit (MSF) measurement model that enables researchers and practitioners to measure the strategic fit of a firm's information systems at both an overall and a detailed level. The steps in the model include identifying the relevant IS capabilities according to the type of system; measuring the current level of support for each capability using a capabilities instrument; identifying the ideal level of support for each capability using an adaptation of Conant et al.'s (1990) instrument to assess strategic archetype; and comparing the ideal and realized level of support for each capability. Evidence from a multiple case study analysis indicates that the fine-grained assessment of strategic fit can strengthen the validity, utility, and ease of corroboration of the strategic fit measurement outputs. The paper also demonstrates how an iterative design science research approach, with its emphasis on evaluating the utility of prototype artifacts, is well suited to developing field-tested and theoretically grounded measurement models and instruments that are accessible to practitioners. This focus on practical utility in turn provides researchers with results that can be more readily corroborated, thus improving the quality and usefulness of the research findings.
|keyword = Strategic alignment,information systems capabilities,configurational theory,strategic archetypes,design science,research methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''UNDERSTANDING THE LINK BETWEEN INFORMATION TECHNOLOGY CAPABILITY AND ORGANIZATIONAL AGILITY: AN EMPIRICAL EXAMINATION'''
{{header}}
{{article
|author= Ying Lu,K. (Ram) Ramamurthy,
|source= MIS QUARTERLY
|year= 2011
|abstract = Information technology is generally considered an enabler of a firm's agility. A typical premise is that greater IT investment enables a firm to be more agile. However, it is not uncommon that IT can also hinder and sometimes even impede organizational agility. We propose and theorize this frequently observed but understudied IT-agility contradiction by which IT may enable or impede agility. We develop the premise that organizations need to develop superior firm-wide IT capability to successfully manage their IT resources to realize agility. We refine the conceptualization and measurement of IT capability as a latent construct reflected in its three dimensions: IT infrastructure capability, IT business spanning capability, and IT proactive stance. We also conceptualize two types of organizational agility: market capitalizing agility and operational adjustment agility. We then conduct a matched-pair field survey of business and information systems executives in 128 organizations to empirically examine the link between a firm's IT capability and agility. Business executives responded to measurement scales of the two types of agility and organizational context variables, and IS executives responded to measurement scales of IT capabilities and IS context variables. The results show a significant positive relationship between IT capability and the two types of organizational agility. We also find a significant positive joint effect of IT capability and IT spending on operational adjustment agility but not on market capitalizing agility. The findings suggest a possible resolution to the contradictory effect of IT on agility: while more IT spending does not lead to greater agility, spending it in such a way as to enhance and foster IT capabilities does. Our study provides initial empirical evidence to better understand essential IT capabilities and their relationship with organizational agility. Our findings provide a number of useful implications for research and managerial practices.
|keyword = Organizational agility,IT-agility contradiction,information technology capability,second-order latent multidimensional construct,IT spending,theory development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FREEDOM OF CHOICE, EASE OF USE, AND THE FORMATION OF INTERFACE PREFERENCES'''
{{header}}
{{article
|author= Kyle B. Murray,Gerald Haeubl,
|source= MIS QUARTERLY
|year= 2011
|abstract = How does users' freedom of choice, or the lack thereof affect interface preferences? The research reported in this article approaches this question from two theoretical perspectives. The first of these argues that an interface with a dominant market share benefits from the absence of competition because users acquire skills that are specific to that particular interface, which in turn reduces the probability that they will switch to a new competitor interface in the future. By contrast, the second perspective proposes that the advantage that a market leader has in being able to install a set of non-transferable skills in its user base is offset by a psychological force that causes humans to react against perceived constraints on their freedom of choice. We test a research model that incorporates the key predictions of these two theoretical perspectives in an experiment involving consequential interface choices. We find strong support for the second perspective, which builds upon the theory of psychological reactance.
|keyword = Interface preferences,ease of use,usability,user skills,consumer choice,psychological reactance,human capital,user based learning,psychological theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTEGRATING TECHNOLOGY ADDICTION AND USE: AN EMPIRICAL INVESTIGATION OF ONLINE AUCTION USERS'''
{{header}}
{{article
|author= Ofir Turel,Alexander Serenko,Paul Giles,
|source= MIS QUARTERLY
|year= 2011
|abstract = Technology addiction is a relatively new mental condition that has not yet been well integrated into mainstream MIS models. This study bridges this gap and incorporates technology addiction into technology use processes in the context of online auctions. It examines how user cognition and ultimately usage intentions toward an information technology are distorted by addiction to the technology. The findings from two empirical studies of 132 and 223 eBay users, using three different operationalizations of addiction, indicate that the level of online auction addiction distorts the way the IT artifact is perceived. Informing a range of cognition-modification processes, addiction to online auctions augments user perceptions of enjoyment, usefulness, and ease of use attributed to the technology, which in turn influence usage intentions. Overall, consistent with behavioral addiction models, the findings indicate that users' levels of online auction addiction influence their reasoned IT usage decisions by altering users' belief systems. The formation of maladaptive perceptions is driven by a combination of memory-, learning-, and bias-based cognition modification processes. Implications of the,findings are discussed.
|keyword = Technology addiction,addiction,online auction,IT continuance,enjoyment,user behavior,obsessive-compulsive behavior,intrinsic and extrinsic motivation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CENTRALITY-IS PROFICIENCY ALIGNMENT AND WORKGROUP PERFORMANCE'''
{{header}}
{{article
|author= Gerald C. Kane,Stephen P. Borgatti,
|source= MIS QUARTERLY
|year= 2011
|abstract = Virtually all of the extensive previous research investigating the effect of information systems proficiency on performance has been conducted at the individual level. Little research has investigated the relationship between IS proficiency and performance at the group level. In this paper, we argue that IS proficiency at the group level may be more than the simple sum or average of the IS proficiency of individual group members. Rather, effective group-level IS proficiency may also be afiinction of how a group's IS proficiency is distributed across its members. Relying on concepts associated with social network analysis (SNA), we introduce the concept of centrality-IS proficiency alignment. We argue that groups will perform better if their more proficient members are highly central in the group's communication and workflows network. Data from 468 employees in 32 workgroups show that centrality-IS proficiency alignment is significantly and positively related to performance across multiple systems examined individually and with the portfolio of systems examined as a whole. This approach effectively integrates the structural and resource perspectives of SNA, providing a roadmap so that others may follow a similar approach to address broader questions of group-level user system interactions in the IS literature and more general questions of central resource alignment in the broader organizational literature.
|keyword = Social network analysis,IS proficiency,centrality,multimodal networks,healthcare delivery,performance,collective use,multilevel analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''VIRTUAL SPACE AND PLACE: THEORY AND TEST'''
{{header}}
{{article
|author= Carol Saunders,Anne F. Rutkowski,Michiel van Genuchten,Doug Vogel,Julio Molina Orrego,
|source= MIS QUARTERLY
|year= 2011
|abstract = Little is known about how individuals come to relate to settings in virtual worlds (VWs), which are defined as digital environments in which individuals, groups, and even organizations interact in virtual (that is to say, nonphysical) spaces. This research develops a theory of virtual space and place (VSP), specifically relating this to the setting of Second Life (SL), a prominent social virtual world. We explore how three-dimensional space, as perceived by users, is able to provide them with an interactive experience with virtual objects, as well as with other VW denizens. To test our theory, we build interactive work tools in SL that are designed to reflect various degrees of motion range and to influence presence. The three information technology tools are evaluated by 150 business professionals who are either familiar or unfamiliar with SL. Implications for practice and directions for future research are discussed.
|keyword = Virtual worlds,Second Life,virtual space,place,cognition,perception,familiarity,presence,social presence,focused immersion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring Information Diffusion in an Online Community'''
{{header}}
{{article
|author= Rajiv Garg,Michael D. Smith,Rahul Telang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Measuring peer influence in social networks is an important business and policy question that has become increasingly salient with the development of globally interconnected information and communication technology networks. However, in spite of the new data sources available today, researchers still face many of the same measurement challenges that have been present in the literature for over four decades: homophily, reflection and selection problems, identifying the source of influence, and determining preexisting knowledge. The goal of this paper is to develop an empirical approach for measuring information diffusion and discovery in online social networks that have these measurement challenges. We develop such an approach and apply it to data collected from 4,000 users of an online music community. We show that peers on such networks significantly increase music discovery. Moreover, we demonstrate how future research can use this method to measure information discovery and diffusion using data from other online social networks.
|keyword = data mining,empirical research,information diffusion,new content discovery,online music community,peer influence,social influence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decommoditization, Resonance Marketing, and Information Technology: An Empirical Study of Air Travel Services amid Channel Conflict'''
{{header}}
{{article
|author= Nelson F. Granados,Robert J. Kauffman,Hsiangchu Lai,Huang-Chi Lin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Digital intermediaries and Internet search technologies have commoditized many products, resulting in intense price competition and channel conflict. Firms use decommoditization strategies to regain control over distribution channels, as well as to implement resonance marketing and hyperdifferentiation, which allows them to improve margins through differentiation. We test two hypotheses: the decommoditization hypothesis and the resonance marketing hypothesis. We use data from an airline with a new a la carte pricing mechanism, which allows consumers to tailor airline ticket bundles to suit their individual preferences. We compare a la carte ticket pricing, whose features can be modified by the purchaser, and fixed (bundled offer) sales, which cannot be modified. We found that a significant number of travelers do use a la carte pricing, which allows the airlines to regain some control over distribution. We find that travelers customized standard bundles when it was possible for them to make a la carte ticket bookings, but mainly for low-feature standard bundles. Frequent-flyer members purchased higher-feature bundles more often when they had the opportunity. The findings support the proposed hypotheses. We discuss the implications for distribution strategy and channel conflict management.
|keyword = air travel services,a la carte pricing,channel conflict,commoditization,decommoditization,disintermediation,information transparency,intermediaries,reintermediation,resonance marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Through a Glass Clearly: Standards, Architecture, and Process Transparency in Global Supply Chains'''
{{header}}
{{article
|author= Charles Steinfield,M. Lynne Markus,Rolf T. Wigand,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Despite evidence that a lack of interoperable information systems results in enormous costs, development, implementation, and effective use of interorganizational systems (IOS) remain an elusive goal for many companies. Lack of interoperability across systems is especially problematic for manufacturers dependent on global supply chains. We develop propositions about the characteristics of IOS that affect information transparency in supply chains. Specifically, we propose that data and process standards are necessary, but not sufficient, to solve such information transparency problems. Instead, standards need to be complemented by hub-type information technology architectures that are shared by organizations participating in an industrial field, not just by the participants in one manufacturer's supply chain. These arguments are supported by an automotive industry case study involving data and process standardization and a shared, cloud-based architecture. We conclude with additional aspects of the case that may be relevant to addressing information transparency problems in global supply chains.
|keyword = automotive industry,case study,data standards,EDI,industry study,information transparency,interorganizational systems,software as a service,supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''R&D Versus Acquisitions: Role of Diversification in the Choice of Innovation Strategy by Information Technology Firms'''
{{header}}
{{article
|author= Rajiv D. Banker,Sunil Wattal,Jose M. Plehn-Dujowich,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = This research examines the role of diversification on incumbent firms' response to the threat of new entry. When faced with threats posed by new technologies, incumbent firms in the information technology (IT) industry can either perform research and development (R&D), or acquire the new entrants who are successful at innovating. We use a two-stage game-theoretic framework to model the relation between diversification and the decision to acquire versus perform R&D. We also collect data on financial indicators for firms in the IT industry using the Compustat database to empirically test the propositions from the analytical model. Our results suggest that firms with a higher degree of diversification are more likely to innovate through acquisition than through R&D. Moreover, diversification has a positive effect on investment in acquisitions, as well as a negative effect on investment in R&D.
|keyword = diversification,firm acquisition,game theory,innovation,R&D,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Study of Sourcing Channels for Electronic Business Transactions'''
{{header}}
{{article
|author= Byungjoon Yoo,Vidyanand Choudhary,Tridas Mukhopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = There are two popular forms of business-to-business (B2B) marketplaces: public marketplaces and private channels. We study why firms choose either or both of these sourcing channels. Using a framework of decision making under uncertainty, we explain firms' choice of B2B channels as a hedging strategy and as a method of obtaining greater managerial flexibility for the future. We show that greater uncertainty can lead to higher investment with firms more likely to invest in both public and private channels. We find that the level of information technology (IT) capability and spending is an important factor in firms' decision making. When a firm chooses its level of IT investment simultaneously with the decision about which sourcing channels to use, the firm choosing both channels selects the highest level of IT capability and the firm implementing only one channel selects lower levels of IT capability.
|keyword = analytical modeling,B2B e-commerce,decision making under uncertainty,economic theory,IT capabilities,managerial decision making,private channels,public marketplaces,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Innovation and Price Competition in a Two-Sided Market'''
{{header}}
{{article
|author= Mei Lin,Shaojin Li,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We examine a platform's optimal two-sided pricing strategy while considering seller-side innovation decisions and price competition. We model the innovation race among sellers in both finite and infinite horizons. In the finite case, we analytically show that the platform's optimal seller-side access fee fully extracts the sellers' surplus, and that the optimal buyer-side access fee mitigates price competition among sellers. The platform's optimal strategy may be to charge or subsidize buyers depending on the degree of variation in the buyers' willingness to pay for quality; this optimal strategy induces full participation on both sides. Furthermore, a wider quality gap among sellers' products lowers the optimal buyer-side fee but leads to a higher optimal seller-side fee. In the infinite innovation race, we perform computations to find the stationary Markov equilibrium of sellers' innovation rate. Our results show that when all sellers innovate, there exists a parameterization under which a higher seller-side access fee stimulates innovation.
|keyword = innovation,price competition,two-sided markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Nonmalicious Security Violations in the Workplace: A Composite Behavior Model'''
{{header}}
{{article
|author= Ken H. Guo,Yufei Yuan,Norman P. Archer,Catherine E. Connelly,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = End users are said to be "the weakest link" in information systems (IS) security management in the workplace. They often knowingly engage in certain insecure uses of IS and violate security policies without malicious intentions. Few studies, however, have examined end user motivation to engage in such behavior. To fill this research gap, in the present study we propose and test empirically a nonmalicious security violation (NMSV) model with data from a survey of end users at work. The results suggest that utilitarian outcomes (relative advantage for job performance, perceived security risk), normative outcomes (workgroup norms), and self-identity outcomes (perceived identity match) are key determinants of end user intentions to engage in NMSVs. In contrast, the influences of attitudes toward security policy and perceived sanctions are not significant. This study makes several significant contributions to research on security-related behavior by (1) highlighting the importance of job performance goals and security risk perceptions on shaping user attitudes, (2) demonstrating the effect of workgroup norms on both user attitudes and behavioral intentions, (3) introducing and testing the effect of perceived identity match on user attitudes and behavioral intentions, and (4) identifying nonlinear relationships between constructs. This study also informs security management practices on the importance of linking security and business objectives, obtaining user buy-in of security measures, and cultivating a culture of secure behavior at local workgroup levels in organizations.
|keyword = information systems security,nonlinear construct relationships,nonmalicious security violation,perceived identity match,perceived security risk,relative advantage for job performance,workgroup norms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Moderated Online Communities and Quality of User-Generated Content'''
{{header}}
{{article
|author= Jianqing Chen,Hong Xu,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Online communities provide a social sphere for people to share information and knowledge. While information sharing is becoming a ubiquitous online phenomenon, how to ensure information quality or induce quality content remains a challenge because of the anonymity of commentators. This paper introduces moderation into reputation systems. We show that moderation directly affects strategic commentators' incentive to generate useful information, and moderation is generally desirable to improve information quality. We find that when being moderated with different probabilities based on their reputations, commentators might display a pattern of reputation oscillation, in which they generate useful content to build up high reputation and then exploit their reputation. As a result, the expected performance from high-reputation commentators can be inferior to that from low-reputation commentators (reverse reputation). We then investigate the optimal moderation resource allocation and conclude that the seemingly abnormal reverse reputation could arise as an optimal result. Our study underscores the importance of moderation and highlights that the frequency of moderation should be properly chosen for better performance of online communities.
|keyword = knowledge management,moderation,online community,reputation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''System Design Features and Repeated Use of Electronic Data Exchanges'''
{{header}}
{{article
|author= Andreas I. Nicolaou,D. Harrison McKnight,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Oftentimes researchers may not only generalize across a population, but may also extrapolate research findings across time. While either assumption can introduce difficulties, generalizing results in one time frame to another time frame may be especially perilous. We study a data exchange, and find that interventions designed to improve exchange features at two points in time have markedly varying effects, from an initial transaction use (time one) to a second transaction occurring two weeks later (time two). Our research objective is to test whether two system design features have the same effects on the intent to continue using an exchange in time two as they had in time one. The two features are control transparency (the availability of information cues) and interim shipping outcome feedback. These effects are mediated, in varying degrees, by perceived information quality. We use social exchange theory and social cognition theory to develop hypotheses regarding changes between time one (the first user transaction) and time two (the second transaction). These are tested using a combined experiment and survey. Supporting the theory, outcome feedback matters at time two even though it did not matter at time one. While control transparency has direct effects on a user's intent to continue use of the exchange in time one, its effects are reduced in time two if negative outcome feedback is communicated to the user. Outcome feedback's effects grow stronger from time one to time two vis-a-vis control transparency's effects. This underscores how critical it is to examine such phenomena at more than one period of time. The study also suggests different strategies for managing data exchanges based on the time frame of use. At the initial transaction use, the exchange should make transparent high-quality information cues to its user. At the next transaction, it should provide feedback showing properly fulfilled orders. These findings have implications for both future research examining effective data exchange design and for professionals who wish to enrich electronic data exchange interactions.
|keyword = control transparency,electronic data exchanges,outcome feedback,perceived information quality,system modifications,two-period model,usage continuance intention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What Motivates Firms to Contribute to Consortium-Based E-Business Standardization?'''
{{header}}
{{article
|author= Kexin Zhao,Mu Xia,Michael J. Shaw,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = E-business standards are a key infrastructure for electronic commerce. In many industries, they are collaboratively developed by firms in an open and neutral industry consortium. It is imperative to understand what drives firms' resource investments in such consortia, as they are critical for the success of e-business standardization. Based on collective action theory, we propose a research model to investigate the drivers of standard development within consortia. We test the model through a data set of 232 firms from 7 consortia. Consistent with collective action theory, our results demonstrate that firms' interests, resource availability, and consortium management effectiveness jointly determine their resource expenditures within the consortium. However, our exploratory investigation indicates differences between vendors and users, as vendors are more motivated by perceived standard benefits whereas users are more motivated by perceived process benefits. Our research provides a deeper understanding of firms' behaviors within consortia and factors driving their standard making.
|keyword = collective action theory,e-business standards,IT vendors,motivations to contribute,standard consortia,user organizations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Automation of Systems on Medical Errors: Evidence from Field Research'''
{{header}}
{{article
|author= Ravi Aron,Shantanu Dutta,Ramkumar Janakiraman,Praveen A. Pathak,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = We use panel data from multiple wards from two hospitals spanning a three-year period to investigate the impact of automation of the core error prevention functions in hospitals on medical error rates. Although there are studies based on anecdotal evidence and self-reported data on how automation impacts medical errors, no systematic studies exist that are based on actual error rates from hospitals. Further, there is no systematic evidence on how incremental automation over time and across multiple wards impacts the rate of medical errors. The primary objective of our study is to fill this gap in the literature by empirically examining how the automation of core error prevention functions affects two types of medical errors. We draw on the medical informatics literature and principal-agency theory and use a unique panel data set of actual documented medical errors from two major hospitals to analyze the interplay between automation and medical errors. We hypothesize that the automation of the sensing function (recording and observing agent actions) will have the greatest impact on reducing error rates. We show that there are significant complementarities between quality management training imparted to hospital staff and the automation of control systems in reducing interpretative medical errors. We also offer insights to practitioners and theoreticians alike on how the automation of error prevention functions can be combined with training in quality management to yield better outcomes. Our results suggest an optimal implementation path for the automation of error prevention functions in hospitals.
|keyword = medical errors,automation,procedural errors,information technology,hospital performance,hospital information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Emerging Infectious Diseases with Information Systems: Reconceptualizing Outbreak Management Through the Lens of Loose Coupling'''
{{header}}
{{article
|author= Yi-Da Chen,Susan A. Brown,Paul Jen-Hwa Hu,Chwan-Chuen King,Hsinchun Chen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Increasing global connectivity makes emerging infectious diseases (EID) more threatening than ever before. Various information systems (IS) projects have been undertaken to enhance public health capacity for detecting EID in a timely manner and disseminating important public health information to concerned parties. While those initiatives seemed to offer promising solutions, public health researchers and practitioners raised concerns about their overall effectiveness. In this paper, we argue that the concerns about current public health IS projects are partially rooted in the lack of a comprehensive framework that captures the complexity of EID management to inform and evaluate the development of public health IS. We leverage loose coupling to analyze news coverage and contact tracing data from 479 patients associated with the severe acute respiratory syndrome (SARS) outbreak in Taiwan. From this analysis, we develop a framework for outbreak management. Our proposed framework identifies two types of causal circles-coupling and decoupling circles-between the central public health administration and the local capacity for detecting unusual patient cases. These two circles are triggered by important information-centric activities in public health practices and can have significant influence on the effectiveness of EID management. We derive seven design guidelines from the framework and our analysis of the SARS outbreak in Taiwan to inform the development of public health IS. We leverage the guidelines to evaluate current public health initiatives. By doing so, we identify limitations of existing public health IS, highlight the direction future development should consider, and discuss implications for research and public health policy.
|keyword = public health information systems,emerging infectious disease,outbreak management,loose coupling,SARS outbreak,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Digitization of Healthcare: Boundary Risks, Emotion, and Consumer Willingness to Disclose Personal Health Information'''
{{header}}
{{article
|author= Catherine L. Anderson,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = As healthcare becomes increasingly digitized, the promise of improved care enabled by technological advances inevitably must be traded off against any unintended negative consequences. There is little else that is as consequential to an individual as his or her health. In this context, the privacy of one's personal health information has escalated as a matter of significant concern for the public. We pose the question: under what circumstances will individuals be willing to disclose identified personal health information and permit it to be digitized? Using privacy boundary theory and recent developments in the literature related to risk-as-feelings as the core conceptual foundation, we propose and test a model explicating the role played by type of information requested (general health, mental health, genetic), the purpose for which it is to be used (patient care, research, marketing), and the requesting stakeholder (doctors/hospitals, the government, pharmaceutical companies) in an individual's willingness to disclose personal health information. Furthermore, we explore the impact of emotion linked to one's health condition on willingness to disclose. Results from a nationally representative sample of over 1,000 adults underscore the complexity of the health information disclosure decision and show that emotion plays a significant role, highlighting the need for re-examining the timing of consent. Theoretically, the study extends the dominant cognitive-consequentialist approach to privacy by incorporating the role of emotion. It further refines the privacy calculus to incorporate the moderating influence of contextual factors salient in the healthcare setting. The practical implications of this study include an improved understanding of consumer concerns and potential impacts regarding the electronic storage of health information that can be used to craft policy.
|keyword = privacy calculus,healthcare,empathy gap,emotion,communication privacy management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Analysis of the Adoption of Digital Health Records Under Switching Costs'''
{{header}}
{{article
|author= Zafer Ozdemir,Jack Barron,Subhajyoti Bandyopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = We investigate the incentive issues that surround the adoption and sharing of electronic health records (EHR) and the potential role of a personal health record (PHR) platform in facilitating data sharing. Through our analysis, we find evidence that health-care providers may not have an incentive to share patients' records electronically even though EHR systems will increase consumer surplus, especially in the presence of provider heterogeneity and myopic consumers. In this context, we find that an independent PHR platform can create incentives for the providers to share their patients' records electronically with other providers by selectively subsidizing them. In a pluralistic health-care system like that in the United States, where health-care providers have varying incentives to implement electronic health records, an online PHR platform can provide a proxy for a "national health information network,'" wherein consumers can freely exchange their health records among competing providers.
|keyword = electronic health records,personal health records,switching costs,national health information network,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IS Avoidance in Health-Care Groups: A Multilevel Investigation'''
{{header}}
{{article
|author= Gerald C. Kane,Giuseppe (Joe) Labianca,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = The information systems (IS) literature has focused considerable research on IS resistance, particularly in the health-care industry. Most of this attention has focused on the impact of IS resistance on systems' initial implementation, but little research has investigated whether and how post-adoption resistance affects performance. We focus on a particular type of post-adoption resistance, which we call IS avoidance, to identify situations in which individuals avoid working with adopted IS despite the need and opportunity to do so. We examine the effects of IS avoidance on patient care delivered by health-care groups across three levels of analysis: the individual level, the shared group level, and the configural group level. We find that IS avoidance is significantly and negatively related to patient care only at the configural group level, which suggests that patient care is not degraded by the number of doctors and/or nurses in a group avoiding a system, but rather by their locations in the group's workflow network configuration. We use qualitative data collected over 16 months at the research site to help explain these results. Implications for theory and practice are discussed.
|keyword = IS use,IS avoidance,IS resistance,multilevel analysis,multimodal networks,multimethod study,social networks,centrality,configural use,healthcare,performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Doctors Do Too Little Technology: A Longitudinal Field Study of an Electronic Healthcare System Implementation'''
{{header}}
{{article
|author= Viswanath Venkatesh,Xiaojun Zhang,Tracy A. Sykes,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = With the strong ongoing push toward investment in and deployment of electronic healthcare (e-healthcare) systems, understanding the factors that drive the use of such systems and the consequences of using such systems is of scientific and practical significance. Elaborate training in new e-healthcare systems is not a luxury that is typically available to healthcare professionals-i.e., doctors, paraprofessionals (e.g., nurses) and administrative personnel-because of the 24 x 7 nature and criticality of operations of healthcare organizations, especially hospitals, thus making peer interactions and support a key driver of or barrier to such e-healthcare system use. Against this backdrop, using social networks as a theoretical lens, this paper presents a nomological network related to e-healthcare system use. A longitudinal study of an e-healthcare system implementation, with data gathered from doctors, paraprofessionals, administrative personnel, patients, and usage logs lent support to the hypotheses that: (1) ingroup and outgroup ties to doctors negatively affect use in all user groups; (2) ingroup and outgroup ties to paraprofessionals and administrative personnel positively affect use in both those groups, but have no effect on doctors' use; and (3) use contributes positively to patient satisfaction mediated by healthcare quality variables-i.e., technical quality, communication, interpersonal interactions, and time spent. This work contributes to the theory and practice related to the success of e-healthcare system use in particular, and information systems in general.
|keyword = IT diffusion and adoption,healthcare and IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Unity in Diversity: Electronic Patient Record Use in Multidisciplinary Practice'''
{{header}}
{{article
|author= Eivor Oborn,Michael Barrett,Elizabeth Davidson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = In this paper we examine the use of electronic patient records (EPR) by clinical specialists in their development of multidisciplinary care for diagnosis and treatment of breast cancer. We develop a practice theory lens to investigate EPR use across multidisciplinary team practice. Our findings suggest that there are oppositional tendencies towards diversity in EPR use and unity which emerges across multidisciplinary work, and this influences the outcomes of EPR use. The value of this perspective is illustrated through the analysis of a yearlong, longitudinal case study of a multidisciplinary team of surgeons, oncologists, pathologists, radiologists, and nurse specialists adopting a new EPR. Each group adapted their use of the EPR to their diverse specialist practices, but they nonetheless orientated their use of the EPR to each others' practices sufficiently to support unity in multidisciplinary teamwork. Multidisciplinary practice elements were also reconfigured in an episode of explicit negotiations, resulting in significant changes in EPR use within team meetings. Our study contributes to the growing literature that questions the feasibility and necessity of achieving high levels of standardized, uniform health information technology use in healthcare.
|keyword = multidisciplinary,practice theory,electronic patient record,unity,IT adoption,information systems and organizational change,case study,longitudinal research,diversity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evolving Work Routines: Adaptive Routinization of Information Technology in Healthcare'''
{{header}}
{{article
|author= Jie Mein Goh,Guodong (Gordon) Gao,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Despite the significant potential for performance gains from health IT (HIT), there has been limited study of the mechanisms underlying successful HIT implementations. We conducted an extensive longitudinal field study to gain an understanding of the interplay between technology and patterns of clinical work embodied in routines. We use the analytical device of narrative networks to identify where and how HIT influences patterns of work. We further draw upon adaptive structuration theory to conceptualize HIT as an intervention that alters the flow of events in a narrative network. Our findings suggest that the key to successful implementation is to manage the co-evolution process between routines and HIT and to actively orchestrate a virtuous cycle through agentic action. We propose a dynamic process model of adaptive routinization of HIT that delineates the major channels through which HIT and routines interact, identifies the different stages in the dynamic co-evolution process, and isolates the pivotal role of two forms of agency in enabling the virtuous cycle of co-evolution. This is one of the first studies to offer a processual, microlevel analysis of HIT implementation in a clinical setting.
|keyword = health information technology,routines,narrative network,adaptive structuration theory,affordances,hospital routines,technological change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Learning Curves of Agents with Diverse Skills in Information Technology-Enabled Physician Referral Systems'''
{{header}}
{{article
|author= Tridas Mukhopadhyay,ParamVir Singh,Seung Hyun Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = To improve operational efficiencies while providing state of the art healthcare services, hospitals rely on information technology enabled physician referral systems (IT-PRS). This study examines learning curves in an IT-PRS setting to determine whether agents achieve performance improvements from cumulative experience at different rates and how information technologies transform the learning dynamics in this setting. We present a hierarchical Bayes model that accounts for different agent skills (domain and system) and estimate learning rates for three types of referral requests: emergency (EM), nonemergency (NE), and nonemergency out of network (NO). Furthermore, the model accounts for learning spillovers among the three referral request types and the impact of system upgrade on learning rates. We estimate this model using data from more than 80,000 referral requests to a large IT-PRS. We find that: (1) The IT-PRS exhibits a learning rate of 4.5% for EM referrals, 7.2% for NE referrals, and 12.3% for NO referrals. This is slower than the learning rate of manufacturing (on average 20%) and more comparable to other service settings (on average, 8%). (2) Domain and system experts are found to exhibit significantly different learning behaviors. (3) Significant and varying learning spillovers among the three referral request types are also observed. (4) The performance of domain experts is affected more adversely in comparison to system experts immediately after system upgrade. (5) Finally, the learning rate change subsequent to system upgrade is also higher for system experts in comparison to domain experts. Overall, system upgrades are found to have a long-term positive impact on the performance of all agents. This study contributes to the development of theoretically grounded understanding of learning behaviors of domain and system experts in an IT-enabled critical healthcare service setting.
|keyword = domain experts,system experts,healthcare IT,learning curves,IT-enabled call centers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When Hackers Talk: Managing Information Security Under Variable Attack Rates and Knowledge Dissemination'''
{{header}}
{{article
|author= Vijay Mookerjee,Radha Mookerjee,Alain Bensoussan,Wei T. Yue,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This paper analyzes interactions between a firm that seeks to discriminate between normal users and hackers that try to penetrate and compromise the firm's information assets. We develop an analytical model in which a variety of factors are balanced to best manage the detection component within information security management. The approach not only considers conventional factors such as detection rate and false-positive rate, but also factors associated with hacker behavior that occur in response to improvements in the detection system made by the firm. Detection can be improved by increasing the system's discrimination ability (i.e., the ability to distinguish between attacks and normal usage) through the application of maintenance effort. The discrimination ability deteriorates over time due to changes in the environment. Also, there is the possibility of sudden shocks that can sharply degrade the discrimination ability. The firm's cost increases as hackers become more knowledgeable by disseminating security knowledge within the hacker population. The problem is solved to reveal the presence of a steady-state solution in which the level of system discrimination ability and maintenance effort are held constant. We find an interesting result where, under certain conditions, hackers do not benefit from disseminating security knowledge among one another. In other situations, we find that hackers benefit because the firm must lower its detection rate in the presence of knowledge dissemination. Other insights into managing detection systems are provided. For example, the presence of security shocks can increase or decrease the optimal discrimination level as compared to the optimal level without shocks.
|keyword = optimal security management,variable attack rates,hacker learning,security shocks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determining Optimal CRM Implementation Strategies'''
{{header}}
{{article
|author= Seung Hyun Kim,Tridas Mukhopadhyay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Although companies have spent a great deal of money to adopt CRM (customer relationship management) technologies, many have not seen satisfactory returns on their CRM implementations. We study optimal CRM implementation strategies and the impact of CRM investments on profitability. For our analysis, we classify CRM technologies into two broad categories: targeting-related and support-related technologies. While targeting CRM improves the success rate of distinguishing between nonloyal and loyal customers, support CRM increases the probability of retaining the loyalty of existing customers. We also consider the costs of implementing each CRM type separately as well as both types simultaneously. We show that the optimal CRM implementation strategy depends on the initial mass of loyal customers and diseconomies of scale in simultaneous implementation. We also find that the two types of CRM technologies are substitutive rather than complementary in generating revenue. We discuss why it is difficult to avoid overinvestments in CRM when the nature of the investments is misunderstood. We study the optimal CRM implementation scope and the impact of different types of CRM on customers. We develop a model that not only considers both the revenue and costs sides but is also helpful in determining the deployment of right CRM technology in the right scope.
|keyword = customer relationship management,IT investments,CRM costs,consumer surplus,complementarity,substitutability,economics of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Putting Yourself in the Picture: An Evaluation of Virtual Model Technology as an Online Shopping Tool'''
{{header}}
{{article
|author= Stephen P. Smith,Robert B. Johnston,Steve Howard,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = The electronic gulf between shoppers and products makes evaluating a physical product on offer at an e-store a potentially problematic activity. We propose that the outcome of the product evaluation task is determined by the fit between the type of information provided and the type of information sought by the consumer and that this, in turn, influences a consumer's attitude toward an e-store. An experiment to compare the impact of one type of advanced evaluation support technology, the virtual model, with a more basic online catalog, is then described. Results indicate that virtual models are potentially valuable when a customer is concerned with self-image and considerably less valuable when concerned with functionality. In more general terms, variation in end-user attitudes toward the object of the task (evaluative attitude) influenced how informed consumers felt about a product when using different technologies. Feeling informed, in turn, had a strong effect on consumer attitudes toward the store. Our results highlight two important issues for online stores: (1) a consumer's information requirements depend on his or her attitude to a product rather than product attributes; and (2) meeting or not meeting these information requirements affects perceptions of the store. Business success in this context therefore appears to hinge on addressing the specific functional and image-related information needs of customers rather than simply providing more interactivity or technical functionality.
|keyword = dual methods,e-store evaluation,electronic commerce,consumer attitude,virtual model,product information,empirical evaluation,value expressive,functional theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Timing of Adaptive Web Personalization and Its Effects on Online Consumer Behavior'''
{{header}}
{{article
|author= Shuk Ying Ho,David Bodoff,Kar Yan Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Web personalization allows online merchants to customize Web content to serve the needs of individual customers. Using data mining and clickstream analysis techniques, merchants can now adapt website content in real time to capture the current preferences of online customers. Though the ability to offer adaptive content in real time opens up new business opportunities for online merchants, it also raises questions of timing. One question is when to present personalized content to consumers. Consumers prefer early presentation that eases their selection process, whereas adaptive systems can make better personalized content if they are allowed to collect more consumers' clicks over time. A review of personalization research confirms that little work has been done on these timing issues in the context of personalized services. The current study aims to fill that gap. Drawing on consumer search theory, we develop hypotheses about consumer responses to differences in presentation timing and recommendation type and the interaction between the two. The findings establish that quality improves over the course of an online session but the probability of considering and accepting a given recommendation diminishes over the course of the session. These effects are also shown to interact with consumer expertise, providing insights on the interplay between the different design elements of a personalization strategy.
|keyword = Web personalization,timing,consumer search theory,online shopping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SYSTEMS RESEARCH BEHAVIORS: WHAT ARE THE NORMATIVE STANDARDS?'''
{{header}}
{{article
|author= Gove N. Allen,Nicholas L. Ball,H. Jeff Smith,
|source= MIS QUARTERLY
|year= 2011
|abstract = Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers' careers influence their judgments of these behaviors. Membership in the Association for Information Systems (A IS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers' judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the A IS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.
|keyword = Normative standards,information systems research,Code of Research Conduct,survey,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PREDICTIVE ANALYTICS IN INFORMATION SYSTEMS RESEARCH'''
{{header}}
{{article
|author= Galit Shmueli,Otto R. Koppius,
|source= MIS QUARTERLY
|year= 2011
|abstract = This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analvtics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.
|keyword = Prediction,causal explanation,theory building,theory testing,statistical model,data mining,modeling process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN EXPLORATION OF ORGANIZATIONAL LEVEL INFORMATION SYSTEMS DISCONTINUANCE INTENTIONS'''
{{header}}
{{article
|author= Brent Furneaux,Michael Wade,
|source= MIS QUARTERLY
|year= 2011
|abstract = Limited attention has been directed toward examining post-adoption stages of the information system life cycle. In particular, the final stages of this life cycle have been largely ignored despite the fact that most systems eventually reach the end of their useful life. This oversight is somewhat surprising given that end-of-life decisions can have significant implications for user effectiveness, the value extracted from IS investments, and organizational performance. Given this apparent gap, a multi-method empirical study was undertaken to improve our understanding of organizational level information system discontinuance. Research commenced with the development of a broad theoretical framework consistent with the technology organization environment (TOE) paradigm. The resulting framework was then used to guide a series of semi-structured interviews with organizational decision makers in an effort to inductively identify salient influences on the formation of IS discontinuance intentions. A set of research hypotheses were formulated based on the understanding obtained during these interviews and subsequently tested via a random survey of senior IS decision makers at U.S. and Canadian organizations. Data obtained from the survey responses was analyzed using partial least squares (PLS). Results of this analysis suggest that system capability shortcomings, limited availability of system support, and low levels of technical integration were key determinants of increased intentions to replace an existing system. Notably, investments in existing systems did not appear to significantly undermine organizational replacement intentions despite support for this possibility from both theory and our semi-structured interviews.
|keyword = Information systems discontinuance,obsolescence,abandonment,replacement,life cycle management,technology-organization-environment (TOE) framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECTS OF DIGITAL TRADING PLATFORMS ON COMMODITY PRICES IN AGRICULTURAL SUPPLY CHAINS'''
{{header}}
{{article
|author= Rajiv Banker,Sabyasachi Mitra,V. Sambamurthy,
|source= MIS QUARTERLY
|year= 2011
|abstract = Digital platforms for buying and selling agricultural commodities have generated significant interest in the trade literature as a way to link rural communities to the Internet. Yet, the extent to which these digital platforms actually translate into higher commodity prices for producers remains an open research question. We investigate this question by comparing transaction data on trading various grades of coffee from a recently implemented digital platform in India with similar transactions from a physical commodity auction held weekly, and firm-gate prices in the coffee producing regions of India. Although the digital platform prices closely track the physical commodity auction prices, producers obtain significantly higher prices when they sell the commodity through the digital platform rather than at the farm-gate through brokers who operate in their regions. However, coffee grades with higher price volatility and premium coffee grades that require face-to-face interactions to verify quality obtain lower prices on the digital platform. Our results also indicate that market participants who control the transaction obtain better prices. We discuss the implications of our findings for governments and platform providers.
|keyword = Digital divide,digital platforms,global IT,commodity auctions,bargaining power,commodity trading,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MEMBERSHIP TURNOVER AND COLLABORATION SUCCESS IN ONLINE COMMUNITIES: EXPLAINING RISES AND FALLS FROM GRACE IN WIKIPEDIA'''
{{header}}
{{article
|author= Sam Ransbotham,Gerald C. Kane,
|source= MIS QUARTERLY
|year= 2011
|abstract = Firms increasingly turn to online communities to create valuable information. These communities are empowered by new information technology-enabled collaborative tools, tools such as blogs, wikis, and social networks. Collaboration on these platforms is characterized by considerable membership turnover, which could have significant effects on collaborative outcomes. We hypothesize that membership retention relates in a curvilinear fashion to effective collaboration: positively up to a threshold and negatively thereafter. The longitudinal history of 2,065 featured articles on Wikipedia offers support for this hypotheses: Contributions from a mixture of new and experienced participants both increases the likelihood that an article will be promoted to featured article status and decreases the risk it will be demoted after having been promoted. These findings imply that, contrary to many of the assumptions in previous research, participant retention does not have a strictly positive effect on emerging collaborative environments. Further analysis of our data provides empirical evidence that knowledge creation and knowledge retention are actually distinct phases of community-based peer production, and that communities may on average experience more turnover than ideal during the knowledge retention phase.
|keyword = Online communities,collaboration,longitudinal study,membership turnover,information generation,information retention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INNOVATION IMPACTS OF USING SOCIAL BOOKMARKING SYSTEMS'''
{{header}}
{{article
|author= Peter H. Gray,Salvatore Parise,Bala Iyer,
|source= MIS QUARTERLY
|year= 2011
|abstract = Many organizational innovations can be explained by the movement of ideas and information from one social context to another, "from where they are known to where they are not" (Hargadon 2002, p. 41). A relatively new technology, social bookmarking, is increasingly being used in many organizations (McAfee 2006), and may enhance employee innovativeness by providing a new, socially mediated channel for discovering information. Users of such systems create publicly viewable lists of bookmarks (each being a hyperlink to an information resource) and often assign searchable keywords ("tags") to these bookmarks. We explore two different perspectives on how accessing others' bookmarks could enhance how innovative an individual is at work. First, we develop two hypotheses around the idea that quantity may be a proxy for diversity, following a well established literature that holds that the more information obtained and the larger the number of sources consulted, the higher the likelihood an individual will come across novel ideas. Next, we offer two hypotheses adapted from social network research that argue that the shape of the network of connections that is created when individuals access each others' bookmarks can reflect information novelty, and that individuals whose networks bridge more structural holes and have greater effective reach are likely to be more innovative. An analysis of bookmarking system use in a global professional services firm provides strong support for the social diversity of information sources as a predictor of employee innovativeness, but no support that the number of bookmarks accessed matters. By extending the social networks literature to theorize the functionalities offered by social bookmarking systems, this research establishes structural holes theory as a valuable lens through which social technologies may be understood.
|keyword = Social tagging systems,social bookmarking systems,social technologies,Web 2.0 technologies,Social network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTROL OVER VIRTUAL WORLDS BY GAME COMPANIES: ISSUES AND RECOMMENDATIONS'''
{{header}}
{{article
|author= Christophe Roquilly,
|source= MIS QUARTERLY
|year= 2011
|abstract = Game companies use five components four core components and one complementary one in a 5Cs model to ensure the control and development of virtual worlds. A multidisciplinary review of the literature reveals that game companies make use of copyright, codes, creativity, and community to do this. They use the contract as a complementary component to reinforce their control over the four basic components and to compensate for the lacunae they present. In order to examine the extent to which game companies use the contract in this way, an analysis is performed of all contractual documents from a sample of 20 virtual worlds, providing evidence of general trends and emphasizing any differences between the virtual worlds in terms of the business and gaming models sought by each game company. An explanation is provided of why these contracts do not constitute a sustainable model for the game companies, given the high level of legal insecurity they present. Some basic recommendations can be made in order to improve the sustainability of the 5Cs model by modifying these contracts in such away that they are enforceable and by matching their content with appropriate business and gaming models. This could lead to further studies aimed at providing answers to some of the intriguing issues affecting scholars and practitioners.
|keyword = Community,computer codes,contract,control,copyright,creativity,EULA,property rights,virtual world,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DESIGN PRINCIPLES FOR VIRTUAL WORLDS'''
{{header}}
{{article
|author= Alok R. Chaturvedi,Daniel R. Dolk,Paul L. Drnevich,
|source= MIS QUARTERLY
|year= 2011
|abstract = In this research note, we examine the design, development, validation, and use of virtual worlds. Our purpose in doing so is to extend the design science paradigm by developing a set of design principles applicable to the context of virtual environments, particularly those using agent-based simulation as their underlying technology. Our central argument is that virtual worlds comprise a new class of information system, one that combines the structural aspects of traditional modeling and simulation systems in concert with emergent user dynamics of systems supporting emergent knowledge processes. Our approach involves two components. First, we review the characteristics of agent-based virtual worlds (ABVWs) to discern design requirements that may challenge current design theory. From this review, we derive a set of design principles based on deep versus emergent structures where deep structures reflect conventional modeling and simulation system architectures and emergent structures capture the unpredictable user system dynamics inherent in emergent knowledge processes, which increasingly characterize virtual worlds. We illustrate how these design challenges are addressed with an exemplar of a complex mirror world, a large-scale ABVW we developed called Sentient World. Our contribution is the insight of partitioning ABVW architectures into deep and emergent structures that mirror modeling systems and emergent knowledge processes respectively, while developing extended design principles to facilitate their integration. We conclude with a discussion of the implications of our design principles for informing and guiding future research and practice.
|keyword = IS Design theory,virtual world systems,emergent knowledge processes,agent-based simulation,deep structure,platform as a methodology (PaaM),user-developed content (UDC),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ARGUING THE VALUE OF VIRTUAL WORLDS: PATTERNS OF DISCURSIVE SENSEMAKING OF AN INNOVATIVE TECHNOLOGY'''
{{header}}
{{article
|author= Nicholas Berente,Sean Hansen,Jacqueline C. Pike,Patrick J. Bateman,
|source= MIS QUARTERLY
|year= 2011
|abstract = With the rapid pace of technological development, individuals are frequently challenged to make sense of equivocal innovative technology while being given limited information. Virtual worlds are a prime example of such an equivocal innovative technology, and this affords researchers an opportunity to study sensemaking and the construction of perspectives about the organizational value of virtual worlds. This study reports on an analysis of the written assessments of 59 business professionals who spent an extended period of time in Second Life, a popular virtual world, and discursively made sense of the organizational value of virtual worlds. Through a Toulminian analysis of the claims, grounds, and warrants used in the texts they generated, we identify 12 common patterns of sensemaking and indicate that themes of confirmation, open-ended rhetoric, demographics, and control are evident in the different types of claims that were addressed. Further, we assert that the Toulminian approach we employ is a useful methodology for the study of sensemaking and one that is not bound to any particular theoretical perspective.
|keyword = Virtual worlds,Second Life,sensemaking,discourse,argument,Toulmin,organizational value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT IF YOUR AVATAR LOOKS LIKE YOU? DUAL-CONGRUITY PERSPECTIVES FOR AVATAR USE'''
{{header}}
{{article
|author= Kil-Soo Suh,Hongki Kim,Eung Kyo Suh,
|source= MIS QUARTERLY
|year= 2011
|abstract = As broadband Internet access and virtual reality technology rapidly expand, virtual worlds and three-dimensional avatars will become more pervasive and widely adopted. In virtual worlds, people assume an identity as an avatar and interact with each other. The objective of this study is to theorize how users form attitudes and intentions regarding avatars in realistic, task-focused virtual world settings. To investigate these effects, this study proposes a conceptual framework based on dual-congruity perspectives (self-congruity and functional congruity). The results show that the more closely an avatar resembles its user, the more the user is likely to have positive attitudes (e.g., affection, connection, and passion) toward the avatar, and the better able to evaluate the quality and performance of apparel products. In the end, these positive attitudes toward an avatar and its usefulness positively affect users' intentions to use the avatar. Based on this study, we propose that avatars representing users' actual appearance may be helpful in experiencing and evaluating some business areas related to users' lives in the real world (e.g., virtual apparel shopping, matchmaking, plastic surgery, fitness clubs, etc.); utilization of such avatars may be a new business opportunity likely to thrive in virtual worlds.
|keyword = Virtual worlds,self-concept,self-congruity,functional congruity,avatar similarity,avatar identification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ENHANCING BRAND EQUITY THROUGH FLOW AND TELEPRESENCE: A COMPARISON OF 2D AND 3D VIRTUAL WORLDS'''
{{header}}
{{article
|author= Fiona Fui-Hoon Nah,Brenda Eschenbrenner,David DeWester,
|source= MIS QUARTERLY
|year= 2011
|abstract = This research uses theories of flow, telepresence, positive emotions, and brand equity to examine the effect of using two-dimensional versus three-dimensional virtual world environments on telepresence, enjoyment, brand equity, and behavioral intention. The findings suggest that the 3D virtual world environment produces both positive and negative effects on brand equity when compared to the 2D environment. The positive effect of the 3D virtual world environment on brand equity occurs through telepresence, a specific aspect of flow, as well as enjoyment. The negative effect on brand equity can be explained using distraction conflict theory in which attentional conflicts faced by users of a highly interactive and rich medium resulted in distractions from attending to the brand. Brand equity, in turn, has a positive effect on behavioral intention. The results suggest that although the 3D virtual world environment has the potential to increase brand equity by offering an immersive and enjoyable virtual product experience, the rich environment can also be a distraction. Therefore, developers of virtual world branding sites need to take into account limitations in the information processing capacity and attention span of users when designing their sites in order to avoid cognitive overload, which can lead to users being distracted from branding information. This paper not only provides a theoretical foundation for explaining users' experience with 2D versus 3D virtual world branding sites, but also provides insights to practitioners for designing 3D virtual world sites to enhance brand equity and intentions through user engagement.
|keyword = Virtual worlds,telepresence,flow,enjoyment,brand equity,behavioral intention,2D,3D,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FROM SPACE TO PLACE: PREDICTING USERS' INTENTIONS TO RETURN TO VIRTUAL WORLDS'''
{{header}}
{{article
|author= Lakshmi Goel,Norman A. Johnson,Iris Junglas,Blake Ives,
|source= MIS QUARTERLY
|year= 2011
|abstract = Virtual worlds have received considerable attention as platforms for entertainment, education, and commerce. But organizations are experiencing failures in their early attempts to lure customers, employees, or partners into these worlds. Among the more grievous problems is the inability to attract users back into a virtual environment. In this study, we propose and test a model to predict users 'intentions to return to a virtual world. Our model is based on the idea that users intend to return to a virtual world having conceived of it as a "place" in which they have had meaningful experiences. We rely on the interactionist theory of place attachment to explain the links among the constructs of our model. Our model is tested via a lab experiment. We find that users' intentions to return to a virtual world is determined by a state of deep involvement (termed cognitive absorption) that users experience as they perform an activity and tend to lose track of time. In turn, cognitive absorption is determined by users' awareness of whom they interact with and how they interact within a virtual world, what they interact about, and where, in a virtual sense, such interaction occurs. Our work contributes to theory in the following ways: it identifies state predictors of cognitive absorption, it conceives of virtual worlds in such a way as to account for users' experiences through the notion of place, and it explains how the properties of a virtual world contribute to users' awareness.
|keyword = Virtual worlds,cognitive absorption,intention to return,social awareness,location awareness,task awareness,sense of place,place attachment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CO-CREATION IN VIRTUAL WORLDS: THE DESIGN OF THE USER EXPERIENCE'''
{{header}}
{{article
|author= Thomas Kohler,Johann Fueller,Kurt Matzler,Daniel Stieger,
|source= MIS QUARTERLY
|year= 2011
|abstract = Emerging virtual worlds, such as the prominent Second Life, offer unprecedented opportunities for companies to collaborate with co-creating users. However, pioneering corporate co-creation systems fail to attract a satisfying level of participation and engagement. The experience users have with the co-creation system is the key to making virtual places a vibrant source of great connections, creativity, and co-creation. While prior research on co-creation serves as a foundation for this work, it does not provide adequate guidance on how to design co-creation systems in virtual worlds. To address this shortcoming, a 20-month action research project was conducted to study the user's experience and to identify design principles for virtual co-creation systems. In two action research cycles, a virtual co-creation system called Ideation Quest was created, deployed, evaluated, and improved. The study reveals how to design co-creation systems and enriches research on co-creation to fit the virtual world context. Practitioners receive a helpful framework to leverage virtual worlds for co-creation.
|keyword = Virtual worlds,Second Life,co-creation,action research,experience design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ODYSSEY INTO VIRTUAL WORLDS: EXPLORING THE IMPACTS OF TECHNOLOGICAL AND SPATIAL ENVIRONMENTS ON INTENTION TO PURCHASE VIRTUAL PRODUCTS'''
{{header}}
{{article
|author= Animesh Animesh,Alain Pinsonneault,Sung-Byung Yang,Wonseok Oh,
|source= MIS QUARTERLY
|year= 2011
|abstract = Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus organism response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants' virtual experiences (telepresence, social presence, and low), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants' virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.
|keyword = Virtual worlds,technological environment,spatial environment,virtual experience,intention to purchase virtual products,S-O-R framework,Second Life,interactivity,sociability,density,stability,symbolic consumption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Embodied Conversational Agent-Based Kiosk for Automated Interviewing'''
{{header}}
{{article
|author= Jr. Jay E. Nunamaker,Douglas C. Derrick,Aaron C. Elkins,Judee K. Burgoon,Mark W. Patton,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We have created an automated kiosk that uses embodied intelligent agents to interview individuals and detect changes in arousal, behavior, and cognitive effort by using psychophysiological information systems. In this paper, we describe the system and propose a unique class of intelligent agents, which are described as Special Purpose Embodied Conversational Intelligence with Environmental Sensors (SPECIES). SPECIES agents use heterogeneous sensors to detect human physiology and behavior during interactions, and they affect their environment by influencing human behavior using various embodied states (i.e., gender and demeanor), messages, and recommendations. Based on the SPECIES paradigm, we present three studies that evaluate different portions of the model, and these studies are used as foundational research for the development of the automated kiosk. The first study evaluates human computer interaction and how SPECIES agents can change perceptions of information systems by varying appearance and demeanor. Instantiations that had the agents embodied as males were perceived as more powerful, while female embodied agents were perceived as more likable. Similarly, smiling agents were perceived as more likable than neutral demeanor agents. The second study demonstrated that a single sensor measuring vocal pitch provides SPECIES with environmental awareness of human stress and deception. The final study ties the first two studies together and demonstrates an avatar-based kiosk that asks questions and measures the responses using vocalic measurements.
|keyword = avatars,deception detection,embodied conversational agents,NeuroIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Global Model of Technological Utilization Based on Governmental, Business-Investment, Social, and Economic Factors'''
{{header}}
{{article
|author= James B. Pick,Rasool Azari,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = This exploratory paper presents a conceptual model of the factors of governmental support and openness, business and technology investment, and socioeconomic level that are posited to influence technological utilization. The conceptual model and conjectures are developed inductively based on logic and prior research about the relationship among variables related to the factors. Structural equation modeling (SEM) is applied to operationalize and test the model. The SEM analysis tests five points of investigation on a large sample of country data from the World Bank and the World Economic Forum. Findings indicate a critical pathway of associations between the factors of government support and openness, investment in business and technology, socioeconomic level, and technology utilization. The paper presents two country case examples of the model and suggests policy steps for national governments of developed and developing countries to prioritize information and communications technology, create openness, strengthen research and development and technology investment, and enhance education and information technology training.
|keyword = global digital divide,government investment,societal openness,socioeconomic factors,structural equation modeling,technological utilization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Changing the Perspective: Using a Cognitive Model to Improve thinkLets for Ideation'''
{{header}}
{{article
|author= Stefan Werner Knoll,Graham Horton,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = In the field of collaboration engineering, thinkLets describe reusable and transferable collaborative activities to reproduce known patterns of collaboration. This paper focuses on thinkLets of the pattern Generate, which define collaboration activities to produce and share new contributions by a group. We address the question whether the small number of published Generate thinkLets can adequately represent the various approaches contained in published idea generation techniques. We used a cognitive model to analyze 101 idea generation techniques with regard to the underlying mental principles that stimulate the ideation process by deliberately activating larger areas of the knowledge network. We present three changes of perspective based on these principles, which can be used to formalize the underlying mechanisms of idea generation techniques. The paper shows how these three principles can be used to improve Generate thinkLets and discusses how this formalization can improve the applicability of information systems for ideation processes.
|keyword = change of perspective,cognitive model of ideation,collaboration engineering,idea generation,thinkLet,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology Spillover and Productivity: The Role of Information Technology Intensity and Competition'''
{{header}}
{{article
|author= Kuns Han,Young Bong Chang,Jungpil Hahn,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We study interindustry information technology (IT) spillover wherein IT investments made by supplier industries increase the productivity of downstream industries. Using data from U.S. manufacturing industries, we find that industries receive significant IT spillover benefits in terms of total factor productivity growth through economic transactions with their respective supplier industries. More importantly, we find that two characteristics of downstream industries, namely, IT intensity and competitiveness, which have been shown to moderate the effect of internal IT investments, play an important role in IT spillovers as well. Our results suggest that IT intensity as well as competitiveness of the downstream industry moderate the effect of IT spillovers industries that are more IT intensive and more competitive benefit more from IT spillovers. Finally, our results suggest that the long-term effects of spillovers are greater than short-term effects, suggesting that learning periods are required to reap the benefits from the IT spillovers.
|keyword = competition,industry analysis,industry characteristics,IT effects,IT intensity,IT spillover,productivity,total factor productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cultural Signifiers of Web Site Images'''
{{header}}
{{article
|author= Fatemeh Mariam Zahedi,Gaurav Bansal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Web sites rely on pictures and animation to convey subtle messages that are more effectively communicated nonverbally. We argue that such messages could have strong cultural content, which should be understood in developing Web sites. Hence, this paper explores the cultural content of Web site images and develops a theory for Web-image signifiers. This is done in two phases. Phase I has an interpretive qualitative approach that uses Grounded Theory to identify signifiers and to develop the Web-image signifiers (WIS) theory. Phase II quantitatively tests the WIS theory. Together, these two phases identify and validate signifiers of cultural dimensions in Web site images. More interestingly, the results uncover that cultural dimensions are signified in five categories, of which two, humans and buildings categories, are the most prominent. The contribution of this paper is in developing a comprehensive theory for the cultural content of Web images, identifying 48 signifiers in Web images, discovering new categories of signifiers, and providing insights into the nature of cultural signification by testing the theory. Such knowledge could heighten our sensitivity and awareness of hidden cultural messages in Web site images. The WIS theory could provide a novel approach to the cultural studies of Web images and other artifacts with cultural content. The results of this work have immediate application in the design of Web sites for a multicultural audience.
|keyword = cultural signifiers,Grounded Theory,Hofstede's cultural dimensions,semiology,Web-image signifiers theory,Web site images,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Automated and Participative Decision Support in Computer-Aided Credibility Assessment'''
{{header}}
{{article
|author= Matthew L. Jensen,Paul Benjamin Lowry,Jeffrey L. Jenkins,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Historically, inaccurate credibility assessments have resulted in tremendous costs to businesses and to society. Recent research offers unobtrusive credibility assessment aids as a solution; however, the accuracy of these decision aids is inadequate, and users often resist accepting the aids' recommendations. We follow the principles of signal detection theory to improve the accuracy of recommendations in computer-aided credibility assessment by combining automated and participatory decision support. We also leverage participation in decision-making theory to explain and predict an increased acceptance of assessment aid recommendations when perceptual cues are elicited from users. Based on these two theories, we design and test a hybrid decision aid to perform automated linguistic analysis and to elicit and analyze perceptual cues from an observer. Results from a laboratory experiment indicate that decision aids that use linguistic and perceptual cues offer more accurate recommendations than aids that use only one type of cue. Automatic analysis of linguistic cues improved both the decision aid's recommendations and the users' credibility assessment accuracy. Challenging the generalizability of past findings, the elicitation of perceptual cues did not improve the decision aid's recommendations or the users' assessment accuracy. Elicitation of perceptual cues, however, did improve user acceptance of the decision aid's recommendations. These findings provide guidance for future development of credibility assessment decision aids.
|keyword = credibility assessment,decision support systems,indirect cues elicitation,linguistic analysis,signal detection theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User Acceptance of Agile Information Systems: A Model and Empirical Test'''
{{header}}
{{article
|author= Weiyin Hong,James Y. L. Thong,Lewis C. Chasalow,Gurpreet Dhillon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = In response to the rapid changes in users' requirements, a new generation of information systems (IS), namely, agile IS, has emerged. Agile IS, defined as information systems developed using agile methods, are characterized by frequent upgrades with a small number of new features released periodically. The existing research on agile IS has mainly focused on the developers' perspective with little research into end users' responses to these agile IS. Drawing upon the tripartite model of attitude, the status quo and the omission bias theories, and the availability heuristic, we propose a model that utilizes constructs from the unified theory of acceptance and use of technology, the IS continuance model, habit, and individual differences to examine the drivers of user acceptance of agile IS. Further, we investigate not only users' intentions to continue using the agile IS but also their intentions to use new features when they are released, which is a surrogate for the ultimate success of agile IS. Data from 477 users of an agile IS showed that users' level of comfort with constant changes, the facilitating conditions provided, and users' habit are predictors of both types of intentions, with users' level of comfort with constant changes being the strongest predictor. Users' intentions to continue using agile IS are also determined by users' satisfaction with and perceived usefulness of the past upgrades. Finally, users who are innovative are more likely to use future releases of new features. The present work fills a gap in the software engineering literature and contributes a technology acceptance model specific to agile IS, which are becoming a mainstay of companies' IT portfolio in a fast-changing business environment.
|keyword = agile methods,agile systems,availability heuristic,comfort with change,habit,information systems continuance,omission bias,personal innovativeness,status quo bias,unified theory of acceptance and use of technology (UTAUT),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Communication and Trust in Global Virtual Teams: A Social Network Perspective'''
{{header}}
{{article
|author= Saonee Sarker,Manju Ahuja,Suprateek Sarker,Sarah Kirkeby,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = The importance of communication and trust in the context of global virtual teams has been noted and reiterated in the information systems (IS) literature. Yet precisely how communication and trust influence certain outcomes within virtual teams remains unresolved. In this study, we seek to contribute some clarity to the understanding of the theoretical linkages among trust, communication, and member performance in virtual teams. To this end, we identify and test three proposed models (additive, interaction, and mediation) describing the role of trust in its relationship with communication to explain performance. In testing the relationships, we note that the concepts of communication and trust are inherently relational and not properties of individuals. Thus, we argue that a social network approach is potentially more appropriate than attribute-based approaches that have been utilized in prior research. Our results indicate that the "mediating" model best explains how communication and trust work together to influence performance. Overall, the study contributes to the existing body of knowledge on virtual teams by empirically reconciling conflicting views regarding the interrelationships between key constructs in the literature. Further, the study, through its adoption of the social network analysis approach, provides awareness within the IS research community of the strengths of applying network approaches in examining new organizational forms.
|keyword = communication,distributed teams,global virtual teams,hybrid teams,individual performance,mediation,networked individualism,social network analysis,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cognitive Conflict and Consensus Generation in Virtual Teams During Knowledge Capture: Comparative Effectiveness of Techniques'''
{{header}}
{{article
|author= Ananth Chiravuri,Derek Nazareth,K. (Ram) Ramamurthy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Effective knowledge management has been increasingly cited as critical for businesses to compete successfully. Knowledge acquisition/capture, the first step in knowledge management, continues to be a bottleneck and is exacerbated when experts are geographically distributed. Furthermore, knowledge from multiple experts is likely to generate inconsistent knowledge for a given problem domain. There is thus a compelling need to generate consensus by resolving inconsistencies and conflicts that may occur among experts during the process of knowledge acquisition. This process is more challenging when dealing with virtual teams of experts. This study addresses task-based or cognitive conflicts among experts. A key objective of this study is to examine the effectiveness of two cognitive techniques the repertory grid (or RepGrid) and Delphi in generating consensus among experts during the knowledge capture process. A field experiment with geographically distributed real-world network experts involving multiple rounds of interaction over an extended period of time was conducted. Findings from this research indicate that, in the short run, Delphi works better than the RepGrid in reducing conflict and generating consensus. However, the RepGrid technique appears to perform better in the long run. We find similar results for satisfaction with the process and outcome. Our findings also indicate that experts using the RepGrid technique elicited more knowledge as well as higher-quality knowledge than experts using the Delphi technique. To sum up, our study indicates that RepGrid is superior to Delphi, and therefore managers should seriously consider the use of RepGrid in capturing knowledge from multiple and distributed experts when dealing with complex real-world issues.
|keyword = cognitive conflict,conflict resolution,consensus generation,Delphi technique,knowledge capture,knowledge management,repertory grid technique,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Virtuality on Social Networks Within and Across Work Groups: A Multilevel Approach'''
{{header}}
{{article
|author= Ayoung Suh,Kyung-Shik Shin,Manju Ahuja,Min Soo Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We examine how the virtuality of work context influences individuals' social networks within and across work groups. Given this purpose, we develop a multilevel research framework that explores the effects of different levels of virtuality on one's intra-group tie strength and extra-group network range based on the computer-mediated communication theory, the proximity theory, and the social network theory. The results of the hierarchical linear modeling indicate that the individual-level virtuality (use of personal and communal communication technologies) significantly influences one's intra-group tie strength and extra-group network range. Moreover, the results show that the effects of individual-level virtuality on social networks vary depending on the group-level virtuality, such as geographic/temporal dispersion and technological support. By illuminating how individuals' social networks can be developed through the appropriate use of personal and communal communication technologies in the context of a virtual group, this study provides useful insights into the mechanics that underlie effective virtual work.
|keyword = computer-mediated communication,hierarchical linear modeling,multilevel analysis,social networks,virtuality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MEASUREMENT AND MEANING IN INFORMATION SYSTEMS AND ORGANIZATIONAL RESEARCH: METHODOLOGICAL AND PHILOSOPHICAL FOUNDATIONS'''
{{header}}
{{article
|author= Richard P. Bagozzi,
|source= MIS QUARTERLY
|year= 2011
|abstract = Despite renewed interest and many advances in methodology in recent years, information systems and organizational researchers face confusing and inconsistent guidance on how to choose amongst, implement, and interpret findings from the use of different measurement procedures. In this article, the related topics of measurement and construct validity are summarized and discussed, with particular focus on formative and reflective indicators and common method bias, and, where relevant, a number of allied issues are considered. The perspective taken is an eclectic and holistic one and attempts to address conceptual and philosophical essentials, raise salient questions, and pose plausible solutions to critical measurement dilemmas occurring in the managerial, behavioral, and social sciences.
|keyword = Construct validity,common method bias,reflective indicators,formative indicators,measurement,structural equation models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONSTRUCT MEASUREMENT AND VALIDATION PROCEDURES IN MIS AND BEHAVIORAL RESEARCH: INTEGRATING NEW AND EXISTING TECHNIQUES'''
{{header}}
{{article
|author= Scott B. MacKenzie,Philip M. Podsakoff,Nathan P. Podsakoff,
|source= MIS QUARTERLY
|year= 2011
|abstract = Despite the fact that validating the measures of constructs is critical to building cumulative knowledge in MIS and the behavioral sciences, the process of scale development and validation continues to be a challenging activity. Undoubtedly, part of the problem is that many of the scale development procedures advocated in the literature are limited by the fact that they (1) fail to adequately discuss how to develop appropriate conceptual definitions of the focal construct, (2) often fail to properly specify the measurement model that relates the latent construct to its indicators, and (3) under utilize techniques that provide evidence that the set of items used to represent the focal construct actually measures what it purports to measure. Therefore, the purpose of the present paper is to integrate new and existing techniques into a comprehensive set of recommendations that can be used to give researchers in MIS and the behavioral sciences a frame work for developing valid measures. First, we briefly elaborate upon some of the limitations of current scale development practices. Following this, we discuss each of the steps in the scale development process while paying particular attention to the differences that are required when one is attempting to develop scales for constructs with formative indicators as opposed to constructs with reflective indicators. Finally, we discuss several things that should be done after the initial development of a scale to examine its generalizability and to enhance its usefulness.
|keyword = Construct validation procedures,Scale development and validation,content, convergent, discriminant and nomological validity,formative and reflective indicator models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INCORPORATING FORMATIVE MEASURES INTO COVARIANCE-BASED STRUCTURAL EQUATION MODELS'''
{{header}}
{{article
|author= Adamantios Diamantopoulos,
|source= MIS QUARTERLY
|year= 2011
|abstract = Formatively measured constructs have been increasingly used in information systems research. With few exceptions, however, extant studies have been relying on the partial least squares (PLS) approach to specify; and estimate structural models involving constructs measured wit h formative indicators. This paper highlights the benefits of employing covariance structure analysis (CSA) when investigating such models and illustrates its application with the LISREL program. The aim is to provide practicing IS researchers with an understanding of key issues and potential problems associated with formatively measured constructs within a covariance-based modeling framework and encourage them to consider using CSA in their future research endeavors.
|keyword = Formative measurement,formative indicators,covariance structure analysis,PLS,MIMIC models,identification,scaling options,model evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EVALUATING EFFECT, COMPOSITE, AND CAUSAL INDICATORS IN STRUCTURAL EQUATION MODELS'''
{{header}}
{{article
|author= Kenneth A. Bollen,
|source= MIS QUARTERLY
|year= 2011
|abstract = Although the literature on alternatives to effect indicators is growing, there has been little attention given to evaluating causal and composite (formative) indicators. This paper provides an overview of this topic by contrasting ways of assessing the validity of effect and causal indicators in structural equation models (SEMs). It also draws a distinction between composite (formative) indicators and causal indicators and argues that validity is most relevant to the latter. Sound validity assessment of indicators is dependent on having an adequate overall model fit and on the relative stability of the parameter estimates for the latent variable and indicators as they appear in different models. If the overall fit and stability of estimates are adequate, then a researcher can assess validity using the unstandardized and standardized validity coefficients and the unique validity variance estimate. With multiple causal indicators or with effect indicators influenced by multiple latent variables, collinearity diagnostics are useful. These results are illustrated with a number of correctly and incorrectly specified hypothetical models.
|keyword = Causal indicators,effect indicators,formative indicators,reflective indicators,measurement,validity,structural equation models,scale construction,composites,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT SIGNAL ARE YOU SENDING? HOW WEBSITE QUALITY INFLUENCES PERCEPTIONS OF PRODUCT QUALITY AND PURCHASE INTENTIONS'''
{{header}}
{{article
|author= John D. Wells,Joseph S. Valacich,Traci J. Hess,
|source= MIS QUARTERLY
|year= 2011
|abstract = An electronic commerce marketing channel is fully mediated by information technology, stripping away much of a product's physical informational cues, and creating information asymmetries (i.e., limited information). These asymmetries may impede consumers' ability to effectively assess certain types of products, thus creating challenges for online sellers. Signaling theory provides a framework for understanding how extrinsic cues signals can be used by sellers to convey product quality information to consumers, reducing uncertainty and facilitating a purchase or exchange. This research proposes a model to investigate website quality as a potential signal of product quality and consider the moderating effects of product information asymmetries and signal credibility. Three experiments are reported that examine the efficacy of signaling theory as a basis for predicting online consumer behavior with an experience good. The results indicate that website quality influences consumers' perceptions of product quality, which subsequently affects online purchase intentions. Additionally, website quality was found to have a greater influence on perceived product quality when consumers had higher information asymmetries. Likewise, signal credibility was found to strengthen the relationship between website quality and product quality perceptions for a high quality website. Implications for future research and website design are examined.
|keyword = Signaling theory,signals,cues,website quality,eCommerce,perceived quality,credibility,information asymmetries,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CORRELATED FAILURES, DIVERSIFICATION, AND INFORMATION SECURITY RISK MANAGEMENT'''
{{header}}
{{article
|author= Pei-yu Chen,Gaurav Kataria,Ramayya Krishnan,
|source= MIS QUARTERLY
|year= 2011
|abstract = The increasing dependence on information networks for business operations has focused managerial attention on managing risks posed by failure of these networks. In this paper, we develop models to assess the risk of failure on the availability of an information network due to attacks that exploit software vulnerabilities. Software vulnerabilities arise from software installed on the nodes of the network. When the same software stack is installed on multiple nodes on the network, software vulnerabilities are shared among them. These shared vulnerabilities can result in correlated failure of multiple nodes resulting in longer repair times and greater loss of availability of the network. Considering positive network effects (e.g., compatibility) alone without taking the risks of correlated failure and the resulting downtime into account would lead to over-investment in homogeneous software deployment. Exploiting characteristics unique to information networks, we present a queuing model that allows us to quantify downtime loss faced by arm as a function of (1) investment in security technologies to avert attacks, (2) software diversification to limit the risk of correlated failure under attacks, and (3) investment in IT resources to repair failures due to attacks. The novelty of this method is that we endogenize the failure distribution and the node correlation distribution, and show how the diversification strategy and other security measures/investments may impact these two distributions, which in turn determine the security loss faced by the firm. We analyze and discuss the effectiveness of diversification strategy under different operating conditions and in the presence of changing vulnerabilities. We also take into account the benefits and costs of a diversification strategy. Our analysis provides conditions under which diversification strategy is advantageous.
|keyword = Security,diversification,downtime loss,software allocation,network effects,risk management,correlated failures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MANAGING CONSUMER PRIVACY CONCERNS IN PERSONALIZATION: A STRATEGIC ANALYSIS OF PRIVACY PROTECTION'''
{{header}}
{{article
|author= Dong-Joo Lee,Jae-Hyeon Ahn,Youngsok Bang,
|source= MIS QUARTERLY
|year= 2011
|abstract = Advances in information technology and e-commerce enable firms to make personalized offers to individual consumers based on information about the consumers. However, the collection and use of private information have caused serious concerns about privacy invasion by consumers, creating a personalization privacy tradeoff The key approach to address privacy concerns is via the protection of privacy through the implementation of fair information practices, a set of standards governing the collection and use of personal information. In this paper, we take a game-theoretic approach to explore the motivation of firms for privacy protection and its impact on competition and social welfare in the context of product and price personalization. We find that privacy protection can work as a competition-mitigating mechanism by generating asymmetry in the consumer segments to which firms offer personalization, enhancing the profit extraction abilities of the firms. In equilibrium, both symmetric and asymmetric choices of privacy protection by the firms can result, depending on the size of the personalization scope and the investment cost of protection. Further, as consumers become more concerned about their privacy, it is more likely that all firms adopt privacy protection. In the perspective of welfare, we show that autonomous choices of privacy protection by personalizing firms can improve social welfare at the expense of consumer welfare. We further find that regulation enforcing the implementation of fair information practices can be efficient from the social welfare perspective mainly by limiting the incentives of the firms to exploit the competition-mitigation effect.
|keyword = Information privacy,consumer privacy concerns,personalization,privacy protection,fair information practices,game theory,competitive analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE MORE, THE MERRIER? HOW THE NUMBER OF PARTNERS IN A STANDARD-SETTING INITIATIVE AFFECTS SHAREHOLDER'S RISK AND RETURN'''
{{header}}
{{article
|author= Nitin Aggarwal,Qizhi Dai,Eric A. Walden,
|source= MIS QUARTERLY
|year= 2011
|abstract = Firms often collaborate with other firms to set information technology standards in order to decrease each firm's individual risk. But does this work? We propose that, in a capital market setting, establishing standards in a group does not decrease the total risk laced by an individual firm's shareholders. However, the market risky its investors lace decrease and idiosyncratic risks increase, changing the risk profiles of the group members. We collected data on standard-setting events from 1996 to 2005. In our dataset, a firm obtained a 4.07 percent, three-day cumulative risk-adjusted return on stock price when engaging in a standard-setting initiative, after controlling event year, firm size, and group size. More importantly, we found that an increase in the number of firms in the group decreased the risk-adjusted abnormal return and the market risk (as measured by beta) of each firm, but increased the idiosyncratic risk (as measured by the variance of firm returns). Our findings suggest that firms electing to participate in a large standardization group obtain a reduction in abnormal returns on stocks on the days of the standard-setting events. They also expect to reduce market risks but increase idiosyncratic risks after the standard-setting events, as compared to firms choosing to participate in a smaller group or attempting to standardize their products unilaterally. This study contributes to the literature on IT standards and standardization, and expands our understanding of the implications of standardization strategy on shareholder risks.
|keyword = Standard,standardization,standard-setting,event study,returns on IT investment,risk of IT investment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPETING PERSPECTIVES ON THE LINK BETWEEN STRATEGIC INFORMATION TECHNOLOGY ALIGNMENT AND ORGANIZATIONAL AGILITY: INSIGHTS FROM A MEDIATION MODEL'''
{{header}}
{{article
|author= Paul P. Tallon,Alain Pinsonneault,
|source= MIS QUARTERLY
|year= 2011
|abstract = Strategic information technology alignment remains a top priority for business and IT executives. Yet with a recent rise in environmental volatility, firms are asking how to be more agile in identifying and responding to market-based threats and opportunities. Whether alignment helps or hurts agility is an unresolved issue. This paper presents a variety of arguments from the literature that alternately predict a positive or negative relationship between alignment and agility. This relationship is then tested using a model in which agility mediates the link between alignment and firm performance under varying conditions of IT infrastructure flexibility and environmental volatility. Using data from a matched survey of IT and business executives in 241 firms, we uncover a positive and significant link between alignment and agility and between agility and firm performance. We also show that the effect of alignment on performance is fully mediated by agility, that environmental volatility positively moderates the link between agility and firm performance, and that agility has a greater impact on firm performance in more volatile markets. While IT infrastructure flexibility does not moderate the link between alignment and agility, except in a volatile environment, we reveal that IT infrastructure flexibility has a positive and significant main effect on agility. In fact, the effect of IT infrastructure flexibility on agility is as strong as the effect of alignment on agility. This research extends and integrates the literature on strategic IT alignment and organizational agility at a time when both alignment and agility are recognized as critical and concurrent organizational goals.
|keyword = Agility,strategic IT alignment,environmental change,volatility,IT infrastructure flexibility,IT rigidity traps,industry clockspeed,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CIO REPORTING STRUCTURE, STRATEGIC POSITIONING, AND FIRM PERFORMANCE'''
{{header}}
{{article
|author= Rajiv D. Banker,Nan Hu,Paul A. Pavlou,Jerry Luftman,
|source= MIS QUARTERLY
|year= 2011
|abstract = Almost 30 years after the introduction of the CIO position, the ideal CIO reporting structure (whether the CIO should report to the CEO or the CFO) is yet to be identified There is an intuitive assumption among some proponents of IT that the CIO should always report to the CEO to promote the importance of IT and the CIO's clout in the firm, while some adversaries of IT call for a CIO CFO reporting structure to keep a tab on IT spending. However, we challenge these two ad hoc prescriptions by arguing that neither CIO reporting structure is necessarily optimal, and that the CIO reporting structure should not be used to gauge the strategic role of IT in the firm. First, extending the strategy structure paradigm, we propose that a firm's strategic positioning (differentiation or cost leadership) should be a primary determinant of its CIO reporting structure. We hypothesize that differentiators are more likely to have their CIO report to the CEO in order to pursue IT initiatives that help the firm's differentiation strategy. We also hypothesize that cost leaders are more likely to have their CIO report to the CFO to lead IT initiatives to facilitate the firm's cost leadership strategy. Second, extending the alignment fit view, we propose that firms that align their CIO reporting structure with their strategic positioning (specifically, differentiation with a CIO CEO reporting structure and cost leadership with a CIO CFO reporting structure) will have superior future performance. Longitudinal data from two periods (1990-1993 and 2006) support the proposed hypotheses, validating the relationship between a firm's strategic positioning and its CIO reporting structure, and also the positive impact of their alignment on firm performance. These results challenge the ad hoc prescriptions about the CIO reporting structure, demonstrating that a CIO CEO reporting structure is only superior for differentiators and a CIO CFO reporting structure is superior only for cost leaders. The CIO reporting structure must, therefore, be designed to align with the firm's strategic positioning, independent of whether IT plays a key strategic role in the firm.
|keyword = Chief information officer (CIO),CIO reporting structure,strategic positioning,Porter's generic strategies,product/service differentiation,cost leadership,firm performance,abnormal stock returns,cash flows from operations,chief executive officer (CEO),chief financial officer (CFO),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Analysis of Incentives for Network Infrastructure Investment Under Different Pricing Strategies'''
{{header}}
{{article
|author= Alok Gupta,Boris Jukic,Dale O. Stahl,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = The Internet is making a significant transition from primarily a network of desktop computers to a network variety of connected information devices such as personal digital assistants and global positioning system-based devices. On the other hand, new paradigms such as overlay networks are defining service-based logical architecture for the network services that make locating content and routing more efficient. Along with Internet2's proposed service-based routing, overlay networks will create a new set of challenges in the provision and management of content over the network. However, a lack of proper infrastructure investment incentive may lead to an environment where network growth may not keep pace with the service requirements. In this paper, we present an analysis of investment incentives for network infrastructure owners under two different pricing strategies: congestion-based negative externality pricing and the prevalent flat-rate pricing. We develop a theoretically motivated gradient-based heuristic to compute maximum capacity that a network provider will be willing to invest in under different pricing schemes. The heuristic appropriates different capacities to different network components based on demand for these components. We then use a simulation model to compare the impact of dynamic congestion-based pricing with flat-rate pricing on the choice of capacity level by the infrastructure provider. The simulation model implements the heuristic and ensures that near-optimal level of capacity is allocated to each network component by checking theoretical optimality conditions. We investigate the impact of a variety of factors, including the per unit cost of capacity of a network resource, average value of the users' requests, average level of users' tolerance for delay, and the level of exogenous demand for services on the network. Our results indicate that relationships between these factors are crucial in determining which of the two pricing schemes results in a higher level of socially optimal network capacity. The simulation results provide a possible explanation for the evolution of the Internet pricing from time-based to flat-rate pricing. The results also indicate that regardless of how these factors are related, the average stream of the net benefits realized under congestion-based pricing tends to be higher than the average net benefits realized under flat-rate pricing. These central results point to the fallacy of the arguments presented by the supporters of net neutrality that do not consider the incentives for private investment in network capacity.
|keyword = Internet pricing,infrastructure investment,simulation,investment incentives,net neutrality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Addressing Digital Inequality for the Socioeconomically Disadvantaged Through Government Initiatives: Forms of Capital That Affect ICT Utilization'''
{{header}}
{{article
|author= J. J. Po-An Hsieh,Arun Rai,Mark Keil,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Digital inequality, or unequal access to and use of information and communication technologies (ICT), is a severe problem preventing the socioeconomically disadvantaged (SED) from participating in a digital society. To understand the critical resources that contribute to digital inequality and inform public policy for stimulating initial and continued ICT usage by the SED, we drew on capital theories and conducted a field study to investigate: (1) the forms of capital for using ICT and how they differ across potential adopters who are SED and socioeconomically advantaged (SEA); (2) how these forms of capitals are relatively impacted for the SEA and the SED through public policy for ICT access; and (3) how each form of capital influences the SED's intentions to use initially and to continue to use ICT. The context for our study involved a city in the southeastern United States that offered its citizens free ICT access for Internet connectivity. Our results show that SED potential adopters exhibited lower cultural capital but higher social capital relative to the SEA. Moreover, the SED who participated in the city's initiative realized greater positive gains in cultural capital, social capital, and habitus than the SEA. In addition, we find that the SED's initial intention to use ICT was influenced by intrinsic motivation for habitus, self-efficacy for cultural capital, and important referents' expectations and support from acquaintances for social capital. Cultural capital and social cultural capital also complemented each other in driving the SED's initial use intention. The SED's continued use intention was affected by both intrinsic and extrinsic motivations for habitus and both knowledge and self-efficacy for cultural capital but was not affected by social capital. We also make several recommendations for future research on digital inequality and ICT acceptance to extend and apply the proposed capital framework.
|keyword = capital theory,habitus,cultural capital,social capital,economic capital,digital divide,digital inequality,ICT policy,socioeconomic inequality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effect of Online Privacy Information on Purchasing Behavior: An Experimental Study'''
{{header}}
{{article
|author= Janice Y. Tsai,Serge Egelman,Lorrie Cranor,Alessandro Acquisti,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Although online retailers detail their privacy practices in online privacy policies, this information often remains invisible to consumers, who seldom make the effort to read and understand those policies. This paper reports on research undertaken to determine whether a more prominent display of privacy information will cause consumers to incorporate privacy considerations into their online purchasing decisions. We designed an experiment in which a shopping search engine interface clearly and compactly displays privacy policy information. When such information is made available, consumers tend to purchase from online retailers who better protect their privacy. In fact, our study indicates that when privacy information is made more salient and accessible, some consumers are willing to pay a premium to purchase from privacy protective websites. This result suggests that businesses may be able to leverage privacy protection as a selling point.
|keyword = privacy,information systems,economics,experimental economics,e-commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using Transaction Prices to Re-Examine Price Dispersion in Electronic Markets'''
{{header}}
{{article
|author= Anindya Ghose,Yuliang Yao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Price dispersion is an important indicator of market efficiency. Internet-based electronic markets have the potential to reduce transaction and search costs, thereby creating more efficient, "frictionless" markets, as predicted by theories in information economics. However, earlier work has reported significant levels of price dispersion on the Internet, which is in contrast to theoretical predictions. A key feature of the existing stream of work has been its use of posted prices to estimate price dispersion. In theory, this can lead to an overestimation of price dispersion because a sale may not have occurred at the posted price. In this research, we use a unique data set of actual transaction prices collected from both the electronic and offline markets of buyers in a business-to-business market to evaluate the extent of price dispersion. We find that price dispersion in the electronic market is as low as 0.22%, which is substantially less than that reported in the existing literature. This near-zero price dispersion suggests that in some electronic markets the "law of one price" can prevail when we consider transaction prices, instead of posted prices. We further develop a theoretical framework that identifies several new drivers of price dispersion using transaction data. In particular, we focus on four product-level and market-level attributes-product cost, order cycle time, own price elasticity, and transaction quantity, and we estimate their impact on price dispersion. We also examine the electronic market's moderating role in the relationship between these drivers and price dispersion. Finally, we estimate the efficiency gains that accrue from transactions in the relatively friction-free market and find that the electronic market can enhance consumer surplus by as much as $97.92 million per year.
|keyword = electronic markets,Internet commerce,price dispersion,transaction prices,demand estimation,consumer surplus,econometrics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Vendor and Client Interaction for Requirements Assessment in Software Development: Implications for Feedback Process'''
{{header}}
{{article
|author= Rajiv Jayanth,Varghese S. Jacob,Suresh Radhakrishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = We study agency problems that arise when prototypes are used for requirements assessment. The precision with which the prototype helps a client assess his requirements depends on (a) the type of prototype provided by the vendor and (b) the client's feedback effort. The vendor can provide either a neutral or nonneutral prototype: The nonneutral prototype influences the client towards one particular set of requirements that may not be the true requirement, and the neutral prototype allows the client to assess his true requirements. This leads to the vendor's moral hazard problem. The client chooses to exert either the high or low feedback effort after the vendor provides the prototype. Because the effort is unobservable to the vendor, it can lead to the client exerting the low feedback effort: the client's commitment problem. In this paper we develop and discuss the role of the contract payment to provide the vendor with incentives to supply the neutral prototype, as well as for the client to commit to the high feedback effort. In this setting, we also examine the "anchoring" effect, wherein even a high-feedback effort can influence the client more toward a particular set of requirements with the nonneutral prototype. Our results highlight the interplay among the feedback effort, anchoring, and vendor payments.
|keyword = requirements assessment,anchoring,software prototyping,game theory,double moral hazard,incentives,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contracting Efficiency and New Firm Survival in Markets Enabled by Information Technology'''
{{header}}
{{article
|author= Anjana Susarla,Anitesh Barua,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Application service providers (ASP), who host and maintain information technology (IT) applications across the Internet, emerged as an innovation in the way IT services are delivered to client firms. In spite of many potential benefits of this model, ASPs experienced business failure and high rates of exit. Drawing on agency theory, we argue that the efficiency of contracting arrangements between ASPs and client organizations is an important determinant of ASP survival. We test this prediction using a unique data set combining multiple sources that allows us to track an ASP from the year of founding through the beginning of 2006. Contractual misalignment, or adopting contracts mismatched with the underlying agency costs, significantly lowers the probability of survival of service providers in the ASP marketplace. The impact of misalignment is particularly severe when coupled with adjustment costs that impede the transition to aligned contracts. To account for potential heterogeneity in ASPs' knowledge of contracting, we test for endogenous self-selection of ASPs in the relationship between contractual misalignment and survival. Our results are robust to a variety of model specifications as well as alternate explanations of survival from multiple theoretical domains.
|keyword = agency theory,contractual misalignment,firm survival,propensity score matching,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Analyzing Sharing in Peer-to-Peer Networks Under Various Congestion Measures'''
{{header}}
{{article
|author= Monica Johar,Syam Menon,Vijay Mookerjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Historically, the use of peer-to-peer (P2P) networks has been limited primarily to user-initiated exchanges of (mostly music) files over the Internet. This traditional view of P2P networks is changing, however, and the use of P2P networks has been suggested for delivering general-purpose content over the Web (or corporate intranets), even in real time. We analyze sharing in a P2P community in this new context under three different congestion measures: delay, jitter, and packet loss. Sharing is important to study in the presence of congestion because most existing research on P2P networks views congestion in the network as a relatively insignificant criterion. However, when delivering general-purpose content, congestion and its relationship to sharing is a critical factor that influences end-user performance. This paper looks at P2P networks from this new perspective by explicitly considering the effects of congestion on user incentives for sharing. We also propose a simple incentive mechanism that induces socially optimal sharing.
|keyword = peer-to-peer networks,congestion,sharing,socially optimal sharing,free-riders,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Experimental Comparison of Linear and Nonlinear Price Combinatorial Auctions'''
{{header}}
{{article
|author= Tobias Scheffel,Alexander Pikovsky,Martin Bichler,Kemal Guler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Combinatorial auctions are used for the efficient allocation of heterogeneous goods and services. They require appropriate software platforms that provide automated winner determination and decision support for bidders. Several promising ascending combinatorial auction formats have been developed throughout the past few years based on primal-dual algorithms and linear programming theory. The ascending proxy auction and iBundle result in Vickrey payoffs when the coalitional value function satisfies buyer submodularity conditions and bidders bid their best responses. These auction formats are based on nonlinear and personalized ask prices. In addition, there are a number of designs with linear prices that have performed well in experiments, the approximate linear prices auction, and the combinatorial clock auction. In this paper, we provide the results of lab experiments that tested these different auction formats in the same setting. We analyze aggregate metrics such as efficiency and auctioneer revenue for small-and medium-sized value models. In addition, we provide a detailed analysis not only of aggregate performance metrics but also of individual bidding behaviour under alternative combinatorial auction formats.
|keyword = laboratory experiments,electronic markets and auctions,decision support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology and Firm Boundaries: Impact on Firm Risk and Return Performance'''
{{header}}
{{article
|author= Sanjeev Dewan,Fei Ren,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = In this paper, we empirically investigate the impact of information technology (IT) investment on firm return and risk financial performance, emphasizing the moderating role of the firm boundary strategies of diversification and vertical integration. Our results indicate a sharp contrast between the direct and interactive effects of IT on both the return (profitability) and risk (variability of returns) dimensions. Although the direct effect of IT capital is to increase firm risk for a given level of return, we find that suitable boundary strategies can moderate the impact of IT on firm performance in a way that increases return and decreases risk, at the margin. This interaction effect is strongest in service firms, in firms with high levels of IT investment intensity, and in more recent time periods. Our results are robust to alternative proxies for firm risk, including an ex ante risk measure (variability of analysts' earnings estimates), and alternative risk-return specifications. Put together, our results provide new insights into how IT and firm boundary strategies interact to affect the risk and return performance of firms.
|keyword = IT investments,risk and return,firm boundaries,diversification,vertical integration,strategic use of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Environmental Uncertainty and IT Infrastructure Governance: A Curvilinear Relationship'''
{{header}}
{{article
|author= Ling Xue,Gautam Ray,Bin Gu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Extant research considers the IT governance choice to be a trade-off between the cost-efficiency of centralization and the responsiveness provided by local information processing. This view predicts that firms tend to decentralize IT governance in more uncertain environments. We investigate this issue by studying the relationship between environmental uncertainty and IT infrastructure governance in a sample of business units from Fortune 1000 companies. The key proposition in this paper is that the relationship between environmental uncertainty and decentralization in IT infrastructure governance is best characterized as a curvilinear relationship. That is, when environmental uncertainty increases from low to high, firms tend to first decentralize their IT infrastructure decisions to the business units to enhance their responsiveness; and then centralize their IT infrastructure decisions to the headquarters as uncertainty increases further, to achieve the benefits of coordination and to mitigate the potential agency problem in uncertain environments. Moreover, the study proposes that business unrelatedness between business units and their headquarters moderates the curvilinear relationship between environmental uncertainty and IT infrastructure governance. We find that both the propositions are supported by the data.
|keyword = IT infrastructure governance,environmental uncertainty,agency theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Punishment, Justice, and Compliance in Mandatory IT Settings'''
{{header}}
{{article
|author= Yajiong Xue,Huigang Liang,Liansheng Wu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = This paper aims to understand the influence of punishment and perceived justice on user compliance with mandatory information technology (IT) policies. Drawing on punishment research and justice theory, a research model is developed. Data collected from a field survey of enterprise resource planning (ERP) users are analyzed to test the proposed hypotheses. The results indicate that IT compliance intention is strongly influenced by perceived justice of punishment, which is negatively influenced by actual punishment. When perceived justice of punishment is considered, the effect of satisfaction on compliance intention decreases and that of perceived usefulness becomes insignificant. This paper contributes to information systems (IS) research and practice by drawing attention to the importance of punishment, particularly perceived justice of punishment, in mandatory IT settings. It delineates the relationships among actual punishment, punishment expectancy, perceived justice of punishment, and IT compliance intention, and thus provides a better understanding of user compliance behavior in mandatory IT settings.
|keyword = punishment,punishment expectancy,distributive justice,procedural justice,informational justice,fairness,mandatory context,compliance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Drivers of the Long Tail Phenomenon: An Empirical Analysis'''
{{header}}
{{article
|author= Oliver Hinz,Jochen Eckert,Bernd Skiera,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = The Internet makes it easy to offer large assortments of products, tempting managers to chase the "long tail" that is, the phenomenon in which niche products gain a significant share of demand among all products. Yet few studies empirically examine the existence and drivers of this long tail phenomenon. This study uses a unique data set with 843,922 purchases from 143,939 customers that a monopolistic video-on-demand operator observed over 111 weeks after its launch of the service. The current analysis centers on the effects of increasing assortment sizes and improved search technologies on measures of the long tail, such as per customer demand, the share of products purchased from the assortment, the distribution of demand across products, and the concentration of demand. Increases in assortment sizes and better assortment quality lead to increases in demand per customer and a longer tail. The length of the tail (i.e., share of purchased products) is also driven by new customers and seasonal effects, such as school vacations, whereas the presence of high-quality blockbuster products shortens the tail. Different search technologies can shift demand toward niche products as well as toward blockbuster products.
|keyword = electronic commerce,long tail,recommendation systems,search technology,video-on-demand,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict'''
{{header}}
{{article
|author= Ofer Arazy,Oded Nov,Raymond Patterson,Lisa Yeo,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (I) the diversity in members' knowledge bases as a determinant of Wikipedia's content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (I) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict notwithstanding its contribution to creative abrasion can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.
|keyword = co-creation,cognitive diversity,collaboration,community-based production,group composition,information quality,task conflict,Wikipedia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Influence of Industry Characteristics on Information Technology Outsourcing'''
{{header}}
{{article
|author= Wen Guang Qu,Alain Pinsonneault,Wonseok Oh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Despite the extensive research on information technology (IT) outsourcing, our knowledge and understanding of how industry characteristics impact the use of IT outsourcing remain limited. Drawing upon theories from organization behavior and industrial economics, this study identifies four major industry characteristics (i.e., munificence, dynamism, concentration, and capital intensity) and investigates how each of these factors affects the use of IT outsourcing. Specifically, we postulate that the extent of industry munificence is positively related to the utilization of IT outsourcing. Since timely strategic actions are the crucial aspects of leveraging munificent resources, IT outsourcing, which can be implemented in short periods of time, is considered to be a preferred option in such environments. Furthermore, industry dynamism is also positively associated with IT outsourcing, given that firms in dynamically evolving industries tend to look for flexibility and avoid a large amount of fixed investments (e.g., IT development in-house). In contrast to these hypotheses, we predict that industry concentration is negatively related to IT outsourcing. Firms in concentrated industries are likely to develop their own IT infrastructures, as they are not constrained by institutional pressures or cost-driven strategic actions. Finally, because firms in capital-intensive industries tend to conform to long-standing traditional practices, and do not highly value novel and risky practices, they will be less likely to use IT outsourcing than firms in industries with low capital intensity. The data from the U.S. Bureau of Economic Analysis along with Compustat empirically validated all of the proposed hypotheses; however, only marginal support was found for the association between industry concentration and IT outsourcing. Our findings offer business executives and IT service providers strategic and managerial insights into the dynamics and complexities involved in the diverse aspects of industry environments and IT outsourcing decisions.
|keyword = capital intensity,industry concentration,industry dynamism,industry environments,industry munificence,IT outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Productivity and Performance Effects of Business Process Reengineering: A Firm-Level Analysis'''
{{header}}
{{article
|author= Kemal Altinkemer,Yasin Ozcelik,Zafer D. Ozdemir,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = We empirically investigate whether business process reengineering (BPR), which requires substantial investment in information technology to integrate separate tasks into complete cross-functional processes, is associated with enhanced firm productivity and performance. We analyze firm-level panel data covering the period 1987-2008 using fixed effects and first differencing, standard methods that account for unobservable firm-level effects. We find that return on assets drops significantly during the project initiation year. According to fixed effects results, the performance and productivity measures improve in a decreasing manner after project initiation, suggesting that BPR indeed positively affects firm performance on average. We also find that enterprise-wide BPR projects are associated with more negative returns during project initiation than functionally focused projects. However, there is no clear evidence regarding their superiority over functionally focused BPR projects in terms of performance improvements after project initiation, perhaps because grand projects are risky and sometimes lead to grand failures.
|keyword = business process reengineering,business value of information technology,panel regression,productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Privacy Concerns Versus Desire for Interpersonal Awareness in Driving the Use of Self-Disclosure Technologies: The Case of Instant Messaging in Two Cultures'''
{{header}}
{{article
|author= Paul Benjamin Lowry,Jinwei Cao,Andrea Everard,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Social computing technologies typically have multiple features that allow users to reveal their personal information to other users. Such self-disclosure (SD) behavior is generally considered positive and beneficial in interpersonal communication and relationships. Using a newly proposed model based on social exchange theory, this paper investigates and empirically validates the relationships between SD technology use and culture. In particular, we explore the effects of culture on information privacy concerns and the desire for online interpersonal awareness, which influence attitudes toward, intention to use, and actual use of SD technologies. Our model was tested using arguably the strongest social computing technology for online SD instant messaging (IM) with users from China and the United States. Our findings reveal that cross-cultural dimensions are significant predictors of information privacy concerns and desire for online awareness, which are, in turn, found to be predictors of attitude toward, intention to use, and actual use of IM. Overall, our proposed model is applicable to both cultures. Our findings enhance the theoretical understanding of the effects of culture and privacy concerns on SD technologies and provide practical suggestions for developers of SD technologies, such as adding additional control features to applications.
|keyword = instant messaging,privacy,self-disclosure,self-disclosure technologies,social computing technologies,social exchange theory,theory of reasoned action,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Network Externalities on the Competition Between Open Source and Proprietary Software'''
{{header}}
{{article
|author= Hsing Kenneth Cheng,Yipeng Liu,Qian (Candy) Tang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = In this paper, we build analytical models to examine the impact of network externalities on the competition between open source software (OSS) and proprietary software. We investigate the competing OSS and proprietary software products with comparable functionalities in four different scenarios depending on whether they are compatible with each other and whether the underlying market is fully covered (i.e., all consumers adopt one of the two products). Furthermore, we study which party has the most incentive to make its product compatible with its counterpart. When the market is fully covered, the installed base and the profit of proprietary software increase at the expense of a decreasing user base for OSS in the presence of network externalities. This competitive imbalance becomes more pronounced when OSS and proprietary software are incompatible and the market is partially covered. Finally, we find that in the presence of network externalities, being compatible with its rival is not desirable for the proprietary software, but highly beneficial to the OSS community.
|keyword = competition,network externalities,open source software,software compatibility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer Acceptance of Recommendations by Interactive Decision Aids: The Joint Role of Temporal Distance and Concrete Versus Abstract Communications'''
{{header}}
{{article
|author= Clemens F. Koehler,Els Breugelmans,Benedict G. C. Dellaert,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Interactive decision aids (IDAs) typically use concrete, feature based approaches to interact with consumers. Recently, however, interaction designs that focus on communicating abstract consumer needs have been suggested as a promising alternative. This paper investigates how temporal distance moderates the effectiveness of these two competing IDA communication designs by its effect on consumers' mental representation of the product decision problem. Temporal distance is inherently connected to IDAs in two ways. Congruency between consumption timing (immediate versus distant) and IDA communication design (concrete versus abstract, respectively) increases the likelihood to accept the IDA's advice. This effect is also achieved by congruency between IDA process timing (immediate versus delayed delivery of recommendations) and IDA communication design (concrete versus abstract, respectively). We further show that this process is mediated by the perceived transparency of the IDA process. Managers and researchers need to take into account the importance of congruency between the user and the interface through which companies interact with their users and can further optimize IDAs so that they better match consumers' mental representations.
|keyword = construal level theory,consumer behavior,e-commerce,interactive decision aids,recommenders,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Mitigating Vendor Silence in Offshore Outsourcing: An Empirical Investigation'''
{{header}}
{{article
|author= Radhika P. Jain,Judith C. Simon,Robin S. Poston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = The tendency to remain silent about project-related issues can contribute to suboptimal project performance or project failure. Prior research in offshore outsourcing suggests that client managers should play a critical role to induce offshore vendors' employees not only to report project problems in a timely fashion but also to brainstorm and contribute ideas to a project. Also, the extant research on cross-cultural teams has emphasized the importance of cultural adaptation in the smooth functioning of these teams, but the role of cultural adaptation in silence mitigation has been largely underdeveloped in the literature. In this research, we bring these concepts of vendor silence and cultural adaptation in cross-cultural teams together and develop a process framework that illustrates how vendor silence may be mitigated in offshore outsourcing through various silence mitigation mechanisms. We then develop three propositions for organizational action toward mitigating vendor silence, which highlight the mediating role of cultural adaptation.
|keyword = collaboration effectiveness,IT outsourcing,mum effect,offshore outsourcing,offshore project management,organizational silence,silence mitigation,vendor silence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Individual Virtual Competence and Its Influence on Work Outcomes'''
{{header}}
{{article
|author= Yinglei Wang,Nicole Haggerty,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2011
|abstract = Witnessing both opportunities and challenges in virtual work arrangements, researchers have explored a number of technological, social, and organizational factors in order to improve virtual work effectiveness. However, there is limited understanding of an important element of virtual work the individuals. Our review of the literature indicates that the composition of individual knowledge, skills, and abilities (KSAs) required to work virtually would benefit from further research. In this study, we theoretically and empirically develop the construct of individual virtual competence that captures the key KSAs required to perform effectively in today's virtualized workplace, within a parsimonious nomological network. Substantiated by its explanatory power on individual perceived performance and satisfaction, individual virtual competence contributes to the literature by acknowledging a distinct workplace competency that can be incorporated in future individual-level studies of virtual phenomena. This research provides managers with a lens to understand differences in individual work outcomes and provides a lever to developing individuals' capabilities so as to improve work outcomes.
|keyword = individual virtual competence,individual work outcomes,virtual organization,virtual work,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing the Versions of a Software Product Under Variable and Endogenous Demand'''
{{header}}
{{article
|author= Kutsal Dogan,Yonghua Ji,Vijay S. Mookerjee,Suresh Radhakrishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Software product versioning (i.e., upgrading the product after its initial release) is a widely adopted practice followed by leading software providers such as Microsoft, Oracle, and IBM. Unlike conventional durable goods, software products are relatively easy to upgrade, making upgrades a strategic consideration in commercial software production. We consider a two-period model with a monopoly software provider who develops and releases a software product to the market. Unlike previous research, we consider demand variability and endogeneity to determine the functionality of the software in the first and second periods. Demand endogeneity is the impact of the word-of-mouth effect that positively relates the features in the initial release of the product to its demand in the second period. We also determine the design effort that should be spent in the first period to prepare for upgrading the product in the second period-upgrade design effort-to tap into the possible future demand. Results show that the upgrade design effort can be lower or higher when there is more market demand uncertainty. We also show that the features of the product in its initial release and upgrade design effort can be complements as well as substitutes, depending on the strength of the word-of-mouth effect. The results in this paper provide insights into how demand-side factors (market demand variability or demand endogeneity) can influence supply-side decisions (initial features and upgrade design effort). A key insight of the analysis is that a high word-of-mouth effect helps manage the product in the face of demand variability.
|keyword = software upgrades,demand endogeneity,upgrade design effort,demand variability,upgrade strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Firms as Incubators of Open-Source Software'''
{{header}}
{{article
|author= Amit Mehra,Rajiv Dewan,Marshall Freimer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Many successful open-source projects have been developed by programmers who were employed by firms but worked on open-source projects on the side because of economic incentives like career improvement benefits. Such side work may be a good thing for the employing firms, too, if they get some strategic value from the open-source software and if the productivity of the programmers on these projects improves through learning-by-doing effects. However, the programmers may work more or less on these projects than what is best for the firms. To manage the programmers' efforts, the firms set appropriate employment policies and incentives. These policies and career concerns then together govern the programmers' effort allocation between the open-source and proprietary projects. We examine this relationship using a variant of the principal/agent model. We derive and characterize optimal employment contracts and show that firms either offer a bonus for only one of the two projects or do not offer any bonuses. However, if attractive alternate employment opportunities are available, they change their strategy and may offer bonuses for both projects simultaneously.
|keyword = open-source software,programmer incentives,programmer compensation,learning by doing,principal/agent,signalling,game theory,business models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the Strategic Alignment and Implementation Success of a KMS: A Subculture-Based Multilevel Analysis'''
{{header}}
{{article
|author= M. N. Ravishankar,Shan L. Pan,Dorothy E. Leidner,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Two important gaps exist in the information systems (IS) alignment research. First, there is scant research on the potential of organizational culture, and specifically subcultures to influence the strategic alignment of IS and organizations. Second, there is a dearth of literature that considers the relationship between alignment and implementation success. In this paper, we address both of these gaps by considering the influence of organizational subcultures on the alignment of a specific IS-a knowledge management system (KMS)-with organizational strategy. Our analysis demonstrates the important roles played by three different subcultures-enhancing, countercultural, and chameleon-in the alignment of the KMS. The analysis also underscores the complementary nature of the alignment and implementation literatures and suggests that they should be used in concert to explain the success of an IS. Drawing on our analysis, we build a subculture model, which depicts the intersection of alignment and implementation. From a managerial perspective, the subculture model highlights three different approaches to managing alignment and implementation. From a theoretical perspective, our paper highlights the need for IS alignment models to be modified, so that subunit-level analyses are incorporated. It also illustrates that organizations confront challenges of alignment and implementation simultaneously rather than sequentially.
|keyword = strategic alignment,information systems implementation,knowledge management systems,organizational subcultures,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Debate on Net Neutrality: A Policy Perspective'''
{{header}}
{{article
|author= Hsing Kenneth Cheng,Subhajyoti Bandyopadhyay,Hong Guo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = The status quo of prohibiting broadband service providers from charging websites for preferential access to their customers-the bedrock principle of net neutrality (NN)-is under fierce debate. We develop a game-theoretic model to address two critical issues of NN: (1) Who are gainers and losers of abandoning NN? (2) Will broadband service providers have greater incentive to expand their capacity without NN? We find that if the principle of NN is abolished, the broadband service provider stands to gain from the arrangement, as a result of extracting the preferential access fees from content providers. Content providers are thus left worse off, mirroring the stances of the two sides in the debate. Depending on parameter values in our framework, consumer surplus either does not change or is higher in the short run. When compared to the baseline case under NN, social welfare in the short run increases if one content provider pays for preferential treatment but remains unchanged if both content providers pay. Finally, we find that the incentive to expand infrastructure capacity for the broadband service provider and its optimal capacity choice under NN are higher than those under the no-net-neutrality (NNN) regime, except in some specific cases. Under NN, the broadband service provider always invests in broadband infrastructure at the socially optimal level but either under-or overinvests in infrastructure capacity in the absence of NN.
|keyword = net neutrality,economics of net neutrality,broadband service providers,content providers,consumer surplus,social welfare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Price Formats as a Source of Price Dispersion: A Study of Online and Offline Prices in the Domestic US Airline Markets'''
{{header}}
{{article
|author= Ramnath K. Chellappa,Raymond G. Sin,S. Siddarth,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = A large body of research in economics, information systems, and marketing has sought to understand sources of price dispersion. Previous empirical work has mainly offered consumer- and/or product-based explanations for this phenomenon. In contrast, our research explores the key role played by vendors' price-format adoption in explaining price dispersion. We empirically analyze over a half-million online and offline prices offered by major U. S. airlines in the top 500 domestic markets. Our study shows that a vendor's price format remains an important source of price dispersion in both channels even after accounting for other factors known to impact dispersion in airline ticket prices. Importantly, this finding is true for both transacted and posted tickets. We document several other interesting empirical findings. First, the lower variance in the prices of "everyday low price" (EDLP) firms serves to reduce the market-level dispersion in prices when such firms are present. Moreover, the price variance of non-EDLP firms in these markets is also lower than in those markets in which EDLP competitors are absent. Second, we also find that dispersion in offered prices increases closer to the departure date, which is consistent with theoretical assertion that price dispersion increases with reservation prices. Finally, we continue to observe dispersion of online prices even after accounting for vendor strategy and other known sources of dispersion, suggesting that the prices are unlikely to converge even in the presence of sophisticated online search mechanisms.
|keyword = online markets,price dispersion,airline industry,EDLP,hierarchical linear modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REQUEST: A Query Language for Customizing Recommendations'''
{{header}}
{{article
|author= Gediminas Adomavicius,Alexander Tuzhilin,Rong Zheng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper, we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. This paper also presents the recommendation algebra RA, shows how REQUEST recommendations can be mapped into this algebra, and analyzes the expressive power of the query language and the algebra. This paper also shows how users can customize their recommendations using REQUEST queries through a series of examples.
|keyword = personalization,recommender systems,recommendation query language,multidimensional recommendations,contextual recommendations,recommendation algebra,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Finite Mixture Logit Model to Segment and Predict Electronic Payments System Adoption'''
{{header}}
{{article
|author= Ravi Bapna,Paulo Goes,Kwok Kee Wei,Zhongju Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Despite much hype about electronic payments systems (EPSs), a 2004 survey establishes that close to 80% of between-business payments are still made using paper-based formats. We present a finite mixture logit model to predict likelihood of EPS adoption in business-to-business (B2B) settings. Our model simultaneously classifies firms into homogeneous segments based on firm-specific characteristics and estimates the model's coefficients relating predictor variables to EPS adoption decisions for each respective segment. While such models are increasingly making their presence felt in the marketing literature, we demonstrate their applicability to traditional information systems (IS) problems such as technology adoption. Using the finite mixture approach, we predict the likelihood of EPS adoption using a unique data set from a Fortune 100 company. We compare the finite mixture model with a variety of traditional approaches. We find that the finite mixture model fits the data better, controlling for the number of parameters estimated; that our explicit model-based segmentation leads to a better delineation of segments; and that it significantly improves the predictive accuracy in holdout samples. Practically, the proposed methodology can help business managers develop actionable segment-specific strategies for increasing EPS adoption by their business partners. We discuss how the methodology is potentially applicable to a wide variety of IS research.
|keyword = finite mixture model,logistic regression,market segmentation,clustering analysis,hierarchical logit regression,electronic payments systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Profiting from Knowledge Management: The Impact of Time and Experience'''
{{header}}
{{article
|author= Dong-Gil Ko,Alan R. Dennis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Although many organizations are implementing knowledge management systems (KMS), there is little empirical evidence about whether KMS use can improve individual performance, and how time and experience influence the value derived from KMS use. Using hierarchical linear modeling (HLM) statistical analysis, we examined the impact of using a codification-based KMS on the sales performance of 2,154 sales representatives in a pharmaceutical firm over a 24-month period. We found that KMS had significant positive impacts on individual performance and that these performance benefits grew over time. Moreover, experience moderated the relationship between KMS use and individual performance. Knowledge workers with more experience were able to more quickly absorb and apply the knowledge from the KMS than were those with less experience, who took longer to benefit from KMS use. However, over time experience played a diminishing role in leveraging performance gains from KMS use, and knowledge workers with less experience eventually derived similar performance benefits as those of their more experienced counterparts.
|keyword = knowledge management systems use,performance improvement,time,job experience,longitudinal study,hierarchical linear modeling (HLM),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competing "Creatively" in Sponsored Search Markets: The Effect of Rank, Differentiation Strategy, and Competition on Performance'''
{{header}}
{{article
|author= Animesh Animesh,Siva Viswanathan,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = Although efficiency-enhancing features of online markets have been well studied, much less is known about firms' differentiation strategies in these competitive markets or the outcomes of such differentiation. This study examines competition among firms in online sponsored search markets-one of the fastest growing and most competitive of online markets. We develop and test a model that predicts the clickthrough rate (CTR) of a seller's listing in a sponsored search setting. Drawing on consumer search theory and competitive positioning strategies, we theorize that CTR is jointly driven by a seller's positioning strategy as reflected by the unique selling proposition (USP) in its "ad creative," by its rank in a sponsored search listing, and by the nature of competition around the focal firm's listing. We use data from a field experiment conducted by a leading firm in the mortgage industry where the firm varied its rank and USP dynamically. Results suggest that sponsored search listings can act as effective customer segmentation mechanisms, consistent with a model of consumer search in directional markets. We further find that the effect on CTR of a firm's positioning strategy and its rank in a listing is strongly moderated by its ability to differentiate itself from adjacent rivals. We discuss the implications of our findings for sellers' strategies in sponsored search markets and for extending the understanding of consumer search behavior in directional markets.
|keyword = online competition,online differentiation,e-commerce,Internet marketing,online search behavior,sponsored search,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Conceptualizing and Testing a Social Cognitive Model of the Digital Divide'''
{{header}}
{{article
|author= Kwok-Kee Wei,Hock-Hai Teo,Hock Chuan Chan,Bernard C. Y. Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = The digital divide has loomed as a public policy issue for over a decade. Yet, a theoretical account for the effects of the digital divide is currently lacking. This study examines three levels of the digital divide. The digital access divide (the first-level digital divide) is the inequality of access to information technology (IT) in homes and schools. The digital capability divide (the second-level digital divide) is the inequality of the capability to exploit IT arising from the first-level digital divide and other contextual factors. The digital outcome divide (the third-level digital divide) is the inequality of outcomes (e. g., learning and productivity) of exploiting IT arising from the second-level digital divide and other contextual factors. Drawing on social cognitive theory and computer self-efficacy literature, we developed a model to show how the digital access divide affects the digital capability divide and the digital outcome divide among students. The digital access divide focuses on computer ownership and usage in homes and schools. The digital capability divide and the digital outcome divide focus on computer self-efficacy and learning outcomes, respectively. This model was tested using data collected from over 4,000 students in Singapore. The results generate insights into the relationships among the three levels of the digital divide and provide a theoretical account for the effects of the digital divide. While school computing environments help to increase computer self-efficacy for all students, these factors do not eliminate knowledge the gap between students with and without home computers. Implications for theory and practice are discussed.
|keyword = digital divide,social cognitive theory,computer ownership,school IT environment,computer self-efficacy,learning outcomes,adoption and impact of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Compatibility and Proprietary Standards: The Impact of Conversion Technologies in IT Markets with Network Effects'''
{{header}}
{{article
|author= Charles Zhechao Liu,Esther Gal-Or,Chris F. Kemerer,Michael D. Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2011
|abstract = In markets that exhibit network effects, the presence of digital conversion technologies provides an alternative mechanism to achieve compatibility. This study examines the impact of conversion technologies on market equilibrium in the context of sequential duopoly competition and proprietary technology standards. We analyze this question by departing from the extant literature to endogenize the decision to provide a converter and incorporate explicit negotiations between firms concerning the extent of conversion. We argue that these choices better reflect the environment facing firms in digital goods industries and find that these decisions change some of the established results in the literature. Specifically, we find that unless network effects are very large, the subgame-perfect equilibrium (SPNE) involves firms' agreeing to provide digital converters at a sufficiently low price to all consumers. At this equilibrium, both the entrant and the incumbent are better off because the provision of converters alleviates price competition in the market and leads to both higher product revenues and higher proceeds from the sale of converters. Moreover, under some circumstances, the provision of converters is welfare enhancing. These findings have important implications for research and practice in the adoption of new digital goods as the introduction of conversion technologies can reduce the social costs of standardization without compromising the benefits of network effects.
|keyword = network effects,conversion technologies,compatibility,technology standards,digital goods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PROFILING THE RESEARCH PRODUCTIVITY OF TENURED INFORMATION SYSTEMS FACULTY AT US INSTITUTIONS'''
{{header}}
{{article
|author= Douglas L. Dean,Paul Benjamin Lowry,Sean Humpherys,
|source= MIS QUARTERLY
|year= 2011
|abstract = How many articles in highly rated journals do Information Systems research faculty publish to earn tenure? Which journals are highly rated outlets? Tenure candidates, promotion and tenure committees, and those who are asked to write external letters are frequently called upon to answer such questions. When Dennis et al. (2006) examined all IS Ph.D. graduates entering academic careers, few faculty had published enough articles in 20 "elite" journals in six years to meet tenure research expectations at research-intensive schools. Our study builds on the dialog started by Dennis et al. In our study, we counted the number of journal articles at the point of tenure for faculty who earned tenure within five to seven years after their Ph.D. graduation date. We also examined the effect of acknowledging different sets of journals as highly rated on the publication rates of faculty who earned tenure. Specifically, we examined the effects of expanding on Dennis et al. by including MIS Quarterly, Information Systems Research, Journal of Management Information Systems, Journal of the AIS, Information Systems Journal, European Journal of Information Systems, Journal of Information Technology, and Journal of Strategic Information Systems in the journal basket. We also looked at the effect of acknowledging highly rated non-IS business journals and highly rated computer science and engineering journals. Finally, we present journal publication benchmarks based on these findings for different types of research institutions.
|keyword = Tenure standards,publication standards,publication benchmarks,faculty productivity,scientometrics,Carnegie classification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A SET OF PRINCIPLES FOR CONDUCTING CRITICAL RESEARCH IN INFORMATION SYSTEMS'''
{{header}}
{{article
|author= Michael D. Myers,Heinz K. Klein,
|source= MIS QUARTERLY
|year= 2011
|abstract = While criteria or principles for conducting positivist and interpretive research have been widely discussed in the IS research literature, criteria or principles for critical research are lacking. Therefore, the purpose of this paper is to propose a set of principles for the conduct of critical research in information systems. We examine the nature of the critical research perspective, clarify; its significance, and review its major discourses, recognizing that its mission and methods cannot be captured by a fixed set of criteria once and for all, particularly as multiple approaches are still in the process of defining their identity However, we suggest it is possible to formulate a set of principles capturing some of the commonalities of those approaches that have so far become most visible in the IS research literature. The usefulness of the principles is illustrated by analyzing three critical field studies in information systems. We hope that this paper will further reflection and debate on the important subject of grounding critical research methodology.
|keyword = Research methods,critical research,interpretive perspective,critical perspective,ethics,values,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ACTION DESIGN RESEARCH'''
{{header}}
{{article
|author= Maung K. Sein,Ola Henfridsson,Sandeep Purao,Matti Rossi,Rikard Lindgren,
|source= MIS QUARTERLY
|year= 2011
|abstract = Design research (DR) positions information technology artifacts at the core of the Information Systems discipline. However, dominant DR thinking takes a technological view of the IT artifact, paying scant attention to its shaping by the organizational context. Consequently, existing DR methods focus on building the artifact and relegate evaluation to a subsequent and separate phase. They value technological rigor at the cost of organizational relevance, and fail to recognize that the artifact emerges from interaction with the organizational context even when its initial design is guided by the researchers' intent. We propose action design research (ADR) as a new DR method to address this problem. ADR reflects the premise that IT artifacts are ensembles shaped by the organizational context during development and use. The method conceptualizes the research process as containing the inseparable and inherently interwoven activities of building the IT artifact, intervening in the organization, and evaluating it concurrently. The essay describes the stages of A DR and associated principles that encapsulate its underlying beliefs and values. We illustrate ADR through a case of competence management at Volvo IT.
|keyword = Action design research,action research,design research,emergence,ensemble artifact,organizational intervention,research method,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DO ONTOLOGICAL DEFICIENCIES IN MODELING GRAMMARS MATTER?'''
{{header}}
{{article
|author= Jan Recker,Michael Rosemann,Peter Green,Marta Indulska,
|source= MIS QUARTERLY
|year= 2011
|abstract = Conceptual modeling grammars are a fundamental means for specifying information systems requirements. However, the actual usage of these grammars is only poorly understood. In particular, little is known about how properties of these grammars inform usage beliefs such as usefulness and ease of use. In this paper, we use an ontological theory to describe conceptual modeling grammars in terms of their ontological deficiencies, and formulate two propositions in regard to how these ontological deficiencies influence primary usage beliefs. Using BPMN OS an example modeling grammar, we surveyed 528 modeling practitioners to test the theorized relationships. Our results show that users of conceptual modeling grammars perceive ontological deficiencies to exist, and that these deficiency perceptions are negatively associated with usefulness and ease of use of these grammars. With our research, we provide empirical evidence in support of the predictions of the ontological theory of modeling grammar expressiveness, and we identify previously unexplored links between conceptual modeling grammars and grammar usage beliefs. This work implies fur practice a much closer coupling of the act of (re-)designing modeling grammars with usage-related success metrics.
|keyword = Conceptual modeling,perception measurement,usage behavior,ontology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PRICE DISCRIMINATION IN E-COMMERCE? AN EXAMINATION OF DYNAMIC PRICING IN NAME-YOUR-OWN PRICE MARKETS'''
{{header}}
{{article
|author= Oliver Hinz,Ii-Horn Hann,Martin Spann,
|source= MIS QUARTERLY
|year= 2011
|abstract = The enhanced abilities of online retailers to learn about their customers' shopping behaviors have increased fears of dynamic pricing, a practice in which a seller sets prices based on the estimated buyer's willingness-to-pay. However, among online retailers, a deviation from a one-price-for-all policy is the exception. When price discrimination is observed, it is often in the context of customer outrage about unfair pricing. One setting where pricing varies is the name-your-own-price (NYOP) mechanism, hi contrast to a typical retail setting, in NYOP markets, it is the buyer who places an initial offer. This offer is accepted if it is above some threshold price set by the seller. If the initial offer is rejected, the buyer can update her offer in subsequent rounds. By design, the final purchase price is opaque to the public; the price paid depends on the individual buyer's willingness-to-pay and offer strategy. Further, most forms of NYOP employ a fixed threshold price policy. In this paper, we compare a fixed threshold price setting with an adaptive threshold price setting. A seller who considers an adaptive threshold price has to weigh potentially greater profits against customer objections about the perceived fairness of such a policy. We first derive the optimal strategy for the seller. We analyze the effectiveness of an adaptive threshold price vis-a-vis a fixed threshold price on seller profit and customer satisfaction. Further, we evaluate the moderating effect of revealing the threshold price policy (adaptive versus fixed) to buyers. We test our model in a series of laboratory experiments and in a large field experiment at a prominent NYOP seller involving real purchases. Our results show that revealing the usage of an adaptive mechanism yields higher profits and more transactions than not revealing this information. In the field experiment, we find that applying a revealed adaptive threshold price can increase profits by over 20 percent without lowering customer satisfaction.
|keyword = Name-your-own-price,bargaining games,dynamic pricing,electronic commerce,customer satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STUCK IN THE CONFLICTED MIDDLE: A ROLE-THEORETIC PERSPECTIVE ON B2B E-MARKETPLACES'''
{{header}}
{{article
|author= Hope Koch,Ulrike Schultze,
|source= MIS QUARTERLY
|year= 2011
|abstract = Over the years, research on the implications of information technology on network governance structures has explored the "move to the market" and the "move to the middle" hypotheses. The middle is a space in which the logic and modalities of markets and hierarchies are intermingled. There is increasing evidence that most network relations reflect mixed-mode or hybrid logic. Despite the apparent advantages that make the middle so populous or "swollen" (Hennart 1993, p. 472), Kambil et al. (1999) highlight that it is riddled with uncertainty and high transaction costs. They label it "the conflicted middle" and propose that online marketplaces, specifically all-in-one markets, are capable of resolving this conflict. Unfortunately, however, Kambil et al. provide limited insight into both the nature of the conflict that plagues the middle and the ability of all-in-one markets to resolve it. To address these questions, this paper applies a role-theoretic perspective to the study of an e-marketplace that served the energy industry and evolved into an all-in-one market. Relying on an interpretive case study, this paper addresses the following research questions: (I) What is the nature of the conflict that characterizes the conflicted middle? (2) How do e-marketplaces, specifically all-in-one markets, help resolve this conflict? Our research highlights that brokers, trading partners, and agents who operate in the middle (where the contradictory logic of markets and hierarchies are mixed) experience goal, behavior, and identity conflict. All-in-one markets can help resolve these conflicts by supporting role integration at the group level and role segmentation at the individual level.
|keyword = B2B e-marketplace,all-in-one markets,network governance structures,market,hierarchy,move to the middle,role theory,role conflict,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHEN FLEXIBLE ROUTINES MEET FLEXIBLE TECHNOLOGIES: AFFORDANCE, CONSTRAINT, AND THE IMBRICATION OF HUMAN AND MATERIAL AGENCIES'''
{{header}}
{{article
|author= Paul M. Leonardi,
|source= MIS QUARTERLY
|year= 2011
|abstract = Employees in many contemporary organizations work with flexible routines and flexible technologies. When those employees find that they are unable to achieve their goals in the current environment, how do they decide whether they should change the composition of their routines or the materiality of the technologies with which they work? The perspective advanced in this paper suggests that the answer to this question depends on how human and material agencies-the basic building blocks common to both routines and technologies-are imbricated. Imbrication of human and material agencies creates infrastructure in the form of routines and technologies that people use to cam, out their work. Routine or technological infrastructure used at any given moment is the result of previous imbrications of human and material agencies. People draw on this infrastructure to construct a perception that a technology either constrains their ability to achieve their goals, or that the technology affords the possibility of achieving new goals. The case of a computer simulation technology for automotive design used to illustrate this framework suggests that perceptions of constraint lead people to change their technologies while perceptions of affordance lead people to change their routines. This imbrication metaphor is used to suggest how a human agency approach to technology can usefully incorporate notions of material agency into its explanations of organizational change.
|keyword = Affordances,agency,materiality,routines,organizational change,technological change,perception,imbrication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PRODUCT-RELATED DECEPTION IN E-COMMERCE: A THEORETICAL PERSPECTIVE'''
{{header}}
{{article
|author= Bo Xiao,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2011
|abstract = With the advent of e-commerce, the potential of new Internet technologies to mislead or deceive consumers has increased considerably. This paper extends prior classifications of deception and presents a typology of product-related deceptive information practices that illustrates the various ways in which online merchants can deceive consumers via e-commerce product websites. The typology can be readily used as educational material to promote consumer awareness of deception in e-commerce and as input to establish benchmarks for good business practices :for online companies. In addition, the paper develops an integrative model and a set of theory-based propositions addressing why consumers are deceived by the various types of deceptive information practices and what factors contribute to consumer success (or failure,) in detecting such deceptions. The model not only enhances our conceptual understanding of the phenomenon of product-based deception and its outcomes in e-commerce but also serves as a foundation further theoretical and empirical investigations. Moreover, a better understanding of the factors contributing to or inhibiting deception detection can also help government agencies and consumer organizations design more effective solutions to fight online deception.
|keyword = Product-based information practices,electronic commerce,typology,stimulus-organism-response framework,model of deception detection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TRANSDISCIPLINARY PERSPECTIVES ON ENVIRONMENTAL SUSTAINABILITY: A RESOURCE BASE AND FRAMEWORK FOR IT-ENABLED BUSINESS TRANSFORMATION'''
{{header}}
{{article
|author= Steve Elliot,
|source= MIS QUARTERLY
|year= 2011
|abstract = The quality and future of human existence are directly related to the condition of our natural environment, but we are damaging the environment. Scientific evidence has mounted a compelling case that human behavior is responsible for deterioration in the Earth's natural environment, with the rate of deterioration predicted to increase in the future. Acknowledging this evidence, the governments of 192 countries have formally agreed to take action to resolve problems with the climate system, one of the most highly stressed parts of the natural environment. While the intention is clear, the question of how best to proceed is not. The research reported here undertook a three-phase approach of selecting, analyzing, and synthesizing relevant literature to develop a holistic, transdisciplinary, integrative framework for IT-enabled business transformation. The focus on business transformation is because business is recognized as being a critical contributor in realizing the challenges of environmental sustainability due to its potential capacity for innovation and change locally, nationally, and globally This article also serves as a resource base for researchers to begin to undertake significant information systems and multidisciplinary work toward the goal of environmental sustainability. Through selection and analysis of illustrative examples of current work from 12 academic disciplines across 6 core categories, the framework addresses the key issues of uncertainty: (1) What is meant by environmental sustainability? (2) What are its major challenges? (3) What is being done about these challenges? (4) What needs to be done?
|keyword = Environmental sustainability,IT-enabled business transformation,literature review,transdisciplinary framework,technology,information systems (IS),information technology (IT),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW INFORMATION MANAGEMENT CAPABILITY INFLUENCES FIRM PERFORMANCE'''
{{header}}
{{article
|author= Sunil Mithas,Narayan Ramasubbu,V. Sambamurthy,
|source= MIS QUARTERLY
|year= 2011
|abstract = How do information technology capabilities contribute to firm performance? This study develops a conceptual model linking IT-enabled information management capability with three important organizational capabilities (customer management capability, process management capability, and performance management capability). We argue that these three capabilities mediate the relationship between information management capability and firm performance. We use a rare archival data set from a conglomerate business group that had adopted a model of performance excellence for organizational transformation based on the Baldrige criteria. This data set contains actual scores from high quality assessments of firms and intraorganizational units of the conglomerate, and hence provides unobtrusive measures of the key constructs to validate our conceptual model. We find that information management capability plays an important role in developing other firm capabilities for customer management, process management, and performance management. In turn, these capabilities favorably influence customer, financial, human resources, and organizational effectiveness measures of firm performance. Among key managerial implications, senior leaders must focus on creating necessary conditions for developing IT infrastructure and information management capability because they play a foundational role in building other capabilities far improved firm performance. The Baldrige model also needs some changes to more explicitly acknowledge the role and importance of information management capability so that senior leaders know where to begin in their journey toward business excellence.
|keyword = Information management capability,information technology,IT capability,customer management capability,process management capability,performance management capability,organizational capital,firm performance,performance excellence,business excellence,resource-based view,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECTS OF TREE-VIEW BASED PRESENTATION ADAPTATION ON MOBILE WEB BROWSING'''
{{header}}
{{article
|author= Boonlit Adipat,Dongsong Zhang,Lina Zhou,
|source= MIS QUARTERLY
|year= 2011
|abstract = Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.
|keyword = Presentation adaptation,mobile handheld device,Web browsing and searching,interface design,usability testing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sponsored Search and Market Efficiency'''
{{header}}
{{article
|author= Vasant Dhar,Anindya Ghose,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Sponsored search is the mechanism whereby advertisers pay a fee to Internet search engines to be displayed alongside organic (nonsponsored) web search results. Based on prior literature, we draw an analogy between these markets and financial markets. We use the analogy as well as the key differences to present a theoretical framework consisting of a set of research questions about the pricing of keywords and design choices available to firms in sponsored search markets. These questions define an agenda for future research in sponsored search markets. They also have practical implications for advertisers and online marketplaces such as search engines and social media sites that support advertising.
|keyword = social media,social commerce,sponsored search,financial markets,user-generated content,market efficiency,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cooperation, Coordination, and Governance in Multisourcing: An Agenda for Analytical and Empirical Research'''
{{header}}
{{article
|author= Ravi Bapna,Anitesh Barua,Deepa Mani,Amit Mehra,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Multisourcing, the practice of stitching together best-of-breed IT services from multiple, geographically dispersed service providers, represents the leading edge of modern organizational forms. While major strides have been achieved in the last decade in the information systems (IS) and strategic management literature in improving our understanding of outsourcing, the focus has been on a dyadic relationship between a client and a vendor. We demonstrate that a straightforward extrapolation of such a dyadic relationship falls short of addressing the nuanced incentive-effort-output linkages that arise when multiple vendors, who are competitors, have to cooperate and coordinate to achieve the client's business objectives. We suggest that when multiple vendors have to work together to deliver end-to-end services to a client, the choice of formal incentives and relational governance mechanisms depends on the degree of interdependence between the various tasks as well as the observability and verifiability of output. With respect to cooperation, we find that a vendor must not only put effort in a "primary" task it is responsible for but also cooperate through "helping" effort in enabling other vendors perform their primary tasks. In the context of coordination, we find that task redesign for modularity, OLAs, and governance structures such as the guardian vendor model represent important avenues for further research. Based on the analysis of actual multisourcing contract details over the last decade, interviews with leading practitioners, and a review of the single-sourcing literature, we lay a foundation for normative theories of multisourcing and present a research agenda in this domain.
|keyword = multisourcing,offshore outsourcing,cooperation,coordination,output verifiability,observability,relational governance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Virtual Worlds: A Performative Perspective on Globally Distributed, Immersive Work'''
{{header}}
{{article
|author= Ulrike Schultze,Wanda J. Orlikowski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Virtual worlds are immersive, simulated, persistent, and dynamic environments that include rich graphical three dimensional spaces, high fidelity audio, motion, viewpoint, and interactivity. Initially dismissed as environments of play, virtual worlds have gained legitimacy in business and educational settings for their application in globally distributed work, project management, online learning, and real-time simulation. Understanding the emergent aspects of these virtual worlds and their implications for organizations will require both new theories and new methods. We propose that a performative perspective may be particularly useful as it challenges the existence of independent objects with fixed or given properties and boundaries, and focuses instead on situated and relational practices that enact entangled and contingent boundaries, entities, identities, and effects.
|keyword = virtual worlds,boundaries,identity,presence,performativity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Seeking the Configurations of Digital Ecodynamics: It Takes Three to Tango'''
{{header}}
{{article
|author= Omar A. El Sawy,Arvind Malhotra,YoungKi Park,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This paper starts from the premise that the simultaneous increase in environmental turbulence, the requisite speed of organizational change, and the intensified ubiquity of digital technologies are spawning a phenomenon that is messy, complex, and chaotic. Accordingly, we need to change the way we examine how information technology (IT) can help organizations build a strategic advantage in turbulent environments. We propose a more systemic and holistic perspective to theory building and testing in the information system (IS) strategy area and correspondingly appropriate methods that capture the complexity of this phenomenon. We term this phenomenon digital ecodynamics, defined as the holistic confluence among environmental turbulence, dynamic capabilities, and IT systems-and their fused dynamic interactions unfolding as an ecosystem. We believe that a more holistic understanding of digital ecodynamics will fuel the next leap in knowledge in the IS strategy area. First, extending the strategic management literature that has mainly focused on two-way interactions between environmental turbulence and dynamic capabilities, we foreground IT systems as a third central element. We use a "threesome tango" analogy(1) with strong mutual interdependence to accentuate our view of digital ecodynamics-while also stressing the emerging role of IT systems in triggering environmental turbulence and shaping dynamic capabilities to build a strategic advantage. Second, we propose a different paradigmatic lens (configuration theories) as an appropriate inquiring system to better understand the complexity of digital ecodynamics. The paper articulates the key aspects of configuration theories as inquiring systems, compares them with the more common variance theories and process theories, and illustrates the power of recent advances in configurational methods. Third, we create a preliminary roadmap for IS researchers to better examine digital ecodynamics using novel structural properties afforded by configuration theories (i.e., mutual causality, discontinuity, punctuated equilibria, nonlinear change). Fourth, we reflect on the broader opportunities that the configurational perspective of digital ecodynamics can create for IS strategy research. The paper ends by highlighting the double-barreled opportunity that digital ecodynamics renders, both as an energizing vision for IS strategy research and also as a reshaper of strategic management research and practice in a turbulent and digitized world.
|keyword = digital ecodynamics,information systems strategy,digital disruption,configuration theory,holistic perspective,environmental turbulence,dynamic capabilities,IT systems,ecosystem dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Let's Shop Online Together: An Empirical Investigation of Collaborative Online Shopping Support'''
{{header}}
{{article
|author= Lei Zhu,Izak Benbasat,Zhenhui Jiang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Prior studies investigating business-to-consumer e-commerce have focused predominantly on online shopping by individuals on their own, although consumers often desire to conduct their shopping activities with others. This study explores the important, but seldom studied, topic of collaborative online shopping. It investigates two design components that are pertinent to collaborative online shopping support tools, namely, navigation support and communication support. Results from a laboratory experiment indicate that compared to separate navigation, shared navigation effectively reduces uncoupling (i.e., the loss of coordination with one's shopping partner) incidents per product discussed and leads to fewer communication exchanges dedicated to resolving each uncoupling incident, thereby enhancing coordination performance. Compared to text chat, voice chat does not help reduce the occurrence of uncoupling, but likely increases the efficiency in resolving uncoupling. The results further show that shared navigation and voice chat can significantly enhance the collaborative shoppers' perceptions of social presence derived from their online shopping experiences. The interaction effect on social presence implies that the benefit of shared navigation is higher in the presence of text chat than in the presence of voice chat.
|keyword = collaborative online shopping,shared navigation,common ground,media richness,uncoupling,social presence,electronic commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Interaction Between Knowledge Codification and Knowledge-Sharing Networks'''
{{header}}
{{article
|author= De Liu,Gautam Ray,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Current knowledge management (KM) technologies and strategies advocate two different approaches: knowledge codification and knowledge-sharing networks. However, the extant literature has paid limited attention to the interaction between them. This research draws on the literature on formal modeling of networks to examine the interaction between knowledge codification and knowledge-sharing networks. The analysis suggests that an increase in codification may damage existing network-sharing ties. Anticipating that, individuals may hoard their knowledge to protect their network ties, even when there are nontrivial rewards for codification. We find that despite the aforementioned tension between the codification and the network approach, a firm may still benefit from combining the two approaches. Specifically, when the future sharing potential between knowledge workers is high, a combination of the two approaches may outperform a codification-only or a network-only approach because the codification reward causes fewer network ties to break down, and the benefit from increased codification can offset the loss of some network ties. However, when the future sharing potential is low, an increase in codification reward can quickly break down the whole network. Thus, firms may be better off by pursuing a codification-only or a network-only strategy.
|keyword = knowledge management,codification,knowledge-sharing network,sharing potential,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Learning and Capabilities for Onshore and Offshore Business Process Outsourcing'''
{{header}}
{{article
|author= Jonathan Whitaker,Sunil Mithas,M. S. Krishnan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = This paper identifies and analyzes firm-level characteristics that facilitate onshore and offshore business process outsourcing (BPO). We use organizational learning and capabilities to develop a conceptual model. We test the conceptual model with archival data on a broad cross section of U.S. firms. Our empirical findings indicate that firms with experience in onshore information technology (IT) outsourcing and capabilities related to IT coordination applications and process codification are more likely to engage in BPO, and firms with experience in internationalization are more likely to engage in offshore BPO. We also find that IT coordination applications have a greater impact on onshore BPO than on offshore BPO, and the effect of process codification is partly mediated through IT outsourcing.
|keyword = business process outsourcing,offshoring,organizational capabilities,organizational learning,outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Disruptive Effect of Open Platforms on Markets for Wireless Services'''
{{header}}
{{article
|author= Atanu Lahiri,Rajiv M. Dewan,Marshall Freimer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Application-based pricing is common in telecommunications. Wireless carriers charge consumers more per byte of traffic for text messages than they do for wireless surfing or voice calls. Such pricing is possible because carriers and handset manufacturers have the ability to tag and meter each application. While tagging and metering are possible in the case of closed platforms such as iPhone, they are not in the case of open platforms such as Android. Android is open source with open application programming interfaces, and anyone can develop applications for it. Because the carriers have little control over applications, Android is inherently disruptive of differential pricing across applications. Users and neutrality advocates support Android, believing that it can increase consumer surplus by disrupting differential pricing. However, we show that the equilibrium under differential pricing is different from the equilibrium under open platforms, and it is particularly so with regard to the sets of consumers served and the quantities consumed. With open platforms, certain consumers are either not served or they are served a quantity that is less than what they would be served under differential pricing. Consequently, the consumer surplus and the social surplus are often lower with open platforms. Similarly, firms are expected to prefer differential pricing. We show that this expectation is also not true under certain circumstances in which open platforms and neutral pricing work like a quasi-bundle.
|keyword = net neutrality,nonlinear pricing,open platforms,quasi-bundling,wireless services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Oligopolistic Pricing with Online Search'''
{{header}}
{{article
|author= Lizhen Xu,Jianqing Chen,Andrew Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = In this paper, we set up a game-theoretic model to examine oligopolistic price competition, considering two features of online search: the existence of a common search ordering and shoppers who have nonpositive search cost. We find that in equilibrium firms set their prices probabilistically rather than deterministically, and different firms follow different price distributions. The equilibrium pricing pattern exhibits an interesting local-competition feature in which direct price competition occurs only between firms adjacent to each other. Further, we incorporate consumers' search strategies into the model so that both search order and stopping rules are determined rationally by consumers. We show that similar patterns may continue to hold in the fully rational framework when consumers have higher inspection costs for inferior positions.
|keyword = local competition,oligopolistic competition,online search,price dispersion,pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Asymmetry in Information Systems Consulting: Toward a Theory of Relationship Constraints'''
{{header}}
{{article
|author= Gregory S. Dawson,Richard T. Watson,Marie-Claude Boudreau,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Opportunism, or self-interest seeking with guile, is often witnessed in human behavior, and it bedevils human interactions and relationships. Organizations expend considerable effort to reduce opportunism. Agency theory espouses formal contracts as effective constraints on opportunism; however, a consultant's use of tacit knowledge subjects clients to information asymmetry that is not amenable to formal contracts. The principal professional lens was developed to accommodate the presence of tacit knowledge, but it ignores formal contracts and, like agency theory, ignores the existence of principal opportunism. This examination of information systems (IS) consulting notes that when information asymmetry is present, both clients and consultants sometimes behave opportunistically. The level of information asymmetry, the type of knowledge, and the level of contract specificity in an IS consulting engagement determine the mixture of legal and social constraints that are efficacious. Based on these revelations and the inadequacy of other theories, a theoretical model of relationship constraints is developed to explain the interplay between signaling and screening, knowledge type, contract specificity, and the levels of information asymmetry in predicting adopted constraint mechanisms. For researchers, this new model offers a lens to study opportunism from a knowledge-based perspective, whereas for practitioners it offers the possibility of forestalling a decline in markets due to rampant opportunism.
|keyword = agency theory,information asymmetry,information systems consulting,opportunism,principal-agent relationship,screening,signaling,tacit knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Developer Heterogeneity and Formation of Communication Networks in Open Source Software Projects'''
{{header}}
{{article
|author= Param Vir Singh,Yong Tan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Over the past few years, open source software (OSS) development has gained a huge popularity and has attracted a large variety of developers. According to software engineering folklore, the architecture and the organization of software depend on the communication patterns of the contributors. Communication patterns among developers influence knowledge sharing among them. Unlike in a formal organization, the communication network structures in an OSS project evolve unrestricted and unplanned. We develop a non-cooperative game-theoretic model to investigate the network formation in an OSS team and to characterize the stable and efficient structures. Developer heterogeneity in the network is incorporated based on their informative value. We find that there may exist several stable structures that are inefficient and there may not always exist a stable structure that is efficient. The tension between the stability and efficiency of structures results from developers acting in their self-interest rather than the group interest. Whenever there is such tension, the stable structure is either underconnected across types or overconnected within type of developers from an efficiency perspective. We further discuss how an administrator can help evolve a stable network into an efficient one. Empirically, we use the latent class model and analyze two real-world OSS projects hosted at Source Forge. For each project, different types of developers and a stable structure are identified, which fits well with the predictions of our model. Overall, our study sheds light on how developer abilities and incentives affect communication network formation in OSS projects.
|keyword = analytical modeling,economics of IS,network formation,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Task and Social Information Seeking: Whom Do We Prefer and Whom Do We Approach?'''
{{header}}
{{article
|author= Yunjie (Calvin) Xu,Hee-Woong Kim,Atreyi Kankanhalli,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Employee information-seeking behavior shapes the formation of organizational communication networks and affects performance. However, it is not easy to facilitate, particularly through information technology, and its motivations are not well understood. Recognizing two broad categories of information-that is, task and social information-this study investigates and compares the antecedents of task and social information seeking. Deriving from the relational communication perspective, informational and relational motivations are modeled as the two main antecedents of source preference and sourcing frequency in dyadic information seeking. Through a survey of employee dyads, our findings indicate that perceived information relevance is a significant antecedent of source preference for both task and social information seeking, whereas perceived relational benefit is significant in the context of task information. The results also show that perceived relational benefit has a stronger effect on source preference in task information seeking than in social information seeking. Furthermore, preference for a source is a significant antecedent of the frequency of sourcing in both contexts. This study provides an explanation of the formation of organizational communication networks. It suggests that organizational information and communication technologies not only need to support information delivery but must also facilitate relationship management for the seeker.
|keyword = perceived information relevance,perceived relational benefit,preference for source,social information seeking,sourcing frequency,task information seeking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Bidding Patterns, Experience, and Avoiding the Winner's Curse in Online Auctions'''
{{header}}
{{article
|author= Robert F. Easley,Charles A. Wood,Sharad Barkataki,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The design and implementation of online auctions has given rise to a unique set of bidding strategies that has stimulated a growing body of research. We make use of a theoretically grounded, well-understood, and empirically observable bidder behavior-the winner's curse adjustment for the expected number of bidders in an auction-to examine the relationships between bidder experience, bidding patterns, and the winner's curse adjustment in rare coin online auctions. We also examine the impact of uncertainty on the winner's curse adjustment, both by using precise measures of uncertainty and by considering seller and bidder strategies for reducing that uncertainty. We analyze a complete record of all auctions in a three-month period for rare U.S. coins, examining 284,681 bids from 62,625 auctions hosted by eBay, the market leader in online auctions. One of the main contributions of this paper is to demonstrate that the bidding patterns associated with different bidders are strongly related to whether they calculate their bids to take into account the number of competing bidders, as predicted for common-value auctions. This is a substantial extension and empirical confirmation of prior work that has explored the implications of different observed patterns of bidding. We also explore new territory by examining the relationships between bidder experience, bidding patterns observed, and the economic outcomes for bidders. We are able to show that bidders with more domain-specific experience (rather than general auction experience) make better adjustments for the winner's curse, that experience has an effect on the type of bidding strategy, and that the type of bidding strategy has a significant effect on the economic outcomes for the bidders.
|keyword = bidder experience,bidder strategies,bidding patterns,online auctions,winner's curse,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Entertainment Without Borders: The Impact of Digital Technologies on Government Cultural Policy'''
{{header}}
{{article
|author= Hsing Kenneth Cheng,Juan Feng,Gary J. Koehler,Sean Marston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Many countries limit the influence of foreign entertainment products, such as music, film, and television programs, to protect their domestic cultural industry. Commonly observed policy tools include quotas, tariffs, and subsidies. However, advances in digital technology enable consumers to access digital versions of foreign entertainment programs via the Internet, a leakage channel that bypasses government protection methods. This calls for a reexamination of the effectiveness of these traditional tools. We build a unified analytical framework to study the impact of digital technology on cultural protection policies. We find that in the presence of Internet leakage, imposing a quota is the least effective protection policy to maximize the total domestic social welfare, but using either a tariff or subsidy policy is optimal, depending on the quality difference between domestic and foreign entertainment programs via the traditional channel and the Internet. Using quotas remains the least effective policy when we extend the analyses to consider the presence of piracy. In addition to the quality difference between foreign and domestic entertainment, the proportion of unethical consumers and the cost of piracy determine whether using tariffs or subsidies is the optimal policy.
|keyword = cultural protection policy,digital entertainment,quotas,subsidies,tariffs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Platform Evolution: Coevolution of Platform Architecture, Governance, and Environmental Dynamics'''
{{header}}
{{article
|author= Amrit Tiwana,Benn Konsynski,Ashley A. Bush,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = The emergence of software-based platforms is shifting competition toward platform-centric ecosystems, although this phenomenon has not received much attention in information systems research. Our premise is that the coevolution of the design, governance, and environmental dynamics of such ecosystems influences how they evolve. We present a framework for understanding platform-based ecosystems and discuss five broad research questions that present significant research opportunities for contributing homegrown theory about their evolutionary dynamics to the information systems discipline and distinctive information technology-artifact-centric contributions to the strategy, economics, and software engineering reference disciplines.
|keyword = platforms,ecosystem,architecture,modularity,environment,evolutionary dynamics,coevolution,governance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing Smart Markets'''
{{header}}
{{article
|author= Martin Bichler,Alok Gupta,Wolfgang Ketter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Electronic markets have been a core topic of information systems (IS) research for last three decades. We focus on a more recent phenomenon: smart markets. This phenomenon is starting to draw considerable interdisciplinary attention from the researchers in computer science, operations research, and economics communities. The objective of this commentary is to identify and outline fruitful research areas where IS researchers can provide valuable contributions. The idea of smart markets revolves around using theoretically supported computational tools to both understand the characteristics of complex trading environments and multiechelon markets and help human decision makers make real-time decisions in these complex environments. We outline the research opportunities for complex trading environments primarily from the perspective of design of computational tools to analyze individual market organization and provide decision support in these complex environments. In addition, we present broad research opportunities that computational platforms can provide, including implications for policy and regulatory research.
|keyword = auctions,design,decision support systems,experimentation,smart markets,software agents,platforms,preferences,trading agent competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Design, Use, and Consequences of Virtual Processes'''
{{header}}
{{article
|author= Eric Overby,Sandra A. Slaughter,Benn Konsynski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Process virtualization occurs when a process that relies upon physical interaction between people and/or objects is transitioned to a virtual environment. Process virtualization is having profound effects on society, as an increasing number of both business and nonbusiness processes such as those related to education, medicine, and dating are being migrated to virtual environments. There is a vast literature that relates to process virtualization topics, but it is fragmented across different domains. The purpose of this paper is to propose a research agenda to develop high-level theories and frameworks that inform the general process virtualization phenomenon. Developing these theories and frameworks will synthesize existing knowledge and provide a theoretical foundation upon which to add new knowledge as it is created. This will help policy makers maximize the substantial benefits of virtual processes while minimizing the risks. Given the background, interests, and skills of IS scholars, the IS discipline is well suited to lead in this endeavor.
|keyword = process virtualization,virtual,physical,theory construction,electronic commerce,online dating,distance learning,telemedicine,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital Natives and Ubiquitous Information Systems'''
{{header}}
{{article
|author= Shahper Vodanovich,David Sundaram,Michael Myers,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Most information systems research until now has focused on information systems in organizations and their use by digital immigrants. Digital immigrants are those who were not born into the digital world-they learnt to use information systems at some stage in their adult lives. An underlying assumption of much of this research is that users "resist" technology or at least have some difficulty in accepting it. Digital natives, conversely, are those who have grown up in a world where the use of information and communications technology is pervasive and ubiquitous. These ubiquitous technologies, networks, and associated systems have proliferated and have woven themselves into the very fabric of everyday life. This article suggests that the rise of the digital native, along with the growth of ubiquitous information systems (UIS), potentially represents a fundamental shift in our "paradigm" for IS research. We propose a research agenda that focuses on digital natives and UIS.
|keyword = digital native,digital immigrant,ubiquitous information systems,pervasive computing,interorganizational information systems,IT diffusion and adoption,user acceptance of IT,mobile computing,enterprise systems,IT and new organizational forms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The New Organizing Logic of Digital Innovation: An Agenda for Information Systems Research'''
{{header}}
{{article
|author= Youngjin Yoo,Ola Henfridsson,Kalle Lyytinen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = In this essay, we argue that pervasive digitization gives birth to a new type of product architecture: the layered modular architecture. The layered modular architecture extends the modular architecture of physical products by incorporating four loosely coupled layers of devices, networks, services, and contents created by digital technology. We posit that this new architecture instigates profound changes in the ways that firms organize for innovation in the future. We develop (1) a conceptual framework to describe the emerging organizing logic of digital innovation and (2) an information systems research agenda for digital strategy and the creation and management of corporate information technology infrastructures.
|keyword = digitization,digital innovation,product architecture,layered modular architecture,organizing logic,doubly distributed networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Long Tails vs. Superstars: The Effect of Information Technology on Product Variety and Sales Concentration Patterns'''
{{header}}
{{article
|author= Erik Brynjolfsson,Yu (Jeffrey) Hu,Michael D. Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = The Internet and related information technologies are transforming the distribution of product sales across products, and these effects are likely to grow in coming years. Both the Long Tail and the Superstar effect are manifestations of these changes, yet researchers lack consistent metrics or models for integrating and extending their insights and predictions. In this paper, we begin with a taxonomy of the technological and nontechnological drivers of both Long Tails and Superstars and then define and compare the key metrics for analyzing these phenomena. The core of the paper describes a large and promising set of questions forming a research agenda. Important opportunities exist for understanding future changes in sales concentration patterns; the impact on supply chains (including cross-channel competition, competition within the Internet channel, implications for the growth of firms, and the balance of power within the supply chain); implications for pricing, promotion, and product design; and, ultimately, the potential effects on society in general. Our approach provides an introduction to some of the relevant research findings and allows us to identify opportunities for cross-pollination of methods and insights from related research topics.
|keyword = Long Tail,Superstar,product variety,sales concentration,information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital Infrastructures: The Missing IS Research Agenda'''
{{header}}
{{article
|author= David Tilson,Kalle Lyytinen,Carsten Sorensen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Since the inauguration of information systems research (ISR) two decades ago, the information systems (IS) field's attention has moved beyond administrative systems and individual tools. Millions of users log onto Facebook, download iPhone applications, and use mobile services to create decentralized work organizations. Understanding these new dynamics will necessitate the field paying attention to digital infrastructures as a category of IT artifacts. A state-of-the-art review of the literature reveals a growing interest in digital infrastructures but also confirms that the field has yet to put infrastructure at the centre of its research endeavor. To assist this shift we propose three new directions for IS research: (1) theories of the nature of digital infrastructure as a separate type of IT artifact, sui generis; (2) digital infrastructures as relational constructs shaping all traditional IS research areas; (3) paradoxes of change and control as salient IS phenomena. We conclude with suggestions for how to study longitudinal, large-scale sociotechnical phenomena while striving to remain attentive to the limitations of the traditional categories that have guided IS research.
|keyword = generativity,digital infrastructure,control points,IT artifact,IS research agenda,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Vigilant Interaction in Knowledge Collaboration: Challenges of Online User Participation Under Ambivalence'''
{{header}}
{{article
|author= Sirkka L. Jarvenpaa,Ann Majchrzak,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Online participation engenders both the benefits of knowledge sharing and the risks of harm. Vigilant interaction in knowledge collaboration refers to an interactive emergent dialogue in which knowledge is shared while it is protected, requiring deep appraisals of each others' actions in order to determine how each action may influence the outcomes of the collaboration. Vigilant interactions are critical in online knowledge collaborations under ambivalent relationships where users collaborate to gain benefits but at the same time protect to avoid harm from perceived vulnerabilities. Vigilant interactions can take place on discussion boards, open source development, wiki sites, social media sites, and online knowledge management systems and thus is a rich research area for information systems researchers. Three elements of vigilant interactions are described: trust asymmetry, deception and novelty. Each of these elements challenges prevailing theory-based assumptions about how people collaborate online. The study of vigilant interaction, then, has the potential to provide insight on how these elements can be managed by participants in a manner that allows knowledge sharing to proceed without harm.
|keyword = online participation,knowledge collaboration,vigilance,trust,distrust,deception,novelty,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Digital Transformation of Healthcare: Current Status and the Road Ahead'''
{{header}}
{{article
|author= Ritu Agarwal,Guodong (Gordon) Gao,Catherine DesRoches,Ashish K. Jha,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = As the United States expends extraordinary efforts toward the digitization of its health-care system, and as policy makers across the globe look to information technology (IT) as a means of making health-care systems safer, more affordable, and more accessible, a rare and remarkable opportunity has emerged for the information systems research community to leverage its in-depth knowledge to both advance theory and influence practice and policy. Although health IT (HIT) has tremendous potential to improve quality and reduce costs in healthcare, significant challenges need to be overcome to fully realize this potential. In this commentary, we survey the landscape of existing studies on HIT to provide an overview of the current status of HIT research. We then identify three major areas that warrant further research: (1) HIT design, implementation, and meaningful use; (2) measurement and quantification of HIT payoff and impact; and (3) extending the traditional realm of HIT. We discuss specific research questions in each domain and suggest appropriate methods to approach them. We encourage information systems scholars to become active participants in the global discourse on health-care transformation through IT.
|keyword = health information technology,health-care transformation,electronic health records,meaningful use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reframing the Dominant Quests of Information Systems Strategy Research for Complex Adaptive Business Systems'''
{{header}}
{{article
|author= Hueseyin Tanriverdi,Arun Rai,N. Venkatraman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = We review and reframe three main quests of research on information systems (IS) strategy: (1) the strategic alignment quest, (2) the integration quest, and (3) the sustained competitive advantage quest. The assumptions and logic of these quests have become less relevant in increasingly complex adaptive business systems (CABS), where the competitive performance landscapes of products and services are highly dynamic and co-evolve. We revise the strategic alignment quest to propose a co-evolution quest that addresses not only competitive strategy questions of a firm but also corporate strategy questions. The co-evolution quest seeks to increase a firm's agility and dynamism in repositioning itself, identifying profitable product-market positions as the evolving competitive landscape erodes the profitability of the firm's existing positions. To support the co-evolution quest, we revise the integration quest and propose a reconfiguration quest that encompasses not only business processes but also products and services, as well as the contracts, resources, and transactions associated with them. As the firm makes repositioning moves to co-evolve with the competitive landscape, the reconfiguration quest seeks to increase the firm's agility in disintegrating its existing nexus of contracts, resources, and transactions that support the old positions and in reconfiguring new ones that support the new positions. Finally, we revise the sustained competitive advantage quest to propose a renewal quest that recognizes the temporary nature of competitive advantage in CABS. The renewal quest seeks to destabilize the firm's old sources of competitive advantage when competitive dynamics erode their utility, rapidly create new sources of competitive advantage, and concatenate a series of temporary advantages over time. The three reframed quests provide the foundation for a research agenda on IS strategy in CABS.
|keyword = information systems strategy,complex adaptive business systems,emergence,strategic alignment,integration,sustained advantage,temporary advantage,co-evolution,reconfiguration,renewal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alliances, Rivalry, and Firm Performance in Enterprise Systems Software Markets: A Social Network Approach'''
{{header}}
{{article
|author= Ramnath K. Chellappa,Nilesh Saraf,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Enterprise systems software (ESS) is a multibillion dollar industry that produces systems components to support a variety of business functions for a widerange of vertical industry segments. Even if it forms the core of an organization's information systems (IS) infrastructure, there is little prior IS research on the competitive dynamics in this industry. Whereas economic modeling has generally provided the methodological framework for studying standards-driven industries, our research employs social network methods to empirically examine ESS firm competition. Although component compatibility is critical to organizational end users, there is an absence of industry-wide ESS standards and compatibility is ensured through interfirm alliances. First, our research observes that this alliance network does not conform to the equilibrium structures predicted by economics of network evolution supporting the view that it is difficult to identify dominant standards and leaders in this industry. This state of flux combined with the multifirm multicomponent nature of the industry limits the direct applicability of extant analytical models. Instead, we propose that the relative structural position acquired by a firm in its alliance network is a reasonable proxy for its standards dominance and is an indicator of its performance. In lieu of structural measures developed mainly for interpersonal networks, we develop a measure of relative firm prominence specifically for the business software network where benefits of alliances may accrue through indirect connections even if attenuated. Panel data analyses of ESS firms that account for over 95% of the industry revenues, show that our measure provides a superior model fit to extant social network measures. Two interesting counterintuitive findings emerge from our research. First, unlike other software industries compatibility considerations can trump rivalry concerns. We employ quadratic assignment procedure to show that firms freely form alliances even with their rivals. Second, we find that smaller firms enjoy a greater value from acquiring a higher structural position as compared to larger firms.
|keyword = technology standards,software industry,enterprise resource planning (ERP),software architecture,partnerships,social network theory,standards competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Willingness-to-Pay Formation of Repeat Bidders in Sequential Online Auctions'''
{{header}}
{{article
|author= Paulo B. Goes,Gilbert G. Karuga,Arvind K. Tripathi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = A growing number of vendors are using a sequence of online auctions to sell large inventories of identical items. Although bidding strategies and bidder behavior in single auctions have been extensively studied, limited research exists on bidding in sequential auctions. We seek to explain how bidders in such an environment learn from the information, and form and update their willingness to pay (WTP). Using a large data set from an online auction retailer, we analyze the evolution of the bidders' WTP as well as the effect of auction design on bidders' WTP in sequential auctions. We see our study in the context of a longitudinal field experiment, in which we were able to track actions of repeat bidders over an extended period of time. Our results show that bidders' WTP in sequential auctions can be explained from their demand characteristics, their participation experience in previous auctions, outcomes in previous auctions, and auction design parameters. We also observe, characterize, and measure what we call a modified demand reduction effect exhibited across different auctions, over time, by multiunit demand bidders. Our findings are important to enable better auction mechanism design, and more sophisticated bidding tools that explore the rich information environment of sequential auctions.
|keyword = sequential online auctions,bidding behavior,willingness to pay,demand reduction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Complementarities in the Diffusion of Personal Computers and the Internet: Implications for the Global Digital Divide'''
{{header}}
{{article
|author= Sanjeev Dewan,Dale Ganley,Kenneth L. Kraemer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This paper studies the cross-country diffusion of personal computers (PCs) and the Internet, and examines how the diffusive interactions across these technologies affect the evolution of the global digital divide. We adopt a generalized diffusion model that incorporates the impact of one technology's installed base on the diffusion of the other technology. We estimate the model on data from 26 developing and developed countries between 1991 and 2005. We find that the codiffusion effects between PCs and the Internet are complementary in nature and the impact of PCs on Internet diffusion is substantially stronger in developing countries as compared to developed ones. Furthermore, our results suggest that these codiffusive effects are a significant driver of the narrowing of the digital divide. We also examine the policy implications of our results, especially with respect to how complementarities in the diffusion of PC and Internet technologies might be harnessed to further accelerate the narrowing of the global digital divide.
|keyword = IT penetration,IT diffusion,digital divide,global IT,diffusion model,codiffusion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Applying the Randomized Response Technique to Elicit Truthful Responses to Sensitive Questions in IS Research: The Case of Software Piracy Behavior'''
{{header}}
{{article
|author= Samuel S. K. Kwan,Mike K. P. So,Kar Yan Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Research on software piracy often relies on self-reports by individual users and thus suffers from possible response distortion attributable to a variety of human motivations. Conclusions drawn directly from distorted self-reports may misguide managerial and policy decisions. The randomized response technique (RRT) was proposed as a remedy to response distortion. In this paper, a model based on RRT was used to illustrate how truthful responses to sensitive questions can be empirically estimated. The model was tested in two empirical studies on software piracy. Consistent with our expectations, respondents responding to RRT were more willing to disclose sensitive information about their attitudes, intentions, and behaviors on software piracy. Nontrivial distortions were demonstrated in causal relationships involving sensitive and nonsensitive variables. The study extends RRT to multivariate analysis and illustrates the feasibility and usefulness of the method in studying sensitive behavioral issues in the information systems (IS) domain.
|keyword = response distortion,software piracy,randomized response technique,unrelated question design,method of moments,socially desirable responding,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Organizational Controls and Boundary Spanning in Software Development Outsourcing: Implications for Project Performance'''
{{header}}
{{article
|author= Anandasivam Gopal,Sanjay Gosain,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Past research has studied how the selection and use of control portfolios in software projects is based on environmental and task characteristics. However, little research has examined the consequences of control mode choices on project performance. This paper reports on a study that addresses this issue in the context of outsourced software projects. In addition, we propose that boundary-spanning activities between the vendor and the client enable knowledge sharing across organizational and knowledge domain boundaries. This is expected to lead to facilitation of control through specific incentives and performance norms that are suited to client needs as well as the vendor context. Therefore, we argue that boundary spanning between the vendor and client moderates the relationship between formal controls instituted by the vendor on the development team and project performance. We also hypothesize the effect of collaboration as a clan control on project performance. We examine project performance in terms of software quality and project efficiency. The research model is empirically tested in the Indian software industry setting on a sample of 96 projects. The results suggest that formal and informal control modes have a significant impact on software project outcomes, but need to be finely tuned and directed toward appropriate objectives. In addition, boundary-spanning activities significantly improve the effectiveness of formal controls. Finally, we find that collaborative culture has provided mixed benefits by enhancing quality but reducing efficiency.
|keyword = software outsourcing,organizational control,software quality,project overruns,boundary spanning,partial least squares,surveys,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Mapping the Field of Virtual Work: A Cocitation Analysis'''
{{header}}
{{article
|author= Sumita Raghuram,Philipp Tuertscher,Raghu Garud,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Interest in the area of virtual work continues to increase with articles being written from different disciplinary perspectives-e. g., information systems (IS), management, psychology, and transportation. In this paper, we map research on virtual work to (a) understand the intellectual base from which this field has emerged, (b) explore how this field has evolved over time, and (c) identify clusters of research themes that have emerged over time and the relationships between them. Specifically, we use cocitation analysis of research published in all social science disciplines to map the field at three points in time-1995, 2000, and 2006. Our results show that the field has grown from 9 research clusters in 1995 to 16 in 2006. A comparison across these maps suggests that research in the cluster of "virtual teams" has gained significance even as research in some earlier clusters such as "urban planning and transportation" has lost ground. Our longitudinal analysis identifies relevant concepts, theories, and methodologies that have emerged in the field of virtual work. This analysis can help interested researchers identify how they may want to contribute to the field of virtual work-by adding to popular clusters, by enriching emerging smaller clusters, or by acting as bridges across clusters.
|keyword = virtual work,virtual teams,bibliometric analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Regulation of Digital Businesses with Natural Monopolies or Third-Party Payment Business Models: Antitrust Lessons from the Analysis of Google'''
{{header}}
{{article
|author= Eric K. Clemons,Nehal Madhani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Some digital business models may be so innovative that they overwhelm existing regulatory mechanisms, both legislation and historical jurisprudence, and require extension to or modification of antitrust law. Regulatory policies that were developed in response to nineteenth- or twentieth-century antitrust concerns dealt principally with economies of scale leading to monopoly power and may not be well suited to the issues of network effects or third-party payer online business models such as sponsored search. From the perspective of information systems economics, we investigate if such third-party payer digital systems require intervention as profound as the government's innovative approach to the problems posed by AT&T in the 1913 Kingsbury Commitment, establishing the first private regulated monopoly. Google provides an example of a company whose innovative digital business model is difficult to fit into current regulatory frameworks, and may provide examples of the issues that might require an extension to regulatory policy.
|keyword = antitrust,bundling and tying,contestability,deterred market entry,digital business strategies,essential facilities doctrine,Google,key word auctions,online search,relevant market share,sponsored search,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Technostress on End-User Satisfaction and Performance'''
{{header}}
{{article
|author= Monideepa Tarafdar,Qiang Tu,T. S. Ragu-Nathan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Organizational use of information and communications technologies (ICT) is increasingly resulting in negative cognitions in individuals, such as information overload and interruptions. Recent literature has encapsulated these cognitions in the concept of technostress, which is stress caused by an inability to cope with the demands of organizational computer usage. Given the critical role of the user in organizational information processing and accomplishing application-enabled workflows, understanding how these cognitions affect users' satisfaction with ICT and their performance in ICT-mediated tasks is an important step in appropriating benefits from current computing environments. The objective of this paper is to (1) understand the negative effects of technostress on the extent to which end users perceive the applications they use to be satisfactory and can utilize them to improve their performance at work and (2) identify mechanisms that can mitigate these effects. Specifically, we draw from the end-user computing and technostress literature to develop and validate a model that analyzes the effects of factors that create technostress on the individual's satisfaction with, and task performance using, ICT. The model also examines how user involvement in ICT development and support mechanisms for innovation can be used to weaken technostress-creating factors and their outcomes. The results, based on survey data analysis from 233 ICT users from two organizations, show that factors that create technostress reduce the satisfaction of individuals with the ICT they use and the extent to which they can utilize ICT for productivity and innovation in their tasks. Mechanisms that facilitate involvement of users, and encourage them to take risks, learn, explore new ideas, and experiment in the context of ICT use, diminish the factors that create technostress and increase satisfaction with the ICT they use. These mechanisms also have a positive effect on users' appropriation of ICT for productivity and innovation in their tasks. The paper contributes to emerging literature on negative outcomes of ICT use by (1) highlighting the influence of technostress on users' satisfaction and performance (i.e., productivity and innovation in ICT-mediated tasks) with ICT, (2) extending the literature on technostress, which has so far looked largely at the general behavioral and psychological domains, to include the domain of end-user computing, and (3) demonstrating the importance of user involvement and innovation support mechanisms in reducing technostress-creating conditions and their ICT use related outcomes.
|keyword = end-user performance,end-user satisfaction,ICT use,information overload,survey research,technostress,user involvement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FOCUS AND DIVERSITY IN INFORMATION SYSTEMS RESEARCH: MEETING THE DUAL DEMANDS OF A HEALTHY APPLIED DISCIPLINE'''
{{header}}
{{article
|author= Hazel Taylor,Stuart Dillon,Melinda Van Wingen,
|source= MIS QUARTERLY
|year= 2010
|abstract = Drawing on sociology of science foundations, we argue that, in order to survive and prosper, healthy applied disciplines must meet the dual demands of academic and practitioner audiences by demonstrating both focus and diversity in their research. First, we use this concomitant modality to explain why previous studies into the structure of the Information Systems discipline have reported contradictory results, with some finding a focused field while others conclude that the field is diverse. In support of our argument, we then present the results of a longitudinal, author co-citation analysis, looking across the full range of journals in which IS research is published. Our results suggest that the IS field has sustained a focus on research within three subfields over a 20-year period from 1986 to 2005: (I) a thematic miscellany of research on development, implementation, and use of systems in various application domains; (2) IS strategy and business outcomes; and (3) group work and decision support. At the same time, the field has demonstrated considerable diversity within and around these core subfields, with researchers responding flexibly to the rapidly changing field by investigating these areas with new paradigms and in new contexts, and by exploring new topics including inter-business and Internet applications, computer-supported collaborative work, virtual teams, and knowledge management. Finally, we demonstrate that, over the 20-year period from 1986 to 2005, the discipline has shifted from fragmented adhocracy to a polycentric state, which is particularly appropriate to an applied discipline such as IS that must address the dual demands of focus and diversity in a rapidly changing technological context.
|keyword = IS discipline,IS research,author co-citation analysis,focus,diversity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WEB 2.0 AND POLITICS: THE 2008 US PRESIDENTIAL ELECTION AND AN E-POLITICS RESEARCH AGENDA'''
{{header}}
{{article
|author= Sunil Wattal,David Schuff,Munir Mandviwalla,Christine B. Williams,
|source= MIS QUARTERLY
|year= 2010
|abstract = The Internet was a major factor in the 2008 U.S. presidential campaign and has become an important tool for political communication and persuasion. Yet, information systems research is generally silent on the role of the Internet in politics. In this paper, we argue that IS is positioned to enhance understanding of the influence of the Internet on politics, and, more specifically, the process of election campaigning using Internet-based technologies such as Web 2.0. In this paper, we discuss how these technologies can change the nature of competition in politics and replace or complement traditional media. Our empirical study on how Web 2.0 technologies were used by the candidates leading up to the 2008 U.S. presidential primaries sheds light on how these technologies influenced candidate performance. Finally, we outline a research agenda highlighting where IS can contribute to the academic discourse on e-politics.
|keyword = New media,Web 2.0,politics,digital democracy,e-politics,elections,online,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE OTHER SIDE OF ACCEPTANCE: STUDYING THE DIRECT AND INDIRECT EFFECTS OF EMOTIONS ON INFORMATION TECHNOLOGY USE'''
{{header}}
{{article
|author= Anne Beaudry,Alain Pinsonneault,
|source= MIS QUARTERLY
|year= 2010
|abstract = Much ado has been made regarding user acceptance of new information technologies. However, research has been primarily based on cognitive models and little attention has been given to emotions. This paper argues that emotions are important drivers of behaviors and examines how emotions experienced early in the implementation of new IT applications relate to IT use. We develop a framework that classifies emotions into four distinct types: challenge, achievement, loss, and deterrence emotions. The direct and indirect relationships between four emotions (excitement, happiness, anger, and anxiety) and IT use were studied through a survey of 249 bank account managers. Our results indicate that excitement was positively related to IT use through task adaptation. Happiness was directly positively related to IT use and, surprisingly, was negatively associated with task adaptation, which is a facilitator of IT use. Anger was not related to IT use directly, but it was positively related to seeking social support, which in turn was positively related to IT use. Finally, anxiety was negatively related to IT use, both directly and indirectly through psychological distancing. Anxiety was also indirectly positively related to IT use through seeking social support, which countered the original negative effect of anxiety. Post hoc ANOVAs were conducted to compare IT usage of different groups of users experiencing similar emotions but relying on different adaptation behaviors. The paper shows that emotions felt by users early in the implementation of a new IT have important effects on IT use. As such, the paper provides a complementary perspective to understanding acceptance and antecedents of IT use. By showing the importance and complexity of the relationships between emotions and IT use, the paper calls for more research on the topic.
|keyword = Emotions,IT use,acceptance,adaptation behaviors,appraisal theory,user reaction,IT-related behaviors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AFFECT IN WEB INTERFACES: A STUDY OF THE IMPACTS OF WEB PAGE VISUAL COMPLEXITY AND ORDER'''
{{header}}
{{article
|author= Liqiong Deng,Marshall Scott Poole,
|source= MIS QUARTERLY
|year= 2010
|abstract = This research concentrates on visual complexity and order as central factors in the design of webpages that enhance users' positive emotional reactions and facilitate desirable psychological states and behaviors. Drawing on existing theories and empirical findings in the environmental psychology, human computer interaction, and marketing research literatures, a research model is developed to explain the relationships among visual complexity and order design features of a webpage, induced emotional responses in users, and users' approach behaviors toward the website as moderated by users' metamotivational states. A laboratory experiment was conducted to test the model and its associated hypotheses. The results of the study suggested that a web user's initial emotional responses (i.e., pleasantness and arousal), evoked by the visual complexity and order design features of a webpage when first encountered, will have carry-over effects on subsequent approach behavior toward the website. The results also revealed how webpage visual complexity and order influence users' emotions and behaviors differently when users are in different metamotivational states. The salience and importance of webpage visual complexity and order for users' feelings of pleasantness were largely dependent on users' metamotivational states.
|keyword = Webpage visual design,webpage visual complexity,webpage order,emotional response,approach behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''UNDERSTANDING ORGANIZATION-ENTERPRISE SYSTEM FIT: A PATH TO THEORIZING THE INFORMATION TECHNOLOGY ARTIFACT'''
{{header}}
{{article
|author= Diane M. Strong,Olga Volkoff,
|source= MIS QUARTERLY
|year= 2010
|abstract = Packaged software applications such as enterprise systems are designed to support generic rather than specific requirements, and hence are likely to be an imperfect fit in any particular instance. Using critical realism as our philosophical perspective, we conducted a three-year qualitative study of misfits that arose from an enterprise system (ES) implementation. A detailed analysis of the observed misfits resulted in a richer understanding of the concept of fit and of the ES artifact itself Specifically, we found six misfit domains (functionality, data, usability, role, control and organizational culture) and within each, two types of misfit (deficiencies and impositions). These misfit types correspond to two newly defined types of fit: fit as coverage and fit as enablement. Our analysis of fit also revealed a new conceptualization of the ES artifact, with implications for IT artifacts in general.
|keyword = Fit,misfits,enterprise systems,IT artifact,critical realism,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPROVING EMPLOYEES' COMPLIANCE THROUGH INFORMATION SYSTEMS SECURITY TRAINING: AN ACTION RESEARCH STUDY'''
{{header}}
{{article
|author= Petri Puhakainen,Mikko Siponen,
|source= MIS QUARTERLY
|year= 2010
|abstract = Employee noncompliance with information systems security policies is a key concern for organizations. If users do not comply with IS security policies, security solutions lose their efficacy. Of the different IS security policy compliance approaches, training is the most commonly suggested in the literature. Yet, few of the existing studies about training to promote IS policy compliance utilize theory to explain what learning principles affect user compliance with IS security policies, or offer empirical evidence of their practical effectiveness. Consequently, there is a need for IS security training approaches that are theory-based and empirically evaluated. Accordingly, we propose a training program based on two theories: the universal constructive instructional theory and the elaboration likelihood model. We then validate the training program for IS security policy compliance training through an action research project. The action research intervention suggests that the theory-based training achieved positive results and was practical to deploy. Moreover, the intervention suggests that information security training should utilize contents and methods that activate and motivate the learners to systematic cognitive processing of information they receive during the training. In addition, the action research study made clear that a continuous communication process was also required to improve user IS security policy compliance. The findings of this study offer new insights for scholars and practitioners involved in IS security policy compliance.
|keyword = IS security,IS security training,employees' compliance with security policies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ALTERNATIVE TO METHODOLOGICAL INDIVIDUALISM: A NON-REDUCTIONIST APPROACH TO STUDYING TECHNOLOGY ADOPTION BY GROUPS'''
{{header}}
{{article
|author= Saonee Sarker,Joseph S. Valacich,
|source= MIS QUARTERLY
|year= 2010
|abstract = Studies on groups within the MIS discipline have largely been based on the paradigm of methodological individualism. Commentaries on methodological individualism within the reference disciplines suggest that studies embracing this paradigm can lead to potentially misleading or incorrect conclusions. This study illustrates the appropriateness of the alternate non-reductionist approach to investigating group-related phenomenon, specifically in the context of technology adoption. Drawing on theories of group influence, prior research on conflict, technology characteristics, task technology fit, group communication media, and recent theoretical work surrounding group technology adoption, the paper proposes and empirically tests a new non-reductionist model for conceptualizing technology adoption by groups. Further, the study also empirically compares this non-reductionist model with a (hypothetical) methodological individualist model of technology adoption by groups. Results strongly support most of the assertions of the non-reductionist model and highlight that this model provides a more robust explanation of technology adoption by groups than a methodological individualist view. Further, the study also highlights some conditions wherein the methodological individualist view fails to provide correct explanations. The implications of the study's findings for future research are discussed.
|keyword = Methodological individualism,non-reductionist view,multilevel theory,group technology adoption,valence,task-technology fit,technology characteristics,PLS analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD ETHICAL INFORMATION SYSTEMS: THE CONTRIBUTION OF DISCOURSE ETHICS'''
{{header}}
{{article
|author= John Mingers,Geoff Walsham,
|source= MIS QUARTERLY
|year= 2010
|abstract = Ethics is important in the Information Systems field as illustrated by the direct effect of the Sarbanes-Oxley Act on the work of IS professionals. There is a substantial literature on ethical issues surrounding computing and information technology in the contemporary world, but much of this work is not published nor widely cited in the mainstream IS literature. The purpose of this paper is to offer one contribution to an increased emphasis on ethics in the IS field. The distinctive contribution is a focus on Habermas's discourse ethics. After outlining some traditional theories of ethics and morality, the literature on IS and ethics is reviewed, and then the paper details the development of discourse ethics. Discourse ethics is different from other approaches to ethics as it is grounded in actual debates between those affected by decisions and proposals. Recognizing that the theory could be considered rather abstract, the paper discusses the need to pragmatize discourse ethics for the IS field through, for example, the use of existing techniques such as soft systems methodology. In addition, the practical potential of the theory is illustrated through a discussion of its application to specific IS topic areas including Web 2.0, open source software, the digital divide, and the UK biometric identity card scheme. The final section summarizes ways in which the paper could be used in IS research, teaching, and practice.
|keyword = Ethics and IS,ethical theories,Habermas,discourse ethics,deliberative democracy,soft systems methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF INFORMATION TECHNOLOGY AND TRANSACTIVE MEMORY SYSTEMS ON KNOWLEDGE SHARING, APPLICATION, AND TEAM PERFORMANCE: A FIELD STUDY'''
{{header}}
{{article
|author= Sue Young Choi,Heeseok Lee,Youngjin Yoo,
|source= MIS QUARTERLY
|year= 2010
|abstract = In contemporary knowledge-based organizations, teams often play an essential role in leveraging knowledge resources. Organizations make significant investments in information technology to support knowledge management practices in teams. At the same time, recent studies show that the transactive memory system (TMS) the specialized division of cognitive labor among team members that relates to the encoding, storage, and retrieval of knowledge is an important factor that affects a team's performance. Yet little is known of how IT support for knowledge management practices in organizations affects the development of TMS. Furthermore, the precise role of TMS on knowledge sharing and knowledge application, which in turn influences team performance, has not been fully explored. In order to close this gap in the literature, we conducted a field study that involved 139 on-going teams of 743 individuals from two major firms in South Korea. Our results show that IT support in organizations has a positive impact on the development of TMS in teams, and that both TMS and IT support have a positive impact on knowledge sharing and knowledge application. Furthermore, we found that knowledge sharing has a positive impact on knowledge application, which in turn has a direct impact on team performance. However, contrary to our expectation, knowledge sharing does not have a direct impact on team performance and its impact on team performance was fully mediated by knowledge application. Our research shows that organizations can improve team members' meta-knowledge of who knows what through the careful investment in information technology. Finally, our results show that sharing knowledge alone is not enough. Organizations must ensure that shared knowledge is in fact applied in order to improve team performance.
|keyword = Transactive memory system,knowledge management,team performance,field study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Safe Contexts for Interorganizational Collaborations Among Homeland Security Professionals'''
{{header}}
{{article
|author= Ann Majchrzak,Sirkka L. Jarvenpaa,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = In many domains of increased turbulence and volatility, interorganizational ad hoc collaborations are common. One such domain is homeland security in which security professionals collaborate virtually with individuals outside of their own organizations in response to a security threat. In such a domain, a safe context is needed to ensure that interactions with collaborators not only help to solve the immediate threat but also avoid the improper use by outside parties of information released during these collaborations. We use the heuristic systematic model of information processing to hypothesize that the relationship between different safe context factors and a security professional's perceptions of collaboration success will be contingent on differences in geographic proximity of the collaborating parties differences in proximity that are not related to differences in physical face-to-face contact but to differences in social proximity. Our exploratory empirical investigation finds support for the hypothesized interaction effect: safe contexts that require deeper processing are related to higher levels of perceived success when the parties are geographically proximal (with no differences in face-to-face contact), whereas safe contexts that involve heuristic-based processing are related to success when parties are geographically less proximal. Our findings suggest that the utility of safe context factors is contextualized based on the proximity of interacting parties, that geographical proximity's social space dimension plays a key role independent of differences in physical face-to-face contact, and that, practically, to be successful, ad hoc collaborators should have access to a range of safe context factors, using them in different combinations depending on the proximity of network members.
|keyword = dual process theories,geographic proximity,knowledge protection,personal networks,safe contexts,security professionals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Systems Development Ambidexterity: Explaining the Complementary and Substitutive Roles of Formal and Informal Controls'''
{{header}}
{{article
|author= Amrit Tiwana,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Although formal and informal control mechanisms are often simultaneously used to govern systems development projects, considerable disagreement exists about whether the use of one strengthens or diminishes the benefits of the other. In other words, are they complements or substitutes? Competing theoretical perspectives favor both sides of the argument, and neither the information systems (IS) controls literature nor the information technology (IT) outsourcing literature has addressed this issue. This study theoretically develops the idea that these competing perspectives are mutually compatible rather than contradictory because informal and formal control mechanisms can simultaneously be complements and substitutes. Using data from 120 outsourced systems development projects, it is shown that informal control mechanisms strengthen the influence of formal behavior control mechanisms on systems development ambidexterity (complementary effects) but weaken the influence of formal outcome control mechanisms (substitutive effects). The key contribution of the paper therefore lies in exploring interactions among control mechanisms in a project's control portfolio to reconcile the competing theoretical perspectives on whether formal and informal controls are complements or substitutes. The findings provide managers guidance on how to carefully combine formal and informal control mechanisms in a project. Combining informal with formal process-based control mechanisms can simultaneously enhance the fulfillment of project goals and development flexibility. However, combining informal with formal outcome-based control mechanisms can instead impair these objectives.
|keyword = ambidexterity,control mechanisms,interaction effects,outsourcing,project governance,signaling,software project control,systems development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''No Free Lunch: Price Premium for Privacy Seal-Bearing Vendors'''
{{header}}
{{article
|author= Bin Mai,Nirup M. Menon,Sumit Sarkar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Privacy is a significant concern of customers in the business-to-consumer online environment. Several technical, economic, and regulatory mechanisms have been proposed to address online privacy. A current market-based mechanism is the privacy seal, under which a third party assures adherence by a vendor to its posted privacy policy. In this paper, we present empirical evidence of the effect of displaying a privacy seal on the product prices of online vendors of electronic books, downloadable audiobooks, and textbooks. Using data collected on these relatively homogeneous products sold by online vendors, we find that while controlling for vendor-specific characteristics, vendors bearing privacy seals charge a premium for such products compared to vendors not bearing a seal. The paper provides empirical evidence of the economic value of privacy assurance from the customers' perspective as measured by the price premium charged for products. The research has implications for researchers and policymakers by providing evidence that privacy is another factor that creates friction in e-commerce, and that prices on the Internet for homogeneous products need not converge.
|keyword = partial least squares,price dispersion,privacy assurance,risk premium,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Donor-to-Nonprofit Online Marketplace: An Economic Analysis of the Effects on Fund-Raising'''
{{header}}
{{article
|author= Zafer D. Ozdemir,Kemal Altinkemer,Prabuddha De,Yasin Ozcelik,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Online intermediaries have recently started offering database services to donors and certification services to nonprofit organizations through the Internet. We conceptualize a donor-to-nonprofit (D2N) marketplace as an online intermediary that offers these two services and examine its effect on fund-raising strategies of nonprofit organizations using an analytical model based on spatial competition under incomplete information with donor search. We characterize the signaling equilibria where certification of quality conveys information about organizational effectiveness in generating socially valuable services. Interestingly, the emergence of the D2N marketplace may lead to a drop in total net fund-raising revenues in the market, despite the fact that the intermediary's database service eliminates search costs for some donors. We also explain why such a marketplace may deliberately lower the accuracy of its certification process.
|keyword = fund-raising,nonprofit organizations,online marketplaces,online search,quality certification,seal of approval,signaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Internal Open Source Development on Reuse: Participatory Reuse in Action'''
{{header}}
{{article
|author= Padmal Vitharana,Julie King,Helena Shih Chapman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Adoption of open source software (OSS) principles to internal software development has gained considerable momentum. Often labeled as "internal open source" (IOS), several large firms have started to implement these programs. Research to date has mostly focused on facilitating IOS adoption. In the present research, we focus on how IOS affects reuse. Employing a qualitative case study, we examine the IOS program at IBM called "Community Source." Analyzing data gathered from multiple sources reveals that IOS adoption facilitates participatory reuse by enhancing information sharing and leveraging of broader community skills. Participatory reuse manifests itself when potential reusers participate in the entire development process leading to the creation of reusable assets. Based on data, we develop a theoretical model to illustrate how IOS affects reuse. While furthering research on IOS and reuse, the model informs managers wishing to foster participatory reuse that they are wise to adopt IOS as a vehicle to promote greater openness of the software development infrastructure for leveraging broader community skills and enhancing information sharing among projects' stakeholders.
|keyword = closed source,internal open source,open source,open source software,participatory reuse,software reuse,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Balancing IT with the Human Touch: Optimal Investment in IT-Based Customer Service'''
{{header}}
{{article
|author= Sulin Ba,Jan Stallaert,Zhongju Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = To cut costs, companies have chosen to deliver a variety of service offerings online. However, the digital systems providing such services (e-service) have always been complemented with or supported by human-based service (h-service). Whereas h-service has total costs that increase with the demand for services, e-service mainly requires a fixed investment upfront, which can be amortized over the totality of customers served. Considering the different nature of the costs of h-service and e-service and the heterogeneity of customer preferences for services, we derive the optimal mix of h-service and e-service for a service-providing company vis-a-vis its competitor. Our theoretical analysis finds the subgame-perfect Nash equilibria that determines the optimal positions in a duopoly setting. We further study the competitive dynamics of the system to examine how firms stay on the equilibrium paths. Using simulation, we investigate the effects of starting positions, small adjustments in h-service and/or e-service, and monotonic expansions of e-service on the final positioning and profits of the firms. Our results demonstrate that when firms follow a local best-reply strategy, they may end up in a position of low profitability, and when only monotonic expansions of e-service are allowed, both firms may end up overinvesting in e-service.
|keyword = e-service quality,customer service,price competition,service differentiation,competitive strategy,competitive dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The "Third Hand": IT-Enabled Competitive Advantage in Turbulence Through Improvisational Capabilities'''
{{header}}
{{article
|author= Paul A. Pavlou,Omar A. El Sawy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Organizations are increasingly engaged in competitive dynamics that are enabled or induced by information technology (IT). A key competitive dynamics question for many organizations is how to build a competitive advantage in turbulence with digital IT systems. The literature has focused mostly on developing and exercising dynamic capabilities for planned reconfiguration of existing operational capabilities in fairly stable environments with patterned "waves," but this may not always be possible, or even appropriate, in highly turbulent environments with unexpected "storms." We introduce improvisational capabilities as an alternative means for managing highly turbulent environments; we define this as the ability to spontaneously reconfigure existing resources to build new operational capabilities to address urgent, unpredictable, and novel environmental situations. In contrast to the planned role of dynamic and operational capabilities and the ambidexterity that they jointly offer, improvisational capabilities are proposed to operate distinctly as a "third hand" that facilitates reconfiguration and change in highly turbulent environments. First, the paper develops the notion of improvisational capabilities and articulates the key differences between the two "reconfiguration"-improvisational and dynamic-capabilities. Second, the paper compares the relative effects of improvisational and dynamic capabilities in the context of new product development in different levels of environmental turbulence. Third, the paper shows how IT-leveraging capability in new product development is decomposed into its three digital IT systems: project and resource management systems, organizational memory systems (OMS), and cooperative work systems-and how each of these IT systems enhances improvisational capabilities, an effect that is accentuated in highly turbulent environments. The results show that although dynamic capabilities are the primary predictor of competitive advantage in moderately turbulent environments, improvisational capabilities fully dominate in highly turbulent environments. Besides discriminant validity, the distinction between improvisational and dynamic capabilities is evidenced by the differential effects of IT-leveraging capability on improvisational and dynamic capabilities. The results show that the more the IT-leveraging capability is catered toward managing resources (through project and resource management systems) and team collaboration (through cooperative work systems) rather than relying on past knowledge and procedures (through organizational memory systems), the more it is positively associated with improvisational capabilities, particularly in more turbulent environments. The paper draws implications for how different IT systems can influence improvisational capabilities and competitive advantage in turbulent environments, thereby enhancing our understanding of the role of IT systems on reconfiguration capabilities. The paper discusses the theoretical and practical implications of building and exercising the " third hand" of improvisational capabilities for IT-enabled competitive dynamics in turbulence.
|keyword = improvisation,improvisational capabilities,dynamic capabilities,environmental turbulence,digital systems,IT-leveraging capability,new product development,competitive advantage,competitive dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Changing the Competitive Landscape: Continuous Innovation Through IT-Enabled Knowledge Capabilities'''
{{header}}
{{article
|author= K. D. Joshi,Lei Chi,Avimanyu Datta,Shu Han,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = We theoretically and empirically investigate the relationship between information technology (IT) and firm innovation. Invoking absorptive capacity (ACAP) theory, we introduce and develop the concepts of three types of IT-enabled knowledge capabilities. Firm innovation is examined through two observable innovation outcomes: patents, and new product and service introductions. These innovation outcomes are often labeled as competitive actions aggressively undertaken by firms to gain market share or to achieve profitability. We use secondary data about IT-enabled knowledge capabilities and innovation outcomes of 110 firms. Our data results provide strong support for our main assertion that knowledge capabilities that are enhanced through the use of IT contribute to firm innovation. The study's findings suggest that the three types of IT-enabled knowledge capabilities have differential effects on firm innovation. This study substantially contributes to the information systems (IS) research, methodology, and practice in multiple ways.
|keyword = absorptive capacity,business value of IT,competitive impacts of IS,firm innovation,IT-enabled knowledge capability,knowledge management,strategic management of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managerial Interpretations of the Role of Information Systems in Competitive Actions and Firm Performance: A Grounded Theory Investigation'''
{{header}}
{{article
|author= Sandra A. Vannoy,A. F. Salam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Using an interpretive grounded theory research approach, we investigate the utilization of organization-wide information systems in the competitive actions and responses undertaken by top managers to sustain their firms' leading competitive position. Our central contribution is a model that explicates the role of information systems in the process by which competitive actions or responses are conceived, enacted, and executed, and resulting impacts on firm performance-issues that have been largely missing from contemporary research in both the information systems and competitive dynamics domains. This study has important implications for both research and practice. Specifically, researchers should consider organizational context; the intentions and actions of key players; and the process of conceiving, enacting, and executing competitive actions or responses carried out by the organization to account for the impact of information systems on firm performance. Findings suggest that when managers envision information systems as a resource that provides opportunities for competitive actions rather than viewing information systems in a service role, competitive advantages will evolve. Furthermore, practitioners will be better able to leverage information systems investments if they recognize the embedded role of information systems within the competitive actions or responses a firm undertakes to maintain or improve relative performance.
|keyword = competitive dynamics,competitive actions,information systems,firm performance,strategy,grounded theory,interpretative research,managerial interpretation,process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging IT Capabilities and Competitive Process Capabilities for the Management of Interorganizational Relationship Portfolios'''
{{header}}
{{article
|author= Arun Rai,Xinlin Tang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Firms are increasingly dependent on external resources and are establishing portfolios of interorganizational relationships (IRs) to leverage external resources for competitive advantage. However, the systems of information technology (IT) and process capabilities that firms should develop to manage IR portfolios dynamically are not well-understood. In order to theorize how key structural IT capabilities (IT integration and IT reconfiguration) and competitive process capabilities (process alignment, partnering flexibility, and offering flexibility) operate as systems of complements, we draw on the competitive dynamics perspective and resource dependency theory and on the literature for IT business value, interorganizational systems, and interorganizational relationship management. We also theorize how a firm's IR portfolio moderates the effects of structural IT capabilities on competitive process capabilities and why a firm's environmental turbulence moderates the effects of complementary process capabilities on competitive performance. We test our model using survey data from 318 firms in 4 industries. Our results provide broad support for the following: (1) structural IT capabilities and process capabilities operating as a system of complements, (2) the effects of structural IT capabilities on competitive process capabilities being contingent on IR portfolio concentration, and (3) the effects of complementary process capabilities on competitive performance being contingent on environmental turbulence. We discuss the theoretical and practical implications of how firms should develop complementary systems of structural IT capabilities and competitive process capabilities to manage IR portfolios dynamically and leverage external resources.
|keyword = competitive dynamics perspective,IT business value,interorganizational relationships,relationship portfolios,competitive performance,structural IT capabilities,competitive process capabilities,complementarities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology, Network Structure, and Competitive Action'''
{{header}}
{{article
|author= Lei Chi,T. Ravichandran,Goce Andrevski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Researchers in competitive dynamics have demonstrated that firms that carry out intense, complex, and heterogeneous competitive actions exhibit better performance. However, there is a need to understand factors that enable firms to undertake competitive actions. In this study, we focus on two antecedents of competitive behavior of firms: (1) access to network resources and (2) use of information technology (IT). We argue that while network structure provides firms with the opportunity to tap into external resources, the extent to which they are actually exploited depends on firms' IT-enabled capability. We develop a theoretical model that examines the relationships between IT-enabled capability, network structure, and competitive action. We test the model using secondary data, about 12 major automakers over 16 years from 1988 to 2003. We find that network structure rich in structural holes has a positive direct effect on firms' ability to introduce a greater number and a wider range of competitive actions. However, the effect of dense network structure is contingent on firms' IT-enabled capability. Firms benefit from dense network structure only when they develop a strong IT-enabled capability. Our results suggest that IT-enabled capability plays both a substitutive role, when firms do not have advantageous access to brokerage opportunities, and a complementary role, when firms are embedded in dense network structure, in the relationship between network structure and competitive actions.
|keyword = competitive action,IT-enabled capability,interfirm network structure,social network theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Network Perspective of Digital Competition in Online Advertising Industries: A Simulation-Based Approach'''
{{header}}
{{article
|author= Ray M. Chang,Wonseok Oh,Alain Pinsonneault,Dowan Kwon,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Using agent-based simulation experiments, we investigate the outcome of SAs between two smaller online search engine companies in competition with a dominant market leader in settings where an advertiser's decision making is the consequence of a combination of NI (e. g., an individual's willingness to follow others' decisions) and IP. In particular, we focus on a context in which the combined search engine company competes with a market leader holding a larger share of the market than the two runner-up "underdogs" combined. Our results indicate that, with the presence of NI and cascading effects, an alliance with "only" 35%-40% combined market share could compete with a leader whose market share, at the time of an alliance, is 60%-65%. Although important, size alone might be insufficient to build the market as suggested by the "vanilla" network effect theory. Another noteworthy finding is that a nonlinear association exists between NI and an alliance outcome; the combined runner-up companies have the best chance of success when the extent of NI is midrange, rather than on the high or low end of continuum. Contrary to the conventional view, this finding might also stimulate discussions among network science researchers. Furthermore, our results suggest that NI substantially moderates the relationship between the combined market share at the time of an alliance and the likelihood of resulting alliance success.
|keyword = digital competition,NI,IP,switching force,SAs,online sponsored advertising,agent-based simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competitive Actions and Dynamics in the Digital Age: An Empirical Investigation of Social Networking Firms'''
{{header}}
{{article
|author= Devi R. Gnyawali,Weiguo Fan,James Penner,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This paper examines two important questions in the context of the social networking services (SNS) firms: what kind of competitive moves do SNS firms undertake and to what extent do the competitive moves impact firm performance? We blend the literature streams on information systems (IS) and strategic management and argue that given the unique characteristics of this nascent industry, SNS firms' competitive moves are likely to focus on value cocreation, as well as enhancement of the repertoire of their moves. We propose a conceptual model by blending value cocreation perspectives from the IS literature and repertoire of competitive actions from the competitive dynamics literature, and test our hypotheses using archival data. Results show that firms that emphasize value cocreation actions through the engagement of codevelopers in their technology platform and formation of strategic alliances enhance their performance. Furthermore, firms that undertake complex action repertoires achieve better performance. This study provides unique insights about the ways in which firms compete in the industry and has several implications for future research.
|keyword = competition strategy,competition dynamics,competitive actions,firm performance,social networking services,page view,business value of information technology,value cocreation,repertoire of actions,network centrality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competing in Crowded Markets: Multimarket Contact and the Nature of Competition in the Enterprise Systems Software Industry'''
{{header}}
{{article
|author= Ramnath K. Chellappa,V. Sambamurthy,Nilesh Saraf,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = As more and more firms seek to digitize their business processes and develop new digital capabilities, the enterprise systems software (ESS) has emerged as a significant industry. ESS firms offer software components (e. g., ERP, CRM, Marketing analytics) to shape their clients' digitization strategies. With rapid rates of technological and market innovation, the ESS industry consists of several horizontal markets that form around these components. As numerous vendors compete with each other within and across these markets, many of these horizontal markets appear to be crowded with rivals. In fact, multimarket contact and presence in crowded markets appear to be the pathways through which a majority of the ESS firms compete. Though the strategy literature has demonstrated the virtues of multimarket contact, paradoxically, the same literature argues that operating in crowded markets is not wise. In particular, crowded markets increase a firm's exposure to the whirlwinds of intense competition and have deleterious consequences for financial performance. Thus, the behavior of ESS firms raises an interesting anomaly and research question: Why do ESS firms continue to compete in crowded markets if they are deemed to be bad for financial performance? We argue that the effects of rivalry in crowded markets are counteracted by a different force, in the form of the economics of demand externalities. Demand externalities occur because the customers of ESS firms expect that software components from one market will be easily integrated with those that they buy from other markets. However, with rapid rates of technological innovation and market formation and dissolution, customers experience significant ambiguity in deciding which markets and components suit their needs. Therefore, they look at crowded markets as an important signal about the legitimacy and viability of specific components for their needs. Through their presence in crowded markets, ESS firms can signal their commitment to many of the components that customers might need for their digital platforms. Customers might find that such firms are attractive because their commitments to crowded markets can mitigate concerns about compatibilities between the components purchased across several markets. This unique potential for demand externality across markets suggests that ESS vendors might, in fact, benefit from competing in many crowded markets. We test our explanations through data across three time periods from a set of ESS firms that account for more than 95% of the revenue in this market. We find that ESS firms do reap performance benefits by competing in crowded markets. More importantly, we find that they can enhance their benefits from crowded markets if they face the same competitors in multiple markets, thereby increasing their multimarket contact with rivals. These results have interesting implications not just for understanding competitive conduct in the ESS industry but also in many of the emerging digital goods industries where the markets have similar competitive characteristics to the ESS industry. Our ideas complement emerging ideas about platform models of competition in the digital goods industry and provide important directions for future research.
|keyword = enterprise software,standards,multimarket contact,crowded markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why Do Software Firms Fail? Capabilities, Competitive Actions, and Firm Survival in the Software Industry from 1995 to 2007'''
{{header}}
{{article
|author= Shanling Li,Jennifer Shang,Sandra A. Slaughter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This study examines why firms fail or survive in the volatile software industry. We provide a novel perspective by considering how software firms' capabilities and their competitive actions affect their ultimate survival. Drawing on the resource-based view (RBV), we conceptualize capabilities as a firm's ability to efficiently transform input resources into outputs, relative to its peers. We define three critical capabilities of software-producing firms-research and development (RD), marketing (MK), and operations (OP)-and hypothesize that in the dynamic, high-technology software industry, RD and MK capabilities are most important for firm survival. We then draw on the competitive dynamics literature to theorize that competitive actions distinguished by a greater emphasis on innovation-related moves will increase firm survival more than actions emphasizing resource-related moves. Finally, we postulate that firms' capabilities will complement their competitive actions in affecting firm survival. Our empirical evaluation examines a cross-sectional, time series panel of 5,827 observations on 870 software companies from 1995 to 2007. We use a stochastic frontier production function to measure the capability for each software firm in each time period. We then use the Cox proportional hazard regression technique to relate capabilities and competitive actions to software firms' failure rates. Unexpectedly, our results reveal that higher OP capability increases software firm survival more than higher MK and RD capabilities. Further, firms with a greater emphasis on innovation-related than resource-related competitive actions have a greater likelihood of survival, and this likelihood increases even further when these firms have higher MK and OP capabilities. Additional analyses of subsectors within the software industry reveal that firms producing visual applications (e. g., graphical and video game software) have the highest MK capability but the lowest OP and RD capabilities and make twice as many innovation-related as resource-related moves. These firms have the highest market values but the worst Altman Z scores, suggesting that they are valued highly but also are at high risk for failure, and indeed the firms in this sector fail at a greater rate than expected. In contrast, firms producing traditional decision-support applications and infrastructure software have different capabilities and make different competitive moves. Our findings suggest that the firms that persist and survive over the long term in the dynamic software industry are able to capitalize on their competitive actions because of their greater capabilities, and particularly OP capabilities.
|keyword = software industry,survival analysis,capability,resource-based view,marketing,operations,research and development,stochastic frontier production function,competitive actions,competitive dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting Collaboration Technology Use: Integrating Technology Adoption and Collaboration Research'''
{{header}}
{{article
|author= Susan A. Brown,Alan R. Dennis,Viswanath Venkatesh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The paper presents a model integrating theories from collaboration research (i.e., social presence theory, channel expansion theory, and the task closure model) with a recent theory from technology adoption research (i.e., unified theory of acceptance and use of technology, abbreviated to UTAUT) to explain the adoption and use of collaboration technology. We theorize that collaboration technology characteristics, individual and group characteristics, task characteristics, and situational characteristics are predictors of performance expectancy, effort expectancy, social influence, and facilitating conditions in UTAUT. We further theorize that the UTAUT constructs, in concert with gender, age, and experience, predict intention to use a collaboration technology, which in turn predicts use. We conducted two field studies in Finland among (1) 349 short message service (SMS) users and (2) 447 employees who were potential users of a new collaboration technology in an organization. Our model was supported in both studies. The current work contributes to research by developing and testing a technology-specific model of adoption in the collaboration context.
|keyword = channel expansion theory,collaboration technologies,social presence theory,task closure model,technology acceptance,technology adoption,unified theory of acceptance and use of technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Are Consumers More Likely to Contribute Online Reviews for Hit or Niche Products?'''
{{header}}
{{article
|author= Chrysanthos Dellarocas,Guodong (Gordon) Gao,Ritu Narayan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = User-generated content has been hailed by some as a democratizing force that enables consumers to discuss niche products that were previously ignored by mainstream media. Nevertheless, the extent to which consumers truly prefer to use these new outlets to discuss lesser-known products as opposed to spending most of their energies on discussing widely marketed or already successful products has so far remained an open question. We explore this question by investigating how a population's propensity to contribute postconsumption online reviews for different products of the same category (motion pictures) relates to various indicators of those products' popularity. We discover that, ceteris paribus, consumers prefer to post reviews for products that are less available and less successful in the market. At the same time, however, they are also more likely to contribute reviews for products that many other people have already commented on online. The presence of these two opposite forces leads to a U-shaped relationship between a population's average propensity to review a movie postconsumption and that movie's box office revenues: moviegoers appear to be more likely to contribute reviews for very obscure movies but also for very high-grossing movies. Our findings suggest that online forum designers who wish to increase the contribution of user reviews for lesser-known products should make information about the volume of previously posted reviews a less-prominent feature of their sites.
|keyword = consumer behavior,econometrics,information intermediaries,online product reviews,online word of mouth,Web 2.0,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Empirical Analysis of the Impact of Recommender Systems on Sales'''
{{header}}
{{article
|author= Bhavik Pathak,Robert Garfinkel,Ram D. Gopal,Rajkumar Venkatesan,Fang Yin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Online retailers are increasingly using information technologies to provide value-added services to customers. Prominent examples of these services are online recommender systems and consumer feedback mechanisms, both of which serve to reduce consumer search costs and uncertainty associated with the purchase of unfamiliar products. The central question we address is how recommender systems affect sales. We take into consideration the interaction among recommendations, sales, and price. We then develop a robust empirical model that incorporates the indirect effect of recommendations on sales through retailer pricing, potential simultaneity between sales and recommendations, and a comprehensive measure of the strength of recommendations. Applying the model to a panel data set collected from two online retailers, we found that the strength of recommendations has a positive effect on sales. Moreover, this effect is moderated by the recency effect, where more recently released recommended items positively affect the cross-selling efforts of sellers. We also show that recommender systems help to reinforce the long-tail phenomenon of electronic commerce, and obscure recommendations positively affect cross-selling. We also found a positive effect of recommendations on prices. These results suggest that recommendations not only improve sales but they also provide added flexibility to retailers to adjust their prices. A comparative analysis reveals that recommendations have a higher effect on sales than does consumer feedback. Our empirical results show that providing value-added services, such as digital word of mouth and recommendations, allows retailers to charge higher prices while at the same time increasing demand by providing more information regarding the quality and match of products.
|keyword = collaborative filtering,electronic commerce,e-tail,experience goods,recommender systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Net Neutrality and Vertical Integration of Content and Broadband Services'''
{{header}}
{{article
|author= Hong Guo,Subhajyoti Bandyopadhyay,Hsing Kenneth Cheng,Yu-Chen Yang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Whether broadband service providers (BSPs) should be allowed to vertically integrate with content providers is a contentious issue. This is even more so when viewed through the lens of the net neutrality debate, since the vertically integrated firm can prioritize the delivery of its own content at the expense of that of its competitors if net neutrality is not enforced. Using a game-theoretic model, we analyze the issues of vertical integration of content and broadband services surrounding this debate from an economic perspective. Our analysis establishes the various equilibria in the game and shows that the vertically integrated BSP does not have any incentive to abide by the principles of net neutrality. If net neutrality is not enforced, social welfare might, in certain cases, decrease with vertical integration, and in such cases, the BSP's objectives are at odds with that of the social planner. With other ranges of parameter values, social welfare increases with vertical integration at the expense of the competing pure-play content provider. Interestingly, we find that it is not always true that the BSP will always degrade the delivery of the competing content, and in fact will sometimes have the incentive to prioritize the latter over its own. The analysis thus provides crucial inputs to policymakers as they decide on whether to allow vertical integration between a BSP and a content provider in the absence of net neutrality.
|keyword = broadband service providers,consumer surplus,content providers,economics of net neutrality,net neutrality,social welfare,vertical integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology Diffusion with Influentials, Imitators, and Opponents'''
{{header}}
{{article
|author= Hasan Cavusoglu,Nan Hu,Yingjiu Li,Dan Ma,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Information technology (IT) innovations follow a diverse set of diffusion patterns. Early diffusion models explaining technology diffusion patterns assumed that there is a single homogeneous segment of potential adopters. It was later shown that a two-segment model considering two groups of adopters explains variations in diffusion patterns better than the existing one-segment models. While the two-segment model considers a group of adopters promoting adoption by exerting a positive influence on prospective adopters, it does not consider the members of society who aim to inhibit the adoption process by exerting a negative influence on prospective adopters. In fact, most IT innovations face opposition. Yet it is not clear how opposition affects the diffusion process. In this paper, we model the diffusion of an IT innovation through its target population with three types of actors: influentials, who are autonomous in adopting new technology and promote its adoption; opponents, who are opposed to the technology and inhibit its adoption; and imitators, who are information seekers, thus affected by both influentials and opponents. We show that opponents play a crucial role in determining the diffusion path of an innovation. The empirical tests using real as well as simulated data sets demonstrate the ability of our model to fit the data better and to identify the segments of adopters correctly.
|keyword = diffusion of innovation,information technology diffusion,technology opposition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DETECTING FAKE WEBSITES: THE CONTRIBUTION OF STATISTICAL LEARNING THEORY'''
{{header}}
{{article
|author= Ahmed Abbasi,Zhu Zhang,David Zimbra,Hsinchun Chen,Jr. Jay F. Nunamaker,
|source= MIS QUARTERLY
|year= 2010
|abstract = Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for e-commerce and online security.
|keyword = Fake website detection,Internet fraud,design science,statistical learning theory,information systems development,website classification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CIRCUITS OF POWER: A STUDY OF MANDATED COMPLIANCE TO AN INFORMATION SYSTEMS SECURITY DE JURE STANDARD IN A GOVERNMENT ORGANIZATION'''
{{header}}
{{article
|author= Stephen Smith,Donald Winchester,Deborah Bunker,Rodger Jamieson,
|source= MIS QUARTERLY
|year= 2010
|abstract = Organizations need to protect information assets against cyber crime, denial-of-service attacks, web hackers, data breaches, identity and credit card theft, and fraud. Criminals often try to achieve financial, political, or personal gain through these attacks, so the threats that their actions prompt are insidious motivators for organizations to adopt information systems security (ISS) approaches. Extant ISS research has traditionally examined ISS in e-commerce business organizations. The present study investigates ISS within government, analyzing power relationships during an ISS standards adoption and accreditation process, where a head of state mandates that all government agencies are to comply with a national de jure ISS standard. Using a canonical action research method, designated managers of ISS services across small, medium, and large agencies were monitored and assessed for progress to accreditation through surveys, interviews, participant observation at round table forums, and focus groups. By 2008, accreditation status across the 89 agencies participating in this study was approximately 33 percent fully accredited, with 67 percent partially compliant. The research uses Clegg's (1989) circuits of power framework to interpret power, resistance, norms, and cultural relationships in the process of compliance. The paper highlights that a strategy based on organization subunit size is helpful in motivating and assisting organizations to move toward accreditation. Mandated standard accreditation was inhibited by insufficient resource allocation, lack of senior management input, and commitment. Factors contributing to this resistance were group norms and cultural biases.
|keyword = Information systems security (ISS),ISS de jure standards,politics and power,circuits of power,resistance,norms,culture,institutionalization,canonical action research,e-commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NEUTRALIZATION: NEW INSIGHTS INTO THE PROBLEM OF EMPLOYEE INFORMATION SYSTEMS SECURITY POLICY VIOLATIONS'''
{{header}}
{{article
|author= Mikko Siponen,Anthony Vance,
|source= MIS QUARTERLY
|year= 2010
|abstract = Employees' failure to comply with information systems security policies is a major concern for information technology security managers. In efforts to understand this problem, IS security researchers have traditionally viewed violations of IS security policies through the lens of deterrence theory. In this article, we show that neutralization theory. a theory prominent in Criminology but not yet applied in the context of IS, provides a compelling explanation for IS security policy violations and offers new insight into how employees rationalize this behavior. In doing so, we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory. Our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices.
|keyword = Neutralization theory,deterrence theory,IS security policies,IS security,compliance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USER PARTICIPATION IN INFORMATION SYSTEMS SECURITY RISK MANAGEMENT'''
{{header}}
{{article
|author= Janine L. Spears,Henri Barki,
|source= MIS QUARTERLY
|year= 2010
|abstract = This paper examines user participation in information systems security risk management and its influence in the context of regulatory compliance via a multi-method study at the organizational level. First, eleven informants across five organizations were interviewed to gain an understanding of the types of activities and security controls in which users participated as part of Sarbanes-Oxley compliance, along with associated outcomes. A research model was developed based on the findings of the qualitative study and extant user participation theories in the systems development literature. Analysis of the data collected in a questionnaire survey of 228 members of ISACA, a professional association specialized in information technology governance, audit, and security, supported the research model. The findings of the two studies converged and indicated that user participation contributed to improved security control performance through greater awareness, greater alignment between IS security risk management and the business environment, and improved control development. While the IS security literature often portrays users as the weak link in security, the current study suggests that users may be an important resource to IS security by providing needed business knowledge that contributes to more effective security measures. User participation is also a means to engage users in protecting sensitive information in their business processes.
|keyword = Information security,user participation,security risk management,Sarbanes-Oxley Act,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SECURITY POLICY COMPLIANCE: AN EMPIRICAL STUDY OF RATIONALITY-BASED BELIEFS AND INFORMATION SECURITY AWARENESS'''
{{header}}
{{article
|author= Burcu Bulgurcu,Hasan Cavusoglu,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2010
|abstract = Many organizations recognize that their employees, who are often considered the weakest link in information security, can also be great assets in the effort to reduce risk related to information security. Since employees who comply with the information security rules and regulations of the organization are the key to strengthening information security, understanding compliance behavior is crucial for organizations that want to leverage their human capital. This research identifies the antecedents of employee compliance with the information security policy (ISP) of an organization. Specifically, we investigate the rationality-based factors that drive an employee to comply with requirements of the ISP with regard to protecting the organization's information and technology resources. Drawing on the theory of planned behavior, we posit that, along with normative belief and self-efficacy, an employee's attitude toward compliance determines intention to comply with the ISP. As a key contribution, we posit that an employee's attitude is influenced by benefit of compliance, cost of compliance, and cost of noncompliance, which are beliefs about the overall assessment of consequences of compliance or noncompliance. We then postulate that these beliefs are shaped by the employee's outcome beliefs concerning the events that follow compliance or noncompliance: benefit of compliance is shaped by intrinsic benefit, safety of resources, and rewards, while cost of compliance is shaped by work impediment; and cost of noncompliance is shaped by intrinsic cost, vulnerability of resources, and sanctions. We also investigate the impact of information security awareness (ISA) on outcome beliefs and an employee's attitude toward compliance with the ISP. Our results show that an employee's intention to comply with the ISP is significantly influenced by attitude, normative beliefs, and self-efficacy to comply. Outcome beliefs significantly affect beliefs about overall assessment of consequences, and they, in turn, significantly affect an employee's attitude. Furthermore, ISA positively affects both attitude and outcome beliefs. As the importance of employees' following their organizations' information security rules and regulations increases, our study sheds light on the role of ISA and compliance-related beliefs in an organization's efforts to encourage compliance.
|keyword = Information security awareness,information security management,compliance,information security policy,behavioral issues of information security,theory of planned behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FEAR APPEALS AND INFORMATION SECURITY BEHAVIORS: AN EMPIRICAL STUDY'''
{{header}}
{{article
|author= Allen C. Johnston,Merrill Warkentin,
|source= MIS QUARTERLY
|year= 2010
|abstract = Information technology executives strive to align the actions of end users with the desired security posture of management and of the firm through persuasive communication. In many cases, some element of fear is incorporated within these communications. However, within the context of computer security and information assurance, it is not yet clear how these fear-inducing arguments, known as fear appeals, will ultimately impact the actions of end users. The purpose of this study is to investigate the influence of fear appeals on the compliance of end users with recommendations to enact specific individual computer security actions toward the mitigation of threats. An examination was performed that culminated in the development and testing of a conceptual model representing an infusion of technology adoption and fear appeal theories. Results of the study suggest that fear appeals do impact end user behavioral intentions to comply with recommended individual acts of security, but the impact is not uniform across all end users. It is determined in part by perceptions of self-efficacy, response efficacy, threat severity, and social influence. The findings of this research contribute to information systems security research, human computer interaction, and organizational communication by revealing a new paradigm in which IT users form perceptions of the technology, not on the basis of performance gains, but on the basis of utility for threat mitigation.
|keyword = Information security,countermeasures,protection motivation theory,fear appeals,persuasive communication,information assurance,threat appraisal,coping appraisal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MARKET VALUE OF VOLUNTARY DISCLOSURES CONCERNING INFORMATION SECURITY'''
{{header}}
{{article
|author= Lawrence A. Gordon,Martin P. Loeb,Tashfeen Sohail,
|source= MIS QUARTERLY
|year= 2010
|abstract = Information security is a fundamental concern for corporations operating in today's digital economy. The number of firms disclosing items concerning their information security on reports filed with the U. S. Securities and Exchange Commission (SEC) has increased in recent years. A question then arises as to whether or not there is value to the voluntary disclosures concerning information security. Thus, the primary objective of this paper is to assess empirically the market value of voluntary disclosures of items pertaining to information security. Based on a sample of 1,641 disclosing and 19,266 non-disclosing firm-years in a cross-sectional pooled model, our primary findings provide strong evidence that voluntarily disclosing items concerning information security is associated positively with the market value of a firm. These findings are based on the use of a market-value relevance model, as well as a bid-ask spread analysis. The study's findings are robust to alternative statistical analyses. The findings also provide support for the signaling argument, which states that managers disclose information in a manner consistent with increased firm value. Finally, the study findings provide some insight into the strategic choice that firms make regarding voluntary disclosures about information security.
|keyword = Information security,market value,voluntary disclosures,selection-bias,bid ask spread,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF MALICIOUS AGENTS ON THE ENTERPRISE SOFTWARE INDUSTRY'''
{{header}}
{{article
|author= Michael R. Galbreth,Mikhael Shor,
|source= MIS QUARTERLY
|year= 2010
|abstract = In this paper, a competitive software market that includes horizontal and quality differentiation, as well as a negative network effect driven by the presence of malicious agents, is modeled. Software products with larger installed bases, and therefore more potential computers to attack, present more appealing targets for malicious agents. One finding is that software firms may profit from increased malicious activity. Software products in a more competitive market are less likely to invest in security, while monopolistic or niche products are likely to be more secure from malicious attack. The results provide insights for IS managers considering enterprise software adoption.
|keyword = Information system security,network externalities,software selection,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PRACTICING SAFE COMPUTING: A MULTIMETHOD EMPIRICAL EXAMINATION OF HOME COMPUTER USER SECURITY BEHAVIORAL INTENTIONS'''
{{header}}
{{article
|author= Catherine L. Anderson,Ritu Agarwal,
|source= MIS QUARTERLY
|year= 2010
|abstract = Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies a survey and an experiment conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user's intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.(2)
|keyword = Behavioral security,protection motivation,home computer user,goal framing,self-view,survey,experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Transparency in Business-to-Consumer Markets: Concepts, Framework, and Research Agenda'''
{{header}}
{{article
|author= Nelson Granados,Alok Gupta,Robert J. Kauffman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = The Internet has brought about significant changes in the availability of market information in many industries. E-commerce technologies provide sellers with opportunities to design electronic mercantile mechanisms that reveal, conceal, bias, and distort market information, depending on their goals and market position (e.g., suppliers versus intermediaries). In particular, in information-intensive industries where electronic markets play an important role, many firms are using advanced technologies to put innovative strategies into play that are based on the provision of differential information to their customers. We examine the role of information transparency in electronic markets. We contend that there is an opportunity to develop research on sellers' strategies regarding information disclosure to customers and competitors. For that purpose, we develop a set of concepts and a framework to guide future research. We then propose an interdisciplinary agenda for research on the emerging and increasingly important topic of transparency strategy, which we define as the set of policies and decisions that a firm makes to disclose, conceal, bias, or distort market information.
|keyword = business-to-business e-commerce,business-to-consumer e-commerce,electronic markets,information transparency,market mechanism design,transparency strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Resource Allocation Policies for Personalization in Content Delivery Sites'''
{{header}}
{{article
|author= Dengpan Liu,Sumit Sarkar,Chelliah Sriskandarajah,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = One of the distinctive features of sites on the Internet is their ability to gather enormous amounts of information about their visitors and to use this information to enhance a visitor's experience by providing personalized information or recommendations. In providing personalized services, a website is typically faced with the following trade-off: When serving a visitor's request, it can deliver an optimally personalized version of the content to the visitor, possibly with a long delay because of the computational effort needed, or it can deliver a suboptimal version of the content more quickly. This problem becomes more complex when several requests are waiting for information from a server. The website then needs to trade off the benefit from providing more personalized content to each user with the negative externalities associated with higher waiting costs for all other visitors that have requests pending. We examine several deterministic resource allocation policies in such personalization contexts. We identify an optimal policy for the above problem when requests to be scheduled are batched, and show that the policy can be very efficiently implemented in practice. We provide an experimental approach to determine optimal batch lengths, and demonstrate that it performs favorably when compared with viable queueing approaches.
|keyword = recommendation systems,user profiling,delay externality,scheduling,batching,queueing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Technological Frames, Organizational Capabilities, and IT Use: An Empirical Investigation of Electronic Procurement'''
{{header}}
{{article
|author= Abhay Nath Mishra,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = The process by which organizations incorporate technological innovations into existing routines and use them on a regular basis persists as a central concern in the literature. Although we now have a fairly robust understanding of the drivers of innovation adoption, the use of innovations is less understood. In this paper, we draw on two streams of literature, managerial and organizational sensemaking, and organizational capabilities that have hitherto been used independently, to investigate organizational use of information technology (IT)-based innovations. Building on and extending prior work, we posit that organizational capabilities serve as complements to managers' technological frames related to an innovation. We focus on the use of an important technological innovation-business-to-business (B2B) electronic markets for procurement. We examine interactions between three technological frames-benefits frame, threat frame, and adjustment frame, and two organizational capabilities-technological opportunism and technological sophistication, and their relationship with the use of B2B electronic markets in firms. We test our research model using survey data collected from 292 firms. Results largely support the proposed conceptualization and shed new light on the key factors associated with firms' use of B2B electronic markets. Theoretical and practical implications of the findings are discussed.
|keyword = electronic procurement,B2B electronic markets,technological frames,benefits frame,threat frame,adjustment frame,organizational capabilities,technological opportunism,technological sophistication,sensemaking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Diffusion Models for Peer-to-Peer (P2P) Media Distribution: On the Impact of Decentralized, Constrained Supply'''
{{header}}
{{article
|author= Kartik Hosanagar,Peng Han,Yong Tan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = In peer-to-peer (P2P) media distribution, users obtain content from other users who already have it. This form of decentralized product distribution demonstrates several unique features. Only a small fraction of users in the network are queried when a potential adopter seeks a file, and many of these users might even free-ride, i.e., not distribute the content to others. As a result, generated demand might not always be fulfilled immediately. We present mixing models for product diffusion in P2P networks that capture decentralized product distribution by current adopters, incomplete demand fulfillment and other unique aspects of P2P product diffusion. The models serve to demonstrate the important role that P2P search process and distribution referrals-payments made to users that distribute files-play in efficient P2P media distribution. We demonstrate the ability of our diffusion models to derive normative insights for P2P media distributors by studying the effectiveness of distribution referrals in speeding product diffusion and determining optimal referral policies for fully decentralized and hierarchical P2P networks.
|keyword = peer-to-peer file diffusion,P2P,supply-constrained diffusion,free-riding,mixing model of diffusion,distributed systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Complementarities Between Organizational IT Architecture and Governance Structure'''
{{header}}
{{article
|author= Amrit Tiwana,Benn Konsynski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This study addresses the theoretically neglected interplay between organizational information technology ( IT) architecture and IT governance structure in shaping IT alignment. We theoretically develop the idea that IT architecture modularity helps sustain IT alignment by increasing IT agility, and that decentralization of IT governance strengthens this relationship. IT architecture therefore complements IT governance structure. Tests of the proposed mediated-moderation model using data from 223 organizations support these ideas. Implications for theory and practice are also discussed.
|keyword = modularity,information technology architecture,governance,mediated moderation,alignment,IT strategy,IT agility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing Screening and Evaluation Decision Support Systems: A Resource-Matching Approach'''
{{header}}
{{article
|author= Chuan-Hoo Tan,Hock-Hai Teo,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = This research explores how consumers use online decision aids with screening and evaluation support functionalities under varying product attribute-load conditions. Drawing on resource-matching theory, we conducted a 3 x 2 factorial experiment to test the interaction between decision aid features (i.e., low versus high-screening support, and aids with weight assignment and computation decision tools) and attribute load (i.e., large versus small number of product attributes) on decision performance. The findings reveal that: (1) where the decision aids render cognitive resources that match those demanded for the task environment, consumers will process more information and decision performance will be enhanced; (2) where the decision aids render cognitive resources that exceed those demanded for the task environment, consumers will engage in less task-related elaboration of decision-making issues to the detriment of decision performance; and (3) where the decision aids render cognitive resources that fall short of those demanded for the task environment, consumers will use simplistic heuristic decision strategies to the detriment of decision performance or invest additional effort in information processing to attain a better decision performance if they perceive the additional investments in effort to be manageable.
|keyword = decision support systems,electronic commerce,resource matching,consumer behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Visualization of Network Concepts: The Impact of Working Memory Capacity Differences'''
{{header}}
{{article
|author= Bin Zhu,Stephanie A. Watts,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = As networks of all forms become ubiquitous, the network-based information they generate is increasingly being used in a wide variety of analysis tasks. In organizations, social network analysis techniques are being applied to a number of domains, particularly the understanding of knowledge stocks and flows. Because this information is generated from large data sets, computerized visualizations of it are very helpful for accomplishing these complex tasks. This paper presents a model for evaluating the effectiveness of network visualizations based on theories of cognitive fit, working memory capacity, and information load. The model was empirically tested in two experiments using two types of data visualizations from two different social networks. Results support the theoretical model, illustrating that variations in cognitive fit and working memory interact. Findings suggest that visualizations can enable superior outcomes when they are designed to support this interaction.
|keyword = information visualization,network visualization,social network analysis,working memory capacity,evaluation of information visualization systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting Web Page Status'''
{{header}}
{{article
|author= Gautam Pant,Padmini Srinivasan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = The World Wide Web has become a key intermediary between producers and consumers of information. Web's linkage structure has been exploited by contemporary search engines to decrease the search cost for consumers while usually also rewarding the producers of higher status Web pages. In addition to influencing visibility and accessibility, in-links, as marks of recognition, accord status to a Web page. In this paper we show how Web page status may be predicted at least in part by page location and topic specificity. Moreover, we observe that the "philanthropic" contributions of a Web page-specifically, contributions of information brokerage function-are also good predictors of in-links. The observations are made in the presence of domain-and topic-specific effects. Interestingly, all of these features that may predict status are "local" to a given Web page and within the control of the owner/author of the page. This is in contrast to the "global" nature of Web linkage-based metrics such as in-link count that are derived as a result of downloading and indexing billions of pages. Because the linkage structure of the Web affects browsing, crawling, and retrieval, our results have implications for vertical and general search, business intelligence, and content management.
|keyword = Web search,search engine marketing,Web visibility,status,influence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward a Causal Interpretation from Observational Data: A New Bayesian Networks Method for Structural Models with Latent Variables'''
{{header}}
{{article
|author= Zhiqiang (Eric) Zheng,Paul A. Pavlou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Because a fundamental attribute of a good theory is causality, the information systems (IS) literature has strived to infer causality from empirical data, typically seeking causal interpretations from longitudinal, experimental, and panel data that include time precedence. However, such data are not always obtainable and observational (cross-sectional, nonexperimental) data are often the only data available. To infer causality from observational data that are common in empirical IS research, this study develops a new data analysis method that integrates the Bayesian networks (BN) and structural equation modeling (SEM) literatures. Similar to SEM techniques (e. g., LISREL and PLS), the proposed Bayesian networks for latent variables (BN-LV) method tests both the measurement model and the structural model. The method operates in two stages: First, it inductively identifies the most likely LVs from measurement items without prespecifying a measurement model. Second, it compares all the possible structural models among the identified LVs in an exploratory (automated) fashion and it discovers the most likely causal structure. By exploring the causal structural model that is not restricted to linear relationships, BN-LV contributes to the empirical IS literature by overcoming three SEM limitations (Lee, B., A. Barua, A. B. Whinston. 1997. Discovery and representation of causal relationships in MIS research: A methodological framework. MIS Quart. 21(1) 109-136)-lack of causality inference, restrictive model structure, and lack of nonlinearities. Moreover, BN-LV extends the BN literature by (1) overcoming the problem of latent variable identification using observed (raw) measurement items as the only inputs, and (2) enabling the use of ordinal and discrete (Likert-type) data, which are commonly used in empirical IS studies. The BN-LV method is first illustrated and tested with actual empirical data to demonstrate how it can help reconcile competing hypotheses in terms of the direction of causality in a structural model. Second, we conduct a comprehensive simulation study to demonstrate the effectiveness of BN-LV compared to existing techniques in the SEM and BN literatures. The advantages of BN-LV in terms of measurement model construction and structural model discovery are discussed.
|keyword = causality,Bayesian networks,structural equation modeling,observational data,Bayesian graphs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does Technological Progress Alter the Nature of Information Technology as a Production Input? New Evidence and New Results'''
{{header}}
{{article
|author= Paul Chwelos,Ronald Ramirez,Kenneth L. Kraemer,Nigel P. Melville,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Prior research at the firm level finds information technology (IT) to be a net substitute for both labor and non-IT capital inputs. However, it is unclear whether these results hold, given recent IT innovations and continued price declines. In this study we extend prior research to examine whether these input relationships have evolved over time. First, we introduce new price indexes to account for varying technological progress across different types of IT hardware. Second, we use the rental price methodology to measure capital in terms of the flow of services provided. Finally, we use hedonic methods to extend our IT measures to 1998, enabling analysis spanning the emergence of the Internet. Analyzing approximately 9,800 observations from over 800 Fortune 1,000 firms for the years 1987-1998, we find firm demand for IT to be elastic for decentralized IT and inelastic for centralized IT. Moreover, Allen Elasticity of Substitution estimates confirm that through labor substitution, the increasing factor share of IT comes at the expense of labor. Last, we identify a complementary relationship between IT and ordinary capital, suggesting an evolution in this relationship as firms have shifted to more decentralized organizational forms. We discuss these results in terms of prior research, suggest areas of future research, and discuss managerial implications.
|keyword = IT business value,productivity,substitute,complement,hedonic,capital services,technological change,rental price,price index,organizational decentralization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Follow the Sun Workflow in Global Software Development'''
{{header}}
{{article
|author= Erran Carmel,J. Alberto Espinosa,Yael Dubinsky,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Follow the sun (FTS) has interesting appeal-hand off work at the end of every day from one site to the next, many time zones away, in order to speed up product development. Although the potential effect on "time to market" can be profound, at least conceptually, FTS has enjoyed few documented industry successes because it is acknowledged to be extremely difficult to implement. In order to address this "FTS challenge," we provide here a conceptual foundation and formal definition of FTS. We then analyze the conditions under which FT'S can be successful in reducing duration in software development. We show that handoff efficiency is paramount to successful FTS practices and that duration can be reduced only when lower within-site coordination and improved personal productivity outweigh the corresponding increase in cross-site coordination. We also develop 12 research propositions based on fundamental issues surrounding FTS, such as calendar efficiency, development method, product architecture and handoff efficiency, within-site coordination, cross-site coordination, and personal productivity. We combine the conceptual analysis with a description of our FTS exploratory comparative field studies and draw out their key findings and learning. The main implication of this paper is that understanding calendar efficiency, handoff efficiency, within-site coordination, and cross-site coordination is necessary to evaluation-if FTS is to be successful in reducing software development duration.
|keyword = calendar-efficient software development,global coordination,round-the-clock development,software development,software handoff efficiency,time to market,24-hour development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Perpetual Versus Subscription Licensing Under Quality Uncertainty and Network Externality Effects'''
{{header}}
{{article
|author= Jie (Jennifer) Zhang,Abraham Seidmann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = We discuss the optimal way for a software vendor to license software: a perpetual license at a posted price, a subscription contract that subscribers receive automatic updates for periodic payment, or a hybrid approach that involves both. By addressing specific issues in the software market such as network effects, quality uncertainty, upgrade compatibility, and the vendor's ability to commit to future prices in a dynamic environment, we demonstrate how a software vendor can manage the trade-offs of perpetual licensing and subscription to optimize profit, as well as the corresponding welfare effect on consumers. Although the subscription model helps the vendor lock in consumers so as to increase profit when there is great uncertainty associated with the next version software, it destroys the path dependence in creating network externalities. Therefore, when the network effect is sufficiently large, it is more profitable for a software vendor to provide both perpetual licensing and subscription.
|keyword = compatibility,network externality,price discrimination,quality uncertainty,software licensing,upgrades,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Power of Patterns and Pattern Recognition When Developing Information-Based Strategy'''
{{header}}
{{article
|author= Eric K. Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Just as scientists in other disciplines use experience and a small set of frequently occurring problems to structure unfamiliar situations, information strategy and economics provides its own patterns to guide and structure the use of experience in managerial settings. These patterns emerged through case studies, theoretical derivations, and empirical analyses of company, industry, and national data sets. The six most frequently observed patterns identified here are (1) newly vulnerable markets experience opportunistic pickoff; (2) transparency of product attributes increases informedness, enabling resonance marketing and increasing the benefits from offering truly differentiated products and services; (3) changes in transaction costs have changed the boundary of the firm; (4) unique resources endowments can confer or sustain competitive advantage; (5) the geometry of distribution determines power and affects profitability; and (6) network-based advantages can form the basis of platform-envelopment strategies. Finally, prospects for the future of information, strategy, and economics over the coming two decades are reviewed.
|keyword = hyperdifferentiation,newly vulnerable markets,outsourcing risks,platform envelopment strategies,resonance marketing,resource-based competitive advantage,strategic information systems,transaction costs of outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Models of Collaboration as the Foundation for Collaboration Technologies'''
{{header}}
{{article
|author= Steven Poltrock,Mark Handel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Can models of collaboration serve as foundations for development of collaborative technologies in much the same way that engineers use models when developing complex systems? We explore this issue by investigating how eight approaches to understanding or modeling collaboration could be used to improve technologies that support processes used in a large aerospace program. Some modeling approaches are ostensive, defining how collaboration should be achieved or how the technology should be used. These approaches provide ways of documenting, analyzing, simulating, and automating the process. Other approaches are performative, describing actual collaboration behavior and actual technology use. Performative approaches reveal the variability in collaboration and deviations from the intended process. Technologies can benefit from and facilitate both types of modeling approaches by recording collaborative events for later analysis. We conclude by considering ways that modeling collaboration could contribute to requirements analysis, new collaboration capabilities, adoption, and maximizing benefit from technologies.
|keyword = collaboration,coordination theory,modeling,social network models,temporal models,workflow,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Bounded Ideation Theory'''
{{header}}
{{article
|author= Robert O. Briggs,Bruce A. Reinig,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Organizations often look to their information systems (IS) professionals to work with system stakeholders to generate new ideas to solve complex problems and to provide information technology (IT) artifacts to support ideation processes. Much research therefore seeks to increase the number of ideas people generate based on Alex F. Osborn's conjecture that more ideas give rise to more good ideas. Recent research, however, calls the quantity-quality conjecture into question. This paper advances bounded ideation theory (BIT), an explanation for the ideation function-the relationship between the number of good ideas and the number of ideas contributed. BIT posits that boundaries of understanding, attention resources, goal congruence, mental and physical stamina, and the solution space moderate a primary relationship between individual ability and idea quality, yielding an ideation function with an inflected curve. We discuss six strategies for improving ideation and call into question the value of the quantity focus of ideation research in the IS/IT literature, arguing that a quality focus would be more useful.
|keyword = bounded ideation theory,brainstorming,group support systems,ideation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network Externalities and Technology Use: A Quantitative Analysis of Intraorganizational Blogs'''
{{header}}
{{article
|author= Sunil Wattal,Pradeep Racherla,Munir Mandviwalla,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = We examine the role of network externalities on the use of blogs in an organization. Prior research has considered social influences such as peer pressure, but there is little prior work on how the extent of others' actual usage can influence an individual's use of technology. We also examine how technology usage is influenced by positive feedback from others. Finally, we look at how the relation between technology usage and network effects is moderated by demographic variables such as age and gender. The results of the study show that usage of blogs within an individual's network is associated with an increase in one's own usage. We also show that network effects are stronger for younger generations and that this relation is nonmonotonic with age. This is interesting considering that prior research suggests that social influences are stronger for older employees. Our results also show that network effects are stronger for women than for men. Further, we show that the impact of age on blog usage in not linear. We also find that feedback or appreciation from others is associated with higher blog usage by an individual. Finally, we subdivide the network effects into various subtypes and find that network effects are strongest for relational networks, and that use of blogs by an employee's managers is associated with higher usage by the employee.
|keyword = corporate blogs,network externalities,social computing,social networks,technology usage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Technology Dominance in Complex Decision Making: The Case of Aided Credibility Assessment'''
{{header}}
{{article
|author= Matthew L. Jensen,Paul Benjamin Lowry,Judee K. Burgoon,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Decision aids have long been an important source of help in making structured decisions. However, decision support for more complex problems has been much more difficult to create. Decision aids are now being developed for very complex problems, and their effects among low- and high-task-knowledge individuals are still being explored. One such task is credibility assessment, in which message recipients or observers must determine a message's veracity and trustworthiness. Credibility assessment is made difficult by lack of constraints, hidden or incomplete information, and mistaken beliefs of the assessor. The theory of technology dominance (TTD) proposes that technology is most effectively applied in intelligent decision aids when an experienced user is paired with a sophisticated decision aid. This work examines TTD in the complex task of credibility assessment. To assist in credibility assessment, we created a decision aid that augments the capabilities of the user-whether novice or professional. Using hypotheses based on TTD, we tested the decision aid using high-stakes deception in recorded interviews and involved both student (novice) and law enforcement (professional) users. Both professionals and novices improved their assessment accuracy by using the decision aid. Consistent with TTD, novices were more reliant on the decision aid than were professionals. However, contrary to TTD, there was no significant difference in the way novices and professionals interacted with the system, and the decision aid was not more beneficial to professionals. Novices and professionals frequently discounted the aid's recommendations, and in many cases professionals did not view explanations when the decision aid contradicted their assessments. Potential reasons for these findings, as well as limitations and future research opportunities, are discussed.
|keyword = credibility,credibility assessment,deception,deception detection,decision aids,decision making,theory of technology dominance (TTD),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Team Size, Dispersion, and Social Loafing in Technology-Supported Teams: A Perspective on the Theory of Moral Disengagement'''
{{header}}
{{article
|author= Omar A. Alnuaimi,Jr. Lionel P. Robert,Likoebe M. Maruping,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Social loafing is the tendency of individuals to withhold contributions to a task in a team setting. Team size and dispersion are two primary drivers of social loafing in technology-supported team settings. However, the mechanisms through which these drivers affect social loafing are not well understood. Consequently, the objective of this study is to identify the cognitive mechanisms that mediate the effect of team size and dispersion on social loafing in technology-supported teams. Drawing on the theory of moral disengagement, we posit that three primary cognitive mechanisms-diffusion of responsibility, attribution of blame, and dehumanization-will mediate the effect of team size and dispersion on social loafing. We conducted a laboratory study involving 140 students randomly assigned to 32 teams performing a brainstorming task using group systems software. The results show that diffusion of responsibility, attribution of blame, and dehumanization all mediate (partially) the effects of team size on social loafing. Meanwhile, only dehumanization mediates (fully) the effect of dispersion on social loafing.
|keyword = computer-mediated communication,creativity,electronic brainstorming,idea generation,individuals in teams,social loafing,team performance,team productivity,technology-mediated collaborative environments,technology-supported team efficacy,theory of moral disengagement,virtual collaboration,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents and Effects of CIO Supply-Side and Demand-Side Leadership: A Staged Maturity Model'''
{{header}}
{{article
|author= Daniel Q. Chen,David S. Preston,Weidong Xia,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = As organizations' information technology (IT) investment goals evolve from improving operational efficiency to enhancing strategic growth, the chief information officer (CIO) is increasingly expected to play not only the traditional supply-side leadership role that focuses on exploiting existing IT competencies to support known business needs but also the demand-side leadership role that focuses on exploring new IT-enabled business opportunities that result in competitive advantage. Using matched CIO business executive responses from 174 firms, we test a staged maturity relationship between CIO supply-side and demand-side leadership and examine three antecedents (CIO human capital, CIO structural power, and organizational support for IT) and two effects (IT contribution to firm efficiency and strategic growth) of CIO leadership. The staged maturity model is supported by our findings and provides insight into how these two stages of CIO leadership influence IT impact within the organization and how they are influenced by these key antecedents.
|keyword = chief information officer,exploitation,exploration,IT functional impact,IT leadership,staged maturity model,strategic value of IT,structural equation modeling,survey research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Experiential and Dispositional Factors in Phishing: An Empirical Investigation of the Deceived'''
{{header}}
{{article
|author= Ryan T. Wright,Kent Marett,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Phishing has been a major problem for information systems managers and users for several years now. In 2008, it was estimated that phishing resulted in close to $50 billion in damages to U.S. consumers and businesses. Even so, research has yet to explore many of the reasons why Internet users continue to be exploited. The goal of this paper is to better understand the behavioral factors that may increase one's susceptibility for complying with a phisher's request for personal information. Using past research on deception detection, a research model was developed to help explain compliant phishing responses. The model was tested using a field study in which each participant received a phishing e-mail asking for sensitive information. It was found that four behavioral factors were influential as to whether the phishing e-mails were answered with sensitive information. The paper concludes by suggesting that the behavioral aspect of susceptible users be integrated into the current tools and materials used in antiphishing efforts.
|keyword = computer-mediated deception,electronic mail fraud,Internet security,interpersonal deception theory,phishing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPUTING IN EVERYDAY LIFE: A CALL FOR RESEARCH ON EXPERIENTIAL COMPUTING'''
{{header}}
{{article
|author= Youngjin Yoo,
|source= MIS QUARTERLY
|year= 2010
|abstract = The information systems field emerged as a new discipline of artificial science as a result of intellectual efforts to understand the nature and consequences of computer and communication technology in modern organizations. As the rapid development of digital technology continues to make computers and computing apart of everyday experiences, we are once again in need of a new discipline of the artificial. In this essay, I argue that the IS community must expand its intellectual boundaries by embracing experiential computing as an emerging field of inquiry in order to fill this growing intellectual void. Experiential computing involves digitally mediated embodied experiences in everyday activities through everyday artifacts that have embedded computing capabilities. Experiential computing is enabled by the mediation of four dimensions of human experiences ('time, space, actors, and artifacts) through digital technology. Drawing on a research framework that encompasses both behavioral and design sciences, six research opportunities that the IS research community can explore are suggested. Ultimately, I propose that the IS field return to its roots, the science of the artificial, by decisively expanding the scope of its inquiry and establishing a new domain of research on computing in everyday life experiences.
|keyword = Experiential computing,science of artificial,design,digitalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SYSTEMS STRATEGY: RECONCEPTUALIZATION, MEASUREMENT, AND IMPLICATIONS'''
{{header}}
{{article
|author= Daniel Q. Chen,Martin Mocker,David S. Preston,Alexander Teubner,
|source= MIS QUARTERLY
|year= 2010
|abstract = Information systems strategy is of central importance to IS practice and research. Our extensive review of the literature suggests that the concept of IS strategy is a term that is used readily; however, it is also a term that is not fully understood. In this study, we fallow a perspective paradigm based on the strategic management literature to define IS strategy as an organizational perspective on the investment in, deployment, use, and management of IS. Through a systematic literature search, we identify the following three conceptions of IS strategy employed implicitly in 48 articles published in leading IS journals that focus on the construct of IS strategy: (1) IS strategy as the use of IS to support business strategy; (2) IS strategy as the master plan of the IS function; and (3) IS strategy as the shared view of the IS role within the organization. We find the third conception best fits our definition of IS strategy. As such, we consequently propose to operationalize IS strategy as the degree to which the organization has a shared perspective to seek innovation through IS. Specifically, our proposed IS strategic typology suggests an organization's IS strategy falls into one of the two defined categories (i.e., IS innovator or IS conservative) or is simply undefined. We also develop measures for this new typology. We argue that the proposed instrument, which was cross-validated across both chief information officers and senior business executives, has the potential to serve as a diagnostic tool through which the organization can directly assess its IS strategy. We contend that our reconceptualization and operationalization of IS strategy provides theoretical and practical implications that advance the current level of understanding of IS strategy from extant studies within three predominant literature streams: strategic IS planning, IS/business strategic alignment, and competitive use of IS.
|keyword = IS strategy,IS strategic alignment,strategic IS planning,competitive advantage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BRAND POSITIONING STRATEGY USING SEARCH ENGINE MARKETING'''
{{header}}
{{article
|author= Wenyu Dou,Kai H. Lim,Chenting Su,Nan Zhou,Nan Cui,
|source= MIS QUARTERLY
|year= 2010
|abstract = Whether and how firms can employ relative rankings in search engine results pages (SERPs) to differentiate their brands from competitors in cyberspace remains a critical, puzzling issue in e-commerce research. By synthesizing relevant literature from cognitive psychology, marketing, and e-commerce, this study identifies key contextual factors that are conducive for creating brand positioning online via SERPs. In two experiments, the authors establish that when Internet users' implicit beliefs (i.e., schema) about the meaning of the display order of search engine results are activated or heightened through feature priming, they will have better recall of an unknown brand that is displayed before the well-known brands in SERPs. Further, those with low Internet search skills tend to evaluate the unknown brand more favorably along the particular brand attribute that activates the search engine ranking schema. This research has both theoretical and practical implications for understanding the effectiveness of search engine optimization techniques.
|keyword = e-commerce,search engine optimization,web design,brand positioning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPECTATION DISCONFIRMATION AND TECHNOLOGY ADOPTION: POLYNOMIAL MODELING AND RESPONSE SURFACE ANALYSIS'''
{{header}}
{{article
|author= Viswanath Venkatesh,Sandeep Goyal,
|source= MIS QUARTERLY
|year= 2010
|abstract = Individual-level information systems adoption research has recently seen the introduction of expectation disconfirmation theory (EDT) to explain how and why user reactions change over time. This prior research has produced valuable in-sights into the phenomenon of technology adoption beyond traditional models, such as the technology acceptance model. First, we identify gaps in EDT research that present potential opportunities advances specifically, we discuss methodological and analytical limitations in EDT research in information systems and present polynomial modeling and response surface methodology as solutions. Second, we draw from research on cognitive dissonance, realistic job preview, and prospect theory to present a polynomial model of expectation-disconfirmation in information systems. Finally, we test our model using data gathered over a period of 6 months among 1,143 employees being introduced to a new technology. The results confirmed our hypotheses that disconfirmation in general was bad, as evidenced by low behavioral intention to continue using a system for both positive and negative disconfirmation, thus supporting the need for a polynomial model to understand expectation disconfirmation in information systems.
|keyword = Polynomial modeling,response surface methodology,nonlinear modeling,difference scores,direct measures,technology acceptance model,expectation disconfirmation theory,IS continuance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MULTI-PROJECT MODEL OF KEY FACTORS AFFECTING ORGANIZATIONAL BENEFITS FROM ENTERPRISE SYSTEMS'''
{{header}}
{{article
|author= Peter B. Seddon,Cheryl Calvert,Song Yang,
|source= MIS QUARTERLY
|year= 2010
|abstract = This paper develops a long-term, multi-project model of factors affecting organizational benefits from enterprise systems (ES), then reports a preliminary test of the model. In the shorter-term half of the model, it is hypothesized that once a system has gone live, two factors, namely functional fit and overcoming organizational inertia, drive organizational benefits flowing from each major ES improvement project. The importance of these Actors may vary from project to project. In the long-term half of the model, it is hypothesized that four additional factors, namely integration, process optimization, improved access to information, and on-going major ES business improvement projects, drive organizational benefits from ES over the long term. Preliminary tests of the model were conducted using data from 126 customer presentations from SAP's 2003 and 2005 Sapphire U.S. conferences. All six factors were found to be important in explaining variance in organizational benefits from enterprise systems from the perspective of senior management.
|keyword = Enterprise system success,packaged software,functional fit,overcoming organizational inertia,change management,IS implementation,IS project management,integration,process optimization,improved access to information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INVESTIGATING Two CONTRADICTORY VIEWS OF FORMATIVE MEASUREMENT IN INFORMATION SYSTEMS RESEARCH'''
{{header}}
{{article
|author= Gimun Kim,Bongsik Shin,Varun Grover,
|source= MIS QUARTERLY
|year= 2010
|abstract = The use of formative measurement in the field of Information Systems has increased, arguably due to statistical tools (e.g., PLS) that can test such models. However, in the literature, there exist two contradictory views on the potential deficiency of formative measurement. While opponents who are critical of formative measurement argue that there are native weaknesses of the formative approach in model estimation, proponents who are in favor of using formative measurement counter that opponents research methods in measurement model specification are flawed. The goal of this work is to empirically test these opposing views on whether the alleged estimation instability of formative measurement is due to measurement model misspecification or simply the shortcoming of formative measurement. To assess the integrity of arguments of both parties, we adopt a research design in which four different cases are tested in terms of interpretational confounding and external consistency. We find that regardless of whether there is a specification issue, formative measures can lead to misleading outcomes. Based on the results, we offer guidelines that researchers may adopt in planning and executing data analysis with structural equation modeling. Given that the use of formative measurement is at a critical juncture in the IS field, we believe that the guidelines in this research note are important to promote appropriate use of the approach rather than relegate it to a bandwagon effect.
|keyword = Formative measurement,formative indicators,measurement models,measurement instability,external consistency,interpretational confounding,information systems measures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTRODUCTION TO THE SPECIAL ISSUE ON NOVEL PERSPECTIVES ON TRUST IN INFORMATION SYSTEMS'''
{{header}}
{{article
|author= Izak Benbasat,David Gefen,Paul A. Pavlou,
|source= MIS QUARTERLY
|year= 2010
|abstract = Research on trust has taken center stage in the MIS field in the past few decades, covering a wide range of trust-related topics based on a multitude of theories from sociology and psychology to economics. To extend this rapidly emerging trend and identify some ground-breaking perspectives on the study of trust, this special issue of the MIS Quarterly on "Novel Perspectives on Trust in Information Systems" aims to explore novel aspects of trust in new and under-researched IS contexts. In brief, the intent of the special issue was to publish innovative research articles about (1) novel ante- cedents of trust, (2) the construct of distrust and its relationship to trust, (3) the boundaries of trust, and (4) the study of trust in new and unexplored MIS contexts (Benbasat et al. 2008).
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT DOES THE BRAIN TELL US ABOUT TRUST AND DISTRUST? EVIDENCE FROM A FUNCTIONAL NEUROIMAGING STUDY'''
{{header}}
{{article
|author= Angelika Dimoka,
|source= MIS QUARTERLY
|year= 2010
|abstract = Determining whom to trust and whom to distrust is a major decision in impersonal IT-enabled exchanges. Despite the potential role of both trust and distrust in impersonal exchanges, the information systems literature has primarily focused on trust, alas paying relatively little attention to distrust. Given the importance of studying both trust and distrust, this study alms to shed light on the nature, dimensionality, distinction, and relationship, and relative effects of trust and distrust on economic outcomes in the context of impersonal IT-enabled exchanges between buyers and sellers in online marketplaces. This study uses functional neuroimaging (fMRI) tools to complement psychometric measures of trust and distrust by observing the location, timing, and level of brain activity that underlies trust and distrust and their underlying dimensions. The neural correlates of trust and distrust are identified when subjects interact with four experimentally manipulated seller profiles that differ on their level of trust and distrust. The results show that trust and distrust activate different brain areas and have different effects, helping explain why trust and distrust are distinct constructs associated with different neurological processes. Implications for the nature, distinction and relationship, dimensionality, and effects of trust and distrust are discussed.
|keyword = Trust,distrust,neuroIS,price premiums,functional neuroimaging,fMRI,cognitive neuroscience,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ARE THERE NEURAL GENDER DIFFERENCES IN ONLINE TRUST? AN FMRI STUDY ON THE PERCEIVED TRUSTWORTHINESS OF EBAY OFFERS'''
{{header}}
{{article
|author= Rene Riedl,Marco Hubert,Peter Kenning,
|source= MIS QUARTERLY
|year= 2010
|abstract = Research provides increasing evidence that women and men differ in their decisions to trust. However, information systems research does not satisfactorily explain why these gender differences exist. One possible reason is that, surprisingly, theoretical concepts often do not address the most obvious factor that influences human behavior: biology. Given the essential role of biological factors-and specifically those of the brain in decisions to trust, the biological influences should naturally include those related to gender. As trust considerations in economic decision making have become increasingly complex with the expansion of Internet use, understanding the related biological/brain functions and the involvement of gender provides a range of valuable insights. To show empirically that online trust is associated with activity changes in certain brain areas, we used functional magnetic resonance imaging (fMRI). In a laboratory experiment, we captured the brain activity of female and 10 male participants simultaneous to decisions on trustworthiness of eBay offers. We found that most of the brain areas that encode trustworthiness differ between women and men. Moreover, we found that women activated more brain areas than did men. These results confirm the empathizing systemizing theory, which predicts gender differences in neural information processing modes. In demonstrating that perceived trustworthiness of Internet offers is affected by neurobiology, our study has major implications for both IS research and management. We confirm the value of a category of research heretofore neglected in IS research and practice, and argue that future IS research investigating human behavior should consider the role of biological 'actors. In practice, biologicalfactors are a significant consideration.for management, marketing, and engineering attempts to influence behavior.
|keyword = Online trust,trustworthiness,functional magnetic resonance imaging (fMRI),gender,eBay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT Careers Camp: An Early Intervention Strategy to Increase IS Enrollments'''
{{header}}
{{article
|author= Vivek Choudhury,Alexandre B. Lopes,Doug Arthur,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = T his paper reports on a specific promotional initiative designed to spur enrollment in IT-related fieldsan IT Careers Camp aimed at high school students. The camp was different from most prior computer camps in that it was not aimed at building skills such as programming or Web development. Rather, it was specifically designed to convince participants that (1) job prospects in the field are strong, and (2) IT/IS work is interesting and creative. To this end, the camp was designed in partnership with a number of corporations, and included as a central element a series of experiential opportunities for participants. Each day of the camp featured a visit to a corporation where the students took part in a hands-on activity that involved solving a business problem with IT. Qualitative and quantitative evaluations indicate that the camp was very successful in changing students' perceptions about the nature of IT work and the IT job market. We believe the camp can be a useful tool to create a pipeline of well-informed students interested in IT careers. We present here details of the design and execution of the camp in the hope that others may wish to replicate our efforts.
|keyword = summer camps,IS enrollments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On Evaluating Information Revelation Policies in Procurement Auctions: A Markov Decision Process Approach'''
{{header}}
{{article
|author= Amy Greenwald,Karthik Kannan,Ramayya Krishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Each market session in a reverse electronic marketplace features a procurer and many suppliers. An important attribute of a market session chosen by the procurer is its information revelation policy. The revelation policy determines the information (such as the number of competitors, the winning bids, etc.) that will be revealed to participating suppliers at the conclusion of each market session. Suppliers participating in multiple market sessions use strategic bidding and fake their own cost structure to obtain information revealed at the end of each market session. The information helps to reduce two types of uncertainties encountered in future market sessions, namely, their opponents' cost structure and an estimate of the number of their competitors. Whereas the first type of uncertainty is present in physical and e-marketplaces, the second type of uncertainty naturally arises in IT-enabled marketplaces. Through their effect on the uncertainty faced by suppliers, information revelation policies influence the bidding behavior of suppliers which, in turn, determines the expected price paid by the procurer. Therefore, the choice of information revelation policy has important consequences for the procurer. This paper develops a partially observable Markov decision process model of supplier bidding behavior and uses a multiagent e-marketplace simulation to analyze the effect that two commonly used information revelation policies-complete information policy and incomplete information policy-have on the expected price paid by the procurer. We find that the expected price under the complete information policy is lower than that under the incomplete information policy. The integration of ideas from the multiagents literature, the machine-learning literature, and the economics literature to develop a method to evaluate information revelation policies in e-marketplaces is a novel feature of this paper.
|keyword = auctions,information revelation,MDP,game theory simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contractual Provisions to Mitigate Holdup: Evidence from Information Technology Outsourcing'''
{{header}}
{{article
|author= Anjana Susarla,Ramanath Subramanyam,Prasanna Karhade,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = T he complexity and scope of outsourced information technology (IT) demands relationship-specific investments from vendors, which, when combined with contract incompleteness, may result in underinvestment and inefficient bargaining, referred to as the holdup problem. Using a unique data set of over 100 IT outsourcing contracts, we examine whether contract extensiveness, i.e., the extent to which firms and vendors can foresee contingencies when designing contracts for outsourced IT services, can alleviate holdup. While extensively detailed contracts are likely to include a greater breadth of activities outsourced to a vendor, task complexity makes it difficult to draft extensive contracts. Furthermore, extensive contracts may still be incomplete with respect to enforcement. We then examine the role of nonprice contractual provisions, contract duration, and extendibility terms, which give firms an option to extend the contract to limit the likelihood of holdup. We also validate the ex post efficiency of contract design choices by examining renewals of contracting agreements.
|keyword = contract duration,extendibility clauses,holdup,underinvestment,information technology outsourcing,incomplete contracts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Is Query Reuse Potentially Harmful? Anchoring and Adjustment in Adapting Existing Database Queries'''
{{header}}
{{article
|author= Gove Allen,Jeffrey Parsons,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Reusing database queries by adapting them to satisfy new information requests is an attractive strategy for extracting information from databases without involving database specialists. However, the reuse of information systems artifacts has been shown to be susceptible to the phenomenon of anchoring and adjustment. Anchoring often leads to a systematic adjustment bias in which people fail to make sufficient changes to an anchor in response to the needs of a new task. In a study involving 157 novice query writers from six universities, we examined the effect of this phenomenon on the reuse of Structured Query Language (SQL) queries under varying levels of domain familiarity and for different types of anchors. Participants developed SQL queries to respond to four information requests in a familiar domain and four information requests in an unfamiliar domain. For two information requests in each domain, participants were also provided with sample queries (anchors) that answered similar information requests. We found evidence that the opportunity to reuse sample queries resulted in an adjustment bias leading to poorer quality query results and greater overconfidence in the correctness of results. The results also indicate that the strength of the adjustment bias depends on a combination of domain familiarity and type of anchor. This study demonstrates that anchoring and adjustment during query reuse can lead to queries that are less accurate than those written from scratch. We also extend the concept of anchoring and adjustment by distinguishing between surface-structure and deep-structure anchors and by considering the impact of domain familiarity on the adjustment bias.
|keyword = reuse,anchoring and adjustment,SQL,query formulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Use of Pricing Schemes for Differentiating Information Goods'''
{{header}}
{{article
|author= Vidyanand Choudhary,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Information goods vendors offer different pricing schemes such as per user pricing and site licensing. Why do competing sellers adopt different pricing schemes for the same information good? Pricing schemes affect buyers' usage levels and thus the revenue generated from different segments of buyers. This can allow competing firms in a duopoly to differentiate themselves by offering different pricing schemes. Such strategic use of pricing schemes can allow undifferentiated sellers to earn substantial profits in a friction-free market for a commoditized information good. These conditions would otherwise lead to the Bertrand equilibrium and zero profits. We show that adopting asymmetric pricing schemes can be a Nash equilibrium for information goods with negligible marginal cost of production. We extend our model to the case of information goods that are horizontally differentiated and show that sellers will offer a single-pricing scheme that is different from competitors when the sellers are weakly differentiated. When the sellers are strongly differentiated, each seller will offer multiple pricing schemes. We show that it can be optimal for a seller to offer multiple pricing schemes-metered and flat fee pricing schemes, even in the absence of transactions costs.
|keyword = duopoly,price competition,pricing,pricing schemes,information goods,differentiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design and Analysis of Contracts for Software Outsourcing'''
{{header}}
{{article
|author= Debabrata Dey,Ming Fan,Conglei Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Outsourcing of software development allows a business to focus on its core competency and take advantage of vendors' technical expertise, economies of scale and scope, and their ability to smooth labor demand fluctuations across several clients. However, contracting a software project to an outside developer is often quite challenging because of information asymmetry and incentive divergence. A typical software development contract must deal with a variety of interrelated issues such as the quality of the developed system, the timeliness of delivery, the effort and cost associated with the project, the contract payment, and the postdelivery software support. This paper presents a contract-theoretic model that incorporates these factors to analyze how software outsourcing contracts can be designed. We find that despite their relative inefficiency, fixed-price contracts are often appropriate for simple software projects that require short development time. Time-and-materials contracts work well for more complex projects when the auditing process is efficient and effective. We also examine a type of performance-based contract called quality-level agreement and find that the first-best solution can be reached with such a contract. Finally, we consider profit-sharing contracts that are useful in situations where the developer has more bargaining power.
|keyword = contract design,software engineering,software outsourcing,performance-based contracts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Analysis of Software Vendors' Patch Release Behavior: Impact of Vulnerability Disclosure'''
{{header}}
{{article
|author= Ashish Arora,Ramayya Krishnan,Rahul Telang,Yubao Yang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = A key aspect of better and more secure software is timely patch release by software vendors for the vulnerabilities in their products. Software vulnerability disclosure, which refers to the publication of vulnerability information, has generated intense debate. An important consideration in this debate is the behavior of software vendors. How quickly do vendors patch vulnerabilities and how does disclosure affect patch release time? This paper compiles a unique data set from the Computer Emergency Response Team/Coordination Center (CERT) and SecurityFocus to answer this question. Our results suggest that disclosure accelerates patch release. The instantaneous probability of releasing the patch rises by nearly two and a half times because of disclosure. Open source vendors release patches more quickly than closed source vendors. Vendors are more responsive to more severe vulnerabilities. We also find that vendors respond more slowly to vulnerabilities not disclosed by CERT. We verify our results by using another publicly available data set and find that results are consistent. We also show how our estimates can aid policy makers in their decision making.
|keyword = security vulnerability,disclosure policy,patch release time,open source vendors,information security,software vendors,hazard model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ex Ante Information and the Design of Keyword Auctions'''
{{header}}
{{article
|author= De Liu,Jianqing Chen,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Keyword advertising, including sponsored links and contextual advertising, powers many of today's online information services such as search engines and Internet-based emails. This paper examines the design of keyword auctions, a novel mechanism that keyword advertising providers such as Google and Yahoo! use to allocate advertising slots. In our keyword auction model, advertisers bid their willingness-to-pay per click on their advertisements, and the advertising provider can weight advertisers' bids differently and require different minimum bids based on advertisers' click-generating potential. We study the impact and design of such weighting schemes and minimum-bid policies. We find that weighting scheme determines how advertisers with different click-generating potential match in equilibrium. Minimum bids exclude low-valuation advertisers and at the same time may distort the equilibrium matching. The efficient design of keyword auctions requires weighting advertisers' bids by their expected click-through-rates, and requires the same minimum weighted bids. The revenue-maximizing weighting scheme may or may not favor advertisers with low click-generating potential. The revenue-maximizing minimum-bid policy differs from those prescribed in the standard auction design literature. Keyword auctions that employ the revenue-maximizing weighting scheme and differentiated minimum bid policy can generate higher revenue than standard fixed-payment auctions. We draw managerial implications for pay-per-click and other pay-for-performance auctions and discuss potential applications to other areas.
|keyword = keyword auctions,keyword advertising,sponsored links,weighted unit-price auctions,weighting scheme,Google,Yahoo,minimum bid,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic Markets, Search Costs, and Firm Boundaries'''
{{header}}
{{article
|author= Ramesh Sankaranarayanan,Arun Sundararajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = We study how interorganizational systems (IOS) such as electronic markets and other enabling information technologies that facilitate broader interfirm transactions affect the extent of outsourcing in firms. We do so by modeling firms in a three-tier value chain consisting of buyers, intermediaries, and suppliers, who can interact using IOS that lower the procurement search costs associated with finding appropriate trading partners. In the context of complex business-to-business (B2B) search, we study how decreasing search costs affect a firm's decision to insource or outsource the procurement function, depending on whether the search process is information intensive or communication intensive. Variation in search costs changes the transaction costs of interaction between firms, as well as the contracting costs associated with outsourcing, owing to changes in the costs of moral hazard for delegated search. We study these effects in a new model that integrates search theory into the principal-agent framework, and establish that the optimal outsourcing contract has a simple "all or nothing" performance-based structure under fairly general assumptions. Our model predicts that when B2B search is information intensive, IOS will facilitate an increase in outsourcing, market-based transactions, and a reduction in the vertical scope of extended enterprises. In contrast, when B2B search is primarily communication intensive, IOS will lead to tighter integration and an increase in the vertical scope of the extended enterprise. Our research suggests that the nature of the information technologies and of the business activities supported by IOS are crucial determinants of the organizational and industry changes they induce, and our results have important implications for a variety of industries in which both technological and agency issues will influence the eventual success of global IT-facilitated extended enterprise initiatives.
|keyword = outsourcing,electronic commerce,interorganizational information systems,electronic markets,economics of IS,information systems and organizational change,IT impacts on industry and market structure,IT-enabled supply chains,IT and new organizational forms,search costs,moral hazard,agency theory,analytical modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Induction over Strategic Agents'''
{{header}}
{{article
|author= Fidan Boylu,Haldun Aytug,Gary J. Koehler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = W e study the problem where a decision maker needs to discover a classification rule to classify intelligent, self-interested agents. Agents may engage in strategic behavior to alter their characteristics for a favorable classification. We show how the decision maker can induce a classification rule that anticipates such behavior while still satisfying an important risk minimization principle.
|keyword = discriminant analysis,principal-agent,strategic gaming,generalization,adversarial learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation'''
{{header}}
{{article
|author= Animesh Animesh,Vandana Ramachandran,Siva Viswanathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2010
|abstract = Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.
|keyword = electronic commerce,competitive impacts of IS,IT impacts on industry and market structure,econometrics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Interdisciplinary Perspective on IT Services Management and Service Science'''
{{header}}
{{article
|author= Indranil R. Bardhan,Haluk Demirkan,P. K. Kannan,Robert J. Kauffman,Ryan Sougstad,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The increasing importance of information technology (IT) services in the global economy prompts researchers in the field of information systems (IS) to give special attention to the foundations of managerial and technical knowledge in this emerging arena of knowledge. Already we have seen the computer science discipline embrace the challenges of finding new directions in design science toward making services-oriented computing approaches more effective, setting the stage for the development of a new science service science, management, and engineering (SSME). This paper addresses the issues from the point of view of service science as a fundamental area for IS research. We propose a robust framework for evaluating the research on service science, and the likely outcomes and new directions that we expect to see in the coming decade. We emphasize the multiple roles of producers and consumers of services-oriented technology innovations, as well as value-adding seller intermediaries and systems integrators, and standards organizations, user groups, and regulators as monitors. The analysis is cast in multidisciplinary terms, including computer science and IS, economics and finance, marketing, and operations and supply chain management. Evaluating the accomplishments and opportunities for research related to the SSME perspective through a robust framework enables in-depth assessment in the present, as well as an ongoing evaluation of new knowledge in this area, and the advancement of the related management practice capabilities to improve IT services in organizations.
|keyword = cloud computing,economics,IS,IT services,literature survey,marketing,operations,research directions,service science,services management,services-oriented systems,system science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Connecting IT Services Operations to Services Marketing Practices'''
{{header}}
{{article
|author= Mitzi M. Montoya,Anne P. Massey,Vijay Khatri,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The importance of building relationships with customers and trust in the services provider is well documented in the marketing literature. Conceptually, we extend this logic to the context of internal information technology (IT) services operations through the notion of the service delivery chain. The purpose of the study is to examine how key service mechanisms in operational IT implementation are related to employee perceptions of actual system benefits and trust in the IT services provider. We report on a study with 380 employees of 14 bank affiliates that were recently acquired by a bank holding company. The focus of the study is on postimplementation trust rather than preimplementation or initial trust, and the service provider is viewed as the object of trust rather than the technology. Our findings suggest that training, trial, and social influence are key service mechanisms an IT services provider can use to stimulate trust in the IT services provider and the realization of system benefits.
|keyword = IT services,mandated systems,relational trust,service delivery chain,service mechanisms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Multitask Agency, Modular Architecture, and Task Disaggregation in SaaS'''
{{header}}
{{article
|author= Anjana Susarla,Anitesh Barua,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = We examine contract choices in the provision of "software-as-a-service" (SaaS), which is a business innovation that transforms information technology (IT) resources into a continuously provided service. We draw upon agency theory and modularity theory to propose that one of the central challenges in service disaggregation is that of knowledge interdependencies across client and provider organizations. The resulting lack of verifiability of certain tasks results in a multitask agency problem. Our key research questions involve (1) the suitability of high- versus low-powered incentives in SaaS contracts when the outsourced tasks involve business analytics that are difficult to verify, and (2) how such contract choices are affected by the modularity of interfaces between the client and the provider. Analysis of data collected from 154 providers of SaaS offering a range of IT services supports our contention that when contracting for business analytics characterized by knowledge interdependencies across clients and providers, incentives should be "low powered." Modularity in the interfaces of the service provider increases the desirability of high-powered incentives in such situations. Our results are robust after accounting for endogeneity issues arising from unobserved matching between service providers and the nature of IT services outsourced by clients. With the increasing importance of information systems in services, this paper suggests that arm's-length relationships and high-powered incentives may be ineffective in incentivizing providers to perform on complex business analytic tasks, unless accompanied by the modularization of interfaces.
|keyword = endogenous matching,information technology,modularity,multitask agency,outsourcing,service science,services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coordination Strategies in an SaaS Supply Chain'''
{{header}}
{{article
|author= Haluk Demirkan,Hsing Kenneth Cheng,Subhajyoti Bandyopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The computing industry is gradually evolving to cater to the demand for software-as-a-service (SaaS). Two core competencies that have emerged over the past few years are that of the application service providers (ASPs) and the application infrastructure providers (AIPs). The arrangements between them result in system dynamics that is typical in supply chain networks. We examine the performance of an SaaS set up under different coordination strategies between these two players. Our analysis indicates that coordination between the monopoly ASP and the AIP can result in an outcome with the same overall surplus as can be achieved by a central planner. Even though the players have an incentive to deviate, it is possible to create the right incentives so that the economically efficient outcome is also the Nash equilibrium. The results of the analysis have significant implications for the coordination strategies for providers in the burgeoning business model of delivering software services over the Internet.
|keyword = cloud computing,economic analysis,information sharing,infrastructure-as-a-service,service science,services,services management,software-as-a-service,strategy,supply chain coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the Economic Potential of Service-Oriented Architecture'''
{{header}}
{{article
|author= Benjamin Mueller,Goetz Viering,Christine Legner,Gerold Riempp,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Service-oriented architecture (SOA) is one of the most discussed topics in the information systems (IS) discipline. While most computer scientists agree that the service-oriented paradigm has clear benefits in terms of technical quality attributes, it has been difficult to justify SOA economically. The few studies that have investigated the strategic and economic aspects of SOA are mostly exploratory and lack a more comprehensive framework for understanding the sources of its economic potential. Based on IS and SOA literature, our work goes further in suggesting the SOA economic potential model, which describes the causal relationships between the SOA's style characteristics and value it can provide on the business side. Using this model, we investigate 164 SOA cases published between 2003 and 2008 to explore the economic rationale for adopting SOA. Our findings suggest that SOA's business benefits are currently mainly driven by operational and information technology infrastructural improvements. However, enterprises also realize strategic benefits from SOA; for example, by electronically integrating with their business partners by means of SOA. We use the results of our study to derive propositions and suggest a research model for future studies on SOA's economic potential.
|keyword = business benefits,economic potential,IS value,service-oriented architecture,service science,services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managers' Judgments of Performance in IT Services Outsourcing'''
{{header}}
{{article
|author= Vandana Ramachandran,Anandasivam Gopal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = An important task for managers in information technology (IT) service settings is the judgment of service performance. The complex and intangible nature of IT services, however, renders this task especially difficult. We use a sample of 85 outsourced software development projects to test for the presence of the "input bias," which is defined as the systematic misuse of nondiagnostic input information in forming managerial judgments of outcomes. The service outcome we examine is process performance. The diagnostic inputs are given by objective performance metrics based on the final cost and duration of completed projects, whereas the nondiagnostic inputs are risk anticipations formed by managers prior to the start of the project. We find strong evidence of the input bias, which leads managers' subjective assessments to diverge considerably from objective outcomes, and that it is moderated by contract type. Our study contributes to better service management by improving our understanding of managers' judgments of service performance and how these judgments are formed.
|keyword = decision-making biases,field studies,input bias,IT services outsourcing,judgment,regression analysis,service science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Service Science Perspective on Strategic Choice, IT, and Performance in US Banking'''
{{header}}
{{article
|author= Paul P. Tallon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = With the move to an information-based economy, financial services has become a key contributor to the U.S. gross domestic product. Even as consolidation reduces the number of banks, small banks with under $100 million in assets continue to report higher profit margins than large banks with over $100 million in assets. Lacking scale, small banks employ a service-oriented business strategy (customer intimacy), whereas large banks focus on productivity and throughput (operational excellence). Information technology (IT) plays a key role in applying each strategy, but as banks move toward customer intimacy in general, the challenge is to grow without undermining service quality. Using a balanced panel data set from 43 U.S. banks, this paper finds that banking strategies are becoming more customer focused. Yet for large banks in particular, IT remains resolutely operations focused. This misalignment could restrict future banking performance. In this way, this paper contributes to the service science literature by using size to dissect banking strategies and performance.
|keyword = banking,business value,customer intimacy,financial services,relationship banking,service science,services,strategic alignment,strategic choice,value disciplines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Implementing Service-Oriented Architecture in Organizations'''
{{header}}
{{article
|author= Jae Choi,Derek L. Nazareth,Hemant K. Jain,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = Service-oriented architecture (SOA) has been promoted as a technology that can enhance information systems agility, interoperability between applications, deployment flexibility, and reusability. As with any new information technology (IT), the decision to adopt SOA cannot be taken lightly, given the nontrivial investment in economic and personnel resources. The complexity associated with industry-wide diffusion, coupled with organization, industry, and environment factors, contributes to a lack of a clear strategy for assessing the business value that SOA provides an organization. This research attempts to shed light on this process of value creation for an organization, using a system dynamics approach. A detailed model of the industry diffusion coupled with organization adoption is presented. After suitable calibration and validation, a series of simulations using the model evaluate the efficacy of SOA under a variety of diverse conditions. The results of the simulations indicate clear benefits of SOA over monolithic ITs when employed under appropriate conditions. Situations where SOA fails to live up to expectations are also identified. The model and accompanying simulations can serve as a practical decision support tool for an organization to help make the strategic decisions of adopting and implementing SOA.
|keyword = adoption,design science,service-oriented architecture,service science,services,system dynamics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Demand Information Sharing in Heterogeneous IT Services Environments'''
{{header}}
{{article
|author= Sagnika Sen,T. S. Raghu,Ajay Vinze,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = In an information technology services outsourcing arrangement, variance in demand volume and individual user preferences pose significant challenges to the provider organization in making resource allocation decisions. Such variations affect service levels, especially under fixed resource constraints. We explore the possible role of periodic demand information sharing and subsequent resource-level adjustments as a means of addressing issues arising from demand variation. As information exchange alters the dynamics of the relationship between the customer and provider organizations, incorporating information sharing in service-level agreements requires modifying current pricing schemes. A pricing heuristic is developed and tested under varying levels of information accuracy and granularity. The heuristic is shown to provide better economic welfare for both participants in comparison to the baseline pricing strategies considered. Also, it is shown that information, even at a coarse level of granularity, is very effective in providing stable service levels a finding that is encouraging for enhanced collaborations between customer and provider organizations in outsourcing arrangements.
|keyword = heterogeneous demand,information sharing,IT services,pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Should We Go Our Own Way? Backsourcing Flexibility in IT Services Contracts'''
{{header}}
{{article
|author= Michel Benaroch,Qizhi Dai,Robert J. Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2010
|abstract = The emergence of new service science approaches to business problems in information technology (IT) services offers new, unusually relevant insights for the senior management of vendors in this business area. This research examines how service-level agreement contract flexibility should be designed when the technological and business market environments result in volatility of demand, based on an understanding of related changes in the cost drivers that underlie IT services contracts. Our approach draws on a blend of well-known methods from financial economics the real option pricing method and the contingent claims analysis method. In particular, our research examines a setting in which a vendor provides IT services to a client according to a prenegotiated IT services contract in the presence of demand volatility. We analyze the motivation of and value consequences for a vendor that offers the client the flexibility to opt out of the contract. For example, the client might switch to another vendor, or backsource and provide its own services internally. Our core results offer important foundational thinking for how to specify various forms of IT service-related flexibility in terms of put and call options from the point of view of an IT services vendor, so that their value and exercise timing can be estimated. We show that the client firm's demand trigger value for deciding when to backsource its IT services varies, and it depends on the degree of demand volatility as well as the usage-based fees charged by the vendor. Working from our modeling approach, we also are able to characterize the extent to which a vendor can benefit from bearing the costs of making a backsourcing flexibility option available to its client.
|keyword = contingent claims analysis,demand trigger value,demand uncertainty,financial economics,IT services,outsourcing,real options,service science,valuation,vendors,volatility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SYSTEMS INNOVATION FOR ENVIRONMENTAL SUSTAINABILITY'''
{{header}}
{{article
|author= Nigel P. Melville,
|source= MIS QUARTERLY
|year= 2010
|abstract = Human life, is dependent upon the natural environment, which, most would agree, is rapidly degrading. Business enterprises are a dominant form of social organization and contribute to the worsening, and enhancement, of the natural environment. Scholars in the administrative sciences examine questions spanning organizations and the natural environment but have largely omitted the information systems perspective. We develop a research agenda on information systems innovation for environmental sustainability that demonstrates the critical role that is can play in shaping beliefs about the environment, in enabling and transforming sustainable processes and practices in organizations, and in improving environmental and economic performance. The belief-action-outcome (BAO) framework and associated research agenda provide the basis for a new discourse on IS for environmental sustainability.
|keyword = Belief-action-outcome (BAO) framework,environment,environmental management system,green,information system,innovation,organization,sustainability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SYSTEMS AND ENVIRONMENTALLY SUSTAINABLE DEVELOPMENT: ENERGY INFORMATICS AND NEW DIRECTIONS FOR THE IS COMMUNITY'''
{{header}}
{{article
|author= Richard T. Watson,Marie-Claude Boudreau,Adela J. Chen,
|source= MIS QUARTERLY
|year= 2010
|abstract = While many corporations and Information Systems units recognize that environmental sustainability is an urgent problem to address, the IS academic community has been slow to acknowledge the problem and take action. We propose ways for the IS community to engage in the development of environmentally sustainable business practices. Specifially. as IS researchers, educators, journal editors, and association leaders, we need to demonstrate how the transformative power of IS can be leveraged to create an ecologically sustainable society. In this Issues and Opinions piece. we advocate a research agenda to establish a new subfield of energy informatics, which applies information systems thinking and skills to increase energy efficiency. We also articulate how IS scholars can incorporate environmental sustainability as an underlying foundation in their teaching, and how IS leaders can embrace environmental sustainability in their core principles and foster changes that reduce the environmental impact of our community.
|keyword = Environmental sustainability,energy informatics,IS community,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN EMPIRICAL ANALYSIS OF THE IMPACT OF INFORMATION CAPABILITIES DESIGN ON BUSINESS PROCESS OUTSOURCING PERFORMANCE'''
{{header}}
{{article
|author= Deepa Mani,Anitesh Barua,Andrew Whinston,
|source= MIS QUARTERLY
|year= 2010
|abstract = Organizations today outsource diverse business processes to achieve a wide variety of business objectives ranging from reduction of costs to innovation and business tran formation. We build on the information processing view of the firm to theorize that performance heterogeneity across business process outsourcing (BPO) exchanges is a function of the design of information capabilities (IC) that fit the unique information requirement (IR) of the exchange. Further, we compare performance effects of the fit between IR and IC across dominant categories of BPO relationships to provide insights into the relative benefits of enacting such fit between the constructs. Empirical tests of our hypotheses using survey data on 127 active BPO relationships find a significant increase (decrease) in satisfaction as a result of the fit (misfit) between IR and IC of the relationship. The results have implications for how BPO relationships must be designed and managed to realize significant performance gains. The study also extends the IPV to identify IC that provide the incentives and means to process information in an interfirm relationship.
|keyword = Business process outsourcing,governance,performance,information requirements,information capabilities,information processing view,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CHASING THE HOTTEST IT: EFFECTS OF INFORMATION TECHNOLOGY FASHION ON ORGANIZATIONS'''
{{header}}
{{article
|author= Ping Wang,
|source= MIS QUARTERLY
|year= 2010
|abstract = What happens to organizations that chase the hottest information technologies? This study examines some of the important organizational impacts of the fashion phenomenon in IT. An IT fashion is a transitory collective belief that an information technology is new, efficient, and at the forefront of practice. Using data collected from published discourse and annual IT budgets of 109 large companies for a decade, I have found that firms whose names were associated with IT fashions in the press did not have higher performance, but they had better reputation and higher executive compensation in the near term. Companies investing in IT in fashion also had higher reputation and executive pay but they had lower performance in the short term and then improved performance in the long term. These results support a fashion explanation for the middle phase diffusion of IT innovations, illustrating that following fashion can legitimize organizations and their leaders regardless of performance improvement. the findings also extend institutional theory from its usual focus on taken-for-granted practices to fashion as a novel source of social approval. This study suggests that practitioners balance between performance pressure and social approval when they confront whatever is hottest in IT.
|keyword = Information technology fashion,management fashion,innovation,diffusion,discourse,corporate reputation,executive compensation,legitimacy,performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD AGILE: AN INTEGRATED ANALYSIS OF QUANTITATIVE AND QUALITATIVE FIELD DATA ON SOFTWARE DEVELOPMENT AGILITY'''
{{header}}
{{article
|author= Gwanhoo Lee,Weidong Xia,
|source= MIS QUARTERLY
|year= 2010
|abstract = As business and technology environments change at an unprecedented rate, software development agility to respond to changing user requirements has become increasingly critical for software development performance. Agile software development approaches, which emphasize sense-and-respond, self-organization, cross-functional teams, and continuous adaptation, have been adopted by an increasing number of organizations to improve their software development agility. However, the agile development literature is largely anecdotal and prescriptive, lacking empirical evidence and theoretical foundation to support the principles and practices of agile development. Little research has empirically examined the software development agility construct in terms of its dimensions, determinants, and effects on software development performance. As a result, there is a lack of understanding about how organizations can effectively implement an agile development approach. Using an integrated research approach that combines quantitative and qualitative data analyses, this research opens the black box of agile development by empirically examining the relationships among two dimensions of software development agility (software team response extensiveness and software team response efficiency), two antecedents that can be con trolled (team autonomy and team diversity), and three aspects of software development performance (on-time completion, on-budget completion, and software functionality). Our PLS results of survey, responses of 399 software project managers suggest that the relationships among these variables are more complex than what has been perceived by the literature. The results suggest a tradeoff relationship between response extensiveness and response efficiency. These two agility dimensions impact software development performance differently: response efficiency positively affects all of on-time completion, on-budget completion, and software functionality, whereas response extensiveness positively affects only software functionality. The results also suggest that team autonomy has a positive effect on response efficiency and a negative effect on response extensiveness, and that team diversity has a positive effect on response extensiveness, We conducted 10 post hoc case studies to qualitatively cross-validate our PLS results and provide rich, additional insights regarding the complex, dynamic interplays between autonomy, diversity, agility, and performance. The qualitative analysis also provides explanations for both supported and unsupported hypotheses. We discuss these qualitative analysis results and conclude with the theoretical and practical implications of our research findings for agile development approaches.
|keyword = Software development agility,agile software development,team autonomy,team diversity,software development performance,requirement change,partial least square,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''VITAL SIGNS FOR VIRTUAL TEAMS: AN EMPIRICALLY DEVELOPED TRIGGER MODEL FOR TECHNOLOGY ADAPTATION INTERVENTIONS'''
{{header}}
{{article
|author= Dominic M. Thomas,Robert P. Bostrom,
|source= MIS QUARTERLY
|year= 2010
|abstract = This study explores how team leaders sense the need for technology adaptation intervention in distributed, computer mediated ("virtual') teams. Analysis and coding of critical incident data collected in interviews of practicing leaders produce a five-trigger model including (1) external constraint, (2) internal constraint, (3) information and communication technology (ICT) inadequacy, (4) ICT knowledge, skills, and abilities inadequacy, and (5) trust and relationship inadequacies. The resulting five-trigger model provides several key contributions including (1) a diagnostic tool for examining real, multi-trigger team technology, adaptation contexts, enabling better leader training and evaluation as well as improved research on team technology adaptation and interventions and (2) a better understanding of the relationship between the technology structure strength indicators in adaptive structuration theory and the need for team technology adaptation intervention.
|keyword = Virtual teams,leadership,project management,IS development teams,empirical research,critical incident technique,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''JOB CHARACTERISTICS AND JOB SATISFACTION: UNDERSTANDING THE ROLE OF ENTERPRISE RESOURCE PLANNING SYSTEM IMPLEMENTATION'''
{{header}}
{{article
|author= Michael G. Morris,Viswanath Venkatesh,
|source= MIS QUARTERLY
|year= 2010
|abstract = Little research has examined the impacts of enterprise resource planning (ERP) systems implementation on job satisfaction. Based on a 12-month study of 2,794 employees in a telecommunications firm, we found that ERP system implementation moderated the relationships between three job characteristics (skill variety, autonomy, and feedback) and job satisfaction. Our findings highlight the key role that ERP system implementation can have in altering well established relationships in the context of technology-enabled organizational change situations. This work also extends research on technology diffusion by moving beyond a focus on technology-centric outcomes, such as system use, to understanding broader job outcomes.
|keyword = ERP systems,job characteristics,job satisfaction,technology adoption,system implementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE FORMATION AND VALUE OF IT-ENABLED RESOURCES: ANTECEDENTS AND CONSEQUENCES OF SYNERGISTIC RELATIONSHIPS'''
{{header}}
{{article
|author= Saggi Nevo,Michael R. Wade,
|source= MIS QUARTERLY
|year= 2010
|abstract = This paper informs the literature on the business value of information technology by conceptualizing a path from IT assets-that is, commodity-like or off-the-shelf information technologies-to, sustainable competitive advantage. This path suggests that IT assets can play a strategic role when they are combined with organizational resources to create IT-enabled resources. To the extent that relationships between IT assets and organizational resources are synergistic, the ensuing IT-enabled resources are capable of positively affecting firms' sustainable competitive advantage via their improved strategic potential. This is an important contribution since IT-related organizational benefits have been hard to demonstrate despite attempts to study them through a variety of methods and theoretical lenses. This paper synthesizes systems theory and the resource-based view of the firm to build a unified conceptual model linking IT assets with firm-level benefits. Several propositions are derived from the model and their implications for IS research and practice are discussed.
|keyword = Business value of IT,systems theory,resource-based view of the firm,IT-enabled resources,emergent capabilities,synergy,strategic potential,compatibility,integration effort,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT MAKES A HELPFUL ONLINE REVIEW? A STUDY OF CUSTOMER REVIEWS ON AMAZON.COM'''
{{header}}
{{article
|author= Susan M. Mudambi,David Schuff,
|source= MIS QUARTERLY
|year= 2010
|abstract = Customer reviews are increasingly available online for a wide range of products and services. They supplement other information provided by electronic storefronts such as product descriptions, reviews from experts, and personalized advice generated by automated recommendation systems. While researchers have demonstrated the benefits of the presence of customer reviews to an online retailer, a largely uninvestigated issue is what makes customer reviews helpful to a consumer in the process of making a purchase decision. Drawing on the paradigm of search and experience goods from information economics, we develop and test a model of customer review helpfulness. An analysis of 1,587 reviews from Amazon.com across six products indicated that review extremity, review depth, and product type affect the perceived helpfulness of the review. Product type moderates the effect of revieiv extremity on the helpfulness of the review. For experience goods, reviews with extreme ratings are less helpful than reviews with moderate ratings. For both product types, review depth has a positive effect on the helpfulness of the review, but the product type moderates the effect of review depth on the helpfulness of the revieiv. Review depth has a greater positive effect on the helpfulness of the review for search goods than for experience goods. We discuss the implications of our findings for both theory and practice.
|keyword = Electronic commerce,product reviews,search and experience goods,consumer behavior,information economics,diagnosticity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Analysis of Contract Structures in IT Outsourcing'''
{{header}}
{{article
|author= Yuanyuan Chen,Anandhi Bharadwaj,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Outsourcing of information technology (IT) services has received much attention in the information systems (IS) literature. However, considerably less attention has been paid to actual contract structures used in IT outsourcing (ITO). Examining contract structures yields important insights into how the contracting parties structure the governance provisions and the factors or transaction risks that influence them. Based on insights from prior literature, from practicing legal experts, and through in-depth content analysis of actual contracts, we develop a comprehensive coding scheme to capture contract provisions across four major dimensions: monitoring, dispute resolution, property rights protection, and contingency provisions. We then develop an empirical data set describing the contract structures across these distinct dimensions, using a sample of 112 ITO contracts from the Securities and Exchange Commission (SEC) database from 1993 to 2003. Drawing on transaction cost, agency, and relational exchange theories, we hypothesize the effects of transaction and relational characteristics on the specific contractual provisions, as well as on overall contract extensiveness. Furthermore, we examine how these associations vary under conditions of fixed price and time and materials pricing structures. The results provide good support for the main hypotheses of the study and yield interesting insights about contractual governance of ITO arrangements.
|keyword = outsourcing,IT outsourcing contract,contract structure,fixed-price contracts,time and materials contracts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing Intelligent Software Agents for Auctions with Limited Information Feedback'''
{{header}}
{{article
|author= Gediminas Adomavicius,Alok Gupta,Dmitry Zhdanov,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = This paper presents analytical, computational, and empirical analyses of strategies for intelligent bid formulations in online auctions. We present results related to a weighted-average ascending price auction mechanism that is designed to provide opaque feedback information to bidders and presents a challenge in formulating appropriate bids. Using limited information provided by the mechanism, we design strategies for software agents to make bids intelligently. In particular, we derive analytical results for the important characteristics of the auction, which allow estimation of the key parameters; we then use these theoretical results to design several bidding strategies. We demonstrate the validity of designed strategies using a discrete event simulation model that resembles the mechanisms used in treasury bills auctions, business-to-consumer (B2C) auctions, and auctions for environmental emission allowances. In addition, using the data generated by the simulation model, we show that intelligent strategies can provide a high probability of winning an auction without significant loss in surplus.
|keyword = online auctions,intelligent agents,software agents,limited information feedback,bidding strategies,discrete event simulation,heuristics,parameter estimation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Incorporating Feedback Mechanisms in a DSS Affects DSS Evaluations'''
{{header}}
{{article
|author= Ujwal Kayande,Arnaud De Bruyn,Gary L. Lilien,Arvind Rangaswamy,Gerrit H. van Bruggen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Model-based decision support systems (DSS) improve performance in many contexts that are data-rich, uncertain, and require repetitive decisions. But such DSS are often not designed to help users understand and internalize the underlying factors driving DSS recommendations. Users then feel uncertain about DSS recommendations, leading them to possibly avoid using the system. We argue that a DSS must be designed to induce an alignment of a decision maker's mental model with the decision model embedded in the DSS. Such an alignment requires effort from the decision maker and guidance from the DSS. We experimentally evaluate two DSS design characteristics that facilitate such alignment: (i) feedback on the upside potential for performance improvement and (ii) feedback on corrective actions to improve decisions. We show that, in tandem, these two types of DSS feedback induce decision makers to align their mental models with the decision model, a process we call deep learning, whereas individually these two types of feedback have little effect on deep learning. We also show that deep learning, in turn, improves user evaluations of the DSS. We discuss how our findings could lead to DSS design improvements and better returns on DSS investments.
|keyword = decision support systems,DSS design,feedback,learning,mental models,evaluations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trans-Situated Learning: Supporting a Network of Practice with an Information Infrastructure'''
{{header}}
{{article
|author= Emmanuelle Vaast,Geoff Walsham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = This paper investigates the practice-based learning dynamics that emerge among peers who share occupational practices but do not necessarily work with each other or even know each other because of geographical or organizational distance. To do so, it draws on the literatures on situated learning, networks of practice, and information infrastructures, and on insights from a longitudinal case study of the implementation of a Web-based information system used by people working in the field of environmental health. The system was deeply involved in the transformations of local practices as well as relationships between peers. Based on a dialogue between existing literatures and observations from the case study, this research extends the practice-based perspective on learning to the computer-mediated context of a network of practice. To that effect, it proposes a model of what we call trans-situated learning that is supported by the local universality of an information infrastructure whose use becomes embedded with other infrastructures.
|keyword = situated learning,network of practice,information infrastructure,case study,embeddedness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Investigation of End-User Query Development: The Effects of Improved Model Expressiveness vs. Complexity'''
{{header}}
{{article
|author= Paul L. Bowen,Robert A. O'Farrell,Fiona H. Rohde,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Data models provide a map of the components of an information system. Prior research has indicated that more expressive conceptual data models (despite their increased size) result in better performance for problem solving tasks. An initial experiment using logical data models indicated that more expressive logical data models also enhanced end-user performance for information retrieval tasks. However, the principles of parsimony and bounded rationality imply that, past some point, increases in size lead to a level of complexity that results in impaired performance. The results of this study support these principles. For a logical data model of increased but still modest size, users composing queries for the more expressive logical data model did not perform as well as users composing queries for the corresponding less expressive but more parsimonious logical data model. These results indicate that, when constructing logical data models, data modelers should consider tradeoffs between parsimony and expressiveness.
|keyword = scalability,expressiveness,ontology,ontological clarity,parsimony,logical data models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competitive Environment and the Relationship Between IT and Vertical Integration'''
{{header}}
{{article
|author= Gautam Ray,Dazhong Wu,Prabhudev Konana,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = The information systems (IS) literature suggests that by lowering coordination costs, information technology (IT) will lead to an overall shift towards more use of markets. Empirical work in this area provides evidence that IT is associated with a decrease in vertical integration (VI). Economy-wide data, however, suggests that over the last 25 years the average level of VI has, in fact, increased. This paper studies this empirical anomaly by explicating the moderating impact of two measures of competitive environment, demand uncertainty, and industry concentration, on the relationship between IT and VI. We examine firms included in 1995 to 1997 Information Week 500 and the COMPUSTAT database. Consistent with the IS literature, the analysis suggests that IT is associated with a decrease in VI when demand uncertainty is high or industry concentration is low. However, contrary to the IS literature, IT is found to be associated with an increase in VI when industry concentration is high or demand uncertainty is low. Furthermore, as demand uncertainty increases, less vertically integrated firms invest more in IT, while as industry concentration increases, more vertically integrated firms invest more in IT. The analysis also suggests that firms' choice of the level of VI and IT investment, under different levels of demand uncertainty and industry concentration, are rational. When demand uncertainty is high or industry concentration is low, increase in VI may increase coordination and production costs. Thus, less VI is rational. However, when industry concentration is high or demand uncertainty is low, increase in VI may decrease coordination and production costs. Thus, firms choose more VI in such industries. The implications for research and practice are discussed.
|keyword = information technology,vertical integration,coordination costs,production costs,demand uncertainty,industry concentration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Control in Internal and Outsourced Software Projects'''
{{header}}
{{article
|author= Amrit Tiwana,Mark Keil,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Although the choice of control mechanisms in systems development projects has been extensively studied in prior research, differences in such choices across internal and outsourced projects and their effects on systems development performance have not received much attention. This study attempts to address this gap using data on 57 outsourced and 79 internal projects in 136 organizations. Our results reveal a paradoxical overarching pattern: controllers attempt greater use of control mechanisms in outsourced projects relative to internal projects, yet controls enhance systems development performance in internal projects but not in outsourced projects. We introduce a distinction between attempted control and realized control to explain this disconnect, and show how anticipated transaction hazards motivate the former but meeting specific informational and social prerequisites facilitate the latter. Our results contribute three new insights to the systems development control literature. First, controllers attempt to use controller-driven control mechanisms to a greater degree in outsourced projects but controllee-driven control mechanisms to a greater degree in internal projects. Second, we establish a hitherto-missing control-performance link. The nuanced differences in internal and outsourced projects simultaneously confirm and refute a pervasive assertion in the information systems controls literature that control enhances performance. Finally, we show how requirements volatility-which can be at odds with control-alters the control-performance relationships. Implications for theory and practice are also discussed.
|keyword = attempted control,control theory,information systems development,project controls,realized control,survey research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design and Use of Preference Markets for Evaluation of Early Stage Technologies'''
{{header}}
{{article
|author= Li Chen,Paulo Goes,James R. Marsden,Zhongju Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = In the work presented here, we develop and apply preference markets in evaluating early stage technology. Partnering with a Fortune 5 company, we developed and implemented two internal preference markets (field experiments). In both cases, nonmonetary (play money) incentives were utilized, but one market provided additional nonmonetary (play money) incentives. Working with the partner company, our investigation started with seven emerging technologies and expanded to a total of 17 emerging technologies. Our results suggest that even a simple form of additional nonmonetary play money incentive yielded greater price convergence, increased spread across final market prices, and greater consistency with a costly expert panel that was set up by the partner company. Based on the outcomes of our analyses, the partner company is investing in developing extended applications of preference markets as a potentially scalable approach for dealing with its ongoing and expanding strategic identification of promising emerging technologies.
|keyword = convergence,incentives,information market,market performance,preference aggregation,preference ranking market,technology evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal Policy for Software Patents: Model and Comparative Implications'''
{{header}}
{{article
|author= Matt E. Thatcher,David E. Pingry,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = We use a duopoly model of quality-price competition between a software innovator and an imitator to determine the socially optimal software patent policy and to assess the social welfare implications of alternative patent policies. We find that the optimal patent policy is to grant patent protection for the entire life of the innovative software product (i.e., set infinite patent length) and to set the novelty and nonobviousness requirement for attaining a patent (i.e., patent height) and the scope of patent protection (i.e., patent width) such that both the innovator and imitator produce higher-quality products than they would in the free market. This policy not only maximizes social welfare but also makes the innovator, imitator, and consumers better off than in the free market. However, counter to intuition, we find that firms exhibit lower research and development intensity under the socially optimal patent policy than they do under free market competition. While highly stylized, the model offers a useful framework within which researchers and regulators can think about the economic trade-offs among three patent policy parameters (length, height, and width) simultaneously.
|keyword = patent height,patent length,patent policy,patent width,quality-price competition,R&D intensity,software patents,vertical product differentiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Push-Pull Technology in Privacy Calculus: The Case of Location-Based Services'''
{{header}}
{{article
|author= Heng Xu,Hock-Hai Teo,Bernard C. Y. Tan,Ritu Agarwal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Location-based services (LBS) use positioning technologies to provide individual users with reachability and accessibility that would otherwise not be available in the conventional commercial realm. While LBS confer greater connectivity and personalization on consumers, they also threaten users' information privacy through granular tracking of their preferences, behaviors, and identity. To address privacy concerns in the LBS context, this study extends the privacy calculus model to explore the role of information delivery mechanisms (pull and push) in the efficacy of three privacy intervention approaches (compensation, industry self-regulation, and government regulation) in influencing individual privacy decision making. The research model was tested using data gathered from 528 respondents through a quasi-experimental survey method. Structural equations modeling using partial least squares validated the instrument and the proposed model. Results suggest that the effects of the three privacy intervention approaches on an individual's privacy calculus vary based on the type of information delivery mechanism (pull and push). Results suggest that providing financial compensation for push-based LBS is more important than it is for pull-based LBS. Moreover, this study shows that privacy advocates and government legislators should not treat all types of LBS as undifferentiated but could instead specifically target certain types of services.
|keyword = compensation,distributive justice,government regulation,industry self-regulation,information delivery mechanisms,location-based services (LBS),privacy calculus,procedural justice,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trust-Assuring Arguments in B2C E-commerce: Impact of Content, Source, and Price on Trust'''
{{header}}
{{article
|author= Dongmin Kim,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = The research question examined in this paper is whether or not product price can be used as a proxy to predict how customers' trust will be influenced by different trust-assuring arguments displayed on a business-to-consumer e-commerce Web site. Drawing from the elaboration likelihood model (ELM) and Toulmin's model of argumentation, we examine the effects on consumer trust of two levels of source and two levels of content factors, under two levels of product price, in a laboratory experiment with 128 subjects. Product price was predicted as a moderating factor that would influence the customer's motivation to scrutinize more closely the content of the trust-assuring arguments. The results suggest that customers are more influenced by the content of trust-assuring arguments when the price of a product is relatively high than when it is relatively low. Presumably, Internet stores employ a third party's trust-assuring arguments because customers are less likely to trust an unknown Internet store's own trust-assuring arguments. However, the results paradoxically may imply that when customers have more at stake (e.g., buying a high-price product), they do not necessarily have to rely only on an independent third-party source to form high trust beliefs about the store. When customers purchase a high-price product, they seem to form trusting beliefs by scrutinizing argument content rather than by depending on heuristic cues (e.g., an independent party's opinion) as the ELM would predict.
|keyword = elaboration likelihood model (ELM),price,third-party certification,Toulmin argumentation model,trust,trust-assuring arguments,trust in e-commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Speed Matters: The Role of Free Software Offer in Software Diffusion'''
{{header}}
{{article
|author= Zhengrui Jiang,Sumit Sarkar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Many software products are available free of charge. While the benefits resulting from network externality have been examined in the related literature, the effect of free offer on the diffusion of new software has not been formally analyzed. We show in this study that even if other benefits do not exist, a software firm can still benefit from giving away fully functioning software. This is due to the accelerated diffusion process and subsequently the increased net present value of future sales. By adapting the Bass diffusion model to capture the impact of free software offer, we provide a methodology to determine the optimal number of free adopters. We show that the optimal free offer solution depends on the discount rate, the length of the demand window, and the ratio of low-valuation to high-valuation free adopters. Our methodology is shown to be applicable for both fixed and dynamic pricing strategies.
|keyword = Bass model,diffusion of innovations,dynamic pricing,free software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Risks and Benefits of Signaling Information System Characteristics to Strategic Attackers'''
{{header}}
{{article
|author= Marco Cremonini,Dmitri Nizovtsev,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = The paper uses a game-theoretic setting to examine the interaction between strategic attackers who try to gain unauthorized access to information systems, or "targets," and defenders of those targets. Our analysis of the attacker-defender interaction shows that well-protected targets can use signals of their superior level of protection as a deterrence tool. This is due to the fact that, all other things being equal, rational attackers motivated by potential financial gains tend to direct their effort toward less-protected targets. We analyze several scenarios differing in the scope of publicly available information about target parameters and discuss conditions under which greater defenders' ability to signal their security characteristics may improve their welfare. Our results may assist security researchers in devising better defense strategies through the use of deterrence and provide new insight about the efficacy of specific security practices in complex information security environments.
|keyword = cost-benefit analysis,crime deterrence,games of complete and incomplete information,information security,information warfare,interdependent strategies,signaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Estimating Time Required to Reach Bid Levels in Online Auctions'''
{{header}}
{{article
|author= Subhajyoti Bandyopadhyay,Seema Bandyopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Sellers in eBay are often small-business owners whose livelihood depends on fast turnaround of their cash flows. Unlike in traditional auctions, these sellers are content to sell as soon as some target price is reached. While a wealth of literature exists on the final rent of the various stakeholders in a traditional auction setting, what is of interest here is to estimate the time required to reach a certain bid level in ongoing auctions. This paper introduces an analytical model to estimate the time it takes an online auction to reach a prespecified price threshold. The motivation for the research is to avoid unnecessary delays in conducting the transaction. Specifying the right duration would benefit small sellers who would realize the revenue proceeds from the sale faster. To this end, we model the bidding process as an infinite quasi-birth-death process, characterized by bursts of rapid bidding and subsequent lulls. We obtain closed-form solutions for the transient probability distribution in the frequency domain of the bid prices in an ongoing auction, which are then used to compute the transient probability distributions in the time domain. Experienced auctioneers can use these results to estimate expected ending times for their auctions. Sample observations from online auctions indicate that there may be potential room for improvement for sellers in setting their auction ending times. Simulations of the quasi-birth-death processes back up the theoretical observations.
|keyword = auction bid price,auction cash turnaround time,auctions,"buy it now" price,correlated random walk,electronic auctions,quasi-birth-death process,small business,transient analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FASHION WAVES IN INFORMATION SYSTEMS RESEARCH AND PRACTICE'''
{{header}}
{{article
|author= Richard L. Baskerville,Michael D. Myers,
|source= MIS QUARTERLY
|year= 2009
|abstract = Building on neo-institutional theory and theories of innovation and diffusion, recent work in the field of management has suggested that management research and practice is characterized by fashions. A management fashion is a relatively transitory belief that a certain management technique leads rational management progress. Using bibliographic research, we apply Abrahamson's management fashion theory to information systems research and practice. Our findings reveal that information systems research and practice, like management research and practice, is indeed characterized by fashions. These "IS fashion waves" are relatively transitory and represent a burst of interest in particular topics by IS researchers and practitioners. However, while our findings show that IS research closely parallels practice, we suggest that a more proactive engagement of IS academics is needed in the IS fashion-setting process.
|keyword = Innovation,adoption,diffusion,management fashion,information systems fashion,fashion setting,research agenda,information systems practice,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW ETHICS CAN ENHANCE ORGANIZATIONAL PRIVACY: LESSONS FROM THE CHOICEPOINT AND TJX DATA BREACHES'''
{{header}}
{{article
|author= Mary J. Cuinan,Cynthia Clark Williams,
|source= MIS QUARTERLY
|year= 2009
|abstract = Protecting the privacy of personal information continues to pose significant challenges for organizations. Because consumers are vulnerable in their dealings with businesses due to a lack of information about and an inability to control the subsequent use of their personal information, we argue that organizations have a moral responsibility to these individuals to avoid causing harm and to take reasonable precautions toward that end. We further argue that firms can enhance their privacy programs by moving beyond merely complying with laws and other regulations and creating a culture of integrity that combines a concern for the law with an emphasis on managerial responsibility for the firm's organizational privacy behaviors. We use two high-profile data breaches experienced by two U. S. companies, ChoicePoint and TJX, to illustrate our arguments for enhancing organizational level privacy programs based on ethical reasoning. In doing so, this paper contributes to the dearth of prior organizational level privacy research, which has largely overlooked ethical issues or the personal harms often caused by privacy violations. We conclude with recommendations for ways organizations can improve their privacy programs by incorporating moral responsibility.
|keyword = Organizational privacy,information ethics,moral responsibility,information risk management,information management practices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERPRETATION OF FORMATIVE MEASUREMENT IN INFORMATION SYSTEMS RESEARCH'''
{{header}}
{{article
|author= Ronald T. Cenfetelli,Genevieve Bassellier,
|source= MIS QUARTERLY
|year= 2009
|abstract = Within the Information Systems literature, there has been an emerging interest in the use of formative measurement in structural equation modeling (SEM). This interest is exemplified by descriptions of the nature of formative measurement (e.g., Chin 1998a), and more recently the proper specification of formatively measured constructs (Petter et al. 2007) as well as application of such constructs (e.g., Barki et al. 2007). Formative measurement is a useful alternative to reflective measurement. However, there has been little guidance on interpreting the results when formative measures are employed Our goal is to provide guidance relevant to the interpretation of formative measurement results through the examination of the following six issues: multicollinearity; the number of indicators specified for a formatively measured construct; the possible co-occurrence of negative and positive indicator weights; the absolute versus relative contributions made by a formative indicator; nomological network effects; and the possible effects of using partial least squares (PLS) versus covariance-based SEM techniques. We provide prescriptions for researchers to consider when interpreting the results of formative measures as well as an example to illustrate these prescriptions.
|keyword = Structural equation modeling,formative measurement,formative indicators,measurement theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMMUNITY LEARNING IN INFORMATION TECHNOLOGY INNOVATION'''
{{header}}
{{article
|author= Ping Wang,Neil C. Ramiller,
|source= MIS QUARTERLY
|year= 2009
|abstract = In striving to learn about an information technology innovation, organizations draw on knowledge resources available in the community of diverse interests that convenes around that innovation. But even as such organizations learn about the innovation, so too does the larger community. Community learning takes place as its members reflect upon their learning and contribute their experiences, observations, and insights to the community's on-going discourse on the innovation. Community learning and organizational learning thus build upon one another in a reciprocal cycle over time, as the stock of interpretations, adoption rationales, implementation strategies, and utilization patterns is expanded and refined We advance an overall model of this learning cycle, drawing on two community-level theories (management fashion and organizing vision), both of which complement the dominant emphases of the literature on IT innovation and learning. Relative to this cycle, we then empirically examine, in particular, the dependence of community learning on organizational learning. Sampling the public discourse on enterprise resource planning (ERP) over a 14-year period, we explore how different kinds of organizational actors can play different roles, at different times, in contributing different types of knowledge to an innovation's public discourse. The evidence suggests that research analysts and technology vendors took leadership early on in articulating the "know-what" (interpretation) and "know-why" (rationales) for ERP, while later on adopters came to dominate the discourse as its focus shifted to the "know-how" (strategies and capabilities). We conclude by identifying opportunities for further inquiry on and strategic management of community learning and its interactions with organizational learning.
|keyword = Information technology innovation,innovation community,community learning,learning-about,discourse,management fashion,organizing vision,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERFIRM STRATEGIC INFORMATION FLOWS IN LOGISTICS SUPPLY CHAIN RELATIONSHIPS'''
{{header}}
{{article
|author= Richard Klein,Arun Rai,
|source= MIS QUARTERLY
|year= 2009
|abstract = This paper focuses on strategic information flows between buyers and suppliers within logistics supply chain relationships and on subsequent relationship-specific performance outcomes. Our analysis of dyadic data collected from 91 buyer-supplier logistics relationships finds that buyer and supplier strategic information flows positively impact the relationship-specific performance of both sharing and receiving parties. Specifically, each party gains financially from improved management of assets, reduced costs of operations, and enhanced productivity. Moreover, each benefits operationally from improved planning, control, and flexibility of resources. Buyer dependence on the supplier increases buyer strategic information flows to the supplier. Additionally, buyer IT customization and both buyer and supplier trusting beliefs in the receiving party positively impact strategic information sharing with partners. This study suggests that partnerships for supply chain services engage in cooperative initiatives to generate relational rents and are an alternative to conventional "arms length" transactional exchanges. These partnerships need to be motivated to go beyond the sharing of order-related information (which must occur in transactional exchanges) and to share strategic information (which has the potential for both additional rent generation and risks of misappropriation).
|keyword = Interfirm relationships,dyads,relational view,strategic information flows,IT customization,trust,dependence,relationship longevity,organization size,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''OVERLAPPING ONLINE AUCTIONS: EMPIRICAL CHARACTERIZATION OF BIDDER STRATEGIES AND AUCTION PRICES'''
{{header}}
{{article
|author= Ravi Bapna,Seokjoo Andrew Chang,Paulo Goes,Alok Gupta,
|source= MIS QUARTERLY
|year= 2009
|abstract = Online auctions enable market-level interactions or interdependency of outcomes, which were not observed in physical auctions. One such set of interactions takes place when multiple auctions are conducted to sell identical items by an identical seller in an overlapping manner. This research focuses on overlapping auctions, their interactions, and the related impact on bidder behavior. We introduce the notion of auction "overlap" and examine the impact of market-level factors such as the price information revealed from prior auctions, degree of overlap, the auction format, and the overall market supply on a given auction's price. Despite a competitive setting, we find that, ceteris paribus, English auctions, on average, extract roughly 8.6 percent more revenue per unit than multiunit uniform-price Dutch auctions. We discover that the overlapping auctions attract institutional bidders, who bid in a participatory manner across multiple auctions, and that such bidders exert a downward pressure on auction prices. We find that overlap of an auction with other competing auctions has a significant negative influence on prices, and information about following auctions has a stronger negative influence than information about prior closing auctions. By estimating the expected price difference, we provide practitioners, who have private knowledge of their internal holding costs, a benchmark that can be used in deciding between using overlapping single-unit English auctions and multiunit Dutch auctions.
|keyword = Online auctions,overlapping auctions,Dutch auctions,English auctions,bidding strategies,impact of information on pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SELECTIVE STATUS REPORTING IN INFORMATION SYSTEMS PROJECTS: A DYADIC-LEVEL INVESTIGATION'''
{{header}}
{{article
|author= Charalambos L. Iacovou,Ronald L. Thompson,H. Jeff Smith,
|source= MIS QUARTERLY
|year= 2009
|abstract = This study investigates selective reporting behaviors that are pursued by project managers when communicating the status of their information system initiatives to their executives. To understand the types, motivations, impacts, and antecedents of such behaviors, a message-exchange perspective is adopted and the prior literature on IS project status reporting is reviewed This study incorporates an empirical investigation that examined the influence of five dyadic factors on selective reporting using a survey of 561 project managers. The findings of the study reveal a positive effect of reporting quality on project performance and indicate that a specific type of selective reporting behavior (optimistic biasing) has a degrading effect on reporting quality. Moreover, the findings show that all five antecedents have a significant influence on the propensity of project managers to report selectively. Specifically, the project executive's power, the project manager's trust in the executive, and the executive's quality of communication impact selective reporting directly; the executives familiarity with the IS development process and the executive's organizational affiliation vis-a-vis that Of the project manager have an indirect influence (it is mediated through other factors). The effects of each of these factors on the two types of selective reporting (optimistic and pessimistic biasing) are examined, and the implications of these findings for both researchers and managers are discussed in this article.
|keyword = Information systems development,project management,status reporting,communication quality,distortion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''RESOLVING DIFFERENCE SCORE ISSUES IN INFORMATION SYSTEMS RESEARCH'''
{{header}}
{{article
|author= Gary Klein,James J. Jiang,Paul Cheney,
|source= MIS QUARTERLY
|year= 2009
|abstract = A number of models and theories in information systems research include concepts of a match between two variables or states. The development of measures for this concept can present problems, because decisions must be made about the nature of the comparison. Should indirect measures of the match be employed, then methodological issues arise about how to best handle the measure when testing the model. Difference scores are commonly used to measure a match between variables or states in IS research, but these have implicit assumptions about the theory and data characteristics that are often false. Not unexpectedly, false assumptions can lead to erroneous conclusions about the relationships among the variables that are used to determine a match in a research model. The implicit assumptions restrict the form of the relationships and limit the IS researchers ability to understand the possible interplay among theoretical concepts. We suggest some guidelines for the formation and testing of models that measure the match. In addition, we recommend polynomial regression analysis as one means of analyzing the more complex relationships in IS studies. We then use an IS service quality example to illustrate the issues involved in the use of matching variables and make suggestions with regard to using or avoiding difference scores.
|keyword = Difference scores,indirect measures,polynomial regression analysis,IS SERVQUAL,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NONLINEARITIES BETWEEN ATTITUDE AND SUBJECTIVE NORMS IN INFORMATION TECHNOLOGY ACCEPTANCE: A NEGATIVE SYNERGY?'''
{{header}}
{{article
|author= Ryad Titah,Henri Barki,
|source= MIS QUARTERLY
|year= 2009
|abstract = Empirical results both from information technology acceptance research as well as from other fields suggest that attitude and subjective norms may have a nonlinear relationship. Based on the economic theory of complementarities, the present paper hypothesizes a substitution relationship or negative synergy between attitude and subjective norms in organizational IT use contexts. Employing two methods for modeling and measuring nonlinear effects of latent constructs, as well as two approaches for visualizing and interpreting interaction and quadratic terms, structural equation modeling analysis of data collected from 258 users of a variety of IT applications in 14 organizations provides support for the hypothesis that attitude and subjective norms were substitutes in predicting intention to use.
|keyword = IT acceptance,theory of complementarities,latent variable interactions,nonlinear modeling,structural equation modeling,quadratic latent variables,response surface methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coevolving Systems and the Organization of Agile Software Development'''
{{header}}
{{article
|author= Richard Vidgen,Xiaofeng Wang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Despite the popularity of agile methods in software development and increasing adoption by organizations there is debate about what agility is and how it is achieved. The debate suffers from a lack of understanding of agile concepts and how agile software development is practiced. This paper develops a framework for the organization of agile software development that identifies enablers and inhibitors of agility and the emergent capabilities of agile teams. The work is grounded in complex adaptive systems (CAS) and draws on three principles of coevolving systems: match coevolutionary change rate, maximize self-organizing, and synchronize exploitation and exploration. These principles are used to study the processes of two software development teams, one a team using eXtreme Programming (XP) and the other a team using a more traditional, waterfall-based development cycle. From the cases a framework for the organization of agile software development is developed. Time pacing, self-management with discipline and routinization of exploration are among the agile enablers found in the cases studies while event pacing, centralized management, and lack of resources allocated to exploration are found to be inhibitors to agility. Emergent capabilities of agile teams that are identified from the research include coevolution of business value, sustainable working with rhythm, sharing and team learning, and collective mindfulness.
|keyword = agile software development,coevolving systems,complex adaptive systems,time-pacing,rhythm,mindfulness,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Control Theory Perspective on Agile Methodology Use and Changing User Requirements'''
{{header}}
{{article
|author= Likoebe M. Maruping,Viswanath Venkatesh,Ritu Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = In this paper, we draw on control theory to understand the conditions under which the use of agile practices is most effective in improving software project quality. Although agile development methodologies offer the potential of improving software development outcomes, limited research has examined how project managers can structure the software development environment to maximize the benefits of agile methodology use during a project. As a result, project managers have little guidance on how to manage teams who are using agile methodologies. Arguing that the most effective control modes are those that provide teams with autonomy in determining the methods for achieving project objectives, we propose hypotheses related to the interaction between control modes, agile methodology use, and requirements change. We test the model in a field study of 862 software developers in 110 teams. The model explains substantial variance in four objective measures of project quality-bug severity, component complexity, coordinative complexity, and dynamic complexity. Results largely support our hypotheses, highlighting the interplay between project control, agile methodology use, and requirements change. The findings contribute to extant literature by integrating control theory into the growing literature on agile methodology use and by identifying specific contingencies affecting the efficacy of different control modes. We discuss the theoretical and practical implications of our results.
|keyword = agile methodologies,agility,control theory,requirements uncertainty,software development,teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Control of Flexible Software Development Under Uncertainty'''
{{header}}
{{article
|author= Michael L. Harris,Rosann Webb Collins,Alan R. Hevner,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = W hen should software development teams have the flexibility to modify their directions and how do we balance that flexibility with controls essential to produce acceptable outcomes? We use dynamic capabilities theory and an extension of control theory to understand these questions. This work is examined in a case study. Our results demonstrate that flexibility may be needed when the starting conditions are uncertain and that effective control in these situations requires use of traditional controls plus a new type of control we term emergent outcome control.
|keyword = control theory,uncertainty,flexible development,agile development,emergent outcome control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Crossing Spatial and Temporal Boundaries in Globally Distributed Projects: A Relational Model of Coordination Delay'''
{{header}}
{{article
|author= Jonathon N. Cummings,J. Alberto Espinosa,Cynthia K. Pickering,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = In globally distributed projects, members have to deal with spatial boundaries (different cities) and temporal boundaries (different work hours) because other members are often in cities within and across time zones. For pairs of members with spatial boundaries and no temporal boundaries (those in different cities with overlapping work hours), synchronous communication technologies such as the telephone, instant messaging (IM), and Web conferencing provide a means for real-time interaction. However, for pairs of members with spatial and temporal boundaries (those in different cities with nonoverlapping work hours), asynchronous communication technologies, such as e-mail, provide a way to interact intermittently. Using survey data from 675 project members (representing 5,674 pairs of members) across 108 projects in a multinational semiconductor firm, we develop and empirically test a relational model of coordination delay. In our model, the likelihood of delay for pairs of members is a function of the spatial and temporal boundaries that separate them, as well as the communication technologies they use to coordinate their work. As expected, greater use of synchronous web conferencing reduces coordination delay for pairs of members in different cities with overlapping work hours relative to pairs of members with nonoverlapping work hours. Unexpectedly, greater use of asynchronous e-mail does not reduce coordination delay for pairs of members in different cities with nonoverlapping work hours, but rather reduces coordination delay for those with overlapping work hours. We discuss the implications of our findings that temporal boundaries are more difficult to cross with communication technologies than spatial boundaries.
|keyword = virtual teams,computer-mediated communication and collaboration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring Agility in Distributed Information Systems Development Teams: An Interpretive Study in an Offshoring Context'''
{{header}}
{{article
|author= Saonee Sarker,Suprateek Sarker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Agility is increasingly being seen as an essential element underlying the effectiveness of globally distributed information systems development (ISD) teams today. However, for a variety of reasons, such teams are often unable develop and enact agility in dealing with changing situations. This paper seeks to provide a deeper understanding of agility through an intensive study of the distributed ISD experience in TECHCOM, an organization widely recognized for its excellence in IT development and use. The study reveals that agility should be viewed as a multifaceted concept having three dimensions: resource, process, and linkage. Resource agility is based on the distributed development team's access to necessary human and technological resources. Process agility pertains to the agility that originates in the team's systems development method guiding the project, its environmental scanning, and sense-making routines to anticipate possible crises, and its work practices enabling collaboration across time zones. Linkage agility arises from the nature of interactional relationships within the distributed team and with relevant project stakeholders, and is composed of cultural and communicative elements. The paper highlights some of the difficulties in developing agility in distributed ISD settings, provides actionable tactics, and suggests contingencies wherein different facets of agility may become more (or less) critical.
|keyword = agility,global software development,distributed information systems development,virtual teamwork,offshoring,insourcing,interpretive study,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business Models for Monetizing Internet Applications and Web Sites: Experience, Theory, and Predictions'''
{{header}}
{{article
|author= Eric K. Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Almost all attempts to data to monetize Internet applications targetted at individuals have focused on natural extensions of traditional media or traditional retailing, Most are either some form of consumer-focused advertising or of consumer-focused e-commerce And yet the Net is far more powerful than traditional media on one hand. and far more liberating, and thus inappropriate as an alternative to traditional media on the other. There are numerous potential online business models that are not based oil advertising. This paper explores several such areas and divides them into two basic categories, those that sell some product, experience., content, or service and earn revenues from the sale, and those that provide access to consumers and charge for access. It further divides each of these major categories into subcategories, and for each subcategory reviews Current experience, places it in the context of the relevant literature in competitive strategy, and uses that theory to make predictions. The paper concludes with strategic recommendations for corporations as well as with suggestions for further research.
|keyword = business models for the Internet,future of online advertising,monetizing the Internet,strategic information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Investigation of the Value of Integrating Enterprise Information Systems: The Case of Medical Imaging Informatics'''
{{header}}
{{article
|author= Moshe Ayal,Abraham Seidmann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = This paper identifies and measures the various business Value benefits that accrue as a result of implementing and integrating large-scale enterprise information systems. Specifically, we look at the integration of electronic medical records for all patients with the radiology information system and a picture archiving and communication system at a regional medical center. Our work is among the first to carefully study and analyze the impact of enterprise information systems at a large-scale service organization that produces intangible outputs-health. It extends the literature on information economics by quantifying the benefits in process dynamics as a source of ongoing firm-level performance improvements. The key dimensions of measurements Include financial revenues, operating lead times, and subjective satisfaction levels by the clinical staff and by the referring physicians. Analyses of longitudinal data suggest that performance levels along a key metric-clinical process lead time-showed a significant improvement immediately after the deployment and integration of the systems. The evidence reveals that performance kept improving for the following 12 months at ail impressive learning rate of 63 percent. Moreover, the reported satisfaction level after installation was higher among referring physicians who actively used the full spectrum of technological functionalities at their own clinics or at the hospital's site. Finally, we present a general framework for capturing the actual tangible and intangible benefits of enterprise information systems installation and integration in a clinical context.
|keyword = business-value,electronic medical research,empirical regularities,empirical research,enterprise information systems,health care,radiology information systems,satisfaction,systems integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Personalization in a Two-Dimensional Product Differentiation Model'''
{{header}}
{{article
|author= Sunil Wattal,Rahul Telang,Tridas Mukhpadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = We use a game-theoretic model to examine how information personalization by firms interacts with different dimensions of product differentiation (namely, horizontal and vertical differentiation). We consider the possibility that consumers attach different importance to various types of product differentiation, and report the equilibrium in terms of the "quality-fit" ratio, which measures the relative strength of preference for quality compared to preference for product fit and is a function of the cost of quality and the cost of product misfit. We also consider how different market structures (whether firms are similar or differentiated on the horizontal dimension ex ante) lead to different equilibriums when firms adopt personalization. We show that personalization by one firm leads to higher profits for both firms if product quality and misfit costs are high and the firms offer similar products ex ante. On the other hand, if firms offer differentiated products, personalization is profitable only if the effectiveness of the personalization technology is high or if both product quality and misfit costs are low. We also highlight conditions under which investments in personalization and product quality can be complements or Substitutes to each other. Finally, we show that a firm can respond to a competitor's personalization by either increasing (aggressive response) or decreasing (defensive response) investments in its own quality. Our results provide insights to managers on when to invest in personalization technologies and how to adjust their investments in product quality after the firm (or its competitor) adopts personalization.
|keyword = game theory,horizontal and vertical differentiation,market structure,personalization,quality-fit ratio,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Security: Facilitating User Precautions Vis-a-Vis Enforcement Against Attackers'''
{{header}}
{{article
|author= Ivan P. L. Png,Qiu-Hong Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = We compare alternative information security policies-facilitating end-user precautions and enforcement against attackers. The context is mass and targeted attacks, taking account of strategic interactions between end users and attackers. For both mass and targeted attacks. facilitating end-user precautions reduces, the expected loss of end users. However, the impact of enforcement oil expected loss depends oil the balance between deterrence and Slackening of end-user precautions. Facilitating end-user precautions is more effective than enforcement against. attackers when the cost of precautions and the cost of atacks are lower. With targeted attacks, facilitating end-user precautions is more effective for users with relatively high valuation of information security, while enforcement against attackers is more effective for users with relatively low valuation of security
|keyword = enforcement,facilitation,information security,mass attacks,targeted attacks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Organizational Learning and Knowledge Transfer on Investment Decisions Under Uncertainty'''
{{header}}
{{article
|author= Xianjun Geng,Lihui Lin,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Because uncertainties around innovative technologies resolve over time, investments in such technologies are often made in stages so that organizations can use the knowledge gained from earlier stages to decide the next step. Previous studies usually assume that once some uncertainty is resolved, it becomes common knowledge within the investing organization. We develop a game-theoretical model to study how different parties within an organization gain and transfer knowledge about new technologies while investing in these technologies, and how the learning process may affect the investment decisions. We show that managers with incentives misaligned with the organization may transfer their knowledge untruthfully and distort the learning process of decision makers. Such behavior may lead to inefficient investment decisions. We also study the effect of uncertainty on the misreporting problem and the investment decisions. Mechanisms to mitigate or prevent untruthful knowledge transfer are also proposed. In particular., powerful incentive schemes may alleviate, but not prevent. the misreporting problem;, punishing managers who are caught mis-reporting may deter the misreporting behavior, but in practice such mechanisms are difficult to implement.
|keyword = economic analysis,game theory,investment under uncertainty,knowledge transfer,organizational learning,signal jamming,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Learning Model of Information Technology Outsourcing: Normative Implications'''
{{header}}
{{article
|author= Hoon S. Cha,David E. Pingry,Matt E. Thatcher,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = We use an economic learning model to examine how knowledge parameters characterizing a sourcing relationship between it vendor and it client interact with production costs and coordination costs to affect the business value of alternative outsourcing strategies. This Information is then used to determine a fit-in's optimal rate of information technology (IT) Outsourcing. We find that the optimal Outsourcing rate is dependent oil the ability of the Outsourcing client to acquire production Knowledge front its outsourcing vendor and to retain its internal coordination knowledge despite losses of fundamental production skills due to Outsourcing. Specifically, when the Client is unable to acquire sufficient production knowledge from the external vendor, the client's optimal Outsourcing decision is to engage in either one of two extreme strategies-total insourcing or total outsourcing-depending on the rate at which the client's coordination knowledge depreciates. On the other hand, when the client is able to acquire a substantial amount of production knowledge from the external vendor, the firm's optimal decision is to outsource only a portion of its IT services, where the proportion depends on the rate at which the client's coordination knowledge depreciates.
|keyword = coordination,economic analysis,IT outsourcing,knowledge,modeling,organizational learning,outsourcing,production costs,vendor-client relationship,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Unified Procurement Strategy for Enterprise Software: A Test of the "Move to the Middle" Hypothesis'''
{{header}}
{{article
|author= Robert J. Kauffman,Juliana Y. Tsai,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = To increase their firms' competitiveness. information technology (IT) managers are adopting a strategy that many deemed risky in the past. Recent IT advances combined with certain firm and industry characteristics are prompting firms to move toward a unified procurement strategy for enterprise software solutions. Unified procurement Occurs when a firm elects to purchase all compatible products and services from a single vendor. Key benefits of unified procurement involve motivating managers to alter their procurement strategy. At the top of the list of benefits is transferred risk, which includes risks that can be transferred from one participating party to another during a transaction. In the case of unified procurement, they include technology risks and integration costs that are transferred from the procurement firm to its vendor. Firms have been shifting toward a unified procurement strategy for enterprise software solutions. We discuss the evolution of procurement practices in an industry that exemplifies a manifestation of Clemons et al 's [10] "move to the middle " hypothesis predictions. The adoption of unified procurement is being driven by changes in IT, firm, and industry structure. Are explore the "move to the middle:" transaction cost economics. and industry clockspeed theory to explain this phenomenon. We present a series of propositions that extend the prior theory to the more specific setting of enterprise software procurement-an example of middle range theory development-and use mini-cases to validate the various perspectives that we offer.
|keyword = competitive strategy,enterprise software,industry structure,IT procurement,middle range theory development,"move to the middle" hypothesis,opportunism risk,software stacks,unified procurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Transaction Cost Perspective of the "Software as a Service" Business Model'''
{{header}}
{{article
|author= Anjana Susarla,Anitesh Barua,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Application service providers (ASP), which host and maintain information technology (IT) applications across the Internet, offer in alternative to traditional models 4 IT service for user firms. We build oil prior literature in transaction cost economics (TCE) to argue that the contract design should address ex post transaction costs that result due to contractual incompleteness and opportunism. We argue that contract design is multidimensional, and that it is necessary to design governance structures that Call protect user firms from shirking and monitoring costs, as well as provide for efficient adaptation when requirements are incompletely specified at the start. of the initiative. Our empirical analysis suggests that factors such as uncertainty in specifying service requirements, interdependence between the ASP application and IT systems in the client organization, and the need for specific investments favor time and materials contracts, whereas fixed prices are desirable when strong incentives are needed for cost reduction. We also find that contracts that are aligned with transaction attributes in a transaction cost-economizing manner are significantly less likely to experience budget overruns and realize better ex post performance than those that are not. These, results hold normative implications for both user and provider firms to assess the performance implications of choosing contracts in line with prescriptions of TCE.
|keyword = application service providers,contract choice,logit models,transaction cost economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHY BREAK THE HABIT OF A LIFETIME? RETHINKING THE ROLES OF INTENTION, HABIT, AND EMOTION IN CONTINUING INFORMATION TECHNOLOGY USE'''
{{header}}
{{article
|author= Ana Ortiz de Guinea,M. Lynne Markus,
|source= MIS QUARTERLY
|year= 2009
|abstract = One of the most welcome recent developments in Information Systems scholarship has been the growing interest in individuals' continuing use of information technology well after initial adoption, known in the literature as IT usage, IT continuance, and post-adoptive IT usage. In this essay, we explore the theoretical underpinnings of IS research on continuing IT use. Although the IS literature on continuing IT use emphasizes the role of habitual behavior that does not require conscious behavioral intention, it does so in a way that largely remains faithful to the theoretical tradition of planned behavior and reasoned action. However, a close reading of reference literatures on automatic behavior (behavior that is not consciously controlled) and the influences of emotion on behavior suggests that planned behavior and reasoned action may not provide the best theoretical foundation for the study of continuing IT use. As a result, we call for empirical research that directly compares and contrasts the consensus theory of continuing IT use with rival theories that place much greater emphasis on unplanned and unreasoned action.
|keyword = Continuing IT use,IT continuance,post-adoptive IT usage,automatic behavior,environmental triggers,habit,emotion,intention,cognition,reasoned action,planned behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MINIMIZING METHOD BIAS THROUGH PROGRAMMATIC RESEARCH'''
{{header}}
{{article
|author= Andrew Burton-Jones,
|source= MIS QUARTERLY
|year= 2009
|abstract = Researchers have long known that research methods influence construct measurements and that this influence, or method bias, can lead to false conclusions. Despite much work in the methodological literature on specific aspects of method bias, such as common method bias and self-report bias, the meaning of method bias remains unclear, and there is no comprehensive approach for dealing with it. This paper offers a clear definition of method bias, proposes a more comprehensive approach for dealing with it, and describes a demonstration exercise applying the approach in an empirical study of how individual system use and task performance relate. The demonstration suggests that the approach is feasible and illustrates how it can help researchers test theories and identify new research opportunities.
|keyword = Construct,measurement,validity,method bias,method variance,qualitative,quantitative,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ESTIMATING THE EFFECT OF COMMON METHOD VARIANCE: THE METHOD-METHOD PAIR TECHNIQUE WITH AN ILLUSTRATION FROM TAM RESEARCH'''
{{header}}
{{article
|author= Rajeev Sharma,Philip Yetton,Jeff Crawford,
|source= MIS QUARTERLY
|year= 2009
|abstract = This paper presents a meta-analysis-based technique to estimate the effect of common method variance on the validity of individual theories. The technique explains between-study variance in observed correlations as a function of the susceptibility to common method variance of the methods employed in individual studies. The technique extends to mono-method studies the concept of method variability underpinning the classic multitrait-multimethod technique. The application of the technique is demonstrated by analyzing the effect of common method variance on the observed correlations between perceived usefulness and usage in the technology acceptance model literature. Implications of the technique and the findings for future research are discussed.
|keyword = Common method variance,common method bias,meta-analysis,perceived usefulness,use,system use,usage,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WEB STRATEGIES TO PROMOTE INTERNET SHOPPING: IS CULTURAL-CUSTOMIZATION NEEDED?'''
{{header}}
{{article
|author= Choon Ling Sia,Kai H. Lim,Kwok Leung,Matthew K. O. Lee,Wayne Wei Huang,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2009
|abstract = Building consumer trust is important for new or unknown Internet businesses seeking to extend their customer reach globally. This study explores the question: Should website designers take into account the cultural characteristics of prospective customers to increase trust, given that different trust-building web strategies have different cost implications? In this study, we focused on two theoretically grounded practical web strategies of customer endorsement, which evokes unit grouping, and portal affiliation, which evokes reputation categorization, and compared them across two research sites: Australia (individualistic culture) and Hong Kong (collectivistic culture). The results of the laboratory experiment we conducted, on the website of an online bookstore, revealed that the impact of peer customer endorsements on trust perceptions was stronger for subjects in Hong Kong than Australia and that portal (Yahoo) affiliation was effective only in the Australian site. A follow-up study was conducted as a conceptual replication, and provided additional insights on the effects of customer endorsement versus firm affiliation on trust-building. Together, these findings highlight the need to consider cultural differences when identifying the mix of web strategies to employ in Internet store websites.
|keyword = Cross-cultural study,Internet shopping,trust,web strategies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE INTEGRATIVE FRAMEWORK OF TECHNOLOGY USE: AN EXTENSION AND TEST'''
{{header}}
{{article
|author= Sung S. Kim,
|source= MIS QUARTERLY
|year= 2009
|abstract = The integrative framework of technology use (IFTU) posits that to fully explain post-adoption phenomena, four mechanisms-namely, reason-oriented action, sequential updating, feedback and habit-should be taken into account simultaneously in a unified model. Although IFTU sheds light on the four mechanisms underlying technology use, it lacks a coherent theoretical explanation for the underlying force that leads to the four mechanisms. To offer a more generalized and richer description of the four mechanisms, this study extends IFTU by drawing on the process model of memory in cognitive psychology. In addition, based on the extended IFTU paradigm, a three-wave panel model is developed that incorporates not only proximal effects but also distal effects of the four mechanisms on post-adoption phenomena. Three different sets of data (n = 195, 160, and 342, respectively) are used to test the proposed model. The results of data analysis show that, as expected, the four mechanisms have proximal effects on subsequent evaluations and behavior. Furthermore, consistent with the memory perspective, the sequential updating and habit mechanisms are found to have distal effects on post-adoption phenomena even after controlling for their proximal effects. Overall, the findings of this study indicate that the memory perspective offers not only a seamless explanation of the four mechanisms underlying technology use but also yields deeper insights that can be validated only through a three-or-more-wave panel study. This research contributes to the literature by demonstrating that the extended IFTU paradigm has the potential to serve as a coherent theoretical framework on post-adoption phenomena in which prior experiences are internalized into memories, which in turn regulate later experiences.
|keyword = Longitudinal study,panel model,technology use,continued use,theory of planned behavior (TPB),process model of memory,path analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INVESTIGATING USER RESISTANCE TO INFORMATION SYSTEMS IMPLEMENTATION: A STATUS QUO BIAS PERSPECTIVE'''
{{header}}
{{article
|author= Hee-Woong Kim,Atreyi Kankanhalli,
|source= MIS QUARTERLY
|year= 2009
|abstract = User resistance to information systems implementation has been identified as a salient reason for the failure of new systems and hence needs to be understood and managed While previous research has explored the reasons for user resistance, there are gaps in our understanding of how users evaluate change related to a new information system and decide to resist it. In particular, missing in the explanation of user decision making is the concept of status quo bias, that is, that user resistance can be due to the bias or preference to stay with the current situation. Motivated thus, this study develops a model to explain user resistance prior to a new IS implementation by integrating the technology acceptance and resistance literatures with the status quo bias perspective. The results of testing the model in the context of a new enterprise system implementation indicate the central role of switching costs in increasing user resistance. Further, switching costs also mediate the relationship between other antecedents (colleague opinion and self-efficacy for change) and user resistance. Additionally, perceived value and organizational support for change are found to reduce user resistance. This research advances the theoretical understanding of user acceptance and resistance prior to a new IS implementation and offers organizations suggestions for managing such resistance.
|keyword = User resistance,IS implementation,status quo bias theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DIVIDED BY A COMMON LANGUAGE? A RESPONSE TO MARSHALL SCOTT POOLE'''
{{header}}
{{article
|author= Matthew R. Jones,Helena Karsten,
|source= MIS QUARTERLY
|year= 2009
|abstract = Marshall Scott Poole identifies some important issues in the treatment of adaptive structuration theory in our review of the use of Giddens's structuration theory in IS research (Jones and Karsten 2008). We argue, however, that a number of his criticisms reflect differences in our respective use of particular terms and that the statements made in Jones and Karsten are reasonable, especially in the light of Giddens's own writings. There are some substantive differences between our position and that of Poole, though, especially in relation to the distinctiveness and compatibility of positivist and interpretive research, and the immateriality of Giddens's structures. Arguments are presented to show that, as Jones and Karsten discussed, Giddens's position is able to offer a plausible and self-consistent account of IS phenomena, including those such as the role of material artefacts in the U.S. legal system, "distributed cognition," and the use of GDSS that Poole suggests are incompatible with Giddens's account of structuration.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EVOLUTION OF RISK IN INFORMATION SYSTEMS OFFSHORING: THE IMPACT OF HOME COUNTRY RISK, FIRM LEARNING, AND COMPETITIVE DYNAMICS'''
{{header}}
{{article
|author= Eugene D. Hahn,Jonathan P. Doh,Kraiwinee Bunyaratavej,
|source= MIS QUARTERLY
|year= 2009
|abstract = Information systems offshoring has emerged as a significant force in the global political economy, an important source of firm-specific competitive advantage, and a focal point for debates over the benefits and costs of globalization. As worldwide competition exerts increasing pressure on the IS function of firms to become geographically unbundled, and IS services are dispersed among increasingly distant and unfamiliar locations, the issue of risk emerges as a significant factor in decisions about where to locate offshore facilities. Drawing from prior research in IS outsourcing/offshoring and theoretical perspectives from international strateggy and multinational management, we examine the determinants Of risk firms bear in their offshoring decisions. In particular, the current paper explores firm-level and environment-level "push" factors that drive firms to accept increasingly greater degrees of host country risk. We predict that firm-level risk outcomes for locating IS offshore facilities will be influenced by prior firm-specific experience, the relative gap between home and host country risk levels, and the overall movement by IS offshore services providers toward increasingly riskier locations. We test these hypotheses on a proprietary data set of more than 850 information technology and software offshoring projects in 55 host countries worldwide during the period 2000 through 2005. We find that firm-specific experience and the core "risk gap" between home and host country are predictive of companies pursuing progressively riskier locations, but that their effects dissipate as environment-wide experience is incorporated into our model. Our analyst's suggests that broader dynamics in the competitive environment are powerful contributors to the overall observation that IS offshoring is moving to increasingly high-risk locations. This trend has implications for the management, security, and global integration of information systems. Our study contributes to the literature on risk and IS offshoring in providing the first worldwide empirical examination of the determinants of actual firm IS offshoring behavior with respect to off-shoring location risk.
|keyword = IS offshoring,risk,FDI,offshore outsourcing,firm experience,empirical,longitudinal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''OFFSHORE INFORMATION SYSTEMS PROJECT SUCCESS: THE ROLE OF SOCIAL EMBEDDEDNESS AND CULTURAL CHARACTERISTICS'''
{{header}}
{{article
|author= Arun Rai,Likoebe M. Maruping,Viswanath Venkatesh,
|source= MIS QUARTERLY
|year= 2009
|abstract = Agency theory has served as a key basis for identifying drivers of offshore information system project success. Consequently, the role of relational factors in driving project success has been overlooked in this literature. In this paper, we address this gap by integrating the social embeddedness perspective and the culture literature to theorize how and why relational factors affect the success of offshore IS projects that are strategic in nature. We identify organizational and interpersonal cultural differences as critical success factors in this context. Using data from a longitudinal field study of 155 offshore IS projects managed by 22 project leaders, we found evidence of a relationship between hypothesized relational factors and two measures of offshore IS project success-namely, project cost overruns and client satisfaction-over and above the effects of project characteristics and agency factors. Specifically, we found that information exchange, joint problem solving, and trust reduce project cost overruns and improve client satisfaction. We also found a relationship between cultural differences at the organizational and team level, and offshore IS project success. The model explained 40 percent and 41 percent of the variance in project cost overruns and client satisfaction, respectively, for projects with a client representative. For projects with no client representative, the model explained 35 percent and 3 7 percent of the variance in project cost overruns and client satisfaction, respectively. Collectively, the results have important theoretical and practical implications for how client-vendor relationships should be managed when partnering with offshore firms and designing offshore IS project teams.
|keyword = Offshoring,social embeddedness,project management,agency theory,culture,multilevel,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents of IS Strategic Alignment: A Nomological Network'''
{{header}}
{{article
|author= David S. Preston,Elena Karahanna,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Alignment of information systems (IS) strategy with business strategy is a top concern of both the chief information officer (CIO) and the top management team (TMT) of organizations. Even though researchers and key decision makers in organizations recognize the importance of IS strategic alignment, they often struggle to understand how this alignment is created. In this paper, we develop a nomological network in which shared understanding between the CIO and TMT about the role of IS in the organization (which represents the social dimension of IS strategic alignment) is posited to be a proximal antecedent of the intellectual dimension of IS strategic alignment. We further posit that shared language, shared domain knowledge manifest in the CIO's business knowledge and the TMT's strategic IS knowledge, systems of knowing (structural and social), and CIO-TMT experiential similarity are important determinants of this shared understanding. Data were collected from 243 matched CIO-TMT pairs. Results largely support the proposed nomological network. Specifically, shared understanding between the CIO and TMT is a significant antecedent of IS strategic alignment. Furthermore, shared language, shared domain knowledge, and structural systems of knowing influence the development of shared understanding between the CIO and the TMT. Contrary to expectations and to findings of prior research, social systems of knowing, representing informal social interactions between the CIO and TMT, and experiential similarity did not have a significant effect on shared understanding.
|keyword = IS leadership,chief information officer,IS strategic alignment,shared understanding,strategic management of IT,top management team,matched-pair questionnaire surveys,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Governance-Knowledge Fit in Systems Development Projects'''
{{header}}
{{article
|author= Amrit Tiwana,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = This study addresses the theoretically underexplored question of how fit between project governance configurations, and the knowledge of specialized information technology (IT) and client departments, influences information systems development (ISD) performance. It conceptualizes project governance configurations using two classes of project decisions rights-decision control rights and decision management rights. The paper then develops a middle-range theory of how governance-knowledge fit shapes ISD performance by influencing the effective exercise of these decision rights during the development process. Further, the two dimensions of ISD performance-efficiency and effectiveness-are shaped by different classes of project decision rights. Data from 89 projects in 89 firms strongly support the proposed ideas. Implications for theory and practice are also discussed.
|keyword = systems development,project governance,decision rights,governance-knowledge fit,middle-range theory,project management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Configuration of and Interaction Between Information Security Technologies: The Case of Firewalls and Intrusion Detection Systems'''
{{header}}
{{article
|author= Huseyin Cavusoglu,Srinivasan Raghunathan,Hasan Cavusoglu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Proper configuration of security technologies is critical to balance the needs for access and protection of information. The common practice of using a layered security architecture that has multiple technologies amplifies the need for proper configuration because the configuration decision about one security technology has ramifications for the configuration decisions about others. Furthermore, security technologies rely on each other for their operations, thereby affecting each other's contribution. In this paper we study configuration of and interaction between a firewall and intrusion detection systems (IDS). We show that deploying a technology, whether it is the firewall or the IDS, could hurt the firm if the configuration is not optimized for the firm's environment. A more serious consequence of deploying the two technologies with suboptimal configurations is that even if the firm could benefit when each is deployed alone, the firm could be hurt by deploying both. Configuring the IDS and the firewall optimally eliminates the conflict between them, ensuring that if the firm benefits from deploying each of these technologies when deployed alone, it will always benefit from deploying both. When optimally configured, we find that these technologies complement or substitute each other. Furthermore, we find that while the optimal configuration of an IDS does not change whether it is deployed alone or together with a firewall, the optimal configuration of a firewall has a lower detection rate (i.e., allowing more access) when it is deployed with an IDS than when deployed alone. Our results highlight the complex interactions between firewall and IDS technologies when they are used together in a security architecture, and, hence, the need for proper configuration to benefit from these technologies.
|keyword = information security,software configuration,information security technologies,firewalls,intrusion detection systems,economics of information systems,analytical modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Willingness to Pay in an Open Source Software Environment'''
{{header}}
{{article
|author= T. S. Raghu,Rajiv Sinha,Ajay Vinze,Orneita Burton,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Competition from open source software and free software (OSS/FS) alternatives is causing proprietary software producers to reevaluate product strategies. OSS/FS alternatives complicate an already complex information goods market plagued by piracy concerns. Although producer perspectives on software pricing and piracy controls have been addressed extensively, consumers' perspective and willingness to pay for commercial software is not very well understood. This paper empirically determines willingness to pay for a leading commercial software application (Microsoft Office) in the presence of an OSS/FS alternative. A contingent valuation approach is used to elicit willingness to pay for the application. The research design employs a 2 x 2 x 2 experiment to investigate the impact of preventive control, deterrence control, and OSS/FS alternative. The results indicate that the availability of an OSS/FS alternative has little impact on willingness to pay for Microsoft Office. However, piracy controls significantly increase willingness to pay for Microsoft Office, even in the presence of OSS/FS alternatives.
|keyword = software piracy,intellectual property,open source software,willingness to pay,contingent valuation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trust and Satisfaction, Two Stepping Stones for Successful E-Commerce Relationships: A Longitudinal Exploration'''
{{header}}
{{article
|author= Dan J. Kim,Donald L. Ferrin,H. Raghav Rao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Trust and satisfaction are essential ingredients for successful business relationships in business-to-consumer electronic commerce. Yet there is little research on trust and satisfaction in e-commerce that takes a longitudinal approach. Drawing on three primary bodies of literature, the theory of reasoned action, the extended valence framework, and expectation-confirmation theory, this study synthesizes a model of consumer trust and satisfaction in the context of e-commerce. The model considers not only how consumers formulate their prepurchase decisions, but also how they form their long-term relationships with the same website vendor by comparing their prepurchase expectations to their actual purchase outcome. The results indicate that trust directly and indirectly affects a consumer's purchase decision in combination with perceived risk and perceived benefit, and also that trust has a longer term impact on consumer e-loyalty through satisfaction. Thus, this study extends our understanding of consumer Internet transaction behavior as a three-fold (prepurchase, purchase, and postpurchase) process, and it recognizes the crucial, multiple roles that trust plays in this process. Implications for theory and practice as well as limitations and future directions are discussed.
|keyword = trust in e-commerce,consumer satisfaction,purchase and repurchase intentions in B2C e-commerce,e-loyalty,extended valence framework,expectation-confirmation theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Demand Heterogeneity in IT Infrastructure Services: Modeling and Evaluation of a Dynamic Approach to Defining Service Levels'''
{{header}}
{{article
|author= Sagnika Sen,T. S. Raghu,Ajay Vinze,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = A key feature of service-oriented models of information technology is the promise of prespecified quality levels enforceable via service level agreements (SLAs). This poses difficult management problems when considerable variability exists in user preferences and service demand within any organization. Because variance in expectations impact service levels, effective pricing and resource allocation mechanisms are needed to deliver services at the promised quality level. In this paper, we propose a mechanism for SLA formulation that is responsive to demand fluctuations and user preference variance, with the objective of maximizing organizational welfare of the participants. This formulation features a dynamic priority based price-penalty scheme targeted to individual users. An analytical model is presented and evaluated for effectiveness of a proposed dynamic priority-based pricing scheme vis-a-vis a baseline fixed-price single-quality level SLA. Simulations using data from an existing SLA is used to provide evidence that the proposed dynamic pricing scheme is likely to be more effective than a fixed-price approach from a system welfare perspective.
|keyword = IT services,service level agreements,outsourcing,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social Interactions and the "Digital Divide": Explaining Variations in Internet Use'''
{{header}}
{{article
|author= Ritu Agarwal,Animesh Animesh,Kislaya Prasad,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Given the increasingly important role of the Internet in education, healthcare, and other essential services, it is important that we develop an understanding of the "digital divide." Despite the widespread diffusion of the Web and related technologies, pockets remain where the Internet is used sparingly, if at all. There are large geographic variations, as well as variations across ethnic and racial lines. Prior research suggests that individual, household, and regional differences are responsible for this disparity. We argue for an alternative explanation: Individual choice is subject to social influence ("peer effects") that emanates from geographic proximity; this influence is the cause of the excess variation. We test this assertion with empirical analysis of a data set compiled from a number of sources. We find, first, that widespread Internet use among people who live in proximity has a direct effect on an individual's propensity to go online. Using data on residential segregation, we test the proposition that the Internet usage patterns of people who live in more ethnically isolated regions will more closely resemble usage patterns of their ethnic group. Finally, we examine the moderating impact of housing density and directly measured social interactions on the relationship between Internet use and peer effects. Results are consistent across analyses and provide strong evidence of peer effects, suggesting that individual Internet use is influenced by local patterns of usage. Implications for public policy and the diffusion of the Internet are discussed.
|keyword = social interactions,digital divide,peer effects,instrumental variables,fixed effects,residential isolation,Internet use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Association to Causation via a Potential Outcomes Approach'''
{{header}}
{{article
|author= Sunil Mithas,M. S. Krishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Despite the importance of causal analysis in building a valid knowledge base and in answering managerial questions, the issue of causality rarely receives the attention it deserves in information systems (IS) and management research that uses observational data. In this paper, we discuss a potential outcomes framework for estimating causal effects and illustrate the application of the framework in the context of a phenomenon that is also of substantive interest to IS researchers. We use a matching technique based on propensity scores to estimate the causal effect of an MBA on information technology (IT) professionals' salary in the United States. We demonstrate the utility of this counterfactual or potential outcomes-based framework in providing an estimate of the sensitivity of the estimated causal effects because of selection on unobservables. We also discuss issues related to the heterogeneity of treatment effects that typically do not receive as much attention in alternative methods of estimation, and show how the potential outcomes approach can provide several new insights into who benefits the most from the interventions and treatments that are likely to be of interest to IS researchers. We discuss the usefulness of the matching technique in IS and management research and provide directions to move from establishing association to assessing causation.
|keyword = business value of IT,returns on MBA,causal analysis,propensity score,matching estimator,counterfactual approach,treatment effect heterogeneity,selection on unobservables,sensitivity analysis,MBA,IT professionals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decentralization in Wikipedia Governance'''
{{header}}
{{article
|author= Andrea Forte,Vanessa Larco,Amy Bruckman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = How does "self-governance" happen in Wikipedia? Through in-depth interviews with 20 individuals who have held a variety of responsibilities in the English-language Wikipedia, we obtained rich descriptions of how various forces produce and regulate social structures on the site. Although Wikipedia is sometimes portrayed as lacking oversight, our analysis describes Wikipedia as an organization with highly refined policies, norms, and a technological architecture that supports organizational ideals of consensus building and discussion. We describe how governance on the site is becoming increasingly decentralized as the community grows and how this is predicted by theories of commons-based governance developed in offline contexts. We also briefly examine local governance structures called WikiProjects through the example of WikiProject Military History, one of the oldest and most prolific projects on the site.
|keyword = governance,online communities,self-organizing systems,Wikipedia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Engaging Group E-Learning in Virtual Worlds'''
{{header}}
{{article
|author= Katherine Franceschi,Ronald M. Lee,Stelios H. Zanakis,David Hinds,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = E-learning has seen tremendous growth in recent years. More and more, university courses are now available online to a potentially global audience. However, a significant shortcoming of e-learning technologies has been poor support for group-oriented learning. We believe that virtual worlds offer a potential solution. Unlike videoconferencing (for instance), virtual worlds provide a shared visual space for students to meet and interact (via avatars). Not only do students share the quasi-realism of a 3D environment where participants can see and hear one another, they also have the capability to manipulate artifacts together. These factors provide a strong sense of group presence, which leads to engaging group learning interactions.
|keyword = business education,e-learning,MMOG,MMORPG,online learning,social presence,virtual engagement,virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Chaos Theory as a Lens for Interpreting Blogging'''
{{header}}
{{article
|author= Xitong Guo,Doug Vogel,Zhongyun (Phil) Zhou,Xi Zhang,Huaping Chen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Blogging is becoming increasingly popular as a global phenomenon. Individual blog traffic and blogosphere structure are of interest to academia and practice. Although it is difficult to get a snapshot of the blogosphere with enough blogs over a long enough period to capture the real situation, chaos theory finds underlying order in this apparent random and complex phenomenon. This study provides an overall view of blogging from micro (individual blog traffic dynamics) and macro (blogosphere structure) levels through a chaos theory lens. Key concepts of chaos theory are used to construct an interpretive framework to illustrate blog system behavior dynamics. Blog systems tend to be nonlinear, dynamic, and deterministic, as well as sensitive to initial conditions. The study also demonstrates the feasibility of applying chaos theory thinking to areas such as knowledge management and the recent global financial crisis. Implications for practice and research opportunities are presented.
|keyword = blogging,blogosphere,blogosphere structure,blog traffic,chaos theory,global financial crisis,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing Knowledge in Light of Its Evolution Process: An Empirical Study on Citation Network-Based Patent Classification'''
{{header}}
{{article
|author= Xin Li,Hsinchun Chen,Zhu Zhang,Jiexun Li,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Knowledge management is essential to modem organizations. Due to the information overload problem, managers are facing critical challenges in utilizing the data in organizations. Although several automated tools have been applied, previous applications often deem knowledge items independent and use solely contents, which may limit their analysis abilities. This study focuses on the process of knowledge evolution and proposes to incorporate this perspective into knowledge management tasks. Using a patent classification task as an example, we represent knowledge evolution processes with patent citations and introduce a labeled citation graph kernel to classify patents under a kernel-based machine learning framework. In the experimental study, our proposed approach shows more than 30 percent improvement in classification accuracy compared to traditional content-based methods. The approach can potentially affect the existing patent management procedures. Moreover, this research ends strong support to considering knowledge evolution processes in other knowledge management tasks.
|keyword = citation analysis,classification,kernel-based method,knowledge management,machine learning,patent management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging Crowdsourcing: Activation-Supporting Components for IT-Based Ideas Competition'''
{{header}}
{{article
|author= Jan Marco Leimeister,Michael Huber,Ulrich Bretschneider,Helmut Krcmar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Ideas competitions appear to be a promising tool for crowdsourcing and open innovation processes, especially for business-to-business software companies. Active participation of potential lead users is the key to success. Yet a look at existing ideas competitions in the software field leads to the conclusion that many information technology (IT)-based ideas competitions fail to meet requirements upon which active participation is established. The paper describes how activation-enabling functionalities can be systematically designed and implemented in an IT-based ideas competition for enterprise resource planning software. We proceeded to evaluate the outcomes of these design measures and found that participation can be supported using a two-step model. The components of the model support incentives and motives of users. Incentives and motives of the users then support the process of activation and consequently participation throughout the ideas competition. This contributes to the successful implementation and maintenance of the ideas competition, thereby providing support for the development of promising innovative ideas. The paper concludes with a discussion of further activation-supporting components yet to be implemented and points to rich possibilities for future research in these areas.
|keyword = activation,crowdsourcing,ERP software,ideas competition,incentives,motivation,open innovation,theory-driven design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Design Approach for Collaboration Processes: A Multimethod Design Science Study in Collaboration Engineering'''
{{header}}
{{article
|author= [Anonymous],
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Collaboration engineering is an approach for the design and deployment of repeatable collaboration processes that can be executed by practitioners without the support of collaboration professionals such as facilitators. A critical challenge in collaboration engineering concerns how the design activities have to be executed and which design choices have to be made to create a process design. We report on a four-year design science study in which we developed a design approach for collaboration engineering that incorporates existing process design methods, pattern-based design principles, and insights from expert facilitators regarding design challenges and choices. The resulting approach was evaluated and continuously improved in four trials with 37 students. Our findings suggest that this approach is useful to support the design of repeatable collaboration processes. Our study further serves as an example of how a design approach can be developed and improved following a multimethod design science approach.
|keyword = collaboration engineering,design science,facilitation,group support systems (GSS),pattern language,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Assimilation of Electronic Procurement Innovations'''
{{header}}
{{article
|author= Arun Rai,Paul Brown,Xinlin Tang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = We investigate the assimilation of electronic procurement innovations (EPIs) and its impact on procurement productivity in buyer organizations. We identify online reverse auctions, electronic catalog management, electronic order fulfillment, and electronic payment and settlement as moderate complements for the performance of the procurement process. We develop a theoretical model that is informed by the literature on innovation assimilation and by structuration theory to explain the aggregated assimilation of EPIs. Our empirical study is based on survey data collected about EMS from 166 buyer firms. Based on our analysis, we isolate the organizational, technological, and interorganizational factors that shape the meta-structures for the aggregated assimilation of EMS. Our results also provide evidence of a substantial impact of the assimilation of these innovations on procurement productivity. Our post hoc analysis provides insights on differences across stages and across EMS on the factors and meta-structures that enable assimilation.
|keyword = electronic procurement innovations,IT assimilation,productivity,structuration theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Differential Effects of the Two Types of Information Systems: A Hospital-Based Study'''
{{header}}
{{article
|author= Nirup M. Menon,Ulku Yaylacicegi,Asunur Cezar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = A new empirical model for the production function of the hospital incorporating two types of information systems (IS) is developed. One type of IS is representative of information technology (IT) used in primary, clinical, value-chain activities, and the other is representative of the IT used in support (administrative) value-chain activities. The model innovation is that it accommodates up to a seven-year lag for each type of IS. The output variables for the production function are hospital output and medical labor productivity. Using data spanning from 1979 to 2006 from several hospitals, it was found that clinical IS improve hospital output in the short run (of two years). Administrative IS were found to be negatively associated with organizational performance in the short run, but positively associated with these performance measures over the long run (over four years). These results highlight the importance of timing IT investments and the sequencing chosen for the implementation of IS presenting various value-chain activities, and the resulting pattern of business value over time. Differential lag length of the types of IS is to be considered in estimating the rate of return of new IT projects.
|keyword = health-care informatics,IT productivity,value-chain model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHOM ARE WE INFORMING? ISSUES AND RECOMMENDATIONS FOR MIS RESEARCH FROM AN INFORMING SCIENCES PERSPECTIVE'''
{{header}}
{{article
|author= Grandon Gill,Anol Bhattacherjee,
|source= MIS QUARTERLY
|year= 2009
|abstract = This paper provides an introspective assessment of the current state of management information systems as a research discipline using the "lens" of the informing sciences. Based on this assessment, we observe that the degree to which MIS research is informing its key external clients practitioners, students, and researchers it? other disciplines has declined over the years. This problem is particularly acute with respect to informing practitioners. Unfortunately, practitioner support may be critical in making up for lost resources caused by declining student enrollments. Despite this dire prognostication, we believe that it is possible to reverse this trend Drawing upon cognitive science and diffusion of innovations research, we analyze the source of the problem and then present five recommendations aimed at leading MIS journals, scholars, and professional societies for improving the ability of MIS research to engage and inform its external clients.
|keyword = Management information systems,informing sciences,practitioner,rigor,relevance,resonance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A SCIENTIFIC BASIS FOR RIGOR IN INFORMATION SYSTEMS RESEARCH'''
{{header}}
{{article
|author= Allen S. Lee,Geoffrey S. Hubona,
|source= MIS QUARTERLY
|year= 2009
|abstract = Qualitative research is just as able as quantitative research to follow certain fundamental principles of logic in general and scientific reasoning in particular. Two such principles are the logic of modus ponens and the logic of modus tollens. In this essay, we frame different research approaches positivist research, interpretive research, action research, and design research-in the forms of modus ponens and modus tollens. Three issues emerge from this framing and call into question how research is now conducted in the discipline of information systems. They are the issue of a common scientific basis, the issue of the fallacy of affirming the consequent, and the issue of summative validity. Both rigor and relevance in information systems research may be better achieved by attending to the three issues.
|keyword = Theory testing,theory building,research methods,methodology,epistemology,positivist research,interpretive research,action research,design research,quantitative research,qualitative research,rigor,relevance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERNET EXCHANGES FOR USED GOODS: AN EMPIRICAL ANALYSIS OF TRADE PATTERNS AND ADVERSE SELECTION'''
{{header}}
{{article
|author= Anindya Ghose,
|source= MIS QUARTERLY
|year= 2009
|abstract = In the past few years, we have witnessed the increasing ubiquity of user-generated content on seller reputation and product condition in Internet-based used-good markets. Recent theoretical models of trading and sorting in used-good markets provide testable predictions to use to examine the presence of adverse selection and trade patterns in such dynamic markets. A key aspect of such empirical analyses is to distinguish between product-level uncertainty and seller level uncertainty, an aspect the extant literature has largely ignored. Based on a unique, 5-month panel data set of user-generated content on used good quality and seller reputation feedback collected from Amazon, this paper examines trade patterns in online used-good markets across four product categories (PDAs, digital cameras, audio players, and laptops). Drawing on two different empirical tests and using content analysis to mine the textual feedback of seller reputations, the paper provides evidence that adverse selection continues to exist in online markets. First, it is shown that after controlling for price and other product, and for seller-related factors, higher quality goods take a longer time to sell compared to lower quality goods. Second, this result also holds when the relationship between sellers' reputation scores and time to sell is examined. Third, it is shown that price declines are larger for more unreliable products, and that products with higher levels of intrinsic unreliability exhibit a more negative relationship between price decline and volume of used good trade. Together, our findings suggest that despite the presence of signaling mechanisms such as reputation feedback and product condition disclosures, the information asymmetry problem between buyers and sellers persists in online markets due to both product-based and seller-based information uncertainty. No consistent evidence of substitution or complementarity effects between product-based and seller-level uncertainty are found. Implications for research and practice are discussed.
|keyword = Information uncertainty,adverse selection,user-generated content,text analysis,seller reputation,product quality,used goods,electronic markets,information asymmetry,trade patterns,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTERACTIVE DECISION AIDS FOR CONSUMER DECISION MAKING IN E-COMMERCE: THE INFLUENCE OF PERCEIVED STRATEGY RESTRICTIVENESS'''
{{header}}
{{article
|author= Weiquan Wang,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2009
|abstract = This paper extends the effort-accuracy framework of cognition by taking into account the perceived strategy restrictiveness of decision aids, and tests the extended framework in a context in which online decision aids are used to elicit consumers' preferences, automate the processing of the preferences, and provide product advice for consumers. Three types of decision aids with different decision strategy support capabilities (an additive-compensatory based aid, an elimination-based aid, and a hybrid aid supporting both strategies) are, compared in terms of users' perceptions of strategy restrictiveness, advice quality, and cognitive effort. These comparisons are grounded on the properties of normativeness and complementarity of decision strategies employed by the aids. A normative strategy takes into account both the users' attribute preferences and the relative importance of such preferences, and allows for trade-offs among preferences (e.g., additive-compensatory). Strategy complementarity indicates support for decision rules based on multiple strategies (e.g., both additive-compensatory and elimination strategies). The experimental results support the validity of the extended effort-accuracy-restrictiveness framework and the effects of strategy normativeness, but not the effects of strategy complementarity. In addition to the perceptions of cognitive effort and advice quality, perceived strategy restrictiveness exerts a significant influence on consumers' intentions to use online decision aids, The additive-compensatory aid is perceived to be less restrictive, of higher quality, and less effortful than the elimination aid, whereas the hybrid aid is not perceived to be any different from the additive-compensatory aid.
|keyword = Decision aid,decision strategy,restrictiveness,cognitive effort,advice quality,strategy normativeness,strategy complementarity,explanation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPETING WITH FREE: THE IMPACT OF MOVIE BROADCASTS ON DVD SALES AND INTERNET PIRACY'''
{{header}}
{{article
|author= Michael D. Smith,Rahul Telang,
|source= MIS QUARTERLY
|year= 2009
|abstract = The creative industries have frequently expressed concern that they can't compete with freely available copies of their content. Competing with free is particularly concerning for movie studios, whose content may be more prone to single use consumption than other industries such as music. This issue has gained renewed importance recently with the advent of new digital video recording and distribution technologies, and the widespread availability of Internet piracy. We examine competition between "free" and paid video content in two important contexts., the impact of legitimate free distribution in one channel on sales through paid channels, and the impact of illegitimate free distribution in pirated channels on sales through paid channels. We do this by studying the impact of movie broadcasts on D VD demand and the impact of piracy availability at the time of broadcast on DVD demand. Our data include all movies shown on over-the-air and cable television during an eight-month period in 2005-2006. With respect to the impact of movie broadcasts on piracy and sales, we find that movie broadcasts on over-the-air networks result in a significant increase in both D VD sales at Amazon. coin and illegal downloads for those movies that are available on BitTorrent at the time of broadcast. With respect to the impact of piracy on sales, we use the television broadcast as an exogenous demand shock and find that the availability of pirated content at the time of broadcast has no effect on post-broadcast D VD sales gains. Together our results suggest that creative artists can use product differentiation and market segmentation strategies to compete with freely available copies of their content. Specifically, the post-broadcast increase in DVD sales suggests that giving away content in one channel can stimulate sales in a paid channel if the free content is sufficiently differentiated from its paid counterpart. Likewise, our finding that the presence of pirated content does not cannibalize sales for the movies in our sample suggests that if free and paid products appeal to separate customer segments, the presence of free products need not harm paid sales.
|keyword = Information goods,movie broadcasts,movie promotion,DVD sales,movie piracy,broadcast flag,consumer surplus,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODEL OF ACCEPTANCE WITH PEER SUPPORT: A SOCIAL NETWORK PERSPECTIVE TO UNDERSTAND EMPLOYEES' SYSTEM USE'''
{{header}}
{{article
|author= Tracy Ann Sykes,Viswanath Venkatesh,Sanjay Gosain,
|source= MIS QUARTERLY
|year= 2009
|abstract = Prior research has extensively studied individual adoption and use of information systems, primarily using beliefs as predictors of behavioral intention to use a system that in turn predicts system use. We propose a model of acceptance with peer support (MAPS) that integrates prior individual-level research with social networks constructs. We argue that an individual's embeddedness in the social network of the organizational unit implementing a new information system can enhance our understanding of technology use. An individual's coworkers can be important sources of help in overcoming knowledge barriers constraining use of a complex system, and such interactions with others can determine an employee's ability to influence eventual system configuration and features. We incorporate network density (reflecting "get-help" ties for an employee) and network centrality (reflecting "give-help" ties for an employee), drawn from prior social network research, as key predictors of system use. Further, we conceptualize valued network density and valued network centrality, both of which take into account ties to those with relevant system-related information, knowledge, and resources, and employ them as additional predictors. We suggest that these constructs together are coping and influencing pathways by which they have an effect on system use. We conducted a 3-month long study of 87 employees in one business unit in an organization. The results confirmed our theory that social network constructs can significantly enhance our understanding of system use over and above predictors from prior individual-level adoption research.
|keyword = TAM,UTAUT,social networks,behavioral intention,system use,network centrality,network density,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does Fit Matter? The Impact of Task-Technology Fit and Appropriation on Team Performance in Repeated Tasks'''
{{header}}
{{article
|author= Robert M. Fuller,Alan R. Dennis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Prior research on technology and team performance concludes that the fit of the technology to tasks influences team performance. It also suggests that the way teams appropriate technology influences performance. This research examines how fit and appropriation (from the Fit Appropriation Model) influence performance over time. Initially, the results show that fit better predicted performance; teams using poor-fitting technology performed worse than teams with better fitting technology. However, over a short time period (two days in this study), this initial fit no longer predicted performance; performance of teams using better fitting technology remained constant while teams using poor-fitting technology innovated and adapted, improving performance. There are two key findings from this study. First, fit can predict team performance soon after technology adoption, but initial assessments of fit are temporary as teams innovate and adapt; thus, our current theoretical models of fitting technology to a task likely will not be useful beyond the first use. Second, teams should understand how to better adapt existing technology and work structures. Because our current theories of task-technology fit failed to predict performance beyond the first use of technology, we believe that this calls for a reconsideration of what fit means for teams using technology.
|keyword = fit-appropriation model,team performance,repeated tasks,time,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology in Supply Chains: The Value of IT-Enabled Resources Under Competition'''
{{header}}
{{article
|author= Shutao Dong,Sean Xin Xu,Kevin Xiaoguo Zhu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = In this study, we seek to better understand the value of information technology (IT) in supply chain contexts. Grounded in the resource-based theory in conjunction with transaction cost economics, we develop a conceptual model that links three IT-related resources (backend integration, managerial skills, and partner support) to firm performance improvement. The model differs from previous studies by proposing a moderating effect of competition on the resource-performance relationships. Using data of 743 manufacturing firms, our analysis indicates significant contribution of IT to supply chains, which is generated through development of the digitally enabled integration capability and manifested at the process level along the supply chain. The technological resource alone, however, does not hold the answer to IT value creation. In fact, managerial skills, which enable adaptations on supply chain processes and corporate strategy to accommodate the use of IT, are shown to play the strongest role in IT value creation. Furthermore, backend integration and managerial skills are found to be more valuable in more competitive environments. While commodity-like resources have diminishing value under competition, integrational and managerial resources become even stronger. Overall, our results shed light on the key drivers of IT-enabled supply chains, and provide insights into how competition shapes IT value.
|keyword = IT business value,competition,supply chain,resource-based view,moderation effect,intangible resources,backend integration,managerial skills,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Computational Analysis of Linear Price Iterative Combinatorial Auction Formats'''
{{header}}
{{article
|author= Martin Bichler,Pasha Shabalin,Alexander Pikovsky,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Iterative combinatorial auctions (ICAs) are IT-based economic mechanisms where bidders submit bundle bids in a sequence and an auctioneer computes allocations and ask prices in each auction round. The literature in this field provides equilibrium analysis for ICAs with nonlinear personalized prices under strong assumptions on bidders' strategies. Linear pricing has performed very well in the lab and in the field. In this paper, we compare three selected linear price ICA formats based on allocative efficiency and revenue distribution using different bidding strategies and bidder valuations. The goal of this research is to benchmark different ICA formats and design and analyze new auction rules for auctions with pseudodual linear prices. The multi-item and discrete nature of linear price iterative combinatorial auctions and the complex price calculation schemes defy much of the traditional game theoretical analysis in this field. Computational methods can be of great help in exploring potential auction designs and analyzing the virtues of various design options. In our simulations, we found that ICA designs with linear prices performed very well for different valuation models even in cases of high synergies among the valuations. There were, however, significant differences in efficiency and in the revenue distributions of the three ICA formats. Heuristic bidding strategies using only a few of the best bundles also led to high levels of efficiency. We have also identified a number of auction rules for ask price calculation and auction termination that have shown to perform very well in the simulations.
|keyword = iterative combinatorial auction,pseudodual prices,allocative efficiency,computational experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Website Characteristics on a Consumer's Urge to Buy Impulsively'''
{{header}}
{{article
|author= D. Veena Parboteeah,Joseph S. Valacich,John D. Wells,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = With the proliferation of e-commerce, there is growing evidence that online impulse buying is occurring, yet relatively few researchers have studied this phenomenon. This paper reports on two studies that examine how variations in a website influence online impulse buying. The results reveal some relevant insights about this phenomenon. Specifically, although many participants had the urge to buy impulsively, regardless of website quality, this behavior's likelihood and magnitude was directly influenced by varying the quality of task-relevant and mood-relevant cues. Task-relevant cues include characteristics, such as navigability, that help in the attainment of the online consumer's shopping goal. Conversely, mood-relevant cues refer to the characteristics, such as visual appeal, that affect the degree to which a user enjoys browsing a website but that do not directly support a particular shopping goal. The implications of the results for both future research and the design of human-computer interfaces are discussed.
|keyword = impulse buying,electronic commerce,human-computer interface,environmental psychology,website characteristics,scenario,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of the Union and Difference Operations on the Quality of Information Products'''
{{header}}
{{article
|author= Amir Parssian,Sumit Sarkar,Varghese S. Jacob,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = Information derived from relational databases is routinely used for decision making. However, little thought is usually given to the quality of the source data, its impact on the quality of the derived information, and how this in turn affects decisions. To assess quality, one needs a framework that defines relevant metrics that constitute the quality pro. le of a relation, and provides mechanisms for their evaluation. We build on a quality framework proposed in prior work, and develop quality profiles for the result of the primitive relational operations Difference and Union. These operations have nuances that make both the classification of the resulting records as well as the estimation of the different classes quite difficult to address, and very different from that for other operations. We first determine how tuples appearing in the results of these operations should be classified as accurate, inaccurate or mismember, and when tuples that should appear do not (called incomplete) in the result. Although estimating the cardinalities of these subsets directly is difficult, we resolve this by decomposing the problem into a sequence of drawing processes, each of which follows a hyper-geometric distribution. Finally, we discuss how decisions would be influenced based on the resulting quality profiles.
|keyword = information quality framework,relational data model,probability calculus,hyper-geometric distributions,database marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Choice and Chance: A Conceptual Model of Paths to Information Security Compromise'''
{{header}}
{{article
|author= Sam Ransbotham,Sabyasachi Mitra,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = No longer the exclusive domain of technology experts, information security is now a management issue. Through a grounded approach using interviews, observations, and secondary data, we advance a model of the information security compromise process from the perspective of the attacked organization. We distinguish between deliberate and opportunistic paths of compromise through the Internet, labeled choice and chance, and include the role of countermeasures, the Internet presence of the firm, and the attractiveness of the firm for information security compromise. Further, using one year of alert data from intrusion detection devices, we find empirical support for the key contributions of the model. We discuss the implications of the model for the emerging research stream on information security in the information systems literature.
|keyword = information security management,computer crime,information systems risk management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investments in Information Technology: Indirect Effects and Information Technology Intensity'''
{{header}}
{{article
|author= Neeraj Mittal,Barrie R. Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2009
|abstract = M any studies measure the value of information technology (IT) by focusing on how much value is added rather than on the mechanisms that drive value addition. We argue that value from IT arises not only directly through changes in the factor input mix but also indirectly through IT-enabled augmentation of non-IT inputs and changes in the underlying production technology. We develop an augmented form of the Cobb-Douglas production function to separate and measure different productivity-enhancing effects of IT. Using industry-level data from the manufacturing sector, we find evidence that both direct and indirect effects of IT are significant. Partitioning industries into IT-intensive and non-IT-intensive, we find that the indirect effects of IT predominate in the IT-intensive sector. In contrast, the direct effects of IT predominate in the non-IT intensive sector. These results indicate structural differences in the role of IT in production between industries that are IT-intensive and those that are not. The implication for decision-makers is that for IT-intensive industries the gains from IT come primarily through indirect effects such as the augmentation of non-IT capital and labor.
|keyword = IT productivity,indirect effects,IT investment,output elasticity,technological change,production theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding Sustained Participation in Open Source Software Projects'''
{{header}}
{{article
|author= Yulin Fang,Derrick Neufeld,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Prior research into open source software (OSS) developer participation has emphasized individuals' motivations for joining these volunteer communities, but it has failed to explain why people stay or leave in the long run. Building upon Lave and Wenger's theory of legitimate peripheral participation (LPP), this paper offers a longitudinal investigation of one OSS community in which sustained participation is hypothesized to be associated with the coevolution of two major elements of LPP theory: "situated learning" (the process of acting knowledgeably and purposefully in the world) and "identity construction" (the process of being identified within the community). To test this hypothesis, data were collected from multiple sources, including online public project documents, electronic mail messages, tracker messages, and log files. Results from qualitative analyses revealed that initial conditions to participate did not effectively predict long-term participation, but that situated learning and identity construction behaviors were positively linked to sustained participation. Furthermore, this study reveals that sustained participants distinguished themselves by consistently engaging in situated learning that both made conceptual (advising others) and practical contributions (improving the code). Implications and future research are discussed.
|keyword = communities of practice,legitimate peripheral participation,open source projects,open source software community,qualitative study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competence of IT Professionals in E-Business Venture Teams: The Effect of Experience and Expertise on Preference Structure'''
{{header}}
{{article
|author= Tobias Kollmann,Matthias Haesel,Nicola Breugst,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = The ascension of e-business has significantly changed competence requirements of information technology (IT) professionals. In this paper, we derive a competence set that addresses these changes and investigate individual preferences for specific competence components within e-business teams. We connect these preferences and competence valuation with personal characteristics of team members that were found to influence the perception of competence requirements in previous research. To empirically address this issue, we apply a Web-based questionnaire with adaptive conjoint measurement. By cluster analysis, we identify four competence profiles preferred by team members. Data from 176 respondents suggest that experience is related to the preferred profile, whereas expertise is related to overall competence valuation. Our research suggests that immature teams should consider that preferences regarding IT professionals may change with venture maturation, whereas interdisciplinary teams should discuss each member's value contribution. Considering our results, these suggestions could optimize the process of team composition.
|keyword = competence of IT professionals,competence profile,competence valuation,conjoint analysis,e-business,team composition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Knowledge Validation Processes Affect Knowledge Contribution'''
{{header}}
{{article
|author= Alexandra Durcikova,Peter Gray,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = To ensure that knowledge repositories contain high-quality knowledge, knowledge management research recommends that contributions to a repository undergo stringent validation processes. To date, however, no research has studied the impact of such processes on contributors' repository-related perceptions or behaviors. To address this gap, we develop a model based on signaling theory and reinforcement theory to explain how individuals' perceptions of three primary validation process characteristics (duration, transparency, and restrictiveness) impact their perceptions of repository knowledge quality and their contribution behaviors. Our empirical results confirm the importance of implementing review processes that are transparent and developmentally oriented as a way of encouraging knowledge contribution. More broadly, this study underscores the need to develop integrated theoretical models that draw from a variety of reference theories when attempting to explain knowledge-related behaviors.
|keyword = knowledge contribution processes,knowledge management,knowledge repositories,knowledge sharing,knowledge sourcing,reinforcement theory,signaling theory,validation processes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Preserving User Preferences in Automated Document-Category Management: An Evolution-Based Approach'''
{{header}}
{{article
|author= Chih-Ping Wei,Paul Jen-Hwa Hu,Yen-Hsien Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Analysis of prevalent document management practices shows the popular use of categories (e.g., folders) to organize documents for subsequent searches and retrievals. The coherence and distinction of an existing document category can diminish considerably as influxes of new documents arrive over time. The complexity of and effort requirements for document-category management favor an automated approach that can be supported by appropriate document-clustering techniques. A review of the extant literature shows a predominant focus on document content analysis in automated document-category management, which cannot preserve the user's document-grouping preferences. This research develops two advanced evolution-based techniques for preserving user preferences in their management of document categories. The first technique (CE2), which supports the automated evolution of a set of flat (i.e., nonhierarchical) document categories, extends a promising evolution-based technique (category evolution, CE) by addressing its fundamental limitations inherent to the use of holistic measures. The second technique, category hierarchy evolution (CHE), is developed on the basis of CE2 to support scenarios where document categories are organized with a hierarchical structure. Empirical evaluations of the effectiveness of each technique in various category evolution scenarios created using two different document corpora (i.e., news documents from Reuters and research articles from the ACM digital library), as compared with those of associated salient techniques for benchmark purposes, show that CE2 and CHE outperform their respective benchmark techniques. Their performance is reasonably robust and appears more effective when the quality (coherence) of the previously created categories does not deteriorate excessively. According to our results, the evolution-based approach is viable, appealing, and capable of preserving user preferences in automatic reorganizations of document categories.
|keyword = category evolution,category hierarchy evolution,document category management,document clustering,hierarchical agglomerative clustering,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating Anthropomorphic Product Recommendation Agents: A Social Relationship Perspective to Designing Information Systems'''
{{header}}
{{article
|author= Lingyun Qiu,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = In online shopping environments, the product-advising function originally performed by salespeople is being increasingly taken over by software-based product recommendation agents (PRAs). However, the literature has mostly focused on the functionality design and utilitarian value of such decision support systems, mostly ignoring the potential social influence they could exert on their users. The objective of this study is to apply a social relationship perspective to the design of interfaces for PRAs. We investigate the effects of applying anthropomorphic interfaces-namely, humanoid embodiment and voice output-on users' perceived social relationship with a technological and software-based artifact designed for electronic commerce contexts. The findings from a laboratory experiment indicate that using humanoid embodiment and human voice-based communication significantly influences users' perceptions of social presence, which in turn enhances users' trusting beliefs, perceptions of enjoyment, and ultimately, their intentions to use the agent as a decision aid. These results extend the applicability of theories concerning traditional shopper-sales person relationships to customers' interactions with technological artifacts residing on Web sites-that is, the recommendation agent software-and provide practitioners with guidelines on how to design Internet stores with the goal of building social relationships with online shoppers and enhancing their overall shopping experiences.
|keyword = avatar,decision support systems,electronic commerce,human voice,laboratory experiments,recommendation agents,social presence,text-to-speech (TTS),user acceptance of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managerial Entrenchment with Strategic Information Technology: A Dynamic Perspective'''
{{header}}
{{article
|author= Xiaotong Li,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Some economic and informational problems associated with organizational information technology (IT) spending may be attributed to managerial rent-seeking. Because of the unavoidable incompleteness of labor contracts, managers with misaligned incentives and budgetary discretion could entrench themselves through their non-value-maximizing adoption decisions. In order to boost their bargaining power in future contract renegotiation, they invest excessively in technologies they manage more effectively than their potential rivals. In addition, they tend to adopt technologies that can create large information asymmetries giving them significant knowledge advantage over their potential rivals ex post. We study the implications and effects of their rent-seeking behavior within the context of organizational IT adoption and management. The efficacies and the limitations of formal incentive contracting are discussed to underscore the need for additional governance mechanisms. While knowledge management may mitigate some of the agency problems associated with entrenchment, managerial self-policing issue remains a challenge. We further explore the incentive provision potential of relational labor contracts in combating entrenchment.
|keyword = agency problems,bargaining,contract renegotiation,entrenchment,incomplete contracts,information technology adoption,knowledge management,organizational decision making,relational contracts,rent-seeking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal Digital Content Distribution Strategy in the Presence of the Consumer-to-Consumer Channel'''
{{header}}
{{article
|author= Yunfang Feng,Zhiling Guo,Wei-Yu Kevin Chiang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = Although the online business-to-consumer (B2C) channel is the primary selling channel for digital content (e.g., videos, images, and music), modern digital technology has made possible the legal dissemination of such content over the consumer-to-consumer (C2C) channel through personal computing devices, such as PCs, mobile phones, and portable media players. This paper investigates the optimal channel structure and the corresponding pricing and service strategies for digital content distribution in order to understand the business value of introducing the C2C channel alongside the prevailing B2C channel. We identify conditions under which it is more profitable to use both B2C and C2C channels simultaneously (i.e., the dual-channel distribution). In such cases, the seller performs price discrimination among consumers but provides them with a higher level of service. Our analysis further characterizes the benefits of service provision. We show that service provision can increase the dual-channel pricing flexibility, reduce the seller's B2C channel dependence, and allow the seller to tolerate higher C2C channel redistribution costs. Finally, in examining the effect of a competitively determined B2C channel price on optimal channel strategy, we find that the seller prefers a dual-channel distribution under higher B2C channel prices.
|keyword = B2C,C2C,channel strategy,digital content,dual channel,pricing,service,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Patterns of Transition: The Shift from Traditional to Object-Oriented Development'''
{{header}}
{{article
|author= H. James Nelson,Deborah J. Armstrong,Kay M. Nelson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2009
|abstract = While many of the changes that information systems professionals encounter are incremental requiring only modifications to an existing mind-set, others represent radical changes that require a fundamental shift in mind-set in order to fully benefit from the change. The goal of this study was to examine how software development concept thinking changes as individuals shift their mind-set to the object-oriented software development approach. The results indicate that the transition may not be as simple as a single "aha!" moment, but rather a series of cognitive shifts as constructs from the old mind-set are replaced by constructs in the new mind-set. In addition, this study identifies problems that may be encountered during the transition due to a lack of cognitive fit, confusion, and reverting to an inappropriate mind-set. The findings from this study may be used to decrease the learning curve and minimize the number of cognitive shifts experienced during the transition.
|keyword = learning models,object-oriented approach,organizational change,personnel IS,personnel training,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE SHOEMAKER'S CHILDREN: USING WIKIS FOR INFORMATION SYSTEMS TEACHING, RESEARCH, AND PUBLICATION'''
{{header}}
{{article
|author= Gerald C. Kane,Robert G. Fichman,
|source= MIS QUARTERLY
|year= 2009
|abstract = This paper argues that Web 2.0 tools, specifically wikis, have begun to influence business and knowledge sharing practices in many organizations. Information Systems researchers have spent considerable time exploring the impact and implications of these tools in organizations, but those same researchers have not spent sufficient time considering whether and how these new technologies may provide opportunities for us to reform our core practices of research, review, and teaching. To this end, this paper calls for the IS discipline to engage in two actions related to wikis and other Web 2.0 tools. First, the IS discipline ought to engage in critical reflection about how wikis and other Web 2.0 tools could allow us to conduct our core processes differently. Our existing practices were formulated during an era of,paper based exchange; wikis and other Web 2.0 tools may enable processes that could be substantively better. Nevertheless, users can appropriate information technology tools in unexpected ways, and even when tools are appropriated as expected there can be unintended negative consequences. Any potential changes to our core processes should, therefore, be considered critically and carefully, leading to our second recommended action. We advocate and describe a series of controlled experiments that will help assess the impact of these technologies on our core processes and the associated changes that would be necessary to use them. We argue that these experiments can provide needed information regarding Web 2.0 tools and related practice changes that could help the discipline better assess whether or not new practices would be superior to existing ones and under which circumstances.
|keyword = Web 2.0,Wiki,teaching,review,research,AIS,collaboration,academia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATIONAL CASCADES AND SOFTWARE ADOPTION ON THE INTERNET: AN EMPIRICAL INVESTIGATION'''
{{header}}
{{article
|author= Wenjing Duan,Bin Gu,Andrew B. Whinston,
|source= MIS QUARTERLY
|year= 2009
|abstract = Online users often need to make adoption decisions without accurate information about the product values. An informational cascade occurs when it is optimal for an online user, having observed others' actions, to follow the adoption decision of the preceding individual without regard to his own information. Informational cascades are often rational for individual decision making; however, it may lead to adoption of inferior products. With easy availability of information about other users' choices, the Internet offers an ideal environment for informational cascades. In this paper, we empirically examine informational cascades in the context of online software adoption. We find user behavior in adopting software products is consistent with the predictions of the informational cascades literature. Our results demonstrate that online users' choices of software products exhibit distinct jumps and drops with changes in download ranking, as predicted by informational cascades theory. Furthermore, we find that user reviews have no impact on user adoption of the most popular product, while having an increasingly positive impact on the adoption of lower ranking products. The phenomenon persists after controlling for alternative explanations such as network effects, word-of-mouth effects, and product diffusion. Our results validate informational cascades as an important driver for decision making on the Internet. The finding also offers an explanation for the mixed results reported in prior studies with regard to the influence of online user reviews on product sales. We show that the mixed results could be due to the moderating effect of informational cascades.
|keyword = E-commerce,herding,informational cascades,decision making,network effects,word-of-mouth,software download,online communities,online user review,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''OUT OF DEDICATION OR CONSTRAINT? A DUAL MODEL OF POST-ADOPTION PHENOMENA AND ITS EMPIRICAL TEST IN THE CONTEXT OF ONLINE SERVICES'''
{{header}}
{{article
|author= Sung S. Kim,Jai-Yeol Son,
|source= MIS QUARTERLY
|year= 2009
|abstract = Sustained website traffic through consumers' patronage at the post-adoption stages is known as a key to the survival of an online service provider. Although a firm's survival depends much on repeated use, whether or not a firm survives is also influenced by a variety of other behavioral outcomes that include, but are not limited to, word-of-mouth, willingness to pay, and inattentiveness to alternatives. Whereas post-adoption research has recently paid attention to repeated use, the information systems field still lacks a systematic investiation into other behavioral outcomes that transcend mere usage. In an attempt to extend the horizons of post-adoption research, we develop and test a model that explains post-adoption behaviors in the context of online services. First, drawing on a dual model of relationship maintenance in consumer behavior research, we propose a conceptual framework to study and explain online consumer behavior. In particular, our model predicts that two contrasting mechanisms, that is, dedication and constraint, are the main drivers of post-adoption phenomena (i.e., consumers' post-adoption reactions to online services-beliefs, attitudes, intentions, and behaviors). We empirically test the proposed dual model through the use of data collected from 510 users of online portals. The results of structural equation modeling analysis indicate that, as expected, the dedication- and constraint-based mechanisms simultaneously, yet differentially, determine online consumer behavior. In general, our findings suggest that it is essential in examining the complex nature of post-adoption phenomena to take into account the interplay of the dedication- and constraint-based mechanisms.
|keyword = Online consumer behavior,post-adoption behavior,loyalty,switching costs,service-specific investments,survey research,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ARE TWO HEADS BETTER THAN ONE FOR SOFTWARE DEVELOPMENT? THE PRODUCTIVITY PARADOX OF PAIR PROGRAMMING'''
{{header}}
{{article
|author= VenuGopal Balijepally,RadhaKanta Mahapatra,Sridhar Nerur,Kenneth H. Price,
|source= MIS QUARTERLY
|year= 2009
|abstract = Extreme programming is currently gaining popularity as an alternate software development methodology. Pair programming, a core practice of this methodology, involves two programmers working collaboratively to develop software. This study examined the efficacy of pair programming by comparing the performance effectiveness and affective responses of collaborating pairs with those of individual programmers treated as nominal pairs. In a controlled laboratory experiment involving student subjects, proxies for entry level programmers working on entry level tasks, two factors were manipulated; programming setting (collaborative pair versus individuals) and programming task complexity (high versus low). Participants who worked in the individual condition were randomly combined into nominal pairs. The performance and affective responses of the collaborating pairs were then compared with those of the best performers and the second best performers of each nominal pair. Results indicated that programming pairs performed at the level above the second best performers and at the level of the best performers in each nominal pair. This relationship was found to be consistent across both levels of task complexity. Consequently, there was no evidence of an "assembly bonus effect," where the performance of a collaborating pair exceeds the performance of its best member working alone. While this finding may appear counterintuitive due to the general perception of two heads being better than one, it is consistent with the findings in small group research. When affective responses were considered, programming pairs reported higher levels of satisfaction than those of the best and second-best performing members in nominal pairs. They also showed higher levels of confidence in their performance compared to those of the second-best members. But the confidence levels of pairs were no different from those of the best performing members in nominal pairs. Theoretical and practical implications of these findings are presented.
|keyword = Software development,agile methodology,pair programming,group problem solving,nominal group,team performance,assembly bonus effect,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ROLE OF SERVICE LEVEL AGREEMENTS IN RELATIONAL MANAGEMENT OF INFORMATION TECHNOLOGY OUTSOURCING: AN EMPIRICAL STUDY'''
{{header}}
{{article
|author= Jahyun Goo,Rajiv Kishore,H. R. Rao,Kichan Nam,
|source= MIS QUARTERLY
|year= 2009
|abstract = This study extends the view that formal contracts and relational governance function as complements rather than as substitutes. We investigate how specific characteristics of service level agreements (SLAs) impact relational governance in information technology outsourcing relationships. Eleven contractual elements (categorized into three SLA characteristics: foundation, change, and governance characteristics) are hypothesized to act as complements of three relational governance attributes: relational norms, harmonious conflict resolution, and mutual dependence. Data for the study were collected through a survey of South Korean IT executives. Results of the study support the fundamental proposition of complementarity between formal contracts and relational governance, and indicate that well-structured SLAs have significant positive influence on the various aspects of relational governance in IT outsourcing relationships. However, the study also reveals that change characteristics of SLAs may act as a substitute for relational governance as these characteristics were found to dampen the level of trust and commitment through moderation effects. Overall, the findings support the proposition that well-developed SLAs not only provide a way to measure the service provider's performance, but also enable effective management of outsourcing engagements through the development of partnership-style relationships with high levels of trust and commitment.
|keyword = IT outsourcing,interorganizational relationship,service level agreement (SLA),formal contract,relational governance,relational norms,harmonious conflict resolution,mutual dependencies,trust,commitment,partnership,relational exchange theory,PLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USING PLS PATH MODELING FOR ASSESSING HIERARCHICAL CONSTRUCT MODELS: GUIDELINES AND EMPIRICAL ILLUSTRATION'''
{{header}}
{{article
|author= Martin Wetzels,Gaby Odekerken-Schroder,Claudia van Oppen,
|source= MIS QUARTERLY
|year= 2009
|abstract = In this paper, the authors show that PLS path modeling can be used to assess a hierarchical construct model. They provide guidelines outlining four key steps to construct a hierarchical construct model using PLS path modeling. This approach is illustrated empirically using a reflective, fourth order latent variable model of online experiential value in the context of online book and CD retailing. Moreover, the guidelines for the use of PLS path modeling to estimate parameters in a hierarchical construct model are extended beyond the scope of the empirical illustration. The findings of the empirical illustration are used to discuss the use of covariance-based SEM versus PLS path modeling. The authors conclude with the limitations of their study and suggestions for future research.
|keyword = PLS path modeling,hierarchical construct model,empirical illustration,experiential value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ASSESSING BETWEEN-GROUP DIFFERENCES IN INFORMATION SYSTEMS RESEARCH: A COMPARISON OF COVARIANCE- AND COMPONENT-BASED SEM'''
{{header}}
{{article
|author= Israr Qureshi,Deborah Compeau,
|source= MIS QUARTERLY
|year= 2009
|abstract = Multigroup or between-group analyses are common in the information systems literature. The ability to detect the presence or absence of between-group differences and accurately estimate the strength of moderating effects is important in studies that attempt to show contingent effects. In the past, IS scholars have used a variety of approaches to examine these effects, with the partial least squares (PLS) pooled significance test for multigroup becoming the most common (e.g., Ahuja and Thatcher 2005; Enns et al. 2003; Zhu et al. 2006). In other areas of social sciences (Epitropaki and Martin 2005) and management (Mayer and Gavin 2005; Song et al. 2005) research, however, there is greater emphasis on the use of covariance-based structural equation modeling multigroup analysis. This paper compares these two methods through Monte Carlo simulation. Our findings demonstrate the conditions under which covariance-based multigroup analysis is more appropriate as well as those under which there either is no difference or the component-based approach is preferable. In particular, we find that when data are normally distributed, with a small sample size and correlated exogenous variables, the component-based approach is more likely to detect differences between-group than is the covariance-based approach. Both approaches will consistently detect differences under conditions of normality with large sample sizes. With non-normally distributed data, neither technique could consistently detect differences across the groups in two of the paths, suggesting that both techniques struggle with the prediction of a highly skewed and kurtotic dependent variable. Both techniques detected the differences in the other paths consistently under conditions of non-normality, with the component-based approach preferable at moderate effect sizes, particularly for smaller samples.
|keyword = Multigroup analysis,Monte Carlo simulation,covariance-based structural equation modeling,partial least squares,measurement invariance,research methodology,nested models,pooled significance test,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer Surplus in Online Auctions'''
{{header}}
{{article
|author= Ravi Bapna,Wolfgang Jank,Galit Shmueli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Despite the growing research interest in Internet auctions, particularly those on eBay, little is known about quanti. able consumer surplus levels in such mechanisms. Using an ongoing novel field experiment that involves real bidders participating in real auctions, and voting with real dollars, we collect and examine a unique data set to estimate consumer surplus in eBay auctions. The estimation procedure relies mainly on knowing the highest bid, which is not disclosed by eBay but is available to us from our experiment. At the outset we assume a private value second-price sealed-bid auction setting, as well as a lack of alternative buying options within or outside eBay. Our analysis, based on a sample of 4,514 eBay auctions, indicates that consumers extract a median surplus of at least $4 per eBay auction. This estimate is unbiased under the above assumptions; otherwise it is a lower bound. The surplus distribution is highly skewed given the diverse nature of the data. We find that eBay's auctions generated at least $7.05 billion in total consumer surplus in 2003 and could generate up to $7.68 billion if the private value sealed-bid assumption does not hold. We check for the validity of our assumptions and the robustness of our estimates using an additional data set from 2005 and a randomly sampled validation data set from eBay.
|keyword = eBay,sniping,highest bid,consumer surplus,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Self-Selection and Information Role of Online Product Reviews'''
{{header}}
{{article
|author= Xinxin Li,Lorin M. Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Online product reviews may be subject to self-selection biases that impact consumer purchase behavior, online ratings' time series, and consumer surplus. This occurs if early buyers hold different preferences than do later consumers about the quality of a given product. Readers of early product reviews may not successfully correct for these preference differences when interpreting ratings and making purchases. In this study, we develop a model that examines how idiosyncratic preferences of early buyers can affect long-term consumer purchase behavior as well as the social welfare created by review systems. Our model provides an explanation for the structure of product ratings over time, which we empirically test using online book reviews posted on Amazon.com. Our analysis suggests that firms could benefit from altering their marketing strategies such as pricing, advertising, or product design to encourage consumers likely to yield positive reports to self-select into the market early and generate positive word-of-mouth for new products. On the other hand, self-selection bias, if not corrected, decreases consumer surplus.
|keyword = online product reviews,self-selection,consumer heterogeneity,herding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From Peer Production to Productization: A Study of Socially Enabled Business Exchanges in Open Source Service Networks'''
{{header}}
{{article
|author= Joseph Feller,Patrick Finnegan,Brian Fitzgerald,Jeremy Hayes,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Peer production phenomena such as open source software (OSS) have been posited as a viable alternative to traditional production models. However, community-based development often falls short of creating software "products" in the sense that consumers understand. Our research identifies an emerging business network archetype in the OSS sector, the open source service network (OSSN), which seeks to address the "productization" challenge. To do so, OSSNs must overcome the problems associated with exchanging resources between firms. We demonstrate that OSSNs overcome exchange problems by primarily relying on social, rather than legal, mechanisms; similar to the OSS communities from which they emerged. This is made possible because OSSNs use IT infrastructures that provide high visibility for primary value-creating activities. The research utilizes a multimethod theory-building approach, deriving a model from extant research, re. ning the model through qualitative case study analysis, and further re. ning the model through quantitative analysis of survey data. The paper reveals the manifestation of social mechanisms in OSSNs and how these are used for coordinating and safeguarding exchanges between firms. Specifically, we illustrate the primary importance of a shared macroculture (goals and norms) and collective sanctions for punishing firms who violate these goals/norms. Furthermore, our research highlights the interplay between digital and social networks within OSSNs, demonstrating that the use of social mechanisms is inherently dependent upon the underlying IT infrastructure.
|keyword = business networks,exchange problems,open source service network,open source software,peer production,social mechanisms,multimethod research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Role of Feedback in Managing the Internet-Based Volunteer Work Force'''
{{header}}
{{article
|author= Jae Yun Moon,Lee S. Sproull,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = This paper explores a new phenomenon at the intersection of digital networks and organizations-the Internet-based volunteer work force-people who use Internet applications to pursue a personal interest through volunteering contributions of time and talent that may create value for organizations and their customers or members. This work force is not centrally organized, managed, or measured. It is an emergent phenomenon resulting from discretionary small actions taken by large numbers of people, enabled by technology and human initiative. This paper proposes a general framework for understanding the phenomenon and offers an empirical investigation of one component of it-the role of feedback in producing and sustaining high-quality contributions from this work force. In a comparative study of Internet-based voluntary technical support groups for software problems, we found that in groups who implement systematic quality feedback systems (compared to those that do not), question askers return over a longer duration, answer providers contribute more often, and technical problem resolution is more effective. We also found that with systematic feedback, volunteers who produce higher quality contributions have longer participation duration, and participation duration is positively associated with community maintenance contributions.
|keyword = systematic quality feedback system,feedback,volunteer work force,voluntary technical support group,volunteer turnover,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Complementary Effects of E-Markets on Existing Supplier-Buyer Relationships in a Supply Chain'''
{{header}}
{{article
|author= Mu Xia,Nan Xia,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = E-markets have been established in many industries as a sourcing option for buyers. The existing literature focuses on the substitutional effect of e-markets on the traditional supply chain, yet in many situations, e-markets are used by buyers as a benchmarking tool in negotiations with traditional suppliers. This paper examines the role of e-markets in price negotiations and relationship-specific investments. We find that e-markets can be an effective tool to stimulate the traditional supplier's relationship-specific investments, lower the procurement prices, and improve the buyer's profitability and the supply-chain efficiency. Therefore, e-markets can complement rather than substitute for the traditional relationship-based supply chain. When there is quality uncertainty in the e-market offering, two effects of quality uncertainty on e-market adoption are identified. Better quality on average will increase e-market adoption, but surprisingly, increasing quality dispersion of e-markets will also help. Therefore, e-markets should strive to enlist suppliers with better quality products, but do not need to worry too much about the quality dispersion. Having better price transparency will also help in attracting more business from buyers.
|keyword = electronic market,negotiation,outside options,supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Auctioning Vertically Integrated Online Services: Computational Approaches for Real-Time Allocation'''
{{header}}
{{article
|author= Ravi Bapna,Paulo Goes,Alok Gupta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = We develop three auction-based pricing and allocation solution methods for the case where a capacity-constrained online service provider offers multiple classes of unique, one-time services with differentiated quality. Consumers desire exactly one of the many service classes offered. We call such a setting a vertically integrated online services market. Examples of these services are webcasting of special events over the Inter-net, provision of video-on-demand, and allocation of grid computing resources. We model the pricing and allocation decision faced by firms in such a setting as a knapsack problem with an added preference elicitation dimension. We present a variety of computational Solution approaches based on adaptations of the traditional g for knapsack problems. The solution approaches vary in efficacy greedy heuristic I depending on whether bidders are restricted to bid in one service class or allowed to bid in multiple service classes, as well as on the overall variability of the demand. In the case bidders can bid in multiple classes but are interested in consuming only one class, a direct application of the heuristics developed for the single service case results in a nonfair allocation. We develop a novel data structure to eliminate the unfair allocation while maintaining the original computation complexity of the simpler setting. The paper contributes by presenting a menu of auction clearing mechanisms for selling vertically integrated online services.
|keyword = auction-based pricing,online services,service classes,service computing,service pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trust and Electronic Government Success: An Empirical Study'''
{{header}}
{{article
|author= Thompson S. H. Teo,Shirish C. Srivastava,Li Jiang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Electronic government is being increasingly recognized as a means for transforming public governance. Despite this increasing interest, information systems (IS) literature is mostly silent on what really contributes to the success of e-government Web sites. To fill this gap, this study examines the role of trust in e-government success using the updated Delone and McLean IS Success model as e theoretical framework. The model is tested via a survey of 214 Singapore e-government Web site users. The results show that trust in government, but not trust in technology, is positively related to trust in e-government Web sites. Further, trust in e-government Web sites is positively related to information quality, system quality, and service quality. The quality constructs have different effects on "intention to continue" using the Web site and "satisfaction" with the Web site. Post hoc analysis indicates that the nature of usage (active versus passive users) may help us better understand the interrelationships among success variables examined in this study. This result suggests that the DeLone and McLean model can be further extended by examining the nature of IS use. In addition, it is important to consider the role of trust as well as various Web site quality attributes in understanding e-government success.
|keyword = e-government,intention to continue,IS success model,public sector,quality,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Change Management in Interorganizational Systems for the Public'''
{{header}}
{{article
|author= Juliana Sutanto,Atreyi Kankanhalli,Junyun Tay,K. S. Raman,Bernard C. Y. Tan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = It is recognized that change management is necessary for information technology implementation success. While there are a growing number of interorganizational systems (IOS) designed for the public, there is little study of and lack of clear guidelines on managing change related to their implementation. This research explores the phenomenon through the case study of a country-wide farecard system implemented in Singapore's public transportation system that involved several organizations and the public. Through the case analysis, we identified critical success factors (CSFs) for change management in IOS for the public and interrelated them using a causal loop diagram (CLD). These factors included refinements of existing CSFs identified from the literature as well as new CSFs from our case study. Our case analysis showed that communication through senior management and cooperation of affected organizations in the system implementation was able to overcome resistance to change in these organizations. We also found that while comprehensive publicity C could initiate change in the public, a critical mass had to be built up for managing public 7 change by coopting public opinion leaders as well. By interrelating CSFs identified in the case via a CLD, this study provides a preliminary theoretical framework for studying change management in IOS for the public and aims to guide practitioners in implementing Such systems.
|keyword = causal loop diagram,change management,critical success factors,IOS for the public,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Individual Adaptation to IT-Induced Change: The Role of Social Networks'''
{{header}}
{{article
|author= Sebastian Bruque,Jose Moyano,Jacob Eisenberg,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = In order to better understand the sociopsychological factors involved in employees' adaptation to new technology in organizations, we examine the role that two types of social networks-supportive and informational-play in individual adaptation to IT-induced change in a large financial company. Using survey data from 371 employees working in 133 different branches of the organization, we find that several aspects of the social networks relate to quality of employees' adaptation to the new technology as assessed by the company's departmental directors. Specifically, the size of the support network as well as the strength and density of the information network significantly predict employees' adaptation to the new system. We conclude the paper by discussing theoretical implications for the relevance of social network research for members' adaptation to organizational changes as well as outlining specific implications for practice.
|keyword = individual adaptation,informational networks,IT-induced change,social networks,supportive networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of the Choice of Open Source Software License'''
{{header}}
{{article
|author= Ravi Sen,Chandrasekar Subramaniam,Matthew L. Nelson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = In this paper, we examine how the motivations and attitudes of open source software (OSS) developers affect their preference among the three common OSS license types-Strong-Copyleft, Weak-Copyleft,and Non-Copyleft. Despite the importance of the license type and developers to OSS projects, there is little understanding in open source literature of the license choice from a developer's perspective. The results from our empirical study of OSS developers reveal that the intrinsic motivation of challenge (problem solving) is associated with the developers' preference for licenses with moderate restrictions, while the extrinsic motivation of status (through peer recognition) is associated with developers' preference for licenses with least restrictions. We also find that when choosing an OSS license, a developer's attitude toward the software redistribution rights conflicts with his or her attitude toward preserving the social benefits of open source. A major implication of our findings is that OSS managers who want to attract a limited number of highly skilled programers to their open source project should choose a restrictive OSS license. Similarly, managers of software projects for social programs could attract more developers by choosing a restrictive OSS license.
|keyword = Copyleft,Copyright,FLOSS,open source,OSS,software license,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Open Source Software on the Strategic Choices of Firms Developing Proprietary Software'''
{{header}}
{{article
|author= Jeevan Jaisingh,Eric W. K. See-To,Kar Yan Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Open source software (OSS) is now posing significant competition to proprietary or closed source software (CSS) in several software markets. In this paper, we characterize the response of a firm developing CSS (where the CSS is a revenue earner) to the presence of an OSS in its market. In particular, we look at the firm's choice of resource investments to improve quality and the firm's pricing decisions. We are primarily motivated by the following questions: Would a firm producing CSS produce higher-quality software when it faces competition from ail OSS than when there is no OSS in its market? Would there be a change in the firm's response if the CSS faced competition from another CSS in addition to competition from the OSS? We show that the firm produces lower-quality CSS when it faces competition from an OSS than when it does not. Also. the quality of the CSS decreases as the quality of the OSS increases. This result holds true even if we consider network effects. When we consider competition from another CSS, in addition to competition from the OSS, then the quality of the CSS could increase or decrease as the quality of the OSS increases. The change in quality depends on how closely substitutable the two CSS are. We also extend our base model to consider (1) competition for resources, (2) uncertainty in resources available to the OSS, and (3) uncertainty about the software development process.
|keyword = competition,network externality,open source software,quality,resources,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of Knowledge Support on the Performance of Software Process Tailoring'''
{{header}}
{{article
|author= Peng Xu,Balasubramaniam Ramesh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The use of a well-defined process is a widely recognized approach to increasing quality and productivity in software development. Building software processes from scratch each time is expensive and risky. Therefore, they are often created by tailoring existing processes and standards. Process tailoring is a knowledge-intensive activity. This research explores the link between knowledge support and software process tailoring performance under different levels of tailoring task complexity. It theoretically develops and tests how the fit between knowledge (generalized and contextualized) and software tailoring task complexity influences process tailoring performance. Process tailoring performance is conceptualized in terms of effectiveness and efficiency. The results from an experiment and a protocol analysis show that contextualized knowledge outperforms generalized knowledge in improving tailoring performance, and that such improvement in performance is greater in complex process tailoring tasks when compared to simple tasks.
|keyword = contextualized knowledge,generalized knowledge,knowledge management,software process,software process tailoring,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Tuning Data Mining Methods for Cost-Sensitive Regression: A Study in Loan Charge-Off Forecasting'''
{{header}}
{{article
|author= Gaurav Bansal,Atish P. Sinha,Huimin Zhao,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Real-world predictive data mining (classification or regression) problems are often cost sensitive, meaning that different types of prediction errors are not equally costly. While cost-sensitive learning methods for classification problems have been extensively studied recently, cost-sensitive regression has not been adequately addressed in the data mining literature yet. In this paper, we first advocate the use of average misprediction cost as a measure for assessing the performance of a cost-sensitive regression model. We then propose an efficient algorithm for tuning a regression model to further reduce its average misprediction cost. In contrast with previous statistical methods, which are tailored to particular cost functions. this algorithm can deal with any convex cost functions without modifying the underlying regression methods. We have evaluated the algorithm in bank loan charge-off forecasting, where underforecasting is considered much more costly than overforecasting. Our results show that the proposed algorithm significantly reduces the average misprediction costs of models learned with various base regression methods,such as linear regression, model tree, and neural network. The amount of cost reduction increases as the difference between the unit costs of the two types of errors (overprediction and underprediction) increases.
|keyword = asymmetric costs,cost-sensitive regression,data mining,forecasting,loan charge-off,model tuning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investments in Information Security: A Real Options Perspective with Bayesian Postaudit'''
{{header}}
{{article
|author= Hemantha S. B. Herath,Tejaswini C. Herath,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The application of real options techniques to information security is significantly different than in the case of general information technology investments due to characteristics unique to information security. Emerging research in the economics of information security has suggested real options analysis (ROA) as a potential technique for assessing the value of information security assets, but has focused primarily on the most effective level of investment and the configuration of intrusion prevention/detection systems. In this paper, we attempt to address significant gaps ill the literature by developing an integrated real options model for information security investments using Bayesian statistics that Incorporates learning and postauditing in the analysis. By using the proposed model with actual data on e-mail and Spain, we demonstrate that ROA with Bayesian postauditing offers a systematic valuation and risk management framework for evaluating information security spending by firms. We also discuss the managerial implications.
|keyword = Bayesian revisions,conjugate prior distributions,economics of information security,information security investments,postaudit,real options,return on investment,ROC curves,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MARSHALING THE PROFESSIONAL EXPERIENCE OF DOCTORAL STUDENTS: A CONTRIBUTION TO THE PRACTICAL RELEVANCE DEBATE'''
{{header}}
{{article
|author= Heinz K. Klein,Frantz Rowe,
|source= MIS QUARTERLY
|year= 2008
|abstract = In this paper, we propose a partial solution to the problem of the relevance of information systems research by adjusting doctoral programs to the specific needs and talents of doctoral students that have significant prior professional life experience. The purpose of this paper is first to recognize that the "professionally qualified doctoral student" (PQDS) has a different type of knowledge that may give her/him some advantages over other students, including greater symbolic capital. We examine the epistemic evidence for the claim that part of their practical experience constitutes a specific type of "applicative" knowledge that should be considered as different from but of equal value to theory, which has been the mainstay of academic education. Three independent lines of academic research contribute such evidence: the communities of practice literature, philosophical perspectives on applicative knowledge, and cognitive sciences. We argue that PQDSs may benefit from doctoral programs with specific features designed to leverage their practical knowledge. In turn, they may be able to "boundary span" and publish research results in forms that are appreciated by their professional communities. Finally we discuss some practical institutional issues that could be addressed if we are to sustain this profile of researchers.
|keyword = Professionally Qualified Doctoral Students (PQDS),applicative knowledge,technical knowledge,communities of practice,qualitative research,practice,relevance,symbolic capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A FAST FORM APPROACH TO MEASURING TECHNOLOGY ACCEPTANCE AND OTHER CONSTRUCTS'''
{{header}}
{{article
|author= Wynne W. Chin,Norman Johnson,Andrew Schwarz,
|source= MIS QUARTERLY
|year= 2008
|abstract = Nearly all prior studies on the technology acceptance model (TAM) have used Likert scales to measure the model's constructs, but the use of only this type of scale has two short-comings. One is that such use prevents us from exposing the model's constructs to a robust test of their measure and relationships to each other, termed their nomological validity. The other is that such use leaves us unsure about whether or not we have selected an efficient way, in terms of survey completion time, to assess these constructs, Past researchers have used short form scales to address the issue of efficiency, but there are problems that may result from such efforts. In this study, we address both shortcomings by exploring the use of a semantic differential scale, which we refer to as a fast form, to assess the constructs of TAM. In this regard, we do three things. First, we describe how fast form as a scale may be developed. Second, we conduct a psychometric evaluation of the constructs that are measured by the fast form and examine their relationships. Third, we assess the efficiency of the fast form by comparing the time required to complete a survey with it to that which is required to complete a survey with Likert scales. Our results confirm that the constructs that are measured by the fast form are psychometrically equivalent to those that are measured by the Likert scales. The relationship among these constructs was unchanged. providing strong evidence for nomological validity. The fast form also yielded a 40 percent reduction in the survey completion time, proving its superior efficiency. We conclude with a description of the implications of these results for research and practice.
|keyword = Technology acceptance,semantic differential,scale development,item decomposition,nomological validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUYER INTENTION TO USE INTERNET-ENABLED REVERSE AUCTIONS: THE ROLE OF ASSET SPECIFICITY, PRODUCT SPECIALIZATION, AND NON-CONTRACTIBILITY'''
{{header}}
{{article
|author= Sunil Mithas,Joni L. Jones,Will Mitchell,
|source= MIS QUARTERLY
|year= 2008
|abstract = Information technology enabled exchanges in electronic markets have significant implications for buyer-supplier relationships. Building oil studies that emphasize the role of intangible assets in interolganizational relationships, this study argues that buyers are less likely to use reverse auctions for supplier relationships involving a high degree of non-contractibility. The argument complements traditional transaction cost economics arguments that focus oil the impact of asset specificity and product specialization. We identify six dimensions of non-contractibility-quality, supplier technological investments, information exchange, responsiveness, trust, and flexibility-which encompass task-based and interaction-based non-contractibility. The study finds that, together with product specialization, these non-contractible elements of interorganizational relationships have greater explanatory power for re verse auction use than asset specificity. This result highlights the importance of supplier investments in non-contractible elements of exchange relationships in an increasingly dynamic service- and knowledge-based economy.
|keyword = Reverse auctions,procurement auctions,electronic markets,transaction cost economics,interorganizational relationships,buyer-supplier relationships,incomplete contracts approach,non-contractibility,business-to-business auctions,asset specificity,uncertainty,customer satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PROCESS GRAMMAR AS A TOOL FOR BUSINESS PROCESS DESIGN'''
{{header}}
{{article
|author= Jintae Lee,George M. Wyner,Brian T. Pentland,
|source= MIS QUARTERLY
|year= 2008
|abstract = The space of design alternatives for a business process is typically very large, with technology. location, and other factors combining to generate seemingly endless possibilities This paper introduces a set of artifacts that support process designers in their efforts to manage this critical business problem: (1) a grammar-based method to generate and manage business process design alternatives and (2) a soft are prototype that provides support for the use of this method. The method and prototype software are demonstrated with a grammar for a sales process. The method improves on existing approaches by offering the generative power of grammar-based methods while addressing the principal challenge to using such approaches in the design of business processes: the limitations on automated evaluation of alternatives and thus the need to provide designers with tools to manage the potentially overwhelming array of choices generated by design grammars.
|keyword = Design science,process design,design space,business process design,business process reengineering,business process alternative generation,grammatical approach,grammar-based design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MAKING SENSE OF TECHNOLOGY TRENDS IN THE INFORMATION TECHNOLOGY LANDSCAPE: A DESIGN SCIENCE APPROACH'''
{{header}}
{{article
|author= Gediminas Adomavicius,Jesse C. Bockstedt,Alok Gupta,Robert J. Kauffman,
|source= MIS QUARTERLY
|year= 2008
|abstract = A major problem for firms making information technology investment decisions is predicting and understanding the effects of future technological developments on the value of present technologies. Failure to adequately address this problem can result in wasted organization resources in acquiring, developing, managing, and training employees in the use of technologies that are short-lived and fail to produce adequate return on investment. The sheer number of available technologies and the complex set of relationships among them make IT landscape analysis extremely challenging. Most IT-consuming firms rely on third parties and suppliers for strategic recommendations on IT investments, which can lead to biased and generic advice. We address this problem by defining a new set of constructs and methodologies upon which we develop an IT ecosystem model. The objective of these artifacts is to provide a formal problem representation structure for the analysis of information technology development trends and to reduce the complexity of the IT landscape for practitioners making IT investment decisions. We adopt a process theory perspective and use a combination of visual mapping and quantification strategies to develop our artifacts and a state diagram-based technique to represent evolutionary transitions over time. We illustrate our approach using two exemplars: digital music technologies and wireless networking technologies. We evaluate the utility of our approach by conducting in-depth interviews with IT industry experts and demonstrate the contribution of our approach relative to existing techniques for technology forecasting.
|keyword = Design science,IT ecosystem model,IT landscape analysis,management of technology,technology evolution,IT investment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CYBERGATE: A DESIGN FRAMEWORK AND SYSTEM FOR TEXT ANALYSIS OF COMPUTER-MEDIATED COMMUNICATION'''
{{header}}
{{article
|author= Ahmed Abbasi,Hsinchun Chen,
|source= MIS QUARTERLY
|year= 2008
|abstract = Content analysis of computer-mediated communication (CMC) is important for evaluating the effectiveness of electronic communication in various organizational settings. CMC text analysis relies on systems capable of providing suitable navigation and knowledge discovery functionalities. However, existing CMC systems focus on structural features, with little support for features derived from message text. This deficiency is attributable to the informational richness and representational complexities associated with CMC text. In order to address this shortcoming, we propose a design framework for CMC text analysis systems. Grounded in systemic functional linguistic theory the proposed framework advocates the development of systems capable of representing the rich array of information types inherent in CMC text. It also provides guidelines regarding the choice of features, feature selection, and visualization techniques that CMC text analysis systems should employ. The CyberGate system was developed as an instantiation of the design framework. CyberGate incorporates a rich feature set and complementary feature selection and visualization methods, including the writeprints and ink blots techniques. An application example was used to illustrate the system's ability to discern important patterns in CMC text. Furthermore, results from numerous experiments conducted in comparison with benchmark methods confirmed the viability of CyberGate's features and techniques. The results revealed that the CyberGate system and its underlying design framework can dramatically improve CMC text analysis capabilities over those provided by existing systems,
|keyword = Computer-mediated communication,design framework,text analysis systems,design science,information visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USING COGNITIVE PRINCIPLES To GUIDE CLASSIFICATION IN INFORMATION SYSTEMS MODELING'''
{{header}}
{{article
|author= Jeffrey Parsons,Yair Wand,
|source= MIS QUARTERLY
|year= 2008
|abstract = Organizing phenomena into classes is a pervasive human activity. The ability to classify phenomena encountered in daily life in useful ways is essential to human survival and adaptation. Not surprisingly, then, classification-oriented activities are widespread in the information systems field. Classes or entity types play a central role in conceptual modeling for information systems requirements analysis, as well as in the design of databases and object-oriented software. Furthermore, classification is the primary task it applications such as data mining and the development of domain ontologies to support information sharing in semantic web applications. However, despite the pervasiveness of classification, little research has proposed well-grounded guidelines for identifying, evaluating, and choosing classes when modeling a domain or designing information systems artifacts. In this paper. we adopt the cognitive notions of inference and economy to derive a set of principles to guide effective and efficient classification. We present a model for characterizing what may be considered useful classes in a given context based on the inferences that can be drawn from membership in a class. This foundation is then used to suggest practical design rules for evaluating and refining potential classes. We illustrate the use of the rules by showing that applying them to a previously published example yields meaningful changes. We then present an evaluation by a panel of experts who compared the published and revised models. The evaluation shows that following the rules leads to semantically clearer models that are preferred by, experts. The paper concludes by outlining possible future research directions.
|keyword = Conceptual modeling,classification principles,classes and types,information modeling,design science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Casting the net: A multimodal network perspective on user-system interactions'''
{{header}}
{{article
|author= Gerald C. Kane,Maryam Alavi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Information systems (IS) researchers have typically examined the user-system relationship as an isolated dyad between a single, independent user and an individual, freestanding information system. We argue that this conceptualization does not adequately represent most organizations today, in which multiple users interact with multiple information systems within a group. Relying heavily on the theory and methods behind social network analysis, we introduce the concept of multimodal networks to assess both users and information systems as equivalent nodes in a single social network. This perspective allows us to examine the influence of information systems on organizational outcomes as a function of all of the user-system and interpersonal interactions in a group. We explore two different possible mechanisms for this influence: (1) direct user-system interactions by aggregating the strength of all the dyadic user-system interactions in a group, and (2) indirect user-system interactions by assessing the centrality of the information systems within the social network. We survey approximately 600 individuals in 40 healthcare groups to test whether either or both of these mechanisms are associated with two types of organizational performance outcomes-efficiency and quality of care. We find that the centrality of the information systems within the network is significantly and positively associated with both efficiency and quality outcomes, but that the average strength of the user-system interactions is not. Implications are that managers and researchers should examine the wider multimodal network of multiple users and multiple systems when assessing the role of IS in organizations in relation to organizational performance outcomes.
|keyword = IS use,social networks,multimodal networks,centrality,performance,group-level,indirect use,information use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of the social structure of digital networks on viral marketing performance'''
{{header}}
{{article
|author= Mauro Bampo,Michael T. Ewing,Dineli R. Mather,David Stewart,Mark Wallace,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Viral marketing is a form of peer-to-peer communication in which individuals are encouraged to pass on promotional messages within their social networks. Conventional wisdom holds that the viral marketing process is both random and unmanageable. In this paper, we deconstruct the process and investigate the formation of the activated digital network as distinct from the underlying social network. We then consider the impact of the social structure of digital networks (random, scale free, and small world) and of the transmission behavior of individuals on campaign performance. Specifically, we identify alternative social network models to understand the mediating effects of the social structures of these models on viral marketing campaigns. Next, we analyse an actual viral marketing campaign and use the empirical data to develop and validate a computer simulation model for viral marketing. Finally, we conduct a number of simulation experiments to predict the spread of a viral message within different types of social network structures under different assumptions and scenarios. Our findings confirm that the social structure of digital networks play a critical role in the spread of a viral message. Managers seeking to optimize campaign performance should give consideration to these findings before designing and implementing viral marketing campaigns. We also demonstrate how a simulation model is used to quantify the impact of campaign management inputs and how these learnings can support managerial decision making.
|keyword = digital communication,social structure of digital networks,viral marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the relationship between reviews and sales: The role of reviewer identity disclosure in electronic markets'''
{{header}}
{{article
|author= Chris Forman,Anindya Ghose,Batia Wiesenfeld,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Consumer-generated product reviews have proliferated online, driven by the notion that consumers' decision to purchase or not purchase a product is based on the positive or negative information about that product they obtain from fellow consumers. Using research on information processing as a foundation, we suggest that in the context of an online community, reviewer disclosure of identity-descriptive information is used by consumers to supplement or replace product information when making purchase decisions and evaluating the helpfulness of online reviews. Using a unique data set based on both chronologically compiled ratings as well as reviewer characteristics for a given set of products and geographical location-based purchasing behavior from Amazon, we provide evidence that community norms are an antecedent to reviewer disclosure of identity-descriptive information. Online community members rate reviews containing identity-descriptive information more positively, and the prevalence of reviewer disclosure of identity information is associated with increases in subsequent online product sales. In addition, we show that shared geographical location increases the relationship between disclosure and product sales, thus highlighting the important role of geography in electronic commerce. Taken together, our results suggest that identity-relevant information about reviewers shapes community members' judgment of products and reviews. Implications for research on the relationship between online word-of-mouth (WOM) and sales, peer recognition and reputation systems, and conformity to online community norms are discussed.
|keyword = digital markets,information processing,social identity,online reviews,Internet retailing,virtual communities,identity disclosure,user-generated content,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Towards dynamic visualization for understanding evolution of digital communication networks'''
{{header}}
{{article
|author= Matthias Trier,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = The capabilities offered by digital communication are leading to the evolution of new network structures that are grounded in communication patterns. As these structures are significant for organizations, much research has been devoted to understanding network dynamics in ongoing processes of electronic communication. A valuable method for this objective is Social Network Analysis. However, its current focus on quantifying and interpreting aggregated static relationship structures suffers from some limitations for the domain of analyzing online communication with high volatility and massive exchange of timed messages. To overcome these limitations, this paper presents a method for event-based dynamic network visualization and analysis together with its exploratory social network intelligence software Commetrix. Based on longitudinal data of corporate email communication, the paper demonstrates how exploration of animated graphs combined with measuring temporal network changes identifies measurement artifacts of static network analysis, describes community formation processes and network lifecycles, bridges actor level with network level analysis by analyzing the structural impact of actor activities, and measures how network structures react to external events. The methods and findings improve our understanding of dynamic phenomena in online communication and motivate novel metrics that complement Social Network Analysis.
|keyword = dynamic network analysis,network dynamics,social network analysis,online communication,communities,network visualization,evolution,email,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of information diffusion on bidding behavior in secret reserve price auctions'''
{{header}}
{{article
|author= Oliver Hinz,Martin Spann,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = The interactive nature of the Internet promotes collaborative business models (e. g., auctions) and facilitates information-sharing via social networks. In Internet auctions, an important design option for sellers is the setting of a secret reserve price that has to be met by a buyer's bid for a successful purchase. Bidders have strong incentives to learn more about the secret reserve price in these auctions, thereby relying on their own network of friends or digital networks of users with similar interests and information needs. Information-sharing and flow in digital networks, both person-to-person and via communities, can change bidding behavior and thus can have important implications for buyers and sellers in secret reserve price auctions. This paper uses a multiparadigm approach to analyze the impact of information diffusion in social networks on bidding behavior in secret reserve price auctions. We first develop an analytical model for the effect of shared information on individual bidding behavior in a secret reserve price auction with a single buyer facing a single seller similar to eBay's Best Offer and some variants of NYOP. Next, we combine the implications from our analytical model with relational data that describe the individual's position in social networks. We empirically test the implications of our analytical model in a laboratory experiment, and examine the impact of information diffusion in social networks on bidding behavior in a field study with real purchases where we use a virtual world as proxy for the real world. We find that the amount and dispersion of information in the individualized context, and betweenness centrality in the social network context, have a significant impact on bidding behavior. Finally, we discuss the implications of our results for buyers and sellers.
|keyword = information diffusion,social networks,secret reserve price auctions,name-your-own-price,eBay best offer,virtual worlds,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Emergence of new project teams from open source software developer networks: Impact of prior collaboration ties'''
{{header}}
{{article
|author= Jungpil Hahn,Jae Yun Moon,Chen Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Recent years have witnessed a surge in self-organizing voluntary teams collaborating online to produce goods and services. Motivated by this phenomenon, this research investigates how these teams are formed and how individuals make decisions about which teams to join in the context of open source software development (OSSD). The focus of this paper is to explore how the collaborative network affects developers' choice of newly initiated OSS projects to participate in. More specifically, by analyzing software project data from real-world OSSD projects, we empirically test the impact of past collaborative ties with and perceived status of project members in the network on the self-assembly of OSSD teams. Overall, we find that a developer is more likely to join a project when he has strong collaborative ties with its initiator. We also find that perceived status of the noninitiator members of a project influences its probability of attracting developers. We discuss the implications of our results with respect to self-organizing teams and OSSD.
|keyword = open source software development (OSSD),team formation,developer social networks,collaborative ties,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How information changes consumer behavior and how consumer behavior determines corporate strategy'''
{{header}}
{{article
|author= Eric K. Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Information availability has increased consumers' informedness, the degree to which they know what is available in the marketplace, with precisely which attributes and at precisely what price. This informedness has altered the demand side of market behavior: customers now discount more heavily when comparable products are available from competitors and when products do not meet their wants, needs, cravings, and longings, but they no longer discount as heavily when purchasing unfamiliar products. Changes in the demand side are producing comparable changes in the supply side: firms earn less than their expectations when competing in traditional mass-market fat spots, while earning far more than previously when entering newly created resonance marketing sweet spots. We trace the impact of hyperdifferentiation and resonance marketing on strategy, with a clear progression from a limited number of fat spots, through reliance on line extensions, and ultimately to fully differentiated market sweet spots.
|keyword = consumer informedness,long tail,marketing strategy,online reviews,resonance marketing,word-of-mouth marketing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How does information technology shape supply-chain structure? Evidence on the number of suppliers'''
{{header}}
{{article
|author= Jason Dedrick,Sean Xin Xu,Kevin Xiaoguo Zhu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = This research investigates the relationship between a manufacturer's use of information technology (IT) (particularly electronic procurement) and the number of suppliers in its supply chain. Will a manufacturer use more or fewer suppliers due to the increasing use of IT? Based on data from a sample of 150 U.S. manufacturers, we find no direct relationship between e-procurement and number of suppliers at the aggregate level. However, when we distinguish the type of goods purchased, we find that the use of electronic procurement is associated with buying from more suppliers for custom goods but from fewer suppliers for standard (or commodity) goods. It is possible that for commodity goods, an efficiently functioning transparent market ensures that a few suppliers are sufficient, whereas for custom goods the need for protection from opportunistic vendor holdup leads to the use of more suppliers. Further, the positive relationship between number of suppliers and electronic procurement for custom goods is negatively moderated by deeper buyer-supplier system integration. This implies that such integration can help buyers obtain better "fit" for their customized requirements, an alternative to increasing fit by employing more suppliers as proposed in the extant literature.
|keyword = electronic procurement,information technology,interfirm coordination,number of suppliers,supply-chain structure,systems integration,transaction costs economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How has electronic travel distribution been transformed? A test of the theory of newly vulnerable markets'''
{{header}}
{{article
|author= Nelson F. Granados,Robert J. Kauffman,Bradley King,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Information technology (IT) advances often create turmoil and disturb existing industry structures. In the travel industry, electronic distribution has existed for decades via the global distribution systems (GDSs), reservation systems that were introduced in the early 1980s on mainframe platforms. Yet with the Internet, new digital intermediaries have threatened the viability of these legacy GDSs. We examine this transformation of e-travel distribution to test the theory of newly vulnerable markets, which predicts how markets become vulnerable to fundamental changes triggered by IT. The tenets of newly vulnerable markets theory are supported. The GDS market became newly easy to enter due to a decrease in barriers to entry caused by the Internet and other technologies, attractive to attack due to their out-of-date and inefficient pricing mechanisms, which made opportunistic pickoff possible across customer profitability gradients, and difficult to defend due to their lack of vision and strategic inflexibility. We use our findings to expand the application of this theory to newly vulnerable e-markets, in general.
|keyword = electronic markets,meta-search agents,newly vulnerable markets,online travel agents,travel distribution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information risk of inadvertent disclosure: An analysis of file-sharing risk in the financial supply chain'''
{{header}}
{{article
|author= M. Eric Johnson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Firms face many different types of information security risk. Inadvertent disclosure of sensitive business information represents one of the largest classes of recent security breaches. We examine a specific instance of this problem-inadvertent disclosures through peer-to-peer file-sharing networks. We characterize the extent of the security risk for a group of large financial institutions using a direct analysis of leaked documents. We also characterize the threat of loss by examining search patterns in peer-to-peer networks. Our analysis demonstrates both a substantial threat and vulnerability for large financial firms. We find a statistically significant link between leakage and leak sources including the firm employment base and the number of retail accounts. We also find a link between firm visibility and threat activity. Finally, we find that firms with more leaks also experience increased threat.
|keyword = data breaches,file-sharing,information security,inadvertent disclosure,intellectual property leaks,peer-to-peer networks,risk management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The deterrent and displacement effects of information security enforcement: International evidence'''
{{header}}
{{article
|author= Ivan P. L. Png,Chen-Yu Wang,Qiu-Hong Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = We adapt the event study methodology from research in financial economics to study the impact of government enforcement and economic opportunities on information security attacks. We found limited evidence that domestic enforcement deters attacks within the country. However, we found compelling evidence of a displacement effect: U.S. enforcement substantially increases attacks originating from other countries. We also found strong evidence that attackers are economically motivated in that the number of attacks is increasing in the U.S. unemployment rate. Our findings were robust to differences in the effective time window of enforcement and the measurement of vulnerabilities.
|keyword = economics,information security,security attacks,security enforcement,unemployment rate,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does competition promote trust and trustworthiness in online trading? An experimental study'''
{{header}}
{{article
|author= Gary Bolton,Claudia Loebbecke,Axel Ockenfels,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = We investigate whether greater market competition improves or inhibits the ability of feedback systems in Internet markets to deliver trust and trustworthiness to the marketplace. Our investigation is grounded in the theory of signaling from information economics. Using methods from experimental economics, we create a laboratory online market where sellers face a moral hazard. We manipulate the level of market competition and the nature of the social network behind the feedback system and study the affect on trust, trustworthiness, and market efficiency. We find that competition in strangers networks, where market encounters are one-shot and reputation information is communicated through outside parties, improves trust, trustworthiness, and market efficiency. The efficiency advantage that partners networks, where a buyer can maintain a repeated relationship with a seller, have over strangers networks largely vanishes with the introduction of competition. This is because the difference in the pattern of social networking largely disappears. Overall, encouraging competition leads to more effective feedback systems in Internet markets. We discuss implications for trader strategy and Internet market design.
|keyword = competitive markets,experimental economics,feedback systems,information economics,moral hazard,online markets,reputation,signaling theory,social networks,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design and ownership of two-sided networks: Implications for Internet platforms'''
{{header}}
{{article
|author= Yannis Bakos,Evangelos Katsamakas,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Many Internet intermediaries operate two-sided networks, that is, they provide platforms to bring together two types of participants, or "sides," such as buyers and sellers. This paper develops a model that characterizes the intermediary's pricing in two-sided networks, the value created by these networks, and the allocation of that value across the two sides. It extends the two-sided networks literature by endogenizing the level of network effects as the result of relevant investments by the intermediary, which determine the design of the network. It shows that under certain assumptions about the available technologies, the design of the two-sided network is highly asymmetric independent of its ownership structure. The paper provides insight into design strategies for Internet platforms, and it discusses their welfare implications.
|keyword = intermediation,Internet platforms,network effects,two-sided markets,two-sided networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Uncertainty and industry structure effects on managerial intuition about information technology real options'''
{{header}}
{{article
|author= Nancy Lankton,Joan Luft,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Real options analysis is an important but costly tool for valuing many information technology (IT) investments. As a low-cost substitute for real options-based methods, firms often depend on managerial intuition, which sometimes approximates real options-based valuations and sometimes does not. Making good choices about how to value IT investments requires an understanding of why, and therefore when, intuitive judgment is more or less likely to be consistent with real options-based valuations. Field and survey studies have provided ex post observations of systematic variations in consistency by option type, but ex ante hypotheses explaining this variation have been rare. This study uses two behavioral economic theories to predict option-type-specific differences between intuitive judgments and real options prescriptions. Regret theory posits that individuals will value decision outcomes based on both the expected utility of payoffs and on anticipated regret for not having made an alternative decision. As a consequence, intuitive IT investment decisions are less aggressive as uncertainty increases (higher valuation of deferral options, lower valuation of growth options), in contrast to higher normative values for both real option types with higher uncertainty. Consistent with competitive behavior theories that predict overaggressive behavior to contest market behavior, intuitive IT investment decisions are more aggressive in the presence of a potential competitor (lower valuation of deferral and higher valuation of growth options), holding constant the normative value of the options. We present experimental evidence consistent with these predictions. An important implication of our results is that future research should not test for general consistency between intuitive judgment and real options theory, but should identify and explain systematic variation in consistency across option types and settings. Such variation is important in practice because it determines when intuitive judgment is and is not likely to be an adequate substitute for costly formal real options valuation. It also determines when training in real options concepts needs to be more intensive to overcome inconsistency with intuitive judgment, and when the outputs of formal real options valuation are likely to be unintuitive and thus not readily acceptable to managers with limited option theory training.
|keyword = behavioral finance,IT investments,real options theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the value of countermeasure portfolios in information systems security'''
{{header}}
{{article
|author= Ram L. Kumar,Sungjune Park,Chandrasekar Subramaniam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Organizations are faced with a variety of information security threats and implement several information system security countermeasures (ISSCs) to mitigate possible damage due to security attacks. These security countermeasures vary in their ability to deal with different types of security attacks and, hence, are implemented as a portfolio of ISSCs. A key challenge for organizations is to understand the economic consequences of security attacks relative to the ISSC portfolio implemented. This paper combines the risk analysis and disaster recovery perspectives to build an integrated simulation model of ISSC portfolio value. The model incorporates the characteristics of an ISSC portfolio relative to the threat and business environments and includes the type of attack, frequency of attacks, possible damage, and the extent and time of recovery from damage. The simulation experiments provide interesting insights into the interactions between ISSC portfolio components and characteristics of business and threat environments in determining portfolio value.
|keyword = business value of IT,economics of IS security,information systems security,IT asset valuation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decision-theoretic and game-theoretic approaches to IT security investment'''
{{header}}
{{article
|author= Huseyin Cavusoglu,Srinivasan Raghunathan,Wei T. Yue,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Firms have been increasing their information technology (IT) security budgets significantly to deal with increased security threats. An examination of current practices reveals that managers view security investment as any other and use traditional decision-theoretic risk management techniques to determine security investments. We argue in this paper that this method is incomplete because of the problem's strategic nature-hackers alter their hacking strategies in response to a firm's investment strategies. We propose game theory for determining IT security investment levels and compare game theory and decision theory approaches on several dimensions such as the investment levels, vulnerability, and payoff from investments. We show that the sequential game results in the maximum payoff to the firm, but requires that the firm move first before the hacker. Even if a simultaneous game is played, the firm enjoys a higher payoff than that in the decision theory approach, except when the firm's estimate of the hacker effort in the decision theory approach is sufficiently close to the actual hacker effort. We also show that if the firm learns from prior observations of hacker effort and uses these to estimate future hacker effort in the decision theory approach, then the gap between the results of decision theory and game theory approaches diminishes over time. The rate of convergence and the extent of loss the firm suffers before convergence depend on the learning model employed by the firm to estimate hacker effort.
|keyword = decision theory,game theory,IT security investments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Uncovering the intellectual core of the information systems discipline'''
{{header}}
{{article
|author= Anna Sidorova,Nicholas Evangelopoulos,Joseph S. Valacich,Thiagarajan Ramakrishnan,
|source= MIS QUARTERLY
|year= 2008
|abstract = What is the intellectual core of the information systems discipline? This study uses latent semantic analysis to examine a large body of published IS research in order to address this question. Specifically, the abstracts of all research papers over the time period from 1985 through 2006 published in three top IS research journals-MIS Quarterly, Information Systems Research, and Journal of Management Information Systems-were analyzed. This analysis identified five core research areas: (1) information technology and organizations; (2) IS development; (3) IT and individuals; (4) IT and markets; and (5) IT and groups. Over the time frame of our analysis, these core topics have remained quite stable. However, the specific research themes within each core area have evolved significantly, reflecting research that has focused less on technology development and more on the social context in which information technologies are designed and used. As such, this analysis demonstrates that the information systems academic discipline has maintained a relatively stable research identity that focuses on how IT systems are developed and how individuals, groups, organizations, and markets interact with IT.
|keyword = IS identity,IS research issues,IS research agenda,organizational identity,latent semantic analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting different conceptualizations of system use: The competing roles of behavioral intention, facilitating conditions, and behavioral expectation'''
{{header}}
{{article
|author= Viswanath Venkatesh,Susan A. Brown,Likoebe M. Maruping,Hillol Bala,
|source= MIS QUARTERLY
|year= 2008
|abstract = Employees' underutilization of new Information systems undermines organizations efforts to gain benefits from such systems. The two main predictors of individual-level system use in prior research-behavioral intention and facilitating conditions-have limitations that we discuss. We introduce behavioral expectation as a predictor that addresses some of the key limitations and provides a better understanding of system use. System use is examined in terms of three key conceptualizations: duration, frequency, and intensity. We develop a model that employs behavioral intention, facilitating conditions, and behavioral expectation as predictors of the three conceptualizations of system use. We argue that each of these three determinants play different roles in predicting each of the three conceptualizations of system use. We test the proposed model in the context of a longitudinal field study of 321 users of a new information system. The model explains 65 percent, 60 percent, and 60 percent of the variance in duration, frequency, and intensity of system use respectively. We offer theoretical and practical implications for our findings.
|keyword = technology adoption,user acceptance,system use,behavioral expectation,behavioral intention,facilitating conditions,duration of use,frequency of use,intensity of use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Internet users' information privacy-protective responses: A taxonomy and a nomological model'''
{{header}}
{{article
|author= Jai-Yeol Son,Sung S. Kim,
|source= MIS QUARTERLY
|year= 2008
|abstract = Although Internet users are expected to respond in various ways to privacy threats from online companies, little attention has been paid so far to the complex nature of how users respond to these threats. This paper has two specific goals in its effort to fill this gap in the literature. The first, so that these outcomes can be systematically investigated, is to develop a taxonomy of information privacy-protective responses (IPPR). This taxonomy consists of six types of behavioral responses-refusal, misrepresentation, removal, negative word-of-mouth, complaining directly to online companies, and complaining indirectly to third-party organizations-that are classified into three categories: information provision, private action, and public action. Our second goal is to develop a nomological model with several salient antecedents-concerns for information privacy, perceived justice, and societal benefits from complaining-of IPPR, and to show how the antecedents differentially affect the six types of IPPR. The nomological model is tested with data collected from 523 Internet users. The results indicate that some discernible patterns emerge in the relationships between the antecedents and the three groups of IPPR. These patterns enable researchers to better understand why a certain type of IPPR is similar to or distinct from other types of IPPR Such an understanding could enable researchers to analyze a variety of behavioral responses to information privacy threats in a fairly systematic manner. Overall, this paper contributes to researchers' theory-building efforts in the area of information privacy by breaking new ground for the study of individuals' responses to information privacy threats.
|keyword = information privacy,responses to information privacy threats,information privacy concerns,ethical issues,structural equation modeling,causal model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business familiarity as risk mitigation in software development outsourcing contracts'''
{{header}}
{{article
|author= David Gefen,Simon Wyss,Yossi Lichtenstein,
|source= MIS QUARTERLY
|year= 2008
|abstract = This study examines the role of business familiarity in determining how software development outsourcing projects are managed and priced to address risks. Increased business familiarity suggests both more prior knowledge, and hence reduced adverse selection risk, and increased implied trust about future behavior, and hence implied reduced moral hazard risk. Preferring high business familiarity partners may also alleviate concerns about incomplete contracts. By reducing these risks, higher business familiarity is hypothesized to be associated with higher priced projects, reduced penalties, and an increased tendency to contract on a time and materials rather than a fixed price basis. These hypotheses were examined with objective contractual legal data from contracts made by a leading international bank. Integrating trust the or into agency theory and into incomplete contract theory and examining unique contract data, the contribution of the study is to show that the premium on business familiarity and the trust it implies is not in directly affecting price, but, rather, in changing how the relationship is managed toward a tendency to sign time and materials contracts. Implications about integrating trust into agency theory and incomplete contract theory, as well as implications regarding trust premiums and software development outsourcing, are discussed.
|keyword = business familiarity,software development outsourcing,fixed price,time and materials,agency theory,incomplete contract theory,trust,contractual governance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Representing part-whole relations in conceptual modeling: An empirical evaluation'''
{{header}}
{{article
|author= Graeme Shanks,Elizabeth Tansley,Jasmina Nuredini,Daniel Tobin,Ron Weber,
|source= MIS QUARTERLY
|year= 2008
|abstract = The part-of construct is a fundamental element of many conceptual modeling grammars that is used to associate one thing (a component) with another thing (a composite). Substantive theoretical issues surrounding the part-of construct remain to be resolved, however. For instance, contrary to widespread claims, some researchers now argue the relationship between components and composites is not always transitive. Moreover, how the part-of construct should be represented in a conceptual schema diagram remains a contentious issue. Some analysts argue composites should be represented as a relationship or association. Others argue they should be represented as an entity. In this paper we use an ontological theory to support our arguments that composites should be represented as entities and not relationships or associations. We also describe an experiment that we undertook to test whether representing composites as relationships or entities enables users to understand a domain better. Our results support our arguments that using entities to represent composites enables users to better understand a domain.
|keyword = conceptual modeling,information systems development,ontology,part-of relations,aggregation,composition,meronymic relations,rnereology,mereotopology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A three-perspective model of culture, information systems, and their development and use'''
{{header}}
{{article
|author= Antonio Kappos,Suzanne Rivard,
|source= MIS QUARTERLY
|year= 2008
|abstract = Culture plays an increasingly important role in information systems initiatives, and it receives considerable attention from researchers who have studied a variety of aspects of its role in IS initiatives. Notwithstanding the contributions of research to date, our knowledge of how culture influences and is influenced by-the development and use processes and an information system itself remains fragmented. Knowledge fragmentation is amplified by the fact that conceptualizations of culture differ among researchers. Indeed, most researchers agree that culture consists of patterns of meaning underlying a variety of manifestations. Researchers diverge, however, on the degree of consensus on these interpretations that they assume to be reached within a collective. In order to integrate these divergent conceptualizations of culture, we adopt the view that no single perspective is sufficient to capture the complexity of interplay between culture, the processes of developing and using an IS, and the IS itself We have, therefore, adopted a conceptualization that views culture from three perspectives-integration, differentiation, and fragmentation-that come into play simultaneously and jointly. Using this conceptualization, the paper synthesizes what is known about the role of culture in IS initiatives, and proposes a model of the relationships between culture, the development and use processes, and an information system.
|keyword = IS development,IS use,IS characteristics,culture,integration,differentiation,fragmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Emotional dissonance and the information technology professional'''
{{header}}
{{article
|author= Paige S. Rutner,Bill C. Hardgrave,D. Harrison McKnight,
|source= MIS QUARTERLY
|year= 2008
|abstract = The information technology professional regularly expected to work with colleagues in both IT and other areas of the organization. During these interactions, the IT employee is expected to conform to occupational or organizational norms regarding the display of emotion. How do these display norms affect the IT professional? This study examines an IT professional's emotional dissonance, the conflict between norms of emotional display and an employee's felt emotion. Emotional dissonance is studied as a factor of IT professionals work exhaustion Job satisfaction, and trunover intention, modeled as an extension to the work of Moore (2000a). The results indicate emotional dissonance predicts work exhaustion better than do perceived workload, role conflict, or role ambiguity, constructs which have long been associated with work exhaustion. Job satisfaction is influenced directly by role ambiguity and work exhaustion. In turn job satisfaction influences employee turnover intention. We discuss implications of these findings for both IT management and future research.
|keyword = IT workforce,emotional dissonance,work exhaustion,turnover intention,job satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dressing your online auction business for success: An experiment comparing two eBay businesses'''
{{header}}
{{article
|author= Dawn G. Gregg,Steven Walczak,
|source= MIS QUARTERLY
|year= 2008
|abstract = Businesses can choose who they want to be online. Product and company attributes that are directly perceivable in the real world can be manipulated tomake a favorable impression on online buyers. This study examines whether creating a more professional online e-image can signal consumers about unobservable product or company quality, and whether this signal influences their willingness to transact with the company, and ultimately the prices they are willing to pay for the company's goods and services. An empirical study is pre-sented that examines two online auction businesses utilizing different company names and auction listing styles to sell items in parallel over the course of one year, The findings suggest that increasing the quality of an auction business's e-image does increase consumers' willingness to transact with the business, and increases prices received at auction. The study also demonstrates the ability to use eBay as an experimental laboratory for testing a variety of hypotheses about purchasing behavior online.
|keyword = signaling theory,website quality,reputation,price premiums,online auctions,empirical research,e-image,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predictors of formal control usage in IT outsourcing partnerships'''
{{header}}
{{article
|author= Sandeep Rustagi,William R. King,Laurie J. Kirsch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Client control over the vendor has been identified as a critical factor in successfully managing information technology outsourcing relationships. Though prior studies have suggested that "how much" control is exercised has significant rami. cations for individuals and firms, relatively few studies have operationalized and studied this important concept. In this study, we de. ne the amount of formal control as the variety of mechanisms used by a client to exercise control over a vendor and the extent to which the mechanisms are used. We use literature on transaction cost economics and organizational control to build a model of the antecedents of the amount of formal control. The study uses data from 138 client-vendor matched pairs working in eight large, long-term, ongoing outsourcing arrangements to test specific hypotheses. The results suggest that clients who have technical or relationship management knowledge, or have high levels of trust in their vendors, use formal control mechanisms to a lesser extent. On the other hand, task uncertainty was found to be positively associated with the amount of formal control, and the degree of core competency involved in the outsourced activity was not found to be related to the amount of formal control. These results are discussed, and implications for research and practice are drawn.
|keyword = outsourcing,control,information systems management,client-vendor relationship,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Capacity provision networks: Foundations of markets for sharable resources in distributed computational economies'''
{{header}}
{{article
|author= Anna Ye Du,Xianjun Geng,Ram Gopal,R. Ramesh,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = With the rapid growth of rich-media content over the Internet, content and service providers (SP) are increasingly facing the problem of managing their service resources cost-effectively while ensuring a high quality of service (QoS) delivery at the same time. In this research we conceptualize and model an Internet-based storage provisioning network for rich-media content delivery. This is modeled as a capacity provision network (CPN) where participants possess service infrastructures and leverage their topographies to effectively serve specific customer segments. A CPN is a network of SPs coordinated through an allocation hub. We first develop the notion of discounted QoS capabilities of storage resources. We then investigate the stability of the discount factors over time and the network topography using a test-bed on the Internet through a longitudinal empirical study. Finally, we develop a market maker mechanism for optimal multilateral allocation and surplus sharing in a network. The proposed CPN is closely tied to two fundamental properties of Internet service technology: positive network externality among cooperating SPs and the property of effective multiplication of capacity allocation among several distributed service sites. We show that there exist significant incentives for SPs to engage in cooperative allocation and surplus sharing. We further demonstrate that intermediation can enhance the allocation effectiveness and that the opportunity to allocation and surplus sharing can play an important role in infrastructure planning. In conclusion, this study demonstrates the practical business viability of a cooperative CPN market.
|keyword = capacity provision networks,resource sharing,online market,distributed computation,quality of service,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Addressing the what and how of online services: Positioning supporting-services functionality and service quality for business-to-consumer success'''
{{header}}
{{article
|author= Ronald T. Cenfetelli,Izak Benbasat,Sameh Al-Natour,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = With the continued growth of business-to-consumer (B2C) e-business, online vendors are providing an increasing array of services that support and enhance their core products or services. For example, Amazon.com does not just sell books; it also enhances that core product with automated product recommendations, "wish list" tracking, order status updates, customer reviews, and many other valuable supporting services. These supporting services are made possible exclusively through the design and deployment of information technology (IT) to provide website supporting services functionality (SSF). In this paper, we de. ne and develop the concept of B2C SSF and investigate how IT can support core products or services. We theorize the role that SSF plays in an environment where individuals who visit B2C websites are not only customers but also technology users. Given the unique online environment that amalgamates vendor services with information systems (IS), our model integrates theories from both services marketing and technology acceptance to help explain the behavior of these customers/users. In doing so, we investigate the role of the extensively researched concept of service quality in relation to SSF. Although service quality provides guidance for how supporting services should be provided (e.g., responsively and reliably), it does not address what those services are (e.g., product recommendations). SSF addresses this deficiency, thus providing both theoretical and practical benefits through a focus on IT design and deployment. The results of field study support that SSF is an important predictor of customer beliefs and behavior, beyond that predicted by service quality alone. SSF is an important concept to consider-theoretically and practically-in IT-mediated B2C service.
|keyword = e-business,online customer service,supporting services functionality,supporting services,service quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Marketspace or marketplace? Online information search and channel outcomes in auto retailing'''
{{header}}
{{article
|author= Jason Kuruzovich,Siva Viswanathan,Ritu Agarwal,Sanjay Gosain,Scott Weitzman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = The growth of the Internet has spawned an increasing number of online information sources (OISs). The effect of OISs on consumer information search processes has been particularly striking in sectors such as auto retailing, where the typical consumer has conventionally been confronted with an unpleasant and inefficient purchase process. However, the relationships between the information found in the online "marketspace," consumer search in the offline "marketplace," and other aspects of the multichannel shopping process are not well understood. This study examines the differential impact of price and product information found in the marketspace, relating consumers' information needs and information retrieval from OISs to three shopping-related outcomes-purchase based on online infomediary referral (i.e., referred purchase), intensity of search in the marketplace, and online search satisfaction. We draw on a large data set of more than 16,000 new vehicle purchasers who reported using the Web for search related to their new vehicle purchase. We find that OISs offer different levels of price and product information and consumers are differentiated in their ability to retrieve this information. Further, the retrieval of price versus product information online has important implications for whether consumers consummate their online search through referred purchase or extend their search into the physical marketplace. Our results suggest different business models for infomediaries providing price and product information and underscore the need for designing information provisioning systems of OISs to facilitate transition between the marketspace and the marketplace.
|keyword = online infomediaries,information needs fulfillment,auto retailing,value chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On vendor preferences for contract types in offshore software projects: The case of fixed price vs. time and materials contracts'''
{{header}}
{{article
|author= Anandasivam Gopal,Konduru Sivaramakrishnan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Prior research has indicated that, on average, offshore vendors have higher profits associated with time and materials (T&M) contracts than fixed price (FP) contracts. This research raises two questions. First, Is the relative importance of various profit drivers different across two contractual regimes? Second, Does it follow that vendors unconditionally prefer T&M contracts for all projects? We address these questions by using data on 93 offshore projects completed by a leading Indian vendor. We use an endogenous switching regression framework and the program evaluation methodology to show that profit equations are distinctly different for the two contractual regimes. Using these two profit equations, we also identify contingencies under which the vendor prefers an FP contract to a T&M contract. We hypothesize that the vendor's ability leverage information asymmetry about capabilities and experiences translates into the vendor preferring FP contract to secure larger information rents. Our results support this hypothesis and suggest that the vendor would prefer the FP contract for larger and longer projects with larger teams. However, vendors would prefer a T&M contract when the risk of employee attrition from the project team is high. In addition, we discuss managerial implications of these results in the paper.
|keyword = software outsourcing,offshore software development,contracts,profitability,regression analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Awareness displays and social motivation for coordinating communication'''
{{header}}
{{article
|author= Laura Dabbish,Robert Kraut,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Researchers and designers have been building awareness displays to improve the coordination of communication between distributed co-workers since the early 1990s. Awareness displays are technology designed to provide contextual information about the activities of group members. Most researchers have assumed that these displays improve the coordination of communication regardless of the relationship between the communicating parties. This article examines the conditions under which awareness displays improve coordination and the types of designs that most effectively support communication timing without overwhelming people with irrelevant information. Results from a pair of laboratory experiments indicate that awareness displays containing information about a remote collaborator's workload lead to communication attempts that are less disruptive, but only when the interrupter has incentives to be concerned about the collaborator's welfare. High-information awareness displays harmed interrupters' task performance, while abstract displays did not. We conclude that a display with an abstract representation of a collaborator's workload is optimal; it leads to better timing of interruptions without overwhelming the person viewing the display.
|keyword = computer-mediated communication and collaboration,virtual teams,laboratory experiments,attention,interruption,awareness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Risk Management of Contract Portfolios in IT Services: The Profit-at-Risk Approach'''
{{header}}
{{article
|author= Robert J. Kauffman,Ryan Sougstad,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Information technology (IT) services providers are exposed to exogenous risks faced by the industry as a whole, and endogenous risks from their Current portfolio of IT contracts. This exposure may lead to cost overruns or legal responsibility for service-level breeches. Providers can leverage information about their risk positions implied by their IT services contract portfolios to gain strategic advantage over their competitors. We build theory in support of a new construct, profit-at-risk, for evaluating the trade-offs between contract profitability and service-level risk, stemming from financial economics theory and models. We simulate an IT services contract portfolio, and show how managers can reduce organizational risk by forgoing profit-maximizing contracts in lieu of more conservative service-level agreements, yet still achieve high returns. Our approach provides decision support for ex ante contract evaluation and negotiation, and a means to conduct ex post efficiency evaluation. It also aligns IT service management with best practices in financial management.
|keyword = efficient frontier,financial economics,IT contracts,IT services,managerial decision making,mechanism design,portfolio management,profit-at-risk,service science,value-at-risk,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Stylometric Identification in Electronic Markets: Scalability and Robustness'''
{{header}}
{{article
|author= Ahmed Abbasi,Hsinchun Chen,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Online reputation systems are intended to facilitate the propagation of word of mouth as a credibility scoring mechanism for improved trust in electronic marketplaces. However, they experience two problems attributable to anonymity abuse-easy identity changes and reputation manipulation. In this study, we propose the use of stylometric analysis to help identify online traders based on the writing style traces inherentin their posted feedback comments. We incorporated a rich stylistic feature set and developed the Writeprint technique for detection of anonymous trader identities. The technique and extended feature set were evaluated on a test bed encompassing thousands of feedback comments posted by 200 eBay traders. Experiments conducted to assess the scalability (number of traders) and robustness (against intentional obfuscation) of the proposed approach found it to significantly outperform benchmark stylometric techniques. The results indicate that the proposed method may help militate against easy identity changes and reputation manipulation in electronic markets.
|keyword = anti-aliasing,electronic markets,online trust,similarity detection,stylometry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Mining Trading Partners' Preferences for Efficient Multi-Issue Bargaining in E-Business'''
{{header}}
{{article
|author= Raymond Y. K. Lau,On Wong,Yuefeng Li,Louis C. K. Ma,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Classical negotiation models are weak in supporting real-world business negotiations because these models often assume that the preference information of each negotiator is made public. Although parametric teaming methods have been proposed for acquiring the preference information of negotiation opponents, these methods suffer from the strong assumptions about the specific utility function and negotiation mechanism employed by the opponents. Consequently, it is difficult to apply these learning methods to the heterogeneous negotiation agents participating in e-marketplaces. This paper illustrates the design, development, and evaluation of a nonparametric negotiation knowledge discovery method which is underpinned by the well-known Bayesian learning paradigm. According to our empirical testing, the novel knowledge discovery method can speed up the negotiation processes while maintaining negotiation effectiveness. To the best of our knowledge, this is the first nonparametric negotiation knowledge discovery method developed and evaluated in the context of multi-issue bargaining over e-marketplaces.
|keyword = Bayesian learning,e-business,knowledge discovery,multi-issue bargaining,negotiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Multiple Group Coordination in Complex and Dynamic Task Environments: Interruptions, Coping Mechanisms, and Technology Recommendations'''
{{header}}
{{article
|author= Yuqing Ren,Sara Kiesler,Susan R. Fussell,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Collaboration in complex and dynamic environments such as hospitals, airlines, and disaster response teams is challenging. High performance requires smooth coordination across multiple groups whose incentives, cultures, and routines call conflict. In this paper, we present an in-depth case study of a hospital's operating room practices to understand challenges associated with multiple group coordination and how information technology may help. We use the concept of trajectory to focus our observations and interviews oil workflow across groups and critical events when coordination breaks down. A careful examination of the sources, coping mechanisms, and consequences of coordination breakdowns suggests three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) information systems integration, and (3) information pooling and learning at the organizational level. We conclude with technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments.
|keyword = collaboration,coordination breakdown,group boundary,multiple group coordination,trajectory awareness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Factors in the Global Assimilation of Collaborative Information Technologies: An Exploratory Investigation in Five Regions'''
{{header}}
{{article
|author= Deepinder S. Bajwa,L. Floyd Lewis,Graham Pervan,Vincent S. Lai,Bjorn E. Munkvold,Gerhard Schwabe,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The diffusion of innovation theory is deployed to investigate the global assimilation of collaborative information technologies (CITs). Based on the concepts of IT acquisition and utilization, an assimilation framework is presented to highlight four states (limited, focused, lagging, and pervasive) that capture the assimilation of conferencing and groupware CITs. Data collected from 538 organizations in the United States, Australia, Hong Kong, Norway, and Switzerland are aggregated and analyzed to explore assimilation patterns and the influence of decision-making pattern, functional integration, promotion of collaboration, organization size, and IT function size on the assimilation of CITs. Although most of these factors influence assimilation of CITs from nonadoption to a state of limited assimilation, and from limited assimilation to a state of pervasive assimilation, they may not be critical when assimilation of CITs deviates from the expected path. The implications of our findings are discussed for practice and research on assimilation of CITs.
|keyword = collaborative information technologies,information technology adoption,information technology assimilation,information technology diffusion,information technology innovations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Manufacturers' Distribution Strategy in the Presence of the Electronic Channel'''
{{header}}
{{article
|author= Dazhong Wu,Gautam Ray,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The Internet provides an additional channel for manufacturers to provide information about and sell their products. The electronic channel has the advantage of reduced search cost and its reach is increasing, but it has limited capability to provide product information. This paper examines how Internet technology affects a monopoly manufacturer's distribution problem in an environment where product information is important for consumers to identify their ideal product. The model suggests that a manufacturer uses the electronic channel in addition to the physical channel when the product information is very valuable and product information is largely about digital attributes, or when the product information is not valuable. The model also suggests that when the manufacturer chooses to sell through both channels, there is an increase in price competition between the two channels such that the manufacturer need not sell through the electronic retailer with the highest reach, Also, when a large proportion of consumers have access to both channels, the manufacturer may sell through only one channel. The paper also examines the case where the manufacturer operates in the electronic channel and the case where the retailers are integrated.
|keyword = channel management,distribution strategy,electronic commerce,free riding,game theory,product information,search cost,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Economic Analysis of Policies for the Protection and Reuse of Noncopyrightable Database Contents'''
{{header}}
{{article
|author= Hongwei Zhu,Stuart E. Madnick,Michael D. Siegel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The availability of data on the Web and new data extraction technologies have made it increasingly easy to reuse existing data to create new databases and provide value-added services. Meanwhile, database creators have been seeking legal protection for their data, such as the European Union's Database Directive. The legislative development shows that there is significant difficulty in finding the right balance between protecting the incentives of creating publicly accessible databases (including semistructured Web sites) and preserving adequate access to factual data for value-creating, activities. We address this issue using an extended spatial competition model that explicitly considers licensing provisions and inefficiencies in policy administration. The results show that, depending on the cost level of database creation, the degree of differentiation of the reuser database, and the efficiency of policy administration, there are different socially beneficial policy choices, such as protecting a legal monopoly, encouraging competition via compulsory licensing, discouraging voluntary licensing, or even allowing free riding. With the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents.
|keyword = database protection,data reuse,intellectual property,noncopyrightable data,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effect of Network Relations on the Adoption of Electronic Trading Systems'''
{{header}}
{{article
|author= Ali Reza Montazemi,John J. Siam,Akbar Esfahanipour,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Information systems can serve as intermediaries between the buyers and the sellers in a market, creating an "electronic marketplace" that lowers the buyers' cost to acquire information about sellers' prices and product offerings. Although electronic trading systems provide potential to create an efficient market structure, we witness that it $45 trillion fixed-income market still makes little use of these systems. Low penetration of electronic trading systems in the marketplace is at odds with the existing information technology research doctrine. The reason is that the creation of efficient market structure through an-electronic marketplace is based on macro-level interfirm relationships that do not take into account the recurrent micro-level, interpersonal interaction among the market actors. Our empirical investigation, based on face-to-face interviews with 90 fixed-income senior managers and traders from 25 financial institutions, provides a unique insight into the social capital based on social networks of interpersonal relationships in the fixed-income market. Our research findings show that the market structure of embedded interpersonal ties enables participants to take advantage of information asymmetry for profit taking. As a result, imposition of solely electronic trading systems on the present fixed-income market structure is at odds with the present interfirm market norms and business processes enacted for large transactions among market makers and institutional investors.
|keyword = alternative trading systems,arm's-length relationships,electronic trading,embedded relationships,fixed-income market,information flow,network ties,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How Endogenous Motivations Influence User Intentions: Beyond the Dichotomy of Extrinsic and Intrinsic User Motivations'''
{{header}}
{{article
|author= Yogesh Malhotra,Dennis F. Galletta,Laurie J. Kirsch,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Information technology (IT) adoption research recognizes theoretical limitations in discerning if and when user behavior results from perceived external influences or from personal volition. A clear understanding of this issue requires a precise distinction between mandatory and volitional behaviors. Consistent with organismic integration theory (OIT), this study situates the locus of user motivations inside the user. Drawing upon an endogenous view of behaviors, this research makes three key contributions. First, it develops the theoretical basis for clearly discerning if and when behavior results from perceived external influences or from personal volition. Specifically, it examines how endogenous psychological feelings of autonomy, freedom, conflict, and external pressure can predict and explain user intentions. Second, it proposes that behavior may result from combinations of perceived external influences and personal volition. Recognizing how such "collections of motivations" together influence behavior advances our understanding beyond the "dichotomy" of extrinsic versus intrinsic motivations often adopted in prior research. Third, it proposes that some desired behaviors may be thwarted or impeded by a conflict between perceived external influences and personal volition. The theoretically grounded research model was empirically validated in a field study on Blackboard, a Web-based education platform at a large university. Data collected from a sample of 211 users were tested using structural equation models of initial system adoption and experienced use. Empirical support was found for the proposed model and related hypotheses. The results of this study advance our understanding about user motivations for adopting IT.
|keyword = endogenous motivations,locus of causality,organismic integration theory,system adoption,system use,system user motivations,technology-enabled learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Two-stage offshoring: An investigation of the Irish bridge'''
{{header}}
{{article
|author= Helena Holmstrom Olsson,Eoin O. Conchuir,Par J. Agerfalk,Brian Fitzgerald,
|source= MIS QUARTERLY
|year= 2008
|abstract = This paper investigates two-stage offshoring as experienced by the Irish sites of two large global companies, head-quartered in the United States, with significant software development operations. As part of these companies, the Irish sites act as abridge in their offshoring arrangements: While the U.S. sites offshore work to Ireland, the Irish sites offshore work further to India and, hence, have experience of being both customer and vendor in two-stage offshore sourcing relationships. Using a framework derived from relational exchange theory (RET), we conducted multiple case study research to investigate and develop an initial theoretical model of the implementation of this two-stage offshoring bridge model. Our study shows that while both companies act as bridges in two-stage offshoring arrangements, their approaches differ in relation to (1) team integration, (2) organizational level implementation, and (3) site hierarchy. Although, there are opportunities afforded by the bridge model at present, the extent to which these opportunities will be viable into the future is open to question. As revealed in our study, temporal location seems to favor a bridge location such as Ireland, certainly with United States-Asian partners. However, location alone will not be enough to maintain position in future two-stage offshoring arrangements. Furthermore, our research supports the view that offshoring tends to progress through a staged sequence of progressively lower cost destinations. Such a development suggests that two-stage offshoring, as described in this paper, will eventually become what we would term multistage offshoring.
|keyword = offshore sourcing,offshoring,customer-vendor relationship,relational exchange,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing the knowledge supply chain: An organizational learning model of information technology offshore outsourcing'''
{{header}}
{{article
|author= Hoon S. Cha,David E. Pingry,Matt E. Thatcher,
|source= MIS QUARTERLY
|year= 2008
|abstract = In this paper, we present an economic learning model that helps to formalize the complex relationships among an offshoring firm's knowledge levels, production costs, and coordination costs. Specifically, we model a domestic firm's use of a selective offshore strategy (i.e., offshoring only a portion of its information technology activities) to exploit, through IT investments or contractual provisions, the foreign vendor's large, scale-driven repository of production knowledge. We illustrate the conditions under which knowledge transfers during offshoring may reduce a domestic firm's in-house production costs, leading to total cost savings in both the short term and the long term. Alternatively, when knowledge transfers are not sufficiently large, some short-lived offshoring projects may generate substantial cost savings to the domestic firm; however, long-lived offshoring projects may cause a disruption in the knowledge supply chain, resulting in substantial losses in the later stages of the project. A firm that fails to realize the costs associated with such a disruption soon enough in the project life may find itself locked into a disadvantageous offshoring agreement without any recourse. However, a domestic firm maybe able to overcome a disruption in its knowledge supply chain by exploiting the learning-by-doing production knowledge generated by the foreign vendor's economies of scale. The managerial implications derived from our learning model may help guide firms as they consider the impacts of offshore contracts and knowledge management investments on firm knowledge, production costs, and coordination costs.
|keyword = IT offshoring,offshore outsourcing,backshoring,organizational learning,learning-by-doing,knowledge management,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Innovating or doing as told? Status differences and overlapping boundaries in offshore collaboration'''
{{header}}
{{article
|author= Natalia Levina,Emmanuelle Vaast,
|source= MIS QUARTERLY
|year= 2008
|abstract = Increasingly, firms source more complex and strategic as well as harder to codify information technology projects to low-cost offshore locations. Completing such projects successfully requires close collaboration among all participants. Yet, achieving such collaboration is extremely difficult because of the complexity of the context: multiple and overlapping boundaries associated with diverse organizational and national contexts separate the participants. These boundaries also lead to a pronounced imbalance of resources among onshore and offshore contributors giving rise to status differences and inhibiting collaboration. This research adopts a practice perspective to investigate how differences in country and organizational contexts give rise to boundaries and associated status differences in offshore application development projects and how these boundaries and status differences can be renegotiated in practice to establish effective collaboration. To illustrate and reline the theory, a qualitative case study of a large financial services firm, which sourced a variety of high-end IT work to its wholly owned subsidiaries ("captive centers") and to third party vendors in multiple global locations (in particular, to India and Russia), is presented. Using a grounded theory approach, the paper finds that differences in country contexts gave rise to a number of boundaries that inhibited collaboration effectiveness, while differences in organizational contexts were largely mediated through organizational practices that treated vendor centers and captive units similarly. It also shows that some key onshore managers were able to alleviate status differences and facilitate effective collaboration across diverse country contexts by drawing on their position and resources. Implications are drawn for the theory and practice of global software development and multiparty collaboration.
|keyword = offshore software development,outsourcing,collaboration,qualitative methods,boundaries,status,power,Bourdieu,practice theory,cross-cultural teams,distributed teams,virtual teams,middle managers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Is the world really flat? A look at offshoring at an online programming marketplace'''
{{header}}
{{article
|author= David Gefen,Erran Carmel,
|source= MIS QUARTERLY
|year= 2008
|abstract = In a world that is flat, where all clients and providers can easily transact with one another, offshoring represents the proposition that information technology providers from low-wage nations can now underbid providers from high-wage nations and win contracts. We examined a particularly flat "world"-an online programming marketplace-and found that this profound tilt to low-wage nations is overstated We analyzed the entire history of transactions at one of the major online programming marketplaces, a marketplace for outsourcing small ITprojects. The data spanned 38 months and included over 263, 000 bids by over 31,000 providers from 70 countries on over 20, 000 small IT projects requested by over 7,900 clients from 59 countries. Contrary to the world-is-flat proposition, the data in this particular site show some client preference for domestic providers. However, the largest group of clients, the American clients, are a marked exception to clients in the rest of the world: they give relatively less preference to domestic providers. In a sense, the American clients have a higher preference for offshore providers. Among non-American clients the preference for domestic providers is mitigated when both client and provider are from an English-speaking nation. Relative bid price, often very low already, also determines the winning bid, as does the ratio of purchasing power parity (PPP) between the country of the client and the country of the provider. Nonetheless, the strongest determinant of the winning bid is client loyalty: the client gives very strong preference to a provider with whom there has been a previous relationship, regardless of whether the provider is offshore or domestic.
|keyword = outsourcing,offshoring,online programming,marketplace,agency theory,bidding,PPP,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Outsourcing to an unknown workforce: Exploring opensourcing as a global sourcing strategy'''
{{header}}
{{article
|author= Par J. Agerfalk,Brian Fitzgerald,
|source= MIS QUARTERLY
|year= 2008
|abstract = This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy-opensourcing, as we term it here-whereby commercial companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product We followed this with a large-scale survey in volving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.
|keyword = open source,opensourcing,offshoring,outsourcing,global software development,crowdsourcing,multi-method research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Transformational technologies and the creation of new work practices: Making implicit knowledge explicit in task-based offshoring'''
{{header}}
{{article
|author= Paul M. Leonardi,Diane E. Bailey,
|source= MIS QUARTERLY
|year= 2008
|abstract = Studies have shown the knowledge transfer problems that arise when communication and storage technologies are employed to accomplish work across time and space. Much less is known about knowledge transfer problems associated with transformational technologies, which afford the creation, modification, and manipulation of digital artifacts. Yet, these technologies play a critical role in offshoring by allo wing the distribution of work at the task level, what we call task-based offshoring For example, computer aided engineering applications transform input like physical dimensions, location coordinates, and material properties into computational models that can be shared electronically among engineers around the world as they work together on analysis tasks. Digital artifacts created via transformational technologies often embody implicit knowledge that must be correctly interpreted to successfully act upon the artifacts. To explore what problems might arise in interpreting this implicit knowledge across time and space, and how individuals might remedy these problems, we studied a firm that sent engineering tasks from home sites in Mexico and the United States to an offshore site in India. Despite having proper formal education and ample tool skills, the Indian engineers bad dfficulty interpreting the implicit knowledge embodied in artifacts sent to them from Mexico and the United States. To resolve and prevent the problems that subsequently arose, individuals from the home sites developed five new work practices to transfer occupational knowledge to the offshore site. The live practices were defining requirements, monitoring progress, fixing returns, routing tasks strategically, and filtering quality. The extent to which sending engineers in our study were free from having to enact these new work practices because on-site coordinators acted on their behalf predicted their perceptions of the effectiveness of the offshoring arrangement, but Indian engineers preferred learning from sending engineers, not on-site coordinators. Our study contributes to theories of knowledge transfer and has practical implications for managing task-based offshoring arrangements.
|keyword = transformational technologies,work practices,offshoring,knowledge transfer,distributed work,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Work dispersion, process-based learning, and offshore software development performance'''
{{header}}
{{article
|author= Narayan Ramasubbu,Sunil Mithas,M. S. Krishnan,Chris F. Kemerer,
|source= MIS QUARTERLY
|year= 2008
|abstract = In this paper we develop a learning-mediated model of offshore software project productivity and quality to examine whether widely adopted structured software processes are effective in mitigating the negative effects of work dispersion in offshore software development. We explicate how the key process areas of the capability maturity model (CMM) can be utilized as a platform to launch learning routines in offshore software development and thereby explain why some offshore software development process improvement initiatives are more effective than others. We validate our learning mediated model of offshore software project performance by utilizing data collected from 42 offshore software projects of a large firm that operates at the CMM level-5 process maturity. Our results indicate that investments in structured processes mitigate the negative effects of work dispersion in offshore software development. We also find that the effect of software process improvement initiatives is mediated through investments in process-based learning activities. These results imply that investments in structured processes and the corresponding process-based learning activities can be an economically viable way to counter the challenges of work dispersion and improve offshore project performance. We discuss the implication of these results for the adoption of normative process models by offshore software firms.
|keyword = offshore software development,capability maturity model,software project performance,software engineering,software productivity,software quality,distributed teams,global service disaggregation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONQUER: A methodology for context-aware query processing on the World Wide Web'''
{{header}}
{{article
|author= Veda C. Storey,Andrew Burton-Jones,Vijayan Sugurnaran,Sandeep Purao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = A major impediment to accurate information retrieval from the World Wide Web is the inability of search engines to incorporate semantics in the search process. This research presents a methodology, CONQUER (CONtext-aware QUERy processing), that enhances the semantic content of Web queries using two complementary knowledge sources: lexicons and ontologies. The methodology constructs a semantic net using the original query as a seed, and refines the net with terms from the two knowledge sources. The enhanced query, represented by the refined semantic net, can be executed by search engines. This paper describes the methodology and its implementation in a prototype. An empirical evaluation shows that queries suggested by the prototype produce more relevant results than those obtained by the original queries. The research, thus, provides a successful demonstration of the use of existing knowledge sources to enhance the semantic content of Web queries. The paper concludes by identifying potential uses of such enhancements of search technology in organizational contexts.
|keyword = query augmentation,semantic retrieval,ontology,lexicon,context,query,semantic web,semantic retrieval system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using self-regulatory learning to enhance e-learning-based information technology training'''
{{header}}
{{article
|author= Radhika Santhanarn,Sharath Sasidharan,Jane Webster,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Technology-mediated learning methods are widely used by organizations and educational institutions to deliver information technology training. One form of technology-mediated learning, e-learning, in which the platform is the tutor, is quickly becoming the cost-effective solution of choice for many corporations. Unfortunately, the learning outcomes have been very disappointing. E-learning training makes an implicit assumption that learners can apply a high level of self-directed learning to assimilate the training content. In contrast, based on perspectives from social cognitive theory, we propose that instructional strategies need to persuade learners to follow self-regulated learning strategies. We test our ideas with participants who were trained through e-learning to design a website. Our findings indicate that participants who were induced to follow self-regulated learning strategies scored significantly higher on learning outcomes than those who were not persuaded to do so. We discuss our findings, and suggest that the interaction among information technology features, instructional strategies, and psychological learning processes offers a fruitful avenue for future information systems training research.
|keyword = e-learning,laboratory experimentation,information technology training,self-regulatory learning,social cognitive perspective,pretraining scripts,website development,self-efficacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Let the pirates patch? An economic analysis of software security patch restrictions'''
{{header}}
{{article
|author= Terrence August,Tunay I. Tunca,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = We study the question of whether a software vendor should allow users of unlicensed (pirated) copies of a software product to apply security patches. We present a joint model of network software security and software piracy and contrast two policies that a software vendor can enforce: (i) restriction of security patches only to legitimate users or (ii) provision of access to security patches to all users whether their copies are licensed or not. We find that when the software security risk is high and the piracy enforcement level is low, or when tendency for piracy in the consumer population is high, it is optimal for the vendor to restrict unlicensed users from applying security patches. When piracy tendency in the consumer population is low, applying software security patch restrictions is optimal for the vendor only when the piracy enforcement level is high. If patching costs are sufficiently low, however, an unrestricted patch release policy maximizes vendor profits. We also show that the vendor can use security patch restrictions as a substitute to investment in software security, and this effect can significantly reduce welfare. Furthermore, in certain cases, increased piracy enforcement levels can actually hurt vendor profits. We also show that governments can increase social surplus and intellectual property protection simultaneously by increasing piracy enforcement and utilizing the strategic interaction of piracy patch restrictions and network security. Finally, we demonstrate that, although unrestricted patching can maximize welfare when the piracy enforcement level is low, contrary to what one might expect, when the piracy enforcement level is high, restricting security patches only to licensed users can be socially optimal.
|keyword = IT security,software piracy,IT policy and management,network economics,economics of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A comparison of pair versus solo programming under different objectives: An analytical approach'''
{{header}}
{{article
|author= Milind Dawande,Monica Johar,Subodha Kumar,Vijay S. Mookerjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = This study compares the performances of pair development (an approach in which a pair of developers jointly work on the same piece of code), solo development, and mixed development under two separate objectives: effort minimization and time minimization. To this end, we develop analytical models to optimize module-developer assignments in each of these approaches. These models are shown to be strongly NP-hard and solved using a genetic algorithm. The solo and pair development approaches are compared for a variety of problem instances to highlight project characteristics that favor one of the two practices. We also propose a simple criterion that can reliably recommend the appropriate approach for a given problem instance. Typically, for efficient knowledge sharing between developers or for highly connected systems, the pair programming approach is preferable. Also, the pair approach is better at leveraging expertise by pairing experts with less skilled partners. Solo programming is usually desirable if the system is large or the effort needed either to form a pair or to code efficiently in pairs is high. Solo programming is also appropriate for projects with a tight deadline, whereas the reverse is true for projects with a lenient deadline. The mixed approach (i.e., an approach where both the solo and pair practices are used in the same project) is only indicated when the system consists of groups of modules that are sufficiently different from one another.
|keyword = extreme programming,software development methodology,pair programming,integer programming,genetic algorithms,heuristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How does personality matter? Relating the five-factor model to technology acceptance and use'''
{{header}}
{{article
|author= Sarv Devaraj,Robert E. Easley,J. Michael Crant,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = The five-factor model (FFM) of personality has been used to great effect in management and psychology research to predict attitudes, cognitions, and behaviors, but has largely been ignored in the IS field. We demonstrate the potential utility of incorporating this model into IS research by using the FFM personality factors in the context of technology acceptance. We propose a dispositional perspective to understanding user attitudes and beliefs, and examine the effect of user personality-captured using the FFM's big five factors-on both the perceived usefulness of and subjective norms toward the acceptance and use of technology. Using logged usage data from 180 new users of a collaborative technology, we found general support for our hypotheses that the FFM personality dimensions can be useful predictors of users' attitudes and beliefs. We also found strong support for the relationships between intention to use and system use.
|keyword = personality,five-factor model,technology acceptance,system use,collaborative technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A value-at-risk approach to information security investment'''
{{header}}
{{article
|author= Jingguo Wang,Aby Chaudhury,H. Raghav Rao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2008
|abstract = Information security investment has been getting increasing attention in recent years. Various methods have been proposed to determine the effective level of security investment. However, traditional expected value methods (such as annual loss expectancy) cannot fully characterize the information security risk confronted by organizations, considering some extremal yet perhaps relatively rare cases in which a security failure may be critical and cause high losses. In this research note we introduce the concept of value-at-risk to measure the risk of daily losses an organization faces due to security exploits and use extreme value analysis to quantitatively estimate the value at risk. We collect a set of internal daily activity data from a large financial institution in the northeast United States and then simulate its daily losses with information based on data snapshots and interviews with security managers at the institution. We illustrate our methods using these simulated daily losses. With this approach, decision makers can make a proper investment choice based on their own risk preference instead of pursuing a solution that minimizes only the expected cost.
|keyword = information assurance,security investment,value-at-risk (VaR),extreme value analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Self-perception-based versus transference-based trust determinants in computer-mediated transactions: A cross-cultural comparison study'''
{{header}}
{{article
|author= Dan J. Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = This study examines the impact of culture on trust determinants in computer-mediated commerce transactions. Adopting trust-building foundations from cross-culture literature and focusing on a set of well-established cultural constructs as groups of culture (Type I and Type 11), this study develops a theoretical model of self-perception-based versus transference-based consumer trust in e-vendors, and empirically tests the model using cross-cultural data. The results show that transference-based trust determinants (i.e., "perceived importance of third-party seal" and "perceived importance of positive referral") are more positively related to consumer trust in e-vendors in a Type II (i.e., collectivist-strong uncertainty avoidance-high long-term orientation-high context) culture than in a Type I (i.e., individualistic-weak uncertainty avoidance-low long-term orientation-low context) culture. Unlike the initial hypothesized expectations, self-perception-based trust determinants (i.e., perceived security protection, perceived privacy concern, and perceived system reliability) do not show stronger roles to consumer trust in e-vendors in a Type I culture than in a Type II culture, although the stronger negative effect of perceived privacy concerns is observed on consumer trust in e-vendors in a Type I culture than in a Type II culture. Theoretical contributions for e-commerce cross-culture literature and implications for multinational online business managers are discussed.
|keyword = cross-cultural comparison,culture impacts,self-perception-based trust,transference-based trust,trust in e-vendor,Type I and Type II cultures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modeling web site design across cultures: Relationships to trust, satisfaction, and e-loyalty'''
{{header}}
{{article
|author= Dianne Cyr,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Despite rapidly increasing numbers of diverse online shoppers, the relationship of Web site design to trust, satisfaction, and loyalty has not previously been modeled across cultures. In the current investigation, three components of Web site design (information design, navigation design, and visual design) are considered for their impact on trust and satisfaction. In turn, relationships of trust and satisfaction to online loyalty are evaluated. Utilizing data collected from 571 participants in Canada, Germany, and China, various relationships in the research model are tested using partial least squares analysis for each country separately. In addition, the overall model is tested for all countries combined as a control and verification of earlier research findings, although this time with a mixed country sample. All paths in the overall model are confirmed. Differences are determined for separate country samples concerning whether navigation design, visual design, and information design result in trust, satisfaction, and ultimately loyalty-suggesting design characteristics should be a central consideration in Web site design across cultures.
|keyword = culture impacts,e-commerce,e-loyalty,satisfaction,trust,Web site design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining trust in information technology artifacts: The effects of system quality and culture'''
{{header}}
{{article
|author= Anthony Vance,Christophe Elie-Dit-Cosaque,Detmar W. Straub,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = The topic of trust in information technology (IT) artifacts has piqued interest among researchers, but studies of this form of trust are not definitive regarding which factors contribute to it the most. Our study empirically tests a model of trust in IT artifacts that increases our understanding in two ways. First, it sets forth two previously unexamined system quality constructs-navigational structure and visual appeal. We found that both of these system quality constructs significantly predict the extent to which users place trust in mobile commerce technologies. Second, our study considers the effect of culture by comparing the trust of French and American potential users in m-commerce technologies. We found that not only does culture directly affect user trust in IT artifacts but it also moderates the extent to which navigational structure affects this form of trust. These findings show that system quality and culture significantly affect trust in the IT artifact and point to rich possibilities for future research in these areas.
|keyword = culture impacts,institution-based trust,m-commerce,m-commerce portals,navigational structure,system quality,systems use,trust in the IT artifact,visual appeal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Establishing trust in electronic commerce through online word of mouth: An examination across genders'''
{{header}}
{{article
|author= Neveen F. Awad,Arik Ragowsky,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = This paper focuses on the cultural effect of gender on the relationship of online word of mouth and trust in e-commerce. To encourage online commerce, many online retailers use online word-of-mouth systems, where consumers can rate products offered for sale. To date, how such ratings affect trust and adoption of e-commerce across genders has been relatively unexplored. We assess whether the effect of online trust on intention to shop online is moderated by gender. Our results show that the effect of trust on intention to shop online is stronger for women than for men. In addition, we find that men value their ability to post content online, whereas women value the responsive participation of other consumers to the content they have posted. Finally, we find that online word-of-mouth quality affects online trust differently across genders.
|keyword = consumer-generated content,electronic commerce,gender,online trust,online word of mouth,sociolinguistic theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''In justice we trust: Predicting user acceptance of e-customer services'''
{{header}}
{{article
|author= Ofir Turel,Yufei Yuan,Catherine E. Connelly,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = High-quality customer service is an integral part of any successful enterprise, but providing it can be a challenge for online merchants, especially when customers are complaining about each other. This study examines how justice and trust affect user acceptance of e-customer services by conducting an online experiment involving 380 participants. The results suggest that trust in the e-customer service fully mediates the effects of trust in the service representative and procedural justice on intentions to reuse the e-customer service. Furthermore, the effect of distributive justice on trust in the e-customer service was fully mediated by trust in the e-service representative. Finally, the effect of informational justice on user intentions to reuse the e-customer service was partially mediated by trust in the service representative and trust in the e-customer service. Theoretical and practical implications are further discussed.
|keyword = e-customer service,justice,online dispute resolution,technology acceptance,trust,trust transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Gaining trust through online privacy protection: Self-regulation, mandatory standards, or Caveat Emptor'''
{{header}}
{{article
|author= Zhulei Tang,Yu (Jeffrey) Hu,Michael D. Smith,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Trust is particularly important in online markets to facilitate the transfer of sensitive consumer information to online retailers. In electronic markets, various proposals have been made to facilitate these information transfers. We develop analytic models of hidden information to analyze the effectiveness of these regimes to build trust and their efficiency in terms of social welfare. We find that firms' ability to influence consumer beliefs about trust depends on whether firms can send unambiguous signals to consumers regarding their intention of protecting privacy. Ambiguous signals can lead to a breakdown of consumer trust, while the clarity and credibility of the signal under industry self-regulation can lead to enhanced trust and improved social welfare. Our results also indicate that although overarching government regulations can enhance consumer trust, regulation may not be socially optimal in all environments because of lower profit margins for firms and higher prices for consumers.
|keyword = consumer information,consumer surplus,internet,privacy,social welfare,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online reverse auctions and the dynamics of trust'''
{{header}}
{{article
|author= Mohamed Hedi Charki,Emmanuel Josserand,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = This research explores the effect of the introduction of online reverse auctions (ORAs) on interorganizational trust between buyers and suppliers in the retail industry. Building upon the notion of the spirit of the technology and the organizing vision, we shed light on the "equivoque" nature of ORAs. In an integrative model, we show how the desocialization associated with the introduction to ORAs can lead to distrust. Our findings show specifically the importance of the role played by technical problems and rumors.
|keyword = distrust,online reverse auctions,online trust,opportunism,rumor,trust,unethical behavior.,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Explaining and predicting the impact of branding alliances and web site quality on initial consumer trust of e-commerce web sites'''
{{header}}
{{article
|author= Paul Benjamin Lowry,Anthony Vance,Greg Moody,Bryan Beckman,Aaron Read,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = Trust is a crucial factor in e-commerce. However, consumers are less likely to trust unknown Web sites. This study explores how less-familiar e-commerce Web sites can use branding alliances and Web site quality to increase the likelihood of initial consumer trust. We use the associative network model of memory to explain brand knowledge and to show how the mere exposure effect can be leveraged to improve a Web site's brand image. We also extend information integration theory to explain how branding alliances are able to increase initial trust and transfer positive effects to Web sites. Testing of our model shows that the most important constructs for increasing initial trust in our experimental context are branding and Web site quality. Finally, we discuss future research ideas, limitations, implications, and ideas for practitioners.
|keyword = associative network model of memory,brand awareness,brand image,branding alliance,e-commerce,information integration theory,internet,trust,Web site quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dynamics of trust revision: Using health infomediaries'''
{{header}}
{{article
|author= Fatemeh Mariam Zahedi,Jaeki Song,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = This study explores the process by which trust evolves over time. There have been a number of studies underscoring the importance of trust in the online environment. However, most trust studies have concentrated on the initial trust, and there is little known about how trust beliefs evolve over time. The dynamics of trust are of particular importance in the use of infomediaries (online information providers), among which health infomediaries are the most important for Web consumers in dealing with their wellness and health issues. We investigate the evolution of trust using the case of health infomediaries. The examination of the temporal changes in trust was carried out through two approaches-comparative statics and dynamic analyses. The research method was laboratory experiment and the data were collected for two episodes of encounters. Two comparative statics models and one dynamic model were estimated in order to examine the parameter changes from one episode of encounter to the next as well as the dynamics of belief changes. The results of analysis show that the structure of trust changes over time and information quality becomes the single most important antecedent in infomediary trust building in the later stages of use. Furthermore, our study also indicates that satisfaction plays an important role in changing Web customers' trust beliefs. Contributions as well as research and managerial implications are discussed.
|keyword = agency theory,change in trust beliefs,emotional trust,information integration theory,satisfaction,trust attitude,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Attributions of trust in decision support technologies: A study of recommendation agents for e-commerce'''
{{header}}
{{article
|author= Weiquan Wang,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = As organizations increasingly utilize Web-based technologies to support customers better, trust in decision support technologies has emerged as an important issue in online environments. In this study, we identify six reasons users trust (or do not trust) a technology in the early stages of its use by extending the theories of trust formation in interpersonal and organizational contexts to that of decision support technologies. We study the particular context of decision support technologies for e-commerce: online recommendation agents (RAs), which facilitate users' decision making by providing advice on what to buy based on user-specified needs and preferences. A laboratory experiment is conducted using a multimethod approach to collect data. Both quantitative data about participants' trust in RAS and written protocols that explain the reasons for their levels of trust are collected. A content analysis of the written protocols identifies both positive and negative trust attributions that are then mapped to six trust reasons. A structural equation modeling analysis is employed to test the causal strengths of the trust reasons in explaining participants' trust in RAS. The results reveal that in the early stages of trust formation, four positive reasons (i.e., knowledge-based, interactive, calculative, and dispositional) are associated with higher trust in RAS and two negative reasons (i.e., calculative and interactive) are associated with lower trust in RAS. The results also demonstrate some distinctive features of trust formation with respect to decision support technologies. We discuss the research and practical implications of the findings and describe opportunities for future research.
|keyword = decision support technology,reasons to trust,recommendation agents,trust attribution,trust in technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A research agenda for trust in online environments'''
{{header}}
{{article
|author= David Gefen,Izak Benbasat,Paul A. Pavlou,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2008
|abstract = We present an agenda for the future research that has the potential to extend the conceptual foundations of trust in online environments and to improve the practice in the domain. The agenda draws on the previous work on trust, the papers included in this Special Issue, and our perspective on the state of the literature. This agenda is structured into four components-nature and role of trust, moderators of trust, antecedents of trust, and empirical methods for examining trust.
|keyword = online trust,research agendas,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward improving the relevance of information systems research to practice: The role of applicability checks'''
{{header}}
{{article
|author= Michael Rosemann,Iris Vessey,
|source= MIS QUARTERLY
|year= 2008
|abstract = This paper takes a first step in aiding researchers to improve the relevance of their research to practice. By proposing that Information Systems researchers conduct applicability checks with practitioners on the research objects (for example, theories, models, frameworks, processes, technical artifacts, or other theoretically based IS artifacts) they either produce or use in theory-focused research, our paper presents an actionable, systematic approach to evaluating, establishing, and further improving research relevance. Furthermore, because it is an approach that can be conducted as an additional step either at the beginning or the end of the traditional research life cycle, it leaves untouched the rigorous methods used to conduct the study, that is, it does not compromise traditional research models. The approach we propose is based on the analyses of three dimensions of relevance that are critical to practitioners' attempts to internalize IS research findings (importance, accessibility, and suitability), and a comprehensive set of solutions that can be used to address them. Our analysis reveals that the most critical dimension for practice is the importance of the research to the needs of practice. The solution we propose to address that need is to conduct an applicability check on the research objects of interest. The applicability check forms an integral part of the research process, either prior to or following engagement in atypical research process. We present principles and criteria for the conduct and evaluation of an applicability check, which is primarily based on the focus group method, and secondarily on a modified nominal group technique.
|keyword = relevance,rigor,academic research,research process,applicability check,focus group method,modified nominal group technique,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contribution behaviors in distributed environments'''
{{header}}
{{article
|author= Fernando Olivera,Paul S. Goodman,Sharon Swee-Lin Tan,
|source= MIS QUARTERLY
|year= 2008
|abstract = In this paper, we develop a framework for understanding contribution behaviors, which we define as voluntary acts of helping others by providing information. Our focus is on why and how people make contributions in geographically distributed organizations where contributions occur primarily through information technologies. We develop a model of contribution behaviors that delineates three mediating mechanisms: (1) awareness; (2) searching and matching; and (3) formulation and delivery. We specify the cognitive and motivational elements involved in these mechanisms and the role of information technology in facilitating contributions. We discuss the implications of our framework for developing theory and for designing technology to support contribution behaviors.
|keyword = contribution behaviors,knowledge sharing,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Extending the understanding of end user information systems satisfaction formation: An equitable needs fulfillment model approach'''
{{header}}
{{article
|author= N. Au,E. W. T. Ngai,T. C. E. Cheng,
|source= MIS QUARTERLY
|year= 2008
|abstract = End user satisfaction (EUS) is critical to successful information systems implementation. Many EUS studies in the past have attempted to identify the antecedents of EUS, yet most of the relationships found have been criticized for their lack of a strong theoretical underpinning. Today it is generally understood that IS failure is due to psychological and organizational issues rather than technological issues, hence individual differences must be addressed. This study proposes a new model with an objective to extend our understanding of the antecedents of EUS by incorporating three well-founded theories of motivation, namely expectation theory, needs theory, and equity theory. The uniqueness of the model not only recognizes the three different needs (i.e., work performance, relatedness, and self-development) that users may have with IS use, but also the corresponding inputs required from each individual to achieve those needs fulfillments, which have been ignored in most previous studies. This input/needs fulfillment ratio, referred to as equitable needs fulfillment, is likely to vary from one individual to another and satisfaction will only result in a user if the needs being fulfilled are perceived as "worthy" to obtain. The partial least squares (PLS) method of structural equation modeling was used to analyze 922 survey returns collected form the hotel and airline sectors. The results of the study show that IS end users do have different needs. Equitable work performance fulfillment and equitable relatedness fulfillment play a significant role in affecting the satisfaction of end users. The results also indicate that the impact of perceived IS performance expectations on EUS is not as significant as most previous studies have suggested. The conclusion is that merely focusing on the technical soundness of the IS and the way in which it benefits employees may not be sufficient. Rather, the input requirements of users for achieving the corresponding needs fulfillments also need to be examined.
|keyword = user satisfaction,information systems,measurement,equitable needs fulfillment,equity,expectations,IS implementation,PLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology governance in information technology investment decision processes: The impact of investment characteristics, external environment, and internal context'''
{{header}}
{{article
|author= Yajiong Xue,Huigang Liang,William R. Boulton,
|source= MIS QUARTERLY
|year= 2008
|abstract = This study identifies governance patterns for information technology investment decision processes and explores the impact of organizations' investment characteristics, external environment, and internal context on the shaping of those patterns. By identifying the lead actors of the initiation, development, and approval stages in IT governance, the patterns of 57IT investment decisions at 6 hospitals are analyzed. The results reveal seven IT governance archetypes: (1) top management monarchy, (2) top management-IT duopoly, (3) IT monarchy, (4) administration monarchy, (5) administration-IT duopoly, (6) professional monarchy, and (7) professional-IT duopoly. Each archetype is analyzed by taking into account four specific factors: IT investment level, external influence, organizational centralization, and IT function power. This study makes several contributions to IT governance theory and practice. First, IT governance is refrained to include pre-decision stages, highlighting the importance of participants other than the final decision maker. Second the variation of IT governance archetypes suggests that even when top management approval is required, the IT department may not play a key role in the IT investment decision process. Third, governance of the pre-decision initiation and development stages is found to be jointly affected by several contextual factors, suggesting that the allocation of final decision rights is only a part of IT governance. While decision rights may be allocated by the organization a priori, the actual patterns of IT governance are contingent on contextual factors. It is important to understand how IT governance archetypes are shaped because they may affect desired outcomes of IT investments.
|keyword = IT investment,decision-making process,IT governance,centralization,IT function power,external environment,investment characteristics,monarchy,duopoly,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using an attribute-based decision support system for user-customized products online: An experimental investigation'''
{{header}}
{{article
|author= Arnold Kamis,Marios Koufaris,Tziporah Stern,
|source= MIS QUARTERLY
|year= 2008
|abstract = In the decision support systems literature, most studies have concentrated on the direct effects of DSS use and design on decision outcomes and user performance in the workplace. Fewer DSS studies have integrated decision process variables, such as user beliefs and attitudes, in their models. In this paper, we examine the mediating role of decision process variables in the use of an online customer DSS. We do so through an experimental study of an alternative-based and an attribute-based DSS for product customization by online customers. Using cognitive fit and flow theories, we develop a theoretical model with four mediating decision process variables (perceived usefulness, perceived ease of use, perceived enjoyment, and perceived control) and two of their antecedents: interface design (attribute-based versus alternative-based) and task complexity (choice set size). Our results show that the impact of DSS interface design on behavioral intentions is fully mediated by perceived usefulness and perceived enjoyment, although not by perceived control. Specifically, we verify that users of an attribute-based DSS express higher perceived usefulness and perceived enjoyment than users of an alternative-based one. In addition, we find that task complexity has an interesting relationship with usefulness and enjoyment, both of which follow an inverted U-shaped curve as choice set size increases. Finally, we find that for users of the alternative-based DSS, perceived ease of use and perceived control decrease as task complexity increases. However, the attribute-based DSS alleviates that decline for both variables. Among other contributions, our results indicate the importance of including decision process variables when studying DSS as well as the complex effect of task complexity on those variables. Our study also provides some important guidelines for online companies that provide customer DSS on their websites, especially the danger of providing too many product choice options that can overwhelm customers and harm their shopping experience.
|keyword = decision support systems,attribute-based decision support systems,decision process,choice set size,task complexity,perceived control,perceived ease of use,perceived usefulness,perceived enjoyment,customization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The relative advantage of electronic channels: A multidimensional view'''
{{header}}
{{article
|author= Vivek Choudhury,Elena Karahanna,
|source= MIS QUARTERLY
|year= 2008
|abstract = The Internet has the potential to fundamentally change the structure of marketing channels, but only if consumers choose to adopt electronic channels. Thus, this paper aims to develop a more nuanced understanding of consumer channel choices. Specifically, it contends that it is important to examine consumers' intent to adopt electronic channels, not as a monolithic decision, but as a choice they make at each of four stages in the purchase process: requirements determination, vendor selection,purchase, and after-sales service. Innovation diffusion theory suggests that consumers make adoption decisions based on their perceptions of the relative advantage of the innovation. The relative advantage of electronic channels is conceptualized as a multidimensional construct involving a cumulative assessment of the perceived relative merits of channels on three dimensions: convenience, trust, and efficacy of information acquisition. Combining the multidimensional nature of relative advantage with the multi-stage purchase process, the central assertion, and intended contribution, of this paper is to show that the relative advantage of electronic channels, and the influence of each dimension of relative advantage on the adoption of electronic channels, will vary across the different stages of the purchase process. Survey data were collected from faculty and staff at a large university about their intention to use the web for auto insurance transactions. The results provide support for the multidimensional nature of relative advantage, although the emergent factors do not align neatly with the hypothesized dimensions (convenience, trust, and efficacy of information acquisition) or stages. Results of the study support three conclusions. First, the dimensions along which consumers assess relative advantage blend hypothesized dimensions such as trust and convenience with stages of the purchase process. Second, consumers consider the relative advantage of channels at two distinct stages of the purchase process: gathering information and executing the transaction. Third, different dimensions of relative advantage are critical in predicting consumer channel choice at each stage.
|keyword = electronic channels,relative advantage,stages in purchasing process,B2C e-commerce,trust,efficacy of information acquisition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Risk and return of information technology initiatives: Evidence from electronic commerce announcements'''
{{header}}
{{article
|author= Sanjeev Dewan,Fei Ren,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = This paper takes an event study approach to jointly examine the wealth and risk effects associated with electronic commerce announcements, contributing to the emerging research on the riskiness of IT investments and the trade-off between risk and return in the information systems literature. We estimate a generalized event study model that allows for both systematic and unsystematic risk changes on data collected for electronic commerce announcements in the 1996-2002 time frame. A striking result emerging from our analysis is that wealth effects are not significant after controlling for contemporaneous risk changes. Both total and unsystematic risk show a significant postevent increase in 1998 and 2000, whereas systematic risk adjusts downward in 1996 and 2002. Put together, our results contribute to our nascent understanding of how IT initiatives affect the risk-return profile of the firm.
|keyword = IT risk,risk and return,electronic commerce,IT event study,wealth effects,risk effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Adoption of information technology under network effects'''
{{header}}
{{article
|author= Deishin Lee,Haim Mendelson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Because information technologies are often characterized by network effects, compatibility is an important issue. Although total network value is maximized when everyone operates in one compatible network, we find that the technology benefits for the users depend on vendor incentives, which are driven by the existence of "de facto" or "de jure" standards. In head-to-head competition, customers are better off "letting a thousand flowers bloom," fostering fierce competition that results in a de facto standard if users prefer compatibility over individual fit, or a split market if fit is more important. In contrast, firms that sponsor these products are better off establishing an up-front, de jure standard to lessen the competitive effects of a network market. However, if a firm is able to enter the market first by choosing a proprietary/incompatible technology, it can use a "divideand-conquer" strategy to increase its profit compared with head-to-head competition, even when there are no switching costs. When there is a first mover, the early adopters, who are "locked in" because of switching costs, never regret their decision to adopt, whereas the late adopters, who are not subject to switching costs, are exploited by the incumbent firm. In head-to-head competition, customers are unified in their preference for incompatibility when there is a first mover; late adopters prefer de jure compatibility because they bear the brunt of the first-mover advantage. This again underscores the interdependence of user net benefits and vendor strategies.
|keyword = information technology,network effects,adoption,compatibility,standards,first mover,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A model of search intermediaries and paid referrals'''
{{header}}
{{article
|author= Thomas A. Weber,Zhiqiang (Eric) Zheng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = In this paper we pursue three main objectives: (1) to develop a model of an intermediated search market in which matching between consumers and firms takes place primarily via paid referrals; (2) to address the question of designing a suitable mechanism for selling referrals to firms; and (3) to characterize and analyze the firms' bidding strategies given consumers' equilibrium search behavior. To achieve these objectives we develop a two-stage model of search intermediaries in a vertically differentiated product market. In the first stage an intermediary chooses a search engine design that specifies to which extent a firm's search rank is determined by its bid and to which extent it is determined by the product offering's performance. In the second stage, based on the search engine design, competing firms place their open bids to be paid for each referral by the search engine. We find that the revenue-maximizing search engine design bases rankings on a weighted average of product performance and bid amount. Nonzero pure-strategy equilibria of the underlying discontinuous bidding game generally exist but are not robust with respect to noisy clicks in the system. We determine a unique nondegenerate mixed-strategy Nash equilibrium that is robust to noisy clicks. In this equilibrium firms of low product performance fully dissipate their rents, which are appropriated by the search intermediary and the firm with the better product. The firms' expected bid amounts are generally nonmonotonic in product performance and depend on the search engine design parameter. The intermediary's profit-maximizing design choice, by attributing a positive weight to the firms' bids, tends to obfuscate search results and reduce overall consumer surplus compared to the socially optimal design of fully transparent results ranked purely on product performance.
|keyword = markets and auctions,paid referrals,search intermediary,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating the influence of the functional mechanisms of online product presentations'''
{{header}}
{{article
|author= Zhenhui Jiang,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Internet-based interactive multimedia technologies enable online firms to employ a variety of formats to present and promote their products: They can use pictures, videos, and sounds to depict products, as well as give consumers the opportunity to try out products virtually. Despite the several previous endeavors that studied the effects of different product presentation formats, the functional mechanisms underlying these presentation methods have not been investigated in a comprehensive way. This paper investigates a model showing how these functional mechanisms (namely, vividness and interactivity) influence consumers' intentions to return to a website and their intentions to purchase products. A study conducted to test this model has largely confirmed our expectations: (1) both vividness and interactivity of product presentations are the primary design features that influence the efficacy of the presentations; (2) consumers' perceptions of the diagnosticity of websites, their perceptions of the compatibility between online shopping and physical shopping, and their shopping enjoyment derived from a particular online shopping experience jointly influence consumers' attitudes toward shopping at a website; and (3) both consumers' attitudes toward products and their attitudes toward shopping at a website contribute to their intentions to purchase the products displayed on the website.
|keyword = virtual product experience,functional control,vividness,interactivity,online product presentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A temporal model of information technology project performance'''
{{header}}
{{article
|author= Andrew Gemino,Blaize Horner Reich,Chris Sauer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Efficiently delivering expected performance from information technology projects remains a critical challenge for many organizations. Improving our understanding of how various factors influence project performance is therefore an important research objective. This study proposes and tests a temporal model of information technology project performance (TMPP). It shows that performance can be better understood by separating risk factors into earlier (a priori) risk factors and later (emergent) risk factors, and modeling the influence of the former on the latter. Project performance, the dependent variable, is measured by considering both process (budget and schedule) and product (outcome) components. The model includes interactions between risk factors, project management practices, and project performance components. The model is tested using partial least squares analysis with data from a survey of 194 project managers. Our results indicate that the TMPP increases explanatory power when compared with models that link risk factors directly to project performance. The results show the importance for active risk management of recognizing, planning for, and managing a priori and emergent risk factors. The finding of a strong relationship between structural risk factors and subsequent volatility shows the need for risk management practice to recognize the interaction of a priori and emergent risk factors. The results confirm the importance of knowledge resources, organizational support, and project management practices, and demonstrate the ways in which they reinforce each other.
|keyword = information technology project management,knowledge management,project performance,software project risk,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A design science research methodology for Information Systems Research'''
{{header}}
{{article
|author= Ken Peffers,Tuure Tuunanen,Marcus A. Rothenberger,Samir Chatterjee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = The paper motivates, presents, demonstrates in use, and evaluates a methodology for conducting design science (DS) research in information systems (IS). DS is of importance in a discipline oriented to the creation of successful artifacts. Several researchers have pioneered DS research in IS, yet over the past 15 years, little DS research has been done within the discipline. The lack of a methodology to serve as a commonly accepted framework for DS research and of a template for its presentation may have contributed to its slow adoption. The design science research methodology (DSRM) presented here incorporates principles, practices, and procedures required to carry out such research and meets three objectives: it is consistent with prior literature, it provides a nominal process model for doing DS research, and it provides a mental model for presenting and evaluating DS research in IS. The DS process includes six steps: problem identification and motivation, definition of the objectives for a solution, design and development, demonstration, evaluation, and communication. We demonstrate and evaluate the methodology by presenting four case studies in terms of the DSRM, including cases that present the design of a database to support health assessment methods, a software reuse measure, an Internet video telephony application, and an IS planning method. The designed methodology effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.
|keyword = case study,design science,design science research,design theory,mental model,methodology,process model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Price mechanism for knowledge transfer: An integrative theory'''
{{header}}
{{article
|author= Ming-Hui Huang,Eric T. G. Wang,Abraham Seidmann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Knowledge transferred in the open market via a price mechanism enjoys the benefits of avoiding internal competition, learning from external competitors, and accumulating diversified knowledge. In the market, users can access a repository of knowledge for a single price (repository pricing) or knowledge items in the repository can be sold individually (knowledge pricing). However, users have been found to prefer repository pricing but not the knowledge in the repository. This irrationality can cause market failure because users derive a suboptimal level of utility from the knowledge repository, and vendors have contradictory pricing and knowledge strategies. We empirically examine a joint explanation from two competing theoretical perspectives that accounts for this inconsistency nicely: The mental accounting perspective endorses repository pricing because it entices users with the benefits of the whole repository, whereas the transaction decoupling perspective finds expression in individually priced knowledge because it prevents the discrete benefit of knowledge from becoming obscure. By integrating the two theoretical perspectives and considering price, knowledge, and user characteristics simultaneously, the results offer important implications for the market transfer of knowledge. Repository pricing attracts users and is essential to initiate the transfer process, whereas knowledge pricing generates knowledge preference and is thus an effective approach for learning.
|keyword = knowledge markets,knowledge pricing,knowledge transfer,mental accounting,multinomial logit model,ordered logit model,transaction decoupling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal pricing of digital experience goods under piracy'''
{{header}}
{{article
|author= Moutaz Khouja,Sungjune Park,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Piracy of digital experience goods such as music recordings has received increased attention in the literature. Much of this research has focused on pricing policies, protection against piracy, and governmental policies in the software industry. In this research, we focus on pricing policies of producers of digital experience goods. We consider a heterogeneous consumer market with different segments, each having a different affinity to piracy. We analyze the effect of different producer pricing policies on the revenue of the creator of the product, who may be different than the producer. Our results indicate that the explicit incorporation of these different consumer segments will cause the producer to charge lower prices and, therefore, lead to higher legal product diffusion. We show that the royalty system does not solve the double marginalization problem and is suboptimal from a supply-chain perspective. Also, the creator of the goods prefers-a lower price than the producer's optimal price, and this tendency increases with the creator's per unit royalty.
|keyword = digital experience goods,information goods piracy,music and movie piracy,online piracy,pricing,social welfare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Selling or advertising: Strategies for providing digital media online'''
{{header}}
{{article
|author= Ming Fan,Subodha Kumar,Andrew B. Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Media and network companies are increasingly providing digital media online. We develop a model to examine optimal strategies for media providers to utilize the online channel to distribute digital media. We examine a number of options for media providers. Our results suggest that media companies should sell programs online when content quality is relative high and online access cost is low. When online access cost is relative high, media providers could use the advertising strategy. Overall, companies are better off providing both pricing and advertising options to consumers. We derive the optimal price and advertising level, and analyze the factors that affect the price and advertising decisions. We find that as advertisement revenue rate increases, advertising level should be kept low. In addition, media companies should set online price and advertising level with consideration of the traditional channel in order to avoid channel cannibalization. We also analyze the advertising level in the traditional channel. Our results suggest that as digital video recorder technologies provide more convenience to consumers, media companies should increase, rather than decrease, revenues from advertising.
|keyword = advertising,digital media,distribution strategies,information goods pricing,online channel,pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The optimal number of versions: Why does goldilocks pricing work for information goods?'''
{{header}}
{{article
|author= Wendy Hui,Byungjoon Yoo,Kar Yan Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = The literature in general suggests that selling multiple versions is more profitable than selling only a single version. However, how many versions should be offered is not as clear. Classical pricing studies suggest providing as many versions as the number of customer types, whereas some studies in information systems suggest providing only one or two versions. In reality, firms typically provide more than one or two versions, such as three in the case of Goldilocks pricing. This study explains the discrepancies in these results and observations by showing that, although profit increases with more versions, the marginal benefit of an additional version decreases rapidly. Therefore, firms sell few versions even in the presence of very small versioning-related costs such as menu and cognitive costs. This study analyzes the effects of these costs, and shows that cognitive costs have more profound effects on versioning than menu costs.
|keyword = analytical modeling,customized bundling,economics of information systems,information goods pricing,pricing,versioning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An economic model of privacy: A property rights approach to regulatory choices for online personalization'''
{{header}}
{{article
|author= Ramnath K. Chellappa,Shivendu Shivendu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Advances in information-acquisition technologies and the increasing strategic importance of this information have created a market for consumers' personal and preference information. Behavioral research suggests that consumers engage in a privacy calculus where they trade off their privacy costs from sharing information against their value from personalization. Through a formal economic model of this personalization-for-privacy (p4p) trade-off, we examine welfare implications by characterizing consumption utilities as "no-free-disposal" functions. We investigate the optimality of four regulatory regimes (through allowance/disallowance of usage-enforcing technologies, and private contracts) by analyzing the strategic interaction between a monopolist who offers personalization services "free of charge" and two consumer types-privacy and convenience seekers. While many privacy watchdog groups have called for technology restrictions and more regulation, our research broadly suggests that society is better off with assignment of property rights over their information to consumers and full allowance of technological control and contractual abilities for the monopolist. However, when private contracts are proscribed, the regulator should also prevent the deployment of usage-enforcing technologies, particularly when the market is predominantly composed of privacy seekers. Interestingly, unlike traditional price-instrument markets for goods with free disposal, a regulator should not only encourage this market's knowledge of consumers' p4p preferences but also the various uses and benefits of preference information to the vendor.
|keyword = economic modeling,incentives,Nash bargaining,personalization,privacy,property rights,social welfare,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A process-oriented perspective on the alignment of information technology and business strategy'''
{{header}}
{{article
|author= Paul P. Tallon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Even after a decade of research and discussion, strategic alignment, denoting the fit between information technology (IT) and business strategy, remains an enduring challenge for firms worldwide. In this paper, we go beyond the dominant firm-level alignment paradigm by utilizing a value disciplines perspective on strategic foci to conceptualize alignment at the process level. Theory would then suggest that alignment should be tightest in processes that are considered critical to each firm's strategic focus. Using data from matched surveys of IT and business executives at 241 firms, we detect support for this locus of alignment argument when alignment is identified using profile deviation or moderation. We also find a positive link between alignment and perceived IT business value in each of five primary processes in the value chain. By bringing a process-level view to the study of alignment and its impacts, we go beyond a discussion on the extent of fit-a cornerstone of the literature-to whether firms are pursuing the right type of fit for the particular mix of processes underlying their strategy. In this way, a process-level perspective can foster a deeper and more meaningful understanding of how alignment affects firm performance. Our results also show a need for managers to reconsider the steps taken to align IT and business strategy by looking more closely at how IT can support individual processes rather than at how IT can support an entire strategy.
|keyword = IT business value,IT-strategy fit,perceptual measures,process-level analysis,process perspective,resource-based view,strategic alignment,value chain,value disciplines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing word mismatch problems in information retrieval: A topic-based query expansion approach'''
{{header}}
{{article
|author= Chih-Ping Wei,Paul Jen-Hwa Hu,Chia-Hung Tai,Chun-Neng Huang,Chin-Sheng Yang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Word mismatch represents a fundamental information retrieval challenge that has become increasingly important as electronic document repositories (e.g., Web resources, digital libraries) grow in number and sheer volume. In general, word mismatch refers to the phenomenon in which a concept is described by different terms in user queries and in source documents. Query expansion represents a promising avenue to address such problems. Previous research predominantly approaches query expansion on the basis of global or local analysis. However, these approaches emphasize a global perspective rather than taking a topic-specific view of term associations. As a consequence, their effectiveness can be severely constrained when the document corpus spans a diverse set of topics. In this study, we propose a topic-based approach for query expansion and develop and empirically evaluate two novel methods-namely, nonfuzzy and fuzzy topic-based query expansion-to address word mismatch problems. According to our evaluation results, the proposed topic-based approach is more effective than a benchmark global analysis method, particularly when user queries consist of multiple query terms.
|keyword = document clustering,fuzzy clustering,information retrieval,query expansion,text mining,topic-based query expansion,word mismatch,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of the use of relational and nonrelational information sources'''
{{header}}
{{article
|author= J. Christopher Zimmer,Raymond M. Henry,Brian S. Butler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Although it has been argued that knowledge is an important organizational resource, little research has investigated where individuals go to search for information or knowledge. Prior work has investigated sources in isolation, but in an organizational setting, sources are encountered as an open portfolio instead of in isolation. it is important to understand how individuals perceive the wide array of sources available to them and how those perceptions affect their use of different types of sources. Building on prior work, this research looks at factors underlying the selection of sources that require direct interpersonal contact (relational sources) and those that do not (nonrelational sources) and explores factors that differentially affect the use of these types of sources. A sample of 204 working professionals recruited from graduate business studies was used to test hypotheses regarding the effects of accessibility and quality, as well as comparisons and trade-offs between relational and nonrelational sources. Consistent with prior work, source accessibility and quality significantly affect usage of a source. This relationship, however, is moderated by the type of source with accessibility having less effect on the use of relational sources. Furthermore, use of each type of source was also affected by the perceived accessibility and quality of alternative types of sources. Together these results highlight the importance of simultaneously considering the relational and nonrelational sources available to individuals. These results also have implications for the design and implementation of systems for managing information and knowledge assets.
|keyword = information accessibility,information quality,information seeking,information sources,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Specifying formative constructs in information systems research'''
{{header}}
{{article
|author= Stacie Petter,Detmar Straub,Arun Rai,
|source= MIS QUARTERLY
|year= 2007
|abstract = While researchers go to great lengths to justify and prove theoretical links between constructs, the relationship between measurement items and constructs is often ignored By default, the relationship between construct and item is assumed to be reflective, meaning that the measurement items are a reflection of the construct. Many times, though, the nature of the construct is not reflective, but rather formative. Formative constructs occur when the items describe and define the construct rather than vice versa. In this research, we examine whether formative constructs are indeed being mistaken for reflective constructs by information systems researchers. By examining complete volumes of MIS Quarterly and Information Systems Research over the last 3 years, we discovered that a significant number of articles have indeed misspecified formative constructs. For scientific results to be valid, we argue that researchers must properly specify formative constructs. This paper discusses the implications of different patterns of common misspecifications of formative constructs on both Type I and Type 11 errors. To avoid these errors, the paper provides a roadmap to researchers to properly specify formative constructs. We also discuss how to address formative constructs within a research model after they are specified While researchers go to great lengths to justify and prove theoretical links between constructs, the relationship between measurement items and constructs is often ignored By default, the relationship between construct and item is assumed to be reflective, meaning that the measurement items
|keyword = formative constructs,reflective constructs,composite constructs,latent constructs,measurement models,methodology,statistical conclusion validity,Type I and Type II errors,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward a deeper understanding of system usage in organizations: A multilevel perspective'''
{{header}}
{{article
|author= Andrew Burton-Jones,Michael J. Gallivan,
|source= MIS QUARTERLY
|year= 2007
|abstract = The objective of this paper is to contribute to a deeper understanding of system usage in organizations by examining its multilevel nature. Past research on system usage has suffered a levels bias, with researchers studying system usage at single levels of analysis only (e.g., the individual, group, or organizational level). Although single-level research can be useful, we suggest that studying organizations one level at a time will ultimately lead to an unnatural, incomplete, and very disjointed view of how information systems are used in practice. To redress this situation, we draw on recent advances in multilevel theory to present system usage as a multilevel construct and provide an illustration for what it takes for researchers to study it as such. The multilevel perspective advanced in this article offers rich opportunities for theoretical and empirical insights and suggests a new foundation for in-depth research on the nature of system usage, its emergence and change, and its antecedents and consequences.
|keyword = system usage,multilevel,construct,configuration,IT impact,longitudinal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Through the eyes of experts: A socio-cognitive perspective on the automation of fingerprint work'''
{{header}}
{{article
|author= Christopher J. Davis,Ellen M. Hufnagel,
|source= MIS QUARTERLY
|year= 2007
|abstract = Prior research on technological frames indicates that many of the difficulties associated with systems implementation stem from differences in the meanings users, managers, and system developers attribute to automation projects. Although the concept of technological frames has been used to explore the bases for intergroup conflict during implementation, it is also a useful device for probing more deeply into the effects complex systems have on users' perceptions of their work and the role-altering effects of new technologies. Drawing upon personal construct theory and job characteristics theory, we adapted the repertory grid technique to explore the technology-in-use frames of a group of occupationally certified fingerprint technicians (FPTs). Our investigation reveals the important role the FPTs' occupationally defined values and norms played in structuring their existing work practices and the tensions produced by organizationally mandated efforts to restructure the logic of their expertise-based hierarchies. These insights illuminate the effects work redesign had on the FPTs' task environment, the process logic that guided specific work practices, and the roles defined by their expertise-based hierarchies, providing a basis for understanding the FPTs' unanticipated reactions to the automation of their work.
|keyword = technological frames,work redesign,job characteristics theory,personal construct theory,repertory grid,technicians,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The interaction of institutionally triggered and technology-triggered social structure change: An investigation of computerized physician order entry'''
{{header}}
{{article
|author= Elizabeth J. Davidson,William G. Chismar,
|source= MIS QUARTERLY
|year= 2007
|abstract = Aligning social structures and technology capabilities is a significant challenge to information technology-related organizational change. It is particularly challenging in institutionalized settings such as hospitals. We report an interpretive field study of computerized physician order entry (CPOE) at an acute-care hospital, in which we investigated how institutionally triggered and technology-triggered change interacted in complementary processes to engender alignment. Social structure changes included increased interdependency among clinical departments, multidisciplinary cooperation across clinical disciplines, and standardization in clinical decision-making. Organization members also enacted institutionalized interaction patterns with physicians, by deferring to their preferences for CPOE use. The cumulative influence of change triggers nonetheless facilitated the hospital's realization of clinical goals. We drew on Barley's (1990) role- and network-based model for technology and structure alignment. Nonetheless, we extended this micro-level analytic approach to account for the influence of change in the macro-institutional environment. Our analysis clarified the extent of structure change attributable to the CPOE technology and highlighted institutional forces that promoted yet inhibited change. The case also highlighted the importance of role networks on the trajectory and outcomes of organizational change processes.
|keyword = social structure change,technology change,computerized physician order entry,health information technology,institutional theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of online trading communities in managing internet auction fraud'''
{{header}}
{{article
|author= Cecil Eng Huang Chua,Jonathan Wareham,Daniel Robey,
|source= MIS QUARTERLY
|year= 2007
|abstract = Internet auctions demonstrate that advances in information technologies can create more efficient venues of exchange between large numbers of traders. However, the growth of Internet auctions has been accompanied by a corresponding growth in Internet auction fraud. Much extant research on Internet auction fraud in the information systems literature is conducted at the individual level of analysis, thereby limiting its focus to the choices of individual traders or trading dyads. The criminology literature, in contrast, recognizes that social and community factors are equally important influences on the perpetration and prevention of crime. We employ social disorganization theory as a lens to explain how online auction communities address auction fraud and how those communities interact with formal authorities. We show how communities may defy, coexist, or cooperate with the formal authority of auction houses. These observations are supported by a qualitative analysis of three cases of online anticrime communities operating in different auction product categories. Our analysis extends aspects of social disorganization theory to online communities. We conclude that community-based clan control may operate in concert with authority-based formal control to manage the problem of Internet auction fraud more effectively.
|keyword = auction fraud,e-commerce,authority,communities,clan control,informal social control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dispositional factors in internet use: Personality versus cognitive style'''
{{header}}
{{article
|author= James C. McElroy,Anthony R. Hendrickson,Anthony M. Townsend,Samuel M. DeMarie,
|source= MIS QUARTERLY
|year= 2007
|abstract = This study directly tests the effect of personality and cognitive style on three measures of Internet use. The results support the use of personality-but not cognitive style-as an antecedent variable. After controlling for computer anxiety, self-efficacy, and gender, including the "Big Five" personality factors in the analysis significantly adds to the predictive capabilities of the dependent variables. Including cognitive style does not. The results are discussed in terms of the role of personality and cognitive style in models of technology adoption and use.
|keyword = human factors,individual differences,individual characteristics,personality,cognitive style,end-user computing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Is the world flat or spiky? Information intensity, skills, and global service disaggregation'''
{{header}}
{{article
|author= Sunil Mithas,Jonathan Whitaker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Which service occupations are the most susceptible to global disaggregation? What are the factors and mechanisms that make service occupations amenable to global disaggregation? This research addresses these questions by building on previous work by Apte and Mason (1995) and Rai et al. (2006) that focuses on the unbundling of information and physical flows. We propose a theory of service disaggregation and argue that high information intensity makes an occupation more amenable to disaggregation because the activities in such occupations can be codified, standardized, and modularized. We empirically validate our theoretical model using data on more than 300 service occupations. We find that at the mean skill level, the information intensity of an occupation is positively associated with the disaggregation potential of that occupation, and the effect of information intensity on disaggregation. potential is mediated by the modularizability of an occupation. We also find that skills moderate the effect of information intensity on service disaggregation. Furthermore, we study the patterns in U.S. employment and salary growth from 2000 to 2004. Contrary to popular perception, we do not find any adverse effect in terms of employment growth or salary growth for high information-intensity occupations at the mean skill level. Our findings show that high-skill occupations have experienced higher employment and salary growth than low-skill occupations at the mean level of information intensity. Notably, high information-intensity occupations that require higher skill levels have experienced higher employment growth, though this employment growth is accompanied by a decline in salary growth. Occupations with a higher need for physical presence have also experienced higher employment growth and lower salary growth. Overall, these results imply that firms and managers need to consider the modularizability of occupations as they reallocate global resources to pursue cost and innovation opportunities. For individual workers, our results highlight the importance of continuous investments in human capital and skill acquisition because high information-intensity and high-skill occupations appear to be relatively less vulnerable to global disaggregation.
|keyword = information intensity,skills,codifiability,standardizability,modularizability,offshoring,global disaggregation,service occupations,services,need for physical presence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging standard electronic business interfaces to enable adaptive supply chain partnerships'''
{{header}}
{{article
|author= Arvind Malhotra,Sanjay Gosain,Omar A. El Sawy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Adaptive supply chain partnerships are a key factor in driving the ability of extended enterprise partners to achieve long-term goals in an environment characterized by disruptive environmental shifts. Adaptive extended enterprise arrangements allow participating enterprises to leverage their combined assets for collective exploration and exploitation. In the context of extended enterprises, where significant investments have been directed toward instituting common interfaces, this study examines the question: How does the use of standard electronic business interfaces (SEB1s) enable supply chain partnerships to become more adaptive? This study conceptualizes the use of SEBIs as a boundary-spanning mechanism that helps overcome boundaries that impede knowledge transfer between enterprises in supply chains. SEBIs enables partners to gain insight into their broader environments, enriching each partner's perspective (enhanced bridging). SEBIs also help strengthen the cooperative ties between partners, motivating each partner to adapt for collective gain (enhanced bonding). Our research model is empirically tested using data collected from 41 demand-side supply chain partnerships (between original equipment manufacturers (OEMs), distributors, and retailers) in the information technology (IT) industry. The results show that collaborative information exchange (CIE) between supply chain partners mediates the relationship between use of SEBIs and mutual adaptation (MA) and adaptive knowledge creation between supply chain partners. Interestingly, the use of SEBIs is found to be directly associated with MA but only indirectly associated with adaptive knowledge creation. The study points out that the strategic impacts of SEBIs go well beyond the exchange of transaction information and process integration. It also shows that multilateral, quasi-open, and information exchange-and process linkage-oriented SEBIs can result in both bonding and bridging across supply chain partners without binding them inflexibly to specific partners. Based on the model and results, the study offers practical implications for how SEBIs should be developed, adopted, and used.
|keyword = standard electronic business interfaces,boundary objects,supply chain partnering,adaptive partnerships,adaptation,digitally enabled extended enterprise,bridging,bonding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The choice of sourcing mechanisms for business processes'''
{{header}}
{{article
|author= Hueseyin Tanriverdi,Prabhudev Konana,Ling Ge,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = There is unprecedented interest in digitally enabled extended enterprises that enable firms to gain access to specialized skills and capabilities globally Given this motivation, firms are unbundling their value chain processes and exploring new sourcing mechanisms. With the emergence of world-class skills and capabilities in offshore locations, new sourcing mechanisms have become available beyond traditional domestic insourcing and outsourcing. However, there is little systematic research examining how firms choose sourcing mechanisms for their business processes. This study views the digitally enabled extended enterprise as a complex system of business processes and examines how sourcing choices are made in such enterprises. It builds on the modular systems theory to posit that modularization of business processes and their underlying information technology (IT) support infrastructures are associated with the choice of sourcing mechanisms for the processes. The study tests this proposition in a sample of business process sourcing choices made by 93 medium and large U.S. firms. The results show that firms tend to choose domestic outsourcing for processes that are high in modularity and offshore outsourcing for processes that are low in modularity. Further, when processes can be detached from a firm's IT infrastructure, firms tend to use offshore outsourcing. However, when processes are tightly coupled with underlying IT infrastructure, it may be infeasible to detach processes and execute them in remote locations. Implications for theory and practice are also discussed.
|keyword = digitally enabled extended enterprise,outsourcing,offshoring,insourcing,new organizational forms,modular systems theory,process modularity,information technology detachability,transaction cost economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of Internet referral services on a supply chain'''
{{header}}
{{article
|author= Anindya Ghose,Tridas Mukhopadhyay,Uday Rajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = In many industries, Internet referral services, hosted either by independent third-party infomediaries or by manufacturers, serve as digitally enabled lead generators in electronic markets, directing consumer traffic to downstream retailers in a distribution network. This reshapes the extended enterprise from the traditional network of upstream manufacturers and downstream retailers to include midstream third-party and manufacturer-owned referral services in the supply chain. We model competition between retailers in a supply chain with such digitally enabled institutions and consider their impact on the optimal contracts among the manufacturer, referral intermediary, and the retailers. Offline, retailers face a higher customer discovery cost. In return, they can engage in price discrimination based on consumer valuations. Online, they save on the discovery costs but lose the ability to identify consumer valuations. This critical trade-off drives firms' equilibrium strategies. We derive the optimal contracts for different entities in the supply chain and highlight how these contracts change with the entry of independent and manufacturer-owned referral services. The establishment of a referral service is a strategic decision by the manufacturer. It leads to diversion of supply chain profit from a third-party infomediary to the manufacturer. Further, it enables the manufacturer to respond to an infomediary, by giving itself greater flexibility in setting the unit wholesale fee to the profit-maximizing level. Both third-party and manufacturer-sponsored referral services play a critical role in enabling retailers to discriminate across consumers' different valuations. Retailers use online referral services to screen out low-valuation consumers and sell only to high-valuation consumers in the online channel. Our model thus endogenously derives a correlation between consumer valuation and online purchase behavior. Finally, we show that under some circumstances, it is too costly for the manufacturer to eliminate the referral infomediary.
|keyword = internet referral services,electronic markets,price dispersion,franchise fees,discovery costs,electronic intermediary,digital supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IS application capabilities and relational value in interfirm partnerships'''
{{header}}
{{article
|author= Nilesh Saraf,Christoph Schlueter Langdon,Sanjay Gosain,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = T his study examines how capabilities of information systems (IS) applications deployed in the context of interfirm relationships contribute to business performance. We propose that these capabilities augment the relational value that a firm derives from its business partners-channel partners and customer enterprises-in the context of the distribution channel. Two cospecialized relational assets are considered as key to realization of relational value-knowledge sharing and process coupling. Hypotheses linking two IS capabilities (IS flexibility and IS integration) to the relational asset dimensions, and ultimately to firm performance, are proposed. The research model is tested based on data collected through a survey of business units of enterprises embedded in customer and channel partner ties in the high-tech and financial services industries. We find that IS integration with channel partners and customers contributes to both knowledge sharing and process coupling with both types of enterprise partners, whereas IS flexibility is a foundational capability that indirectly contributes to value creation in interfirm relationships by enabling greater IS integration with partner firms. We find that two types of relational assets are significantly associated with business performance-knowledge sharing with channel partners and process coupling with customers-pointing to underlying mechanisms that differentially leverage resources of different types of channel partners. Implications for theory development and practice based on these findings are proposed.
|keyword = interorganizational information systems,competitive impacts of IS,strategic management of IT,IS applications management,relational value,marketing channels,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assimilation of interorganizational business process standards'''
{{header}}
{{article
|author= Hillol Bala,Viswanath Venkatesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Organizations have not fully realized the benefits of interorganizational relationships (IORs) due to the lack of cross-enterprise process integration capabilities. Recently, interorganizational business process standards (IBPS) enabled by information technology (IT) have been suggested as a solution to help organizations overcome this problem. Drawing on three theoretical perspectives, i.e., the relational view of the firm, institutional theory, and organizational inertia theory, we propose three mechanisms-relational, influence, and inertial-to explain the assimilation of IBPS in organizations. We theorize that these mechanisms will have differential effects on the assimilation of IBPS in dominant and nondominant firms. Using a cross-case analysis based on data from 11 firms in the high-tech industry, we found evidence to support our propositions that relational depth, relationship extendability, and normative pressure were important for dominant firms while relational specificity and influence mechanisms (coercive, mimetic, and normative pressures) were important for nondominant firms. Inertial mechanisms, i.e., ability and willingness to overcome resource and routine rigidities, were important for both dominant and nondominant firms.
|keyword = interorganizational relationships,business process,process standards,firm dominance,assimilation,deployment,relational view of the firm,institutional influences,organizational inertia,interorganizational system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Overcoming Online information privacy concerns: An information-processing theory approach'''
{{header}}
{{article
|author= Il-Horn Hann,Kai-Lung Hui,Sang-Yong Tom Lee,Ivan P. L. Png,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = The advent of the Internet has made the transmission of personally identifiable information more common and often unintended by the user. As personal information becomes more accessible, individuals worry that businesses misuse the information that is collected while they are online. Organizations have tried to mitigate this concern in two ways: (1) by offering privacy policies regarding the handling and use of personal information and (2) by offering benefits such as financial gains or convenience. In this paper, we interpret these actions in the context of the information-processing theory of motivation. Information-processing theories, also known as expectancy theories in the context of motivated behavior, are built on the premise that people process information about behavior-outcome relationships. By doing so, they are forming expectations and making decisions about what behavior to choose. Using an experimental setting, we empirically validate predictions that the means to mitigate privacy concerns are associated with positive valences resulting in an increase in motivational score. In a conjoint analysis exercise, 268 participants from the United States and Singapore face trade-off situations, where an organization may only offer incomplete privacy protection or some benefits. While privacy protections (against secondary use, improper access, and error) are associated with positive valences, we also find that financial gains and convenience can significantly increase individuals' motivational score of registering with a Web site. We find that benefits-monetary reward and future convenience-significantly affect individuals' preferences over Web sites with differing privacy policies. We also quantify the value of Web site privacy protection. Among U.S. subjects, protection against errors, improper access, and secondary use of personal information is worth $30.49-$44.62. Finally, our approach also allows us to identify three distinct segments of Internet users-privacy guardians, information sellers, and convenience seekers.
|keyword = conjoint analysis,expectancy theory,financial reward,information privacy,online privacy,segmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical investigation of third-party seller rating systems in e-commerce: The case of buySAFE'''
{{header}}
{{article
|author= Eric K. Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = eBay's highly visible feedback-based rating system is also highly flawed, contributing to problems for buyers, which in turn creates problems for sellers. The well-known "market for lemons" phenomenon studied by Akerlof, and the even older Gresham's law effect, are contributing to loss of buyers' confidence in eBay, shrinking sellers' margins, contributing to the erosion of eBay's share price, and, potentially, leading to serious reductions in the value of eBay as an electronic auction site. buySAFE has created an alternative mechanism for reducing buyers' information deficit concerning sellers and their merchandise, involving a third-party certification system and bonding for qualified sellers; the rating is analogous to bond rating services such as Moody's and Standard & Poor's. Analysis of buySAFE's certification and bonding strategies for eBay sellers provides a basis for ongoing theory development related to organizational strategies that recognize the importance of information asymmetries in the digital marketplace and address resolution of consumers' concerns. buySAFE's original business model involves bonding sellers' transactions and protecting consumers for as much as $25,000. This has a number of beneficial effects on the buyers and sellers: it improves the information endowments of the buyers, it increases their willingness to pay for the goods and services offered, and it increases the margins and total revenues of the sellers. Although acceptance of buySAFE has been rapid, it has been slower than anticipated, and slower than theory would suggest. The company's executives are exploring adjusting their approach to the market and finding a way to achieve higher profitability, and working to limit their dependence upon eBay.
|keyword = information asymmetry,market for lemons,online auctions,online consumer fraud,online rating systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Proximity and information technology outsourcing: How local are IT services markets?'''
{{header}}
{{article
|author= Ashish Arora,Chris Forman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = We examine the question of which services are tradable within a concrete Setting: the outsourcing of information technology (IT) services across a broad cross-section of establishments in the United States. If markets for IT services are local, then we should expect increases in local supply would increase the likelihood of outsourcing by lowering the cost of outsourcing. If markets are not local, then local supply will not affect outsourcing demand. We analyze the outsourcing decisions of a large sample of 99,775 establishments in 2002 and 2004, for two types of IT services-programming and design and hosting. Programming and design projects require communication of detailed user requirements whereas hosting requires less coordination between client and service provider than programming and design. Our empirical results bear out this intuition: the probability of outsourcing programming and design is increasing in the local supply of outsourcing, and this sensitivity to local supply conditions has been increasing over time. This suggests there is some nontradable or "local" component to programming and design services that cannot be easily removed. In contrast, the decision to outsource hosting is sensitive to local supply only for firms for which network uptime and security concerns are particularly acute.
|keyword = IT outsourcing,outsourcing,service tradability,services,services outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Option-based risk management: A field study of sequential information technology investment decisions'''
{{header}}
{{article
|author= Michel Benaroch,Mark Jeffery,Robert J. Kauffman,Sandeep Shah,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = This field study research evaluates the viability of applying an option-based risk management (OBRiM) framework, and its accompanying theoretical perspective and methodology, to real-world sequential information technology (IT) investment problems. These problems involve alternative investment structures that bear different risk profiles for the firm, and also may improve the payoffs of the associated projects and the organization's performance. We sought to surface the costs, benefits, and risks associated with a complex sequential investment setting that has the key features that OBRiM treats. We combine traditional, purchased real options that subsequently create strategic flexibility for the decision maker, with implicit or embedded real options that are available with no specific investment required provided the decision maker recognizes them. This combination helps the decision maker to both (1) explicitly surface all of his or her strategic choices and (2) accurately value those choices, including ones that require prior enabling investments. The latter permits senior managers to adjust a project's investment trajectory in the face of revealed risk. This normally is important when there are uncertain organizational, technological, competitive, and market conditions. The context of our research is a data mart consolidation project, which was conducted by a major airline firm in association with a data warehousing systems vendor. Field study inquiry and data collection were essential elements in the retrospective analysis of the efficacy of OBRiM as a means to control risk in a large-scale project. We learned that OBRiM's main benefits are (1) the ability to generate meaningftil option-bearing investment structures, (2) simplification of the complexities of real options for the business context, (3) accuracy in analyzing the risks of IT investments, and (4) support for more proactive planning. These issues, which we show are more effectively addressed by OBRiM than the other methods, have become crucial as more corporate finance-style approaches are applied to IT investment and IT services problems. Our evaluative study shows that OBRiM has the potential to add value for managers looking to structure risky IT investments, although some aspects still require refinements.
|keyword = data marts,data warehouses,investment valuation,IT investment,IT services,options,risk management,services science,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Comparison of software quality under perpetual licensing and software as a service'''
{{header}}
{{article
|author= Vidyanand Choudhary,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Software is available through a number of different licensing models such as the commonly used perpetual licensing model and a relatively new licensing model called software as a service (SaaS). There are several differences between SaaS and perpetual licensing. SaaS licensing offers software using a subscription model, whereas perpetual licensing involves a one-time payment for a perpetual use license and optional additional payments for future upgrades. Prior literature has not considered the impact of these licensing schemes on the publisher's incentive to invest in software quality. We model differences in how new software features are disseminated in SaaS and perpetual licensing. We show that these differences affect the publisher's incentive to invest in product development. We find that the SaaS licensing model leads to greater investment in product development under most conditions. This increased investment leads to higher software quality in equilibrium under SaaS as compared to perpetual licensing. The software publisher earns greater profits and social welfare is higher under SaaS under these conditions.
|keyword = application service providers,information goods,monopoly,on-demand computing,pricing,product development,software as a service,software licensing,software quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A temporary monopolist: Taking advantage of information transparency on the web'''
{{header}}
{{article
|author= Rajiv M. Dewan,Marshall L. Freimer,Yabing Jiang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Information displayed on an e-commerce site can be used not just by the intended customers but also by competitors. While retailers enhance service quality by linking inventory systems to Web servers and making stockout information available in real time, that stockout information could also be used by competitors in determining their prices on current stocks. In this paper, we examine the effect of such proactive use of information in the setting of e-commerce retailing where duopoly retailers set their prices of a commodity that is in short supply. We show that when customer reservation value is relatively high and retailers are differentiated in fill rate, both retailers choose the dynamic pricing strategy in equilibrium. By investing in Web scraping technology, retailers automatically monitor each other's stock status and dynamically adjust prices contingent on rival's stock availability.
|keyword = contingent pricing,e-commerce,information transparency,vertical differentiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital consumer networks and producer-consumer collaboration: Innovation and product development in the video game industry'''
{{header}}
{{article
|author= Reina Y. Arakji,Karl R. Lang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = This paper examines new forms of collaboration between producers and consumers that are emerging in the digital entertainment space. Taking the case of the video game industry, we show how some firms have opened a portion of their proprietary content for transformation by consumers and allowed the development of consumer-designed and consumer-implemented derivative products. By reappropriating these derivatives, video game firms are successfully outsourcing parts of their game design and development process to digital consumer networks. Applying economic analysis, we explore the potential benefits and risks associated with outsourcing to networks of consumers. We also derive the optimal combination of copyright enforcement and consumer compensation. Our results suggest that profit-maximizing producers of video games have incentive to partially open game content to their users and to remunerate the most innovative ones, under the condition that the derivatives constitute complements to, and not substitutes for, the original product. We discuss the implications on firm strategy for innovation.
|keyword = digital entertainment,innovation,outsourcing,producerconsumer collaboration,user-generated content,video games,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Team cognition: Development and evolution in software project teams'''
{{header}}
{{article
|author= Jun He,Brian S. Butler,William R. King,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = In software development, team-based work structures are commonly used to accomplish complex projects. Software project teams must be able to utilize the expertise and knowledge of participants without overwhelming individual members. To efficiently leverage individuals' knowledge and expertise, software project teams develop team cognition structures that facilitate their knowledge activities. This study focuses on the emergence and evolution of team cognition in software project teams, and examines how communication activity and team diversity impact the formation of these structures. A longitudinal study was conducted of 51 database development teams. The results suggest that some forms of communication and team diversity affect the formation of team cognition. Frequency of meetings and phone calls were positively related to the formation of team cognition, while e-mail use had no effect. Gender diversity had a strong and positive effect on the development of team cognition and the effect remained stable over time. Implications for the practical potential and limitations of purposive team construction as a strategy for improving software development team performance are discussed.
|keyword = expertise location,member familiarity,project teams,shared task understanding,software teams,team communications,team diversity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software process tailoring: An empirical investigation'''
{{header}}
{{article
|author= Peng Xu,Balasubramaniam Ramesh,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = A well-defined software process is critical for success in software projects. Software process tailoring refers to the activity of tuning a standardized process to meet the needs of a specific project. We conducted two case studies that address the research question: How is a software process tailored to suit its context? This study identifies process tailoring as a key mechanism to address the challenges faced by a project and develops a model that describes how a process is tailored to resolve these challenges. The model identifies a set of environmental factors, challenges, project goals, process tailoring strategies, and their influences on each other. Specifically, the findings demonstrate the duality of the software process, showing how the project context (i.e., project goals, environmental factors, and challenges) and tailoring decisions dynamically interact with each other and construct the context in which the project is developed and the process is tailored.
|keyword = duality of software process,software development,software process,software process tailoring,software project management,software projects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The spatial, temporal, and configurational characteristics of geographic dispersion in teams'''
{{header}}
{{article
|author= Michael Boyer O'Leary,Jonathon N. Cummings,
|source= MIS QUARTERLY
|year= 2007
|abstract = As organizations operate across greater distances, scholars are increasingly interested in the work of geographically dispersed teams and the technologies that they use to communicate and coordinate their work. However, research has generally not specified the dimensions (spatial, temporal, or configurational) and degrees of team dispersion, nor has it articulated the theoretical connections between those dimensions and important team outcomes. This research essay expands upon previous field and lab studies of dispersed teamwork by presenting a new conceptualization of dispersion as a continuous, multidimensional construct, in which each dimension is theoretically linked with different outcomes. We illustrate this new conceptualization with a series of examples from real dispersed teams and present implications for research regarding technology use.
|keyword = geographically dispersed teams,virtual teams,dispersion,distance,configuration,technology use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding mindshift learning: The transition to object-oriented development'''
{{header}}
{{article
|author= Deborah J. Armstrong,Bill C. Hardgrave,
|source= MIS QUARTERLY
|year= 2007
|abstract = Information systems professionals increasingly face changes in their work environment. Some of these changes are incremental, but many require fundamental shifts in mindset (referred to as a mindshift). Within the domain of software development, previous research has determined that veteran developers experience difficulty making the transition to new forms of development. Although prior research has brought awareness to the problems caused by a mindshift and has provided some insight, it has not answered the question of why software developers have difficulty making the transition. This study begins to answer that question by positing and examining the mindshift learning theory (MLT). The MLT suggests that the degree of perceived novelty of the fundamental concepts that characterize the new mindset will impact learning. Specifically, concepts may be perceived as novel (i.e., not familiar to the learner), changed (i.e., similar to a known concept, but a different meaning in the new context), or carry over (i.e., known concept with a similar meaning in the new context). As an exemplar mindshift learning situation, this study explores the phenomenon in the context of software developers transitioning from traditional to object oriented (OO) software development. Findings indicate that software developers had higher knowledge scores on the OO concepts they perceived as novel or carryover compared to those they perceived as changed. Thus, developers experienced detrimental interference from their existing traditional software development knowledge structure when trying to learn OO software development. The findings have implications for organizations and individuals as an understanding of mindshifts could mean an easier transition through decreased frustration and a more effective learning process.
|keyword = software development,IS personnel,personnel training,learning theory,object-oriented,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of presentation formats and task complexity on online consumers' product understanding'''
{{header}}
{{article
|author= Zhenhui (Jack) Jiang,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2007
|abstract = Two constructs used to measure product understanding performance are actual product knowledge and perceived website diagnosticity (i.e., the extent to which consumers believe a website is helpful for them to understand products). The experimental results show that (1) both videos and VPE lead to higher perceived website diagnosticity than static pictures; (2) under a moderate task complexity condition, VPE and videos lead to the same level of actual product knowledge, but all are more effective than static pictures; (3) under a high task complexity condition, all four presentation formats are equally effective in terms of actual product knowledge. Moreover, the results also indicate that it is perceived website diagnosticity, not actual product knowledge, that affects the perceived usefulness of websites, which further influences consumers' intentions to revisit the websites. This study assesses and compares four product presentation formats currently used online: static pictures, videos without narration, videos with narration, and virtual product experience (VPE), where consumers are able to virtually feel, touch, and try products. The effects of the four presentation formats on consumers' product understanding as well as the moderating role of the complexity of product understanding tasks were examined in a laboratory experiment.
|keyword = product presentation,task complexity,virtual product experience,product understanding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A task-based model of perceived website complexity'''
{{header}}
{{article
|author= Sucheta Nadkarni,Reetika Gupta,
|source= MIS QUARTERLY
|year= 2007
|abstract = In this study, we propose that perceived website complexity (PWC) is central to understanding how sophisticated features of a website (such as animation, audio, video, and rollover effects) affect a visitor's experience at the site. Although previous research suggests that several elements of perceived complexity (e.g., amount of text, animation, graphics, range and consistency of webpages configuring a website, ease of navigating through it, and clarity of hyperlinks) affect important user outcomes, conflicting results yielded by previous research have created an important debate: Does complexity enhance or inhibit user experience at a website? In this study, we draw on the task complexity literature to develop a broad and holistic model that examines the antecedents and consequences of PWC. Our results provide two important insights into the relationship between PWC and user outcomes. First, the positive relationship between objective complexity and PWC was moderated by user familiarity. Second, online task goals (goal-directed search and experiential browsing) moderated the relationship between PWC and user satisfaction. Specifically, the relationship between PWC and user satisfaction was negative for goal-directed users and inverted-U for experiential users. The implications of this finding for the practice of website design are discussed.
|keyword = perceived website complexity,user perception,website usability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enhancing information retrieval through statistical natural language processing: A study of collocation indexing'''
{{header}}
{{article
|author= Ofer Arazy,Carson Woo,
|source= MIS QUARTERLY
|year= 2007
|abstract = In this paper, we provide preliminary evidence for the usefulness of statistical natural language processing (NLP) techniques, and specifically of collocation indexing, for IR in general settings. We investigate the effect of three key parameters on collocation indexing performance: directionality, distance, and weighting. We build on previous work in IR to (1) advance our knowledge of key design elements for collocation indexing, (2) demonstrate gains in retrieval precision from the use of statistical NLP for general-settings IR, and, finally, (3) provide practitioners with a useful cost benefit analysis of the methods under investigation. Although the management of information assets-specifically, of text documents that make up 80 percent of these assets an provide organizations with a competitive advantage, the ability of information retrieval (IR) systems to deliver relevant information to users is severely hampered by the difficulty of disambiguating natural language. The word ambiguity problem is addressed with moderate success in restricted settings, but continues to be the main challenge for general settings, characterized by large, heterogeneous document collections.
|keyword = document management,information retrieval (IR),word ambiguity,natural language processing (NLP),collocations,distance,directionality,weighting,general settings,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technologies in business: A blueprint for education and research'''
{{header}}
{{article
|author= Vasant Dhar,Arun Sundararajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = How are business schools thinking about developing leaders for the emerging digital economy? Is there a set of core principles we can apply to thinking about the enabling potential of information technologies and their consequences for business and society? We present a business-centric framework and a technologycentric framework that together form a blueprint for answering these questions. The business-centric framework articulates three compelling reasons why information technology (IT) matters in business: (1) IT continually transform industry and society, (2) executive decisions about IT investments, governance, and strategy are critical to organizational success, and (3) deriving value from increasingly available data trails defines effective decision making in the digital economy. However, our conversations with the leadership of 45 business schools and our subsequent data indicate that business schools are challenged by effectively trail-ling future executives to think about these reasons and act on them as part of a forward-looking program of business education that is grounded in stable concepts. In response, the technology-centric framework provides a set of grounding concepts and stable principles about IT that have emerged over the last four decades, and leads to a natural set of consequences that can inform thinking about IT in business. We illustrate how these complementary frameworks-business and technology-can be combined to frame an educational program by outlining a set of key questions, by placing these questions in the context suggested by our frameworks, and by providing guidelines toward answering them. These questions also define a natural path for future research about IT in business and society that will lead to stronger intellectual foundations for the field and define future education that is better grounded in concepts and theories that emerge from academic research.
|keyword = IT strategy,corporate strategy,IT investment,education,electronic commerce,business transformation,disruptive technology,platform,business value,decision making,digital goods,network economics,social networks,MBA core,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Those to whom IT matters most: Perspectives of IT faculty on curricula, courses, and class materials'''
{{header}}
{{article
|author= Andrew McAfee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = This paper presents quantitative and qualitative results from a conference on IT teaching held in May of 2006 in Boston. Participants completed a survey in advance, and the conference consisted of presentations and interactive panel discussions. The conference revealed both heterogeneity and convergence across participants' course offerings, and grounds for both optimism and concern about the health and future of IT curricula within business schools. This paper highlights these tensions, synthesizes and extends data and discussions from the conference, and suggests open questions for faculty who teach IT.
|keyword = education,teaching,core curriculum,elective curriculum,teaching cases,theories,framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product development and pricing strategy for information goods under heterogeneous outside opportunities'''
{{header}}
{{article
|author= Ying-Ju Chen,Sridhar Seshadri,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = This paper considers a two-stage development problem for information goods with costless quality degradation. In our model, a seller of information goods faces customers that are heterogeneous with regard to both the marginal willingness to pay for quality and the outside opportunity. In the development stage, the seller determines the quality limit of the product. In the second stage, the seller's problem is to design the price schedule corresponding to different quality levels, taking into account production and distribution costs. We show that versioning is optimal for the seller when customers have multiple outside options or, more generally, convex reservation utilities. In addition, we show that in the optimal solution the seller discards both low-end and high-end customers. Among those that are served, the seller offers a continuum of (inferior) versions to customers with relatively low willingness to pay, and extracts full information rent from each of them. A common version with the quality limit is offered to the rest. We further prove that the seller should offer a single version when reservation utilities are either concave or linear. Through numerical experiments, we study the sensitivity of our results to changes in the cost structure and customer utilities.
|keyword = versioning,quality degradation,price discrimination,information goods,heterogeneous outside opportunities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information system use-related activity: an expanded Behavioral conceptualization of individual-level information system use'''
{{header}}
{{article
|author= Henri Barki,Ryad Titah,Celine Boffo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Despite calls for improving current approaches to conceptualizing and measuring the construct of information system use, theoretical advances in this regard are still insufficient. The present paper proposes to expand the focus of existing conceptualizations that exclusively focus on technology interaction behaviors via the construct of IS use-related activity. Based on task-technology fit and activity theory, IS use-related activity is conceptualized as a second-order aggregate construct that comprises both technology interaction behaviors, as well as activities users undertake to adapt the task-technology-individual system. A multiple-indicators and multiple-causes analysis of data collected from 190 users in 21 organizations is found to support the proposed conceptualization.
|keyword = IS use,IS implementation,user adaptation,user learning,formative constructs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Comparing IT workers' compensation across country contexts: Demographic, human capital, and institutional factors'''
{{header}}
{{article
|author= Natalia Levina,Mingdi Xin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = As the IT workforce becomes global, it is increasingly important to understand the factors affecting IT workers' compensation in labor markets distributed across the globe. Ang et al. (2002) published the first in-depth analysis of compensation for IT professionals juxtaposing human capital endowment (education and experience) and institutional determinants (firm's size, industry, and sector) of compensation in the Singaporean economy. In this paper, we explore the influence of particular national economies on IT workers' compensation. We draw on research into the roots of wage differentials in labor economics and build on the Ang et al. research to present a multilevel analysis of IT workers' compensation in the United States, analyzing the U.S. Bureau of Labor Statistics' Current Population Survey (CPS) data for 1997, 2001, and 2003. We find that, while institutional differences in Singapore mattered only in conjunction with individual factors, in the U.S. institutional differences had a direct effect on IT workers' wages. As tightness of IT labor supply decreased in the United States in the early 2000s, the influence of a firm's size on wages became more pronounced. Also, female IT workers and workers without a college degree fared worse than their male and college-educated counterparts as the IT job market slowed down. We suggest that factors such as presence of job search friction, diversity in the educational system, geographical differences in cost of living, labor mobility, and shortages in IT labor supply vis-A-vis demand help explain the differences among countries. We conclude by outlining the implications of these findings for IT workers, firms, and policy makers.
|keyword = management of IT human resources,compensation,hierarchical linear modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Statistical power in analyzing interaction effects: Questioning the advantage of PLS with product indicators'''
{{header}}
{{article
|author= Dale Goodhue,William Lewis,Ronald Thompson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = A significant amount of information systems (IS) research involves hypothesizing and testing for interaction effects. Chin et al. (2003) completed an extensive experiment using Monte Carlo simulation that compared two different techniques for detecting and estimating such interaction effects: partial least squares (PLS) with a product indicator approach versus multiple regression with summated indicators. By varying the number of indicators for each construct and the sample size, they concluded that PLS using product: indicators was better (at providing higher and presumably more accurate path estimates) than multiple regression using summated indicators. Although we view the Chin et al. (2003) study as an important step in using Monte Carlo analysis to investigate such issues, we believe their results give a misleading picture of the efficacy of the product indicator approach with PLS. By expanding the scope of the investigation to include statistical power, and by replicating and then extending their work, we reach a different conclusion-that although PLS with the product indicator approach provides higher point estimates of interaction paths, it also produces wider confidence intervals, and thus provides less statistical power than multiple regression. This disadvantage increases with the number of indicators and (up to a point) with sample size. We explore the possibility that these surprising results can be explained by capitalization on chance. Regardless of the explanation, our analysis leads us to recommend that if sample size or statistical significance is a concern, regression or PLS with product of the sums should be used instead of PLS with product indicators for testing interaction effects.
|keyword = interaction effects,moderator effects,regression,PLS,product indicators,statistical power,statistical accuracy,Monte Carlo simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Fact or fiction? A sensemaking perspective on the reality behind executives' perceptions of IT business value'''
{{header}}
{{article
|author= Paul P. Tallon,Kenneth L. Kraemer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Although research has made significant strides in recent years in evaluating the performance impacts from information technology (IT), a dearth of easily accessible objective measures, particularly at the process level, continues to limit IT research. Suggestions that researchers use perceptual measures instead are met with claims that the biased nature of perceptions renders them imperfect proxies for the true extent of IT impacts. In this paper, we use sensemaking theory to explore this claim. We outline a model relating what executives notice about process-level IT impacts with sensemaking-based perceptions of IT impacts at the firm level, and firm performance as the ultimate arbiter of perceptual accuracy. Estimating the model with survey data from executives in 196 firms, we find that executives' perceptions are more fact than fiction. While perceptions are not a perfect proxy for hard-to-find objective measures, perceptual accuracy should stimulate greater consideration of executives' perceptions in future IT business value research.
|keyword = executive perceptions,IT business value,IT organizational impacts,IT value measurement,objective measures,perceptual measures,PLSGraph,process orientation,sensemaking,value chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Buyers' adoption and use of B2B electronic marketplaces: Efficiency- and legitimacy-oriented perspectives'''
{{header}}
{{article
|author= Jai-Yeol Son,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Despite the significant opportunities to transform the way that organizations conduct trading activities, few studies have investigated the impetus for organizational strategic moves toward business-to-business (13213) electronic marketplaces. Drawing on transaction cost theory and institutional theory, this paper identifies two groups of factors-efficiency- and legitimacy-oriented factors, respectively-that can influence organizational buyers'initial adoption of, and the level of participation in, B2B e-marketplaces. The effects of these factors on initial adoption of and participation level in 13213 e-marketplaces are empirically tested with data collected, respectively, from 98 potential adopter and 85 current adopter organizations. The results of a partial least squares analysis of the data indicate that the two groups of factors exhibit different patterns in explaining initial adoption in the preadoption period and participation level in the postadoption period. Specifically, all three of the efficiency-oriented factors investigated in this study-product characteristics, demand uncertainty, and market volatility-and their subconstructs exhibit a significant influence on adoption intent or participation level, or both. The results demonstrate that two legitimacy-oriented factors-mimetic pressures and normative pressures-and their subconstructs have a significant impact on adoption intent, but not on participation level. Our findings also indicate that clearly different patterns exist between the two groups of factors in explaining adoption intent and participation level.
|keyword = B2B electronic marketplaces,e-commerce,institutional theory,interorganizational information systems,organizational adoption and use,transaction cost theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of ERP implementation on business process outcomes: A factor-based study'''
{{header}}
{{article
|author= Jahangir Karimi,Toni M. Somers,Anol Bhattacherjee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Failures in large-scale information technology implementation are abundantly documented in the practitioner literature. In this study, we examine why some firms benefit more from enterprise resource planning (ERP) implementation than others. We look at ERP implementation from a technological diffusion perspective, and investigate under what contextual conditions the extent of ERP implementation has the greatest effect on business process outcomes. Using empirical data, we find that the extent of ERP implementation influences business process outcomes, and both ERP radicalness and delivery system play moderating roles. For information systems (IS) practice, this study helps managers direct their attention to the most promising factors, provides insights into how to manage their complex interactions, and elaborates on their differential effects on business process outcomes. For IS research, it integrates innovation diffusion theory into our current knowledge of ERP implementation and provides theoretical explanations for ERP implementation failures.
|keyword = business process outcomes,enterprise systems,ERP effects,ERP implementation,ERP radicalness,information technology innovation,innovation diffusion theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Team knowledge and coordination in geographically distributed software development'''
{{header}}
{{article
|author= J. Alberto Espinosa,Sandra A. Slaughter,Robert E. Kraut,James D. Herbsleb,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Coordination is important in software development because it leads to benefits such as cost savings, shorter development cycles, and better-integrated products. Team cognition research suggests that members coordinate through team knowledge, but this perspective has only been investigated in real-time collocated tasks and we know little about which types of team knowledge best help coordination in the most geographically distributed software work. In this field study, we investigate the coordination needs of software teams, how team knowledge affects coordination, and how this effect is influenced by geographic dispersion. Our findings show that software teams have three distinct types of coordination needs-technical, temporal, and process-and that these needs vary with the members' role; geographic distance has a negative effect on coordination, but is mitigated by shared knowledge of the team and presence awareness; and shared task knowledge is more important for coordination among collocated members. We articulate propositions for future research in this area based on our analysis.
|keyword = coordination,global software development,management of the information technology (IT) function,team knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Postimplementation knowledge transfers to users and information technology professionals'''
{{header}}
{{article
|author= Radhika Santhanam,Larry Seligman,David Kang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Although there is substantial research on learning that occurs before adoption of a new information system, there is a dearth of research on postimplementation learning when a new system is assimilated as a routine element of users'work. Hence, during the postimplementation period of a bank's new work flow system, we conducted a longitudinal participant observation study to observe knowledge transfers of users and information technology (IT) professionals assigned to a help desk. We found that although users turned to IT professionals to obtain knowledge related to conceptual understanding and procedures to use the system, they most often turned to other users to obtain knowledge that allowed them to adapt the system to their work. IT professionals, on the other hand, often turned to their colleagues to obtain knowledge that helped them modify the system to emerging innovative uses. These patterns of knowledge transfers can be explained based upon source expertise. Our findings indicate that organizations must sustain designated sources of knowledge such as help desks, but must also establish conduits for users to acquire knowledge from other users and develop innovative uses of the system. A substantial amount of critical knowledge transfers relevant to system adaptation occurred during face-to-face discussions between users and IT professionals, and therefore future research should examine how this would be affected by the outsourcing of technical support functions.
|keyword = IT professionals,knowledge transfers,learning,postimplementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Broken ties: The impact of organizational restructuring on the stability of information-processing networks'''
{{header}}
{{article
|author= Dowan Kwon,Wonseok Oh,Sangyong Jeon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Information-processing networks (IPNs) denote dynamic network-based information-processing structures that operate as coordination mechanisms that transcend formal hierarchies. Despite growing interest in information technology-enabled IPNs, the literature has been silent in exploring the various ontological structures of IPNs and the structural efficiency embedded in each IPN, especially in the event of radical organizational changes. To fill this gap, this study identifies, from the perspective of graph theory, four ontological IPN archetypes that can serve as blueprints for information processing within and across organizations-random, small world, moderate scale free (MSF), and Barabasi. We then assess how each structure reacts to corporate restructuring (e.g., downsizing) and investigate, based on computer simulation, the extent to which each structure preserves a worker's efficiency and the stability of the network structure in the event of downsizing. Two moderating variables are included in the model-that is, scale of downsizing and the reconnection strategy in the presence of downsizing. In this study, downsizing is viewed not only as the simple elimination of individual workers but also as the elimination of the communication and information-processing conduits necessary for effective communication and coordination. We find that when firms implement a relatively small-scale workforce reduction, centralized coordination structures such as MSF and Barabasi are generally more resilient and facilitate better coordination. However, when the downsizing strategy involves massive and severe layoffs, decentralized coordination structures such as random and small world are more durable, and tend to provide a stronger safety net, irrespective of the strategies employed to create new ties. Although this study focused exclusively on the context of downsizing, the results of the study have important implications for other types of organizational restructuring (e.g., organizational expansion and merger and acquisition) that reconfigure IPNs.
|keyword = computer simulation,information-processing networks,network theory,organizational restructuring,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A strategic analysis of competition between open source and proprietary software'''
{{header}}
{{article
|author= Ravi Sen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = This paper analyzes a software market consisting of a freely available open source software (OSS), the commercial version of this OSS (OSS-SS), and the competing commercial proprietary software (PS). We find that in software markets characterized by low direct network benefits, the PS vendor is better off in the presence of competition from OSS-SS. Furthermore, the OSS-SS vendor in these markets is better off by having lower usability than PS. Therefore, the PS vendor has little incentive to improve the usability of their software in these markets. On the other hand, in software markets characterized by high network benefits, a PS vendor is threatened by the presence of OSS-SS and can survive only if the PS is more usable than the competing OSS-SS.
|keyword = commercial open source,economics of open source,FLOSS,open source software,software competition,software market,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A comparison of transaction cost, agency, and knowledge-based predictors of IT outsourcing decisions: A US-Japan cross-cultural field study'''
{{header}}
{{article
|author= Amrit Tiwana,Ashley A. Bush,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = As outsourcing evolves into a competitive necessity, managers must increasingly contend with the decision about which software development projects to outsource. Although a variety of theories have been invoked to study the initial outsourcing decision, much of this work has relied in isolation on one theoretical perspective. Therefore, the relative importance ascribed by managers to the factors from these theories is poorly understood. The majority of this work also masks interesting insights into outsourcing decisions by focusing on the information technology (IT) function rather than individual projects as the unit of analysis, where many of these decisions occur. In contrast, prior research at the project level has focused on predicting development performance in the postoutsourcing-decision phases of projects. The objective of this study is to examine the relative importance that IT managers ascribe to various factors from three complementary theories-transaction cost economics, agency theory, and knowledge-based theory-as they simultaneously consider them in their project outsourcing decisions. A secondary objective is to assess the cross-cultural robustness (United States versus Japan in this study) of such models in predicting project-level IT outsourcing decisions. We develop and test a multitheoretic model using data on 1,008 project-level decisions collected from 33 Japanese and 55 U.S. managers. Overall, our results provide novel insights into the relative importance that managers ascribe to the factors from these three theories, their complementarities and occasional contradictions, and offer new insights into the differences among U.S. and Japanese IT managers. Implications for theory and practice are also discussed.
|keyword = agency theory,conjoint study,IT sourcing,Japanese software,knowledge-based theory,knowledge management,outsourcing,subcontracting,transaction cost economics,vendor selection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of technostress on role stress and productivity'''
{{header}}
{{article
|author= Monideepa Tarafdar,Qiang Tu,Bhanu S. Ragu-Nathan,T. S. Ragu-Nathan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Based on empirical survey data, this paper uses concepts from sociotechnical theory and role theory to explore the effects of stress created by information and computer technology (ICT)-that is, "technostress"-on role stress and on individual productivity. We first explain different ways in which ICTs can create stress in users and identify factors that create technostress. We next propose three hypotheses: (1) technostress is inversely related to individual productivity, (2) role stress is inversely related to individual productivity, and (3) technostress is directly related to role stress. We then use structural equation modeling on survey data from ICT users in 223 organizations to test the hypotheses. The results show support for them. Theoretically, the paper contributes in three ways. First, the different dimensions of technostress identified here add to existing concepts on stress experienced by individuals in organizations. Second, by showing that technostress inversely affects productivity, the paper reinforces that failure to manage the effects of ICT-induced stress can offset expected increases in productivity. Third, validation of the positive relationship between technostress and role stress adds a new conceptual thread to literature analyzing the relationship between technology and organizational roles and structure. In the practical domain, the paper proposes a diagnostic tool to evaluate the extent to which technostress is present in an organization and suggests that the adverse effects of technostress can be partly countered by strategies that reduce role conflict and role overload.
|keyword = human resource management,role conflict,role overload,role stress,role theory,structural equation modeling,survey methods,technostress,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Intrusion prevention in information systems: Reactive and proactive responses'''
{{header}}
{{article
|author= Wei T. Yue,Metin Cakanyildirim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Intrusion prevention requires effective identification of and response to malicious events. In this paper, we model two important managerial decisions involved in the intrusion prevention process: the configuration of the detection component, and the response by the reaction component. The configuration decision affects the number of alarms the firm has to investigate. It is well known that the traditional intrusion detection system generates too many false alarms. The response decision determines whether alarms are going to be investigated or rejected outright. By jointly optimizing these two decision variables, a firm may apply different strategies in protecting its informational assets: slow but accurate, rapid but inaccurate, or a mixture of the two strategies. We use the optimal control approach to study the problem. Unlike previous literature, which studied the problem with a static model, in our model, the decision on balancing the desire to detect all malicious events with the opportunity costs required to do so is time dependent. Furthermore, we show how the choice of an optimal mixture of reactive and proactive responses depends on the values of cost parameters and investigation rate parameters. We find that in our model, a high damage cost does not immediately translate to a preference of proactive response, or a high false rejection cost does not translate to a preference of reactive response. The dynamics of the problem, such as how fast alarms accumulate and how fast they can be cleared, also affect the decisions.
|keyword = information security,intrusion detection,intrusion prevention,intrusion response,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The contingent effects of training, technical complexity, and task interdependence on successful information systems implementation'''
{{header}}
{{article
|author= Rajeev Sharma,Philip Yetton,
|source= MIS QUARTERLY
|year= 2007
|abstract = Research has investigated the main effect of training on information systems implementation success. However, empirical support for this model is inconsistent. We propose a contingent model in which the effect of training on IS implementation success is a function of technical complexity and task interdependence. A meta-analysis of the literature finds strong support for the model, explaining the inconsistent findings reported in the literature. Implications for theory and practice are discussed
|keyword = training,IS implementation,task interdependence,technical complexity,meta-analysis,IS success,transactive memory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Communication media repertoires: Dealing with the multiplicity of media choices'''
{{header}}
{{article
|author= Mary Beth Watson-Manheim,France Belanger,
|source= MIS QUARTERLY
|year= 2007
|abstract = In today's organizations, employees have an ever-increasing variety of communication media to use in the performance of work activities. In this study, we seek to expand our understanding of media usage in organizations where there is a multiplicity of communication media available to employees. We use communication media repertoires as the lens through which we explore how media is used in the support of communication-based work performed by individuals in complex organizational settings. Data were collected in sales divisions at two large corporations in the information technology industry. We compared multiple media use within and between the two sales divisions, and identified similarities and differences in repertoires. Our findings suggest that use of repertoires is influenced by institutional conditions (e.g., incentives, trust, and physical proximity) and situational conditions (e.g., urgency, task, etc.), and by routine use of the media overtime. Based on the findings, we propose a frame work for investigating the use of multiple media in organizations through examination of communication media repertoires. Implications of these findings for research and practice are discussed.
|keyword = distributed work teams,computer-mediated communication,communication media choice,communication media repertoire,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Power, rationality, and the art of living through socio-technical change'''
{{header}}
{{article
|author= Chrisanthi Avgerou,Kathy McGrath,
|source= MIS QUARTERLY
|year= 2007
|abstract = Most information systems research takes for granted the assumption that IS practice and associated organizational change can be effectively understood as a process of technical reasoning and acting governed by a mix of concerns about software construction, administrative control, and economic gain. Its mission has been to empower managers, IS engineers, and information and communication technology users with knowledge and techniques for effective decision making. However, empirical research frequently encounters human activity that is at odds with the assumed pattern of rational behavior. Recent work tries to explain behavior in IS and organizational change in terms of social processes rather than as a consideration of rational techniques of professional practice. In this paper we address this ambivalence of the IS field with regard to technical/rational knowledge and practice. We draw from the theoretical work of Michel Foucault on power/knowledge and the aesthetics of existence to argue that the rational techniques of IS practice and the power dynamics of an organization and its social context are closely intertwined, requiring each other to be sustained. Furthermore, we develop a context-specific notion of rationality in IS innovation, through which interested parties judge the value of an innovation for their lives and consequently support or subvert its course. We demonstrate these ideas with a case study of a social security organization in Greece.
|keyword = rationality,IS innovation,power/knowledge,regime of truth,aesthetics of existence,techniques of the self,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Developing health information systems in developing countries: The flexible standards strategy'''
{{header}}
{{article
|author= Jorn Braa,Ole Hanseth,Arthur Heywood,Woinshet Mohammed,Vincent Shaw,
|source= MIS QUARTERLY
|year= 2007
|abstract = The development of appropriate integrated and scalable information systems in the health sector in developing countries has been difficult to achieve, and is likely to remain elusive in the face of continued fragmented funding of health programs, particularly related to the HIV/AIDS epidemic. In this article, we propose a strategy for developing information infrastructures in general and in particular for the health care sector in developing countries. We use complexity science to explain the challenges that need to be addressed, in particular the need for standards that can adapt to a changing health care environment, and propose the concept of flexible standards as a key element in a sustainable infrastructure development strategy. Drawing on case material from a number of developing countries, a case is built around the use of flexible standards as attractors, arguing that if they are well defined and simple, they will be able to adapt to the frequent changes that are experienced in the complex health environment. A number of paradoxes are highlighted as useful strategies, integrated independence being one that encourages experimentation and heterogeneity to develop and share innovative solutions while still conforming to simple standards. The article provides theoretical concepts to support standardization processes in complex systems, and to suggest an approach to implement health standards in developing country settings that is sensitive to the local context, allows change to occur through small steps, and provides a mechanism for scaling information systems.
|keyword = health information systems,standards,complexity science,developing countries,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Telemedicine in the Upper Amazon: Interplay with local health care practices'''
{{header}}
{{article
|author= Gianluca Miscione,
|source= MIS QUARTERLY
|year= 2007
|abstract = This article is based on the introduction of a telemedicine system in the jungles of northeastern Peru. The system was designed by a European consortium led by a Spanish polytechnic in cooperation with two universities in Lima and the Peruvian Ministry of Health. The purpose of the system was to improve health conditions by extending science-based medicine into a region with well-established traditional healing practices. The central analytical focus of this article is on the interplay between the public health care system, which used the telemedicine system, and local health care practices. The manner in which scientific medicine was delivered through information technology and public health care services is analyzed in terms of the health personnel's activity, the local population's conceptions of health, and the trajectories followed by patients seeking recovery. The author participated in the design of the second evaluation of the telemedicine system and acted as a participant observer in the regional hospital and peripheral clinics. In addition to interviewing health care staff from the study area, the author also met with traditional healers, and patients in the districts whether or not they were involved in the telemedicine project. New institutional theory provided the analytical framework for the interpretation of the observed behavior of the public healthcare staff traditional healers, and potential patients. Empirically, this study describes the informal aspects of the functioning of the telemedicine system, and its partial mismatch with the definitions of health and illness employed by local communities and healers. An argument is made that people's construction of their health, which is embedded in their normal patterns of action, should be identified, and then considered in the design, implementation, and evaluation of future telemedicine projects. This article problematizes an approach to telemedicine-based health development that is weakly accountable to local social context and their diversity.
|keyword = telemedicine,developing countries,new institutionalism,ethnomethodology,accountability,Amazon,healing practices,knowledge transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decision-centric active learning of binary-outcome models'''
{{header}}
{{article
|author= Maytal Saar-Tsechansky,Foster Provost,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = It can be expensive to acquire the data required for businesses to employ data-driven predictive modeling-for example, to model consumer preferences to optimize targeting. Prior research has introduced "active-learning" policies for identifying data that are particularly useful for model induction, with the goal of decreasing the statistical error for a given acquisition cost (error-centric approaches). However, predictive models are used as part of a decision-making process, and costly improvements in model accuracy do not always result in better decisions. This paper introduces a new approach for active data acquisition that specifically targets decision making. The new decision-centric approach departs from traditional active learning by placing emphasis on acquisitions that are more likely to affect decision making. We describe two different types of decision-centric techniques. Next, using direct-marketing data, we compare various data-acquisition techniques. We demonstrate that strategies for reducing statistical error can be wasteful in a decision-making context, and show that one decision-centric technique in particular can improve targeting decisions significantly. We also show that this method is robust in the face of decreasing quality of utility estimations, eventually converging to uniform random sampling, and that it can be extended to situations where different data acquisitions have different costs. The results suggest that businesses should consider modifying their strategies for acquiring information through normal business transactions. For example, a firm such as Amazon.com that models consumer preferences for customized marketing may accelerate learning by proactively offering recommendations-not merely to induce immediate sales, but for improving recommendations in the future.
|keyword = decision-support systems,active learning,classifier induction,decision making,predictive modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Releasing individually identifiable microdata with privacy protection against Stochastic threat: An application to health information'''
{{header}}
{{article
|author= Robert Garfinkel,Ram Gopal,Steven Thompson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = The ability to collect and disseminate individually identifiable microdata is becoming increasingly important in a number of arenas. This is especially true in health care and national security, where this data is considered vital for a number of public health and safety initiatives. In some cases legislation has been used to establish some standards for limiting the collection of and access to such data. However, all such legislative efforts contain many provisions that allow for access to individually identifiable microdata without the consent of the data subject. Furthermore, although legislation is useful in that penalties are levied for violating the law, these penalties occur after an individual's privacy has been compromised. Such deterrent measures can only serve as disincentives and offer no true protection. This paper considers security issues involved in releasing microdata, including individual identifiers. The threats to the confidentiality of the data subjects come from the users possessing statistical information that relates the revealed microdata to suppressed confidential information. The general strategy is to recode the initial data, in which some subjects are "safe" and some are at risk, into a data set in which no subjects are at risk. We develop a technique that enables the release of individually identifiable microdata in a manner that maximizes the utility of the released data while providing preventive protection of confidential data. Extensive computational results show that the proposed method is practical and viable and that useful data can be released even when the level of risk in the data is high.
|keyword = data security,privacy,health information,optimization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competition among virtual communities and user valuation: The case of investing-related communities'''
{{header}}
{{article
|author= Bin Gu,Prabhudev Konana,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Virtual communities are a significant source of information for consumers and businesses. This research examines how users value virtual communities and how virtual communities differ in their value propositions. In particular, this research examines the nature of trade-offs between information quantity and quality, and explores the sources of positive and negative externalities in virtual communities. The analyses are based on more than 500,000 postings collected from three large virtual investing-related communities (VICs) for 14 different stocks over a period of four years. The findings suggest that the VICs engage in differentiated competition as they face trade-offs between information quantity and quality. This differentiation among VICs, in turn, attracts users with different characteristics. We find both positive and negative externalities at work in virtual communities. We propose and validate that the key factor that determines the direction of network externalities is posting quality. The contributions of the study include the extension of our understanding of the virtual community evaluation by users, the exposition of competition between virtual communities, the role of network externalities in virtual communities, and the development of an algorithmic methodology to evaluate the quality (noise or signal) of textual data. The insights from the study, provide useful guidance for design and management of VICs.
|keyword = network economics,computer-mediated communication and collaboration,virtual communities,IT diffusion and adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of international information technology transfer on national productivity'''
{{header}}
{{article
|author= Jungsoo Park,Seung Kyoon Shin,G. Lawrence Sanders,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = Researchers have widely postulated that the adoption of information technology (IT) products enhances global competitiveness and production efficiency as successful technological innovation replaces and improves traditional inputs and modes of production. This study suggests that when IT products are traded across borders, IT investment in an economy has a positive influence on the productivity of its import partner country. We provide empirical evidence for the positive effect of global IT diffusion on productivity through international trading of IT products. The results show a positive effect of foreign IT transfer on the recipient country's productivity. In addition, we find that the effect of transferred IT is only significant when the source country is an IT-intensive or hi-tech export country. The results and implications are robust, even controlling for other important factors such as openness, innovative capacity, and IT infrastructure in addition to the transferred IT. Finally, a panel cointegration test-a recently developed advanced econometric method-is used to address the common problems of spurious relations that arise in regressions with nonstationary time-series data.
|keyword = information technology,knowledge transfer,openness,innovative capacity,IT infrastructure,national productivity,international IT diffusion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents and consequences of Internet use in procurement: An empirical investigation of US manufacturing firms'''
{{header}}
{{article
|author= Abhay Nath Mishra,Prabhudev Konana,Anitesh Barua,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2007
|abstract = This paper examines the antecedents and consequences of Internet use in the procurement process. Drawing upon the resource-based view (RBV) of the firm and the technology, organization, and environment framework, we develop an integrative model that examines the antecedents and consequences of Internet use in two stages-the search stage and the order initiation and completion (OIC) stage-of the procurement process. The model enables us to deconstruct both the usage and the performance aspects of information technology (IT) in business processes, and to provide insights into the enablers of use and business value. The model is estimated with survey data from 412 firms. Our results suggest that while some resources, such as procurement-process digitization, influence Internet use in both the procurement stages, other resources, such as the diversity of organizational procurement knowledge, impact Internet use in only one stage. We also find that Internet use in the OIC stage has a more significant impact on procurement-process performance than use in search. This study extends the digital capabilities and firm performance literature in the context of electronic procurement. This study, also contributes to the small but emerging stream of literature that investigates antecedents, the extent, and implications of IT use holistically.
|keyword = electronic procurement,business value of IT use,resource-based view,procurement-process digitization,B2B electronic commerce,IT adoption and use,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Host country resource availability and information system control mechanisms in multinational corporations: An empirical test of resource dependence theory'''
{{header}}
{{article
|author= Madhu T. Rao,Carol V. Brown,William C. Perkins,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = The management of the information systems (IS) function is a complex task, particularly in the case of multinational corporations (MNCs), where installations dispersed across distance, time, and cultures can lead to diverse and incompatible systems spreading among foreign subsidiaries. The need to globally control and coordinate the IS management function is often met with resistance from local IS managers, who may perceive corporate standards as intrusive. Resource dependence theory (RDT) argues that control is made easier when a subsidiary unit is dependent on corporate headquarters for critical resources. This study examined the IS management relationship and the use of various mechanisms of control (formal and informal) between 54 headquarters-subsidiary pairs spread across 19 countries of varying resource-richness. While RDT appears to be valid when subsidiaries are dependent on MNC headquarters for resources, the expected relationship between the mechanisms and host country IS resource availability was not observed. Although there was a significant relationship with the use of informal mechanisms and IS resources, it was in the opposite direction to what would be expected by RDT.
|keyword = canonical correlation,control mechanisms,multinational,resource dependence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interoperability of e-government information systems: Issues of identification and data sharing'''
{{header}}
{{article
|author= Benoit Otjacques,Patrik Hitzelberger,Fernand Feltz,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = In business and government organizations, information systems often handle sensitive data about individuals and other organizations, using various kinds of identifiers. The growing cooperation of organizations results in the need to share and exchange such data. This collection and sharing, however, is affected by privacy concerns, and organizational and technical issues have to be solved and taken into account. This paper describes the results of an exploratory study in the government sector, focusing on the way public organizations manage identity-related data and the sharing of such data, either with other public agencies or with private organizations. Despite significant progress in harmonizing the legal and administrative provisions and technical standards in the European Union, there are still considerable cross-country differences regarding this subject. These differences-together with the growing mobility of goods, persons, and related data within the European Union-cause particular challenges for information systems in digital government in this region. After discussing and defining the key notions and methodology of the study, we present the status quo in 18 out of 25 EU member states and compare it to the results of a prior study by the same network done in 2001. Finally, we draw conclusions about identity management in cross-border contexts.
|keyword = data sharing,e-government,identifier,identity management,information sharing,interoperability,interorganizational information systems,privacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of individualism - Collectivism, social presence, and group diversity on group decision making under majority influence'''
{{header}}
{{article
|author= Dongsong Zhang,Paul Benjamin Lowry,Lina Zhou,Xiaolan Fu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Majority influence is the attempt by a majority of group members to impose their common position on group dissenters during group decision making. Because of globalization, the use of cross-cultural teams in group tasks is becoming increasingly common. The objective of this study was to investigate how national culture, social presence, and group diversity may affect majority influence in a group decision-making context. A total of 183 groups participated in a large-scale empirical experiment at multiple sites. The results show that the national culture of group minorities has a significant impact on majority influence and that the use of computer-mediated communication can reduce majority influence. The findings have both theoretical and practical implications for improving the outcome and the effectiveness of group decision making in cross-cultural environments.
|keyword = CMC,computer-mediated communication,culture,group decision making,group decision systems,group diversity,majority influence,social presence,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Genre combinations: A window into dynamic communication practices'''
{{header}}
{{article
|author= Carsten Osterlund,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = The notion of a genre system typically connotes sequences of interrelated communicative genres. This paper suggests that we can find other types of relationships among genres. Data from a field study in a large emergency room illustrate how doctors, nurses, and clerical staff routinely combine document genres not only in sequences but also in various accumulations achieved through proximity and movement. The combinations of genres add flexibility to the emergency room staff's genre use and allow them to employ individual genres for several purposes. The data allow us to explore how organizational members manage the tension between a need for continuity in communicative practices and a need for flexibility in managing a jumble of paper-based and digital information systems. In addition, it demonstrates how end users often tinker with genres' media and form in the process of altering combinations among specific genres.
|keyword = boundary objects,communication genres,documenting work,genre combinations,medical informatics,practice theory,qualitative methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On the measurement of ideation quality'''
{{header}}
{{article
|author= Bruce A. Reinig,Robert O. Briggs,Jr. Jay F. Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Ideation is a key step in organizational problem solving, so researchers have developed a variety of technological interventions for improving ideation quality, which we define as the degree to which an ideation activity produces ideas that are helpful in attaining a goal. In this paper, we examine the four measures typically used to evaluate ideation quality, including idea-count, sum-of-quality, average-quality, and good-idea-count, and discuss their validity and potential biases. An experimental study comparing three levels of social comparison was used to illustrate the differences in the ideation quality measures and revealed that research conclusions were dependent on the measure used. Based on our analysis of the measures and experimental results, we recommend that only good-idea-count be used as a measure to evaluate ideation treatments and call into question research that has based its findings on the other measures. Finally, we discuss implications for research and other potential approaches to evaluating ideation quality.
|keyword = construct validity,electronic brainstorming,group support systems,idea quality,idea quantity,ideation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Attention issues in spatial information systems: Directing mobile users' visual attention using augmented reality'''
{{header}}
{{article
|author= Frank Biocca,Charles Owen,Arthur Tang,Corey Bohil,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Knowledge of objects, situations, or locations in the environment can be productive, useful, or even life-critical for mobile augmented reality (AR) users. Users may need assistance with (1) dangers, obstacles, or situations requiring attention; (2) visual search; (3) task sequencing; and (4) spatial navigation. The omnidirectional attention funnel is a general purpose AR interface technique that rapidly guides attention to any tracked object, person, or place in the space. The attention funnel dynamically directs user attention with strong bottom-up spatial attention cues. In a study comparing the attention funnel to other attentional techniques such as highlighting and audio cueing, search speed increased by over 50 percent, and perceived cognitive load decreased by 18 percent. The technique is a general three-dimensional cursor in a wide array of applications requiring visual search, emergency warning, and alerts to specific objects or obstacles, or for three-dimensional navigation to objects in space.
|keyword = augmented reality,geospatial information system,location-based services,mobile computing,spatial information systems,visual attention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of product, market, and relationship characteristics on interorganizational system integration in manufacturer-supplier dyads'''
{{header}}
{{article
|author= Varun Grover,Khawaja A. Saeed,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = Firms are increasingly using collaborative systems to enhance supply-chain visibility. A key emphasis of these interorganizational systems (IOS) is to improve the coordination between buyers and suppliers through electronic integration. While such IOS integration is purportedly good, because it tightens linkages in the supply chain, it is not clear whether it is the best configuration under all conditions. A review of literature on adoption and use of electronic data interchange (EDI) systems (a type of IOS) shows that this issue has been examined from multiple theoretic perspectives. Researchers have examined how contingencies related to technology, organization, and environment shape EDI use. Limited attention has been directed toward understanding how conditions under which transactions are conducted impact the use of IOS. We argue that transactional characteristics are important antecedents to IOS integration and propose that demand uncertainty, complexity, market fragmentation, and market volatility capture key characteristics. These factors coupled with an open information-sharing environment are hypothesized to influence 10S integration. Data collected from the electronics industry is used to examine the research model. Results show that firms tend to deploy integrated 10S when complexity of the component is high, market fragmentation is low, and an open information-sharing environment exists. Thus, from a managerial perspective, IOS integration is the appropriate configuration under conditions of high product complexity and open information-sharing environment, but it precludes the firm from participating in the open market and gaining brokerage benefits.
|keyword = collaborative systems,electronic integration,interorganizational systems,supply chains,survey research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Recommendation agents for electronic commerce: Effects of explanation facilities on trusting beliefs'''
{{header}}
{{article
|author= Weiquan Wang,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = We empirically test the effects of explanation facilities on consumers' initial trusting beliefs concerning online recommendation agents (RAS). RAS provide online shopping advice based on user-specified needs and preferences. The characteristics of RAs that may hamper consumers' trust building in the RAS are identified, and the provision of explanation facilities is proposed as a knowledge-based approach to enhance consumers' trusting beliefs by dealing with these obstacles. This study examines the effects of three types of explanations about an RA and its use-how, why, and trade-off explanations-on consumers' trusting beliefs in an RA's competence, benevolence, and integrity. An RA was built as the experimental platform and a laboratory experiment was conducted. The results confirm the important role of explanation facilities in enhancing consumers' initial trusting beliefs and indicate that consumers' use of different types of explanations enhances different trusting beliefs: the use of how explanations increases their competence and benevolence beliefs, the use of why explanations increases their benevolence beliefs, and the use of trade-off explanations increases their integrity beliefs.
|keyword = decision support,decisional guidance,electronic commerce,explanations,recommendation agents,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An integrated model of consortium-based e-business standardization: Collaborative development and adoption with network externalities'''
{{header}}
{{article
|author= Kexin Zhao,Mu Xia,Michael J. Shaw,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2007
|abstract = E-business standards are critical for electronic interorganizational transactions. In many industries, firms develop e-business standards collaboratively in a standard consortium. They can choose to become a leading developer, a passive adopter, or a nonadopter. To capture firms' strategic choices at the development stage and the adoption stage, which are related due to the double-sided interactions between the two stages, we propose an integrated model of consortium-based e-business standardization. We find that firms' payoffs from standard adoption increase with the intrinsic value of the standard, but developers' benefits increase faster than passive adopters' benefits. The model examines the value of passive adopters to the standard development via network externalities, even though passive adopters do not contribute directly in the consortium. We find that passive adopters do not always exist. There are two possible equilibria for the endogenous formation of the developer network and the adopter network, one without passive adopters and one with passive adopters. How external conditions affect the endogenous formation of the consortium depends upon whether there are passive adopters in the equilibrium. Based on our analysis, we recommend strategies to e-business standard consortia to motivate firms' participation and enhance social welfare created by the standard.
|keyword = adoption,collaborative development,double-sided interactions,e-business standards,network externalities,standard consortia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT road warriors: Balancing work-family conflict, job autonomy, and work overload to mitigate turnover intentions'''
{{header}}
{{article
|author= Manju K. Ahuja,D. Harrison McKnight,Katherine M. Chudoba,Joey F. George,Charles J. Kacmar,
|source= MIS QUARTERLY
|year= 2007
|abstract = This study examines the antecedents of turnover intention among information technology road warriors. Road warriors are IT professionals who spend most of their workweek away from home at a client site. Building on Moore's (2000) work on turnover intention, this article develops and tests a model that is context-specific to the road warrior situation. The model highlights the effects of work family conflict and job autonomy, factors especially applicable to the road warrior's circumstances. Data were gathered from a company in the computer and software services industry. This study provides empirical evidence for the effects of work family conflict, perceived work overload, fairness of rewards, and job autonomy on organizational commitment and work exhaustion for road warriors. The results suggest that work family conflict is a key source of stress among IT road warriors because they have to juggle family and job duties as they work at distant client sites during the week. These findings suggest that the context of the IT worker matters to turnover intention, and that models that are adaptive to the work context will more effectively predict and explain turnover intention.
|keyword = turnover,turnover intention,IT personnel,road warrior,organizational commitment,work-family conflict,work overload,autonomy,fairness,work exhaustion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The value of privacy assurance: An exploratory field experiment'''
{{header}}
{{article
|author= Kai-Lung Hui,Hock Hai Teo,Sang-Yong Tom Lee,
|source= MIS QUARTERLY
|year= 2007
|abstract = This paper reports the results of an exploratory field experiment in Singapore that assessed the values of two types of privacy assurance: privacy statements and privacy seals. We collaborated with a local firm to host the experiment on its website with its real domain name, and the subjects were not informed of the experiment. Hence, the study provided afield observation of the subjects' behavioral responses toward privacy assurances. We found that (1) the existence of a privacy statement induced more subjects to disclose their personal information but that of a privacy seal did not; (2) monetary incentive had a positive influence on disclosure; and (3) information request had a negative influence on disclosure. These results were robust in other specifications that used alternative measures for some of our model variables. We discuss this study in relation to the extant privacy literature, most of which employs surveys and laboratory experiments for data collection, and draw related managerial implications.
|keyword = privacy assurance,field experiment,privacy statement,privacy seal,monetary incentive,information request,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The relationship between organizational culture and the deployment of systems development methodologies'''
{{header}}
{{article
|author= Juhani Iivari,Magda Huisman,
|source= MIS QUARTERLY
|year= 2007
|abstract = This exploratory study analyzes the relationship between organizational culture and the deployment of systems development methodologies. Organizational culture is interpreted in terms of the competing values model and deployment as perceptions of the support, use, and impact of systems development methodologies. The results show that the deployment of methodologies by IS developers is primarily associated with a hierarchical culture that is oriented toward security, order, and routinization. IT managers' critical attitudes of the deployment of methodologies in organizations with a strong rational culture (focusing on productivity, efficiency, and goal achievement) is also worth noting. Based on its empirical findings, the paper proposes a theoretical model to explain the impact of organizational culture on the deployment of systems development methodologies.
|keyword = systems development,software engineering,systems development methodology,organizational culture,competing values model,information systems developers,information technology managers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cognitive stopping rules for terminating information search in online tasks'''
{{header}}
{{article
|author= Glenn J. Browne,Mitzi G. Pitts,James C. Wetherbe,
|source= MIS QUARTERLY
|year= 2007
|abstract = Online search has become a significant activity in the daily lives of individuals throughout much of the world. The almost instantaneous availability of billions of web pages has caused a revolution in the way people seek information. Despite the increasing importance of online search behavior in decision making and problem solving, very little is known about why people stop searching for information online. In this paper, we review the literature concerning online search and cognitive stopping rules, and then describe specific types of information search tasks. Based on this theoretical development, we generated hypotheses and conducted an experiment with 115 participants each performing three search tasks on the web. Our findings show that people utilize a number of stopping rules to terminate search, and that the stopping rule used depends on the type of task performed. Implications for online information search theory and practice are discussed.
|keyword = information search,cognitive stopping rules,online search behavior,decision making,task types and dimensions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the impact of collaboration software on product design and development'''
{{header}}
{{article
|author= Rajiv D. Banker,Indranil Bardhan,Ozer Asdemir,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Prior research suggests that supply chain collaboration has enabled companies to compete more efficiently in a global economy. We investigate a class of collaboration software for product design and development called collaborative product commerce (CPC). Drawing on prior research in media richness theory and organizational science, we develop a theoretical framework to study the impact of CPC on product development. Based on data collected from 71 firms, we test our research hypotheses on the impact of CPC on product design quality, design cycle time, and development cost. We find that CPC implementation is associated with greater collaboration among product design teams. This collaboration has a significant, positive impact on product quality and reduces cycle time and product development cost. Further analyses reveal that CPC implementation is associated with substantial cost savings that can be attributed to improvements in product design quality, design turnaround time, greater design reuse, and lower product design documentation and rework costs.
|keyword = collaborative product commerce,new product development,collaboration software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Formulating the data-flow perspective for business process management'''
{{header}}
{{article
|author= Sherry X. Sun,J. Leon Zhao,Jay E. Nunamaker,Olivia R. Liu Sheng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Workflow technology has become a standard solution for managing increasingly complex business processes. Successful business process management depends on effective workflow modeling and analysis. One of the important aspects of workflow analysis is the data-flow perspective because, given a syntactically correct process sequence, errors can still occur during workflow execution due to incorrect data-flow specifications. However, there have been only scant treatments of the data-flow perspective in the literature and no formal methodologies are available for systematically discovering data-flow errors in a workflow model. As an indication of this research gap, existing commercial workflow management systems do not provide tools for data-flow analysis at design time. In this paper, we provide a data-flow perspective for detecting data-flow anomalies such as missing data, redundant data, and potential data conflicts. Our data-flow framework includes two basic components: data-flow specification and data-flow analysis; these components add more analytical rigor to business process management.
|keyword = workflow modeling,data-flow specification,data-flow anomalies,data-flow verification,dependency analysis,process data diagram,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The nature and role of feedback text comments in online marketplaces: implications for trust building, price premiums, and seller differentiation'''
{{header}}
{{article
|author= Paul A. Pavlou,Angelika Dimoka,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = For online marketplaces to succeed and prevent a market of lemons, their feedback mechanism (reputation system) must differentiate among sellers and create price premiums for trustworthy sellers as returns to their reputation. However, the literature has solely focused on numerical (positive and negative) feedback ratings, alas ignoring the role of feedback text comments. These text comments are proposed to convey useful reputation information about a seller's prior transactions that cannot be fully captured with crude numerical ratings. Building on the economics and trust literatures, this study examines the rich content of feedback text comments and their role in building a buyer's trust in a seller's benevolence and credibility. In turn, benevolence and credibility are proposed to differentiate among sellers by influencing the price premiums that a seller receives from buyers. This paper utilizes content analysis to quantify over 10,000 publicly available feedback text comments of 420 sellers in eBay's online auction marketplace, and to match them with primary data from 420 buyers that recently transacted with these 420 sellers. These dyadic data show that evidence of extraordinary past seller behavior contained in the sellers' feedback text comments creates price premiums for reputable sellers by engendering buyer's trust in the sellers' benevolence and credibility (controlling for the impact of numerical ratings). The addition of text comments and benevolence helps explain a greater variance in price premiums (R-2 = 50%) compared to the existing literature (R 2 = 20%-30%). By showing the economic value of feedback text comments through trust in a seller's benevolence and credibility, this study helps explain the success of online marketplaces that primarily rely on the text comments (versus crude numerical ratings) to differentiate among sellers and prevent a market of lemon sellers. By integrating the economics and trust literatures, the paper has theoretical and practical implications for better understanding the nature and role of feedback mechanisms, trust building, price premiums, and seller differentiation in online marketplaces.
|keyword = feedback,feedback mechanisms,feedback text comments,price premiums,seller differentiation,seller heterogeneity,trust,benevolence,credibility,numerical ratings,online marketplaces,auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Individual cognition and dual-task interference in group support systems'''
{{header}}
{{article
|author= William G. Heninger,Alan R. Dennis,Kelly McNamara Hilmer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Previous research shows that synchronous text discussion through group support systems (GSS) can improve the exchange of information within teams, but this improved information exchange usually does not improve decisions because participants fail to process the new information they receive. This study examined one potential cause for this failure: Dual-task interference caused by the need to concurrently process new information from others while also contributing one's own information to the discussion. Although prior research argues that dual-task interference should be minimal, we found that it significantly reduced participants' information processing and led to lower decision quality. The effect sizes were large, suggesting that dual-task interference is one of a handful of major factors that exert the greatest influence on information processing and decision-making performance. We believe that these results call for an increased emphasis on and understanding of the cognitive underpinnings of GSS and virtual team decision making.
|keyword = group support systems,GSS,synchronous text discussion,decision making,collaboration technology,dual-task interference,individual cognition,information exchange,information processing,virtual teams,cognitive interference,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enabling customer-centricity using wikis and the wiki way'''
{{header}}
{{article
|author= Christian Wagner,Ann Majchrzak,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Customer-centric business makes the needs and resources of individual customers the starting point for planning new products and services or improving existing ones. While customer-centricity has received recent attention in the marketing literature, technologies to enable customer-centricity have been largely ignored in research and theory development. In this paper, we describe one enabling technology-wikis. Wiki is a Web-based collaboration technology designed to allow anyone to update any information posted to a wiki-based Web site. As such, wikis can be used to enable customers to not only access but also change the organization's Web presence, creating previously unheard of opportunities for joint content development and 11 peer production" of Web content. At the same time, such openness may make the organization vulnerable to Web site defacing, destruction of intellectual property, and general chaos. In this zone of tension-between opportunity and possible failure-an increasing number of organizations are experimenting with the use of wikis and the wiki way to engage customers. Three cases of organizations using wikis to foster customer-centricity are described, with each case representing an ever-increasing level of customer engagement. An examination of the three cases reveals six characteristics that affect customer engagement-comm unity custodianship, goal alignment among contributors, value-adding processes, emerging layers of participation, critical mass of management and monitoring activity, and technologies in which features are matched to assumptions about how the community collaborates. Parallels between our findings and those evolving in studies of the open source software movement are drawn.
|keyword = customer-centricity,knowledge creation,knowledge management,open source,wiki,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Personalized content recommendation and user satisfaction: Theoretical synthesis and empirical findings'''
{{header}}
{{article
|author= Ting-Peng Liang,Hung-Jen Lai,Yi-Cheng Ku,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Personalized services are increasingly popular in the Internet world. This study identifies theories related to the use of personalized content services and their effect on user satisfaction. Three major theories have been identified-information overload, uses and gratifications, and user involvement. The information overload theory implies that user satisfaction increases when the recommended content fits user interests (i.e., the recommendation accuracy increases). The uses and gratifications theory indicates that motivations for information access affect user satisfaction. The user involvement theory implies that users prefer content recommended by a process in which they have explicit involvement. In this research, a research model was proposed to integrate these theories and two experiments were conducted to examine the theoretical relationships. Our findings indicate that information overload and uses and gratifications are two major theories for explaining user satisfaction with personalized services. Personalized services can reduce information overload and, hence, increase user satisfaction, but their effects may be moderated by the motivation for information access. The effect is stronger for users whose motivation is in searching for a specific target. This implies that content recommendation would be more useful for knowledge management systems, where users are often looking for specific knowledge, rather than for general purpose Web sites, whose customers often come for scanning. Explicit user involvement in the personalization process may affect a user's perception of customization, but has no significant effect on overall satisfaction.
|keyword = content recommendation,personalization,recommendation systems,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online consumer search depth: Theories and new findings'''
{{header}}
{{article
|author= Jie (Jennifer) Zhang,Xiao Fang,Olivia R. Liu Sheng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The continuous growth of e-commerce makes it critical for firms to understand consumers' search behavior so that e-commerce Web sites and the underlying information systems can be designed to better cater to consumers' needs. This paper extends the classic search model to analyze online consumer search behavior. The analytical results suggest how consumers' search depth is influenced by a variety of factors such as search cost, individual consumer difference, and product characteristics. Evidence is provided using clickstream data of online searches and purchases of music CDs, computer hardware, and airline tickets during the period from July 2002 to December 2002 collected by an Internet marketing company, ComScore Inc. Compared with the search depth reported in previous works, this study finds that consumers are searching more intensely before purchasing online. This reflects the evolution of Internet users and the growth of online retail business.
|keyword = clickstream data,consumer search behavior,online search behavior,search depth,search model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing web sites for customer loyalty across business domains: A multilevel analysis'''
{{header}}
{{article
|author= Sunil Mithas,Narayan Ramasubbu,M. S. Krishnan,Claes Fornell,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Web Sites are important components of Internet strategy for organizations. This paper develops a theoretical model for understanding the effect of Web site design elements on customer loyalty to a Web site. We show the relevance of the business domain of a Web site to gain a contextual understanding of relative importance of Web site design elements. We use a hierarchical linear modeling approach to model multilevel and cross-level interactions that have not been explicitly considered in previous research. By analyzing, data on more than 12,000 online customer surveys for 43 Web sites in several business domains, we find that the relative importance of different Web site features (e.g., content, functionality) in affecting customer loyalty to a Web site varies depending on the Web site's domain. For example, we find that the relationship between Web site content and customer loyalty is stronger for information-oriented Web sites than for transaction-oriented Web sites. However, the relationship between functionality and customer loyalty is stronger for transaction-oriented Web sites than for information-oriented Web sites. We also find that government Web sites enjoy greater word-of-mouth effect than commercial Web sites. Finally, transaction-oriented Web sites tend to score higher on mean customer loyalty than do information-oriented Web sites.
|keyword = business value of information technology,customer loyalty,customer relationships,customer satisfaction,e-commerce,hierarchical linear modeling (HLM),Web site content,word-of-mouth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using enterprise architecture standards in managing information technology'''
{{header}}
{{article
|author= Wai Fong Boh,Daniel Yellin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Organizations increasingly need to build an enterprise-wide capability to leverage technology that is distributed in different business units. Some organizations establish enterprise architecture (EA) standards to enable greater compatibility of information technology (IT) components and integration of applications and data across the enterprise. Through a firm-level survey, we sought to answer two key questions about the use of EA standards: (1) How do different governance mechanisms affect the use of EA standards? and (2) To what extent does the use of EA standards help organizations to improve the sharing and integration of IT resources across the enterprise? We identified four key governance mechanisms for EA standards management and examined how each mechanism affected the use of EA standards. We also examined how the use of EA standards affects the management of IT infrastructure, applications, and data resources across business units. Our empirical results showed that the use of EA standards is effective in helping organizations to better manage their IT resources. Our study also provides detailed insights into how organizations can set up governance mechanisms to facilitate the use of EA standards in achieving enterprisewide goals.
|keyword = enterprise architecture,information system governance,information technology architecture,organizational standards,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Efficacy in technology-mediated distributed teams'''
{{header}}
{{article
|author= Mark A. Fuller,Andrew M. Hardin,Robert M. Davison,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The concept of collective efficacy within virtual teams has yet to be studied. This study developed and rigorously validated a domain-specific measure of collective efficacy, entitled virtual team efficacy, within a comprehensive research framework. Over a two-year period we collected field study data from multiple samples of information systems project teams-in all, 52 virtual teams comprising 318 students from the United States, Great Britain, and Hong Kong. As we hypothesized, group potency and computer collective efficacy act as antecedents to virtual team efficacy, and virtual team efficacy is in turn predictive of perceptual and objective measures of performance. Further, consistent with efficacy theory, we also find that virtual team efficacy acts on performance outcomes through specific mediating processes. This paper contributes to the academic and practitioner communities by providing a comprehensive model of virtual team efficacy and performance and by providing validated instrumentation that can be immediately applied during further research in this area.
|keyword = collective efficacy,global virtual teams,virtual team efficacy,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Conflict and performance in global virtual teams'''
{{header}}
{{article
|author= Atreyi Kankanhalli,Bernard C. Y. Tan,Kwok-Kee Wei,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Increasing globalization and advances in communication technology have fuelled the emergence of global virtual teams (GVTs). There is much potential for conflict in GVTs as members work across cultural, geographical, and time boundaries. This study examines the antecedents of GVT conflict and the circumstances under which conflict affects team performance. An in-depth study of GVT conflict episodes was carried out using interviews, observations, communication logs, and documents. Based on findings from the teams under study interpreted in the light of prior literature, propositions are developed about the antecedents and effects of GVT conflict as stated. Within GVTs, cultural diversity is likely to contribute to both task and relationship conflict while functional diversity may result in task conflict. Large volumes of electronic communication and lack of immediacy of feedback in asynchronous media can contribute to task conflict. Moreover, the relationship between task conflict and team performance is likely to be contingent upon task complexity and conflict resolution approach. The influence of relationship conflict on performance may depend on task interdependence and conflict resolution approach. The conflict resolution approach may in turn be determined by the nature of conflict attribution. These propositions have been synthesized into a model to guide future empirical research and GVT practice.
|keyword = communication technology,conflict resolution,conflict types,diversity,global virtual teams,task characteristics,team conflict,team performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal strategies for a monopoly intermediary in the supply chain of complementary web services'''
{{header}}
{{article
|author= Qian (Candy) Tang,Hsing (Kenneth) Cheng,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Web services are interoperable and reusable software components that can be dynamically discovered and integrated over the Internet. Developed on open standards, Web services have become a promising solution to inter- and intra-organization application integration. The supply chain of Web services exhibits two distinct features that are not considered in previous literature on information and physical-good supply chain: the integration of multiple Web services and the cross-network externality effect between Web service vendors and users. In a quest to fill in the research gap, this paper studies the optimal pricing strategies of a monopolistic intermediary in the supply chain of complementary Web services. The Web service intermediary (WSI) provides both technical and aggregation services, and seeks to charge optimal subscription and listing fees. Analytical results show that in a supply chain of complementary Web services exhibiting cross-network effects, the optimal strategy for the WSI is to set the listing fee such that all service providers list on it. On the other hand, the optimal subscription fee depends on the intensity of the cross-network effect, consumers' valuation of value-added services, and the characteristics of the Web services under consideration.
|keyword = business intermediary,complementarity,network externalities,supply chains,Web services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A knowledge management success model: Theoretical development and empirical validation'''
{{header}}
{{article
|author= Uday R. Kulkarni,Sury Ravindran,Ronald Freeze,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = We examine a knowledge management (KM) success model that incorporates the quality of available knowledge and KM systems built to share and reuse knowledge such as determinants of users' perception of usefulness and user satisfaction with an organization's KM practices. Perceived usefulness and user satisfaction, in turn, affect knowledge use, which in our model is a measure of how well knowledge sharing and reuse activities are internalized by an organization. Our model includes organizational support structure as a contributing factor to the success of KM system implementation. Data collected from 150 knowledge workers from a variety of organizations confirmed 10 of 13 hypothesized relationships. Notably, the organizational support factors of leadership commitment, supervisor and coworker support, as well as incentives, directly or indirectly supported shared knowledge quality and knowledge use. In line with the proposed model, the study lends support to the argument that, in addition to KM systems quality, firms must pay careful attention to championing and goal setting as well as designing adequate reward systems for the ultimate success of these efforts. This is one of the first studies that encompasses both the supply (knowledge contribution) and demand (knowledge reuse) sides of KM in the same model. It provides more than anecdotal evidence of factors that determine successful KM system implementations. Unlike earlier studies that only deal with knowledge-sharing incentives or quality of shared knowledge, we present and empirically validate an integrated model that includes knowledge sharing and knowledge quality and their links to the desired outcome-namely, knowledge reuse.
|keyword = information systems success,knowledge management,knowledge management success,knowledge management systems,knowledge quality,knowledge reuse,knowledge sharing,system quality,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reconceptualizing compatibility beliefs in technology acceptance research'''
{{header}}
{{article
|author= Elena Karahanna,Ritu Agarwal,Corey M. Angst,
|source= MIS QUARTERLY
|year= 2006
|abstract = Theoretical and empirical research in technology acceptance, while acknowledging the importance of individual beliefs about the compatibility of a technology, has-produced equivocal results. This study focuses on further conceptual development of this important belief in technology acceptance. Unlike much prior research that has focused on only a limited aspect of compatibility, we provide a more comprehensive conceptual definition that disaggregates the content Of compatibility into four distinct and separable constructs: compatibility with preferred work style, compatibility with existing work practices, compatibility with prior experience, and compatibility with values. We suggest that the form of the multidimensional compatibility construct is best modeled as a multivariate structural model. Based on their conceptual definitions, we develop operational measures for the four compatibility variables. We assess the nomological validity of our conceptualization by situating it within the technology acceptance model. In contrast to prior research, which has regarded beliefs of compatibility as an independent antecedent of technology acceptance outcomes, we posit causal linkages not only among the four compatibility beliefs, but also between compatibility beliefs and usefulness, and ease of use. We test our theoretical model with afield sample of 278 users of a customer relationship management system in the context of a large bank. Scale validation indicates that the operational measures of compatibility developed in this study have acceptable psychometric properties, which support the existence of four distinct constructs. Results largely support the theorized relationships.
|keyword = technology acceptance model,TAM,compatibility,innovation diffusion,innovation characteristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Influence processes for information technology acceptance: An elaboration likelihood model'''
{{header}}
{{article
|author= Anol Bhattacherjee,Clive Sanford,
|source= MIS QUARTERLY
|year= 2006
|abstract = This study examines how processes of external influence shape information technology acceptance among potential users, how such influence effects vary across a user population, and whether these effects are persistent over time. Drawing on the elaboration-likelihood model (ELM), we compared two alternative influence processes, the central and peripheral routes, in motivating IT acceptance. These processes were respectively operationalized using the argument qualify and source credibility constructs, and linked to perceived usefulness and attitude, the core perceptual drivers of IT acceptance. We further examined how these influence processes were moderated by users' IT expertise and perceived job relevance and the temporal stability of such influence effects. Nine hypotheses thus developed were empirically validated using a field survey of document management system acceptance at an eastern European governmental agency. This study contributes to the IT acceptance literature by introducing ELM as a referent theory for acceptance research, by elaborating alternative modes of influence, and by specifying factors moderating their effects. For practitioners, this study introduces influence processes as policy tools that managers can employ to motivate IT acceptance within their organizations, benchmarks alternative influence strategies, and demonstrates the need for customizing influence strategies to the specific needs of a user population.
|keyword = information systems acceptance,elaboration likelihood model,influence,persuasion,attitude,survey research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Real options in information technology risk management: An empirical validation of risk-option relationships'''
{{header}}
{{article
|author= Michel Benaroch,Yossi Lichtenstein,Karl Robinson,
|source= MIS QUARTERLY
|year= 2006
|abstract = Recenty, an option-based risk management (OBRiM)framework has been proposed to control risk and maximize value in information technology investment decisions. While the framework is prescriptive in nature, its core logic rests on a set of normative risk-option mappings for choosing which particular real options to embed in an investment in order to control specific risks. This study tests empirically whether these mappings are observed in practice. The research site is a large Irish financial services organization with well established IT risk management practices not tied to any real options framework. Our analysis of the risk management plans developed for a broad portfolio of 50 IT investments finds ample empirical support for OBRiM's risk-option mappings. This shows that IT managers follow the logic of option-based risk management, although purely based on intuition. Unfortunately, reliance on this logic based on intuition alone could lead to suboptimal or counterproductive risk management practices. We therefore argue that managerial intuition ought to be supplemented with the use of formal real option models, which allow for better quantitative insights into which risk mitigations to pursue and combine in order to effectively address the risks most worth controlling.
|keyword = IT investment,risk,real options,risk management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the impact of web personalization on user information processing and decision outcomes'''
{{header}}
{{article
|author= Kar Yan Tam,Shuk Ying Ho,
|source= MIS QUARTERLY
|year= 2006
|abstract = Personalized information technology services have become a ubiquitous phenomenon. Companies worldwide are using the web to provide personalized offerings and unique experiences to their customers. While there is a lot of hype about delivering personalized services over the web, little is known about the effectiveness of web personalization and the link between the IT artifact (the personalization agent) and the effects it exerts on a user's information processing and decision making. To address the impact of personalized content, this article theoretically develops and empirically tests a model of web personalization. The model is grounded on social cognition and consumer research theories adapted to the peculiar features of web personalization. The influence of a personalization agent is mediated by two variables: content relevance and self reference. Hypotheses generated from the model are empirically tested in a laboratory experiment and a field study. The findings indicate that content relevance, self reference, and goal specificity affect the attention, cognitive processes, and decisions of web users in various ways. Also, users are found to be receptive to personalized content and find it useful as a decision aid. Theoretical and practical implications of the findings are discussed.
|keyword = web personalization,processing goal,content relevance,self reference,human computer interface,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Aligning software processes with strategy'''
{{header}}
{{article
|author= Sandra A. Slaughter,Linda Levine,Balasubramaniam Ramesh,Jan Pries-Heje,Richard Baskerville,
|source= MIS QUARTERLY
|year= 2006
|abstract = Although increasing evidence suggests that superior performance requires alignment between firms' strategies and production processes, it is not known if such alignment is relevant for software development processes. This study breaks new ground by examining how firms align their software processes, products, and strategies in Internet application development. Drawing upon the literatures in strategy, operations management, and information systems, we identify four dimensions that influence alignment: the business unit strategy, the level of product customization, the level of process customization, and the volume of customers. To examine how these dimensions are synchronized, we conducted detailed case studies of Internet application development in nine varied firms including both start-ups and established "brick and mortar" companies. Our analyses reveal that the firms in our study do use differing processes for Internet application development, and that many of the firms match their software process choices to product characteristics, customer volume, and business unit strategies. We develop concept maps for the firms that are in alignment to illustrate how managers configure specific product and process dimensions. We also offer potential explanations for why some firms are misaligned, such as attempting to execute incompatible strategies, the lack of coordination between marketing and production strategies, the too rapid expansion of process scope, and inflexible barriers to rapid adaptation of process. Our study contributes detailed insights into how software processes are customized to complement different types of product requirements and strategies.
|keyword = software process,product-process matrix,Internet application development,software development strategy,competitive strategy,contingency theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge integration and information technology project performance'''
{{header}}
{{article
|author= Victoria L. Mitchell,
|source= MIS QUARTERLY
|year= 2006
|abstract = Successful product and process design depends on management's ability to integrate fragmented pockets of specialized knowledge. This integrative capability has important implications for large-scale information technology projects. This article examines the relationship between timely project completion and two dimensions of management's integrative capability: access to external knowledge and internal knowledge integration. Measures of these two dimensions are used to predict on-time project completion, where completion is a function of the duration of IT-related project delays. In a longitudinal study of 74 enterprise application integration projects in the medical sector, integrative capability was measured from the point of view of the CIO and a facility IT manager. Accounting for several project controls, our Cox regression results indicate both integrative dimensions significantly mitigate the duration of IT-related project delays, thus promoting timely project completion. The analysis also reveals the importance of taking management structure into consideration when studying IT phenomena in networked organizations.
|keyword = IT performance,MIS management,knowledge management,management structure,integrative capabilities,dynamic capabilities,project delay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of personalization and familiarity on trust and adoption of recommendation agents'''
{{header}}
{{article
|author= Sherrie Y. X. Komiak,Izak Benbasat,
|source= MIS QUARTERLY
|year= 2006
|abstract = In the context of personalization technologies, such as Web-based product-brokering recommendation agents (RAs) in electronic commerce, existing technology acceptance theories need to be expanded to take into account not only the cognitive beliefs leading to adoption behavior, but also the affect elicited by the personalized nature of the technology. This study takes a trust-centered, cognitive and emotional balanced perspective to study RA adoption. Grounded on the theory of reasoned action, the IT adoption literature, and the trust literature, this study theoretically articulates and empirically examines the effects of perceived personalization and familiarity on cognitive trust and emotional trust in an RA, and the impact of cognitive trust and emotional trust on the intention to adopt the RA either as a decision aid or as a delegated agent. An experiment was conducted using two commercial RAs. PLS analysis results provide empirical support for the proposed theoretical perspective. Perceived personalization significantly increases customers' intention to adopt by increasing cognitive trust and emotional trust. Emotional trust plays an important role beyond cognitive trust in determining customers' intention to adopt. Emotional trust fully mediates the impact of cognitive trust on the intention to adopt the RA as a delegated agent, while it only partially mediates the impact of cognitive trust on the intention to adopt the RA as a decision aid. Familiarity increases the intention to adopt through cognitive trust and emotional trust.
|keyword = trust,electronic commerce,adoption,personalization,familiarity,cognitive trust,emotional trust,recommendation agent,delegation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From IT leveraging competence to competitive advantage in turbulent environments: The case of new product development'''
{{header}}
{{article
|author= Paul A. Pavlou,Omar A. El Sawy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = A burning question for information systems (IS) researchers and practitioners is whether and how IT can build a competitive advantage in turbulent environments. To address this question, this study focuses on the business process level of analysis and introduces the construct of IT leveraging competence-the ability to effectively use IT functionalities. This construct is conceptualized in the context of new product development (NPD). IT leveraging competence is shown to indirectly influence competitive advantage in NPD through two key mediating links: functional competencies (the ability to effectively execute operational NPD processes) and dynamic capabilities (the ability to reconfigure functional competencies to address turbulent environments). Environmental turbulence is also shown to moderate the process by which IT leveraging competence influences competitive advantage in NPD. Empirical data were collected from 180 NPD managers. Through the construct of IT leveraging competence, the study shows that the effective use of IT functionalities, even generic functionalities, by business units can help build a competitive advantage. The study also shows that the strategic effect of IT leveraging competence is more pronounced in higher levels of environmental turbulence. This effect is not direct: It is fully mediated by both dynamic capabilities and functional competencies. Taken together, these findings suggest that IS researchers should look beyond the direct effects of firm-level IT infrastructures and focus their attention on how business units can leverage IT functionalities to better reconfigure and execute business processes. In turbulent environments, focusing on these aspects is even more vital.
|keyword = IT competence,information systems strategy,environmental turbulence,dynamic capabilities,functional competencies,IT-enabled business processes,new product development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reconceptualizing system usage: An approach and empirical test'''
{{header}}
{{article
|author= Andrew Burton-Jones,Jr. Detmar W. Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Although DeLone, McLean, and others insist that system usage is a key variable in information systems research, the system usage construct has received little theoretical scrutiny, boasts no widely accepted definition, and has been operationalized by a diverse set of unsystematized measures. In this article, we present a systematic approach for reconceptualizing the system usage construct in particular nomological contexts. Comprising two stages, definition and selection, the approach enables researchers to develop clear and valid measures of system usage for a given theoretical and substantive context. The definition stage requires that researchers define system usage and explicate its underlying assumptions. In the selection stage, we suggest that system usage be conceptualized in terms of its structure and function. The structure of system usage is tripartite, comprising a user, system, and task, and researchers need to justify which elements of usage are most relevant for their study. In terms of function, researchers should choose measures for each element (i.e., user, system, and/or task) that tie closely to the other constructs in the researcher's nomological network. To provide evidence of the viability of the approach, we undertook an empirical investigation of the relationship between system usage and short-run task performance in cognitively engaging tasks. The results support the benefits of the approach and show how an inappropriate choice of usage measures can lead researchers to draw opposite conclusions in an empirical study. Together, the approach and the results of the empirical investigation suggest new directions for research into the nature of system usage, its antecedents, and its consequences.
|keyword = system usage,theoretical conceptualization,measure,performance,IS success,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On the optimality of fixed-up-to tariff for telecommunications service'''
{{header}}
{{article
|author= Yasushi Masuda,Seungjin Whang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = A tariff is the total charge payable by a customer for services provided. We study the design of tariffs for a telecommunications service provider. We develop an economic model that captures the negative externalities of the network and the diversity of customers. The tariff is designed so that it reflects the expected response of different customers and the system congestion it would induce. We study a simple tariff structure in wide use by mobile phone carriers-a menu of "fixed-up-to (FUT)" plans like "fixed access fee $35 up to 300 minutes, and $0.40 per minute beyond the limit." We derive the optimal menu of FUT plans and show that such a simple FUT menu structure delivers as good performance to the monopolistic carrier as any nonlinear pricing schedule.
|keyword = tariff design,queuing delays,nonlinear pricing,menu of plans,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Privacy protection in data mining: A perturbation approach for categorical data'''
{{header}}
{{article
|author= Xiao-Bai Li,Sumit Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = To respond to growing concerns about privacy of personal information, organizations that use their customers' records in data-mining activities are forced to take actions to protect the privacy of the individuals involved. A common practice for many organizations today is to remove identity-related attributes from the customer records before releasing them to data miners or analysts. We investigate the effect of this practice and demonstrate that many records in a data set could be uniquely identified even after identity-related attributes are removed. We propose a perturbation method for categorical data that can be used by organizations to prevent or limit disclosure of confidential data for identifiable records when the data are provided to analysts for classification, a common data-mining task. The proposed method attempts to preserve the statistical properties of the data based on privacy protection parameters specified by the organization. We show that the problem can be solved in two phases, with a linear programming formulation in Phase I (to preserve the first-order marginal distribution), followed by a simple Bayes-based swapping procedure in Phase 11 (to preserve the joint distribution).
|keyword = privacy,data confidentiality,data mining,linear programming,Bayesian estimation,data swapping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research note - How often should reputation mechanisms update a trader's reputation profile?'''
{{header}}
{{article
|author= Chrysanthos Dellarocas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Reputation mechanisms have become an important component of electronic markets, helping to build trust and elicit cooperation among loosely connected and geographically dispersed economic agents. Understanding the impact of different reputation mechanism design parameters on the resulting market efficiency has thus emerged as a question of theoretical and practical interest. Along these lines, this note studies the impact of the frequency of reputation profile updates on cooperation and efficiency. The principal finding is that, in trading settings with pure moral hazard and noisy ratings, if the per-period profit margin of cooperating sellers is sufficiently high, a mechanism that does not publish every single rating it receives but rather only updates a trader's public reputation profile every k transactions with a summary statistic of a trader's most recent k ratings can induce higher average levels of cooperation and market efficiency than a mechanism that publishes all ratings as soon as they are posted. This paper derives expressions for calculating the optimal profile updating interval k, discusses the implications of this finding for existing systems, such as eBay, and proposes alternative reputation mechanism architectures that attain higher maximum efficiency than the, currently popular, reputation mechanisms that publish summaries of a trader's recent ratings.
|keyword = electronic markets,reputation mechanisms,game theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of trust-assuring arguments on consumer trust in Internet stores: Application of Toulmin's model of argumentation'''
{{header}}
{{article
|author= Dongmin Kim,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = A trust-assuring argument refers to "a claim and its supporting statements used in an Internet store to address trust-related issues." Although trust-assuring arguments often appear in Internet stores, little research has been conducted to understand their effects on consumer trust in an Internet store. The goals of this study are (1) to investigate whether or not the provision of trust-assuring arguments on the website of an Internet store increase consumer trust in that Internet store and (2) to identify the most effective form of trust-assuring arguments to provide guidelines for their implementation. Toulmin's (1958) model of argumentation is proposed as a basis to identify the elements of an argument and to strengthen the effects of trust-assuring arguments on consumer trust in an Internet store. Based on Toulmin's (1958) model of argumentation, three elements of arguments that commonly appear in daily communication; namely, claim, data, and backing, are identified. Data refers to the grounds for a claim, while backing is used for providing reasons for why the data should be accepted. By combining these three elements, three forms of trust-assuring arguments (claim only, claim plus data, and claim plus data and backing) are developed. The effects of these three forms of trust-assuring arguments on consumer trust in an Internet store are tested by comparing them to a no trust-assuring argument condition in a laboratory experiment with 112 participants. The results indicate (1) providing trust-assuring arguments that consist of claim plus data or claim plus data and backing increases consumers' trusting belief but displaying arguments that contain claim only does not and (2) trust-assuring arguments that include claim plus data and backing lead to the highest level of trusting belief among the three forms of arguments examined in this study. Based on the results, we argue that Toulmin's (1958) model of argumentation is an effective basis for website designers to develop convincing trust-assuring arguments and to improve existing trust-assuring arguments in Internet stores.
|keyword = trust-assuring arguments,trust,electronic commerce,Toulmin,model of argumentation,claim,data,backing,human-computer interaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effectiveness of knowledge transfer portfolios in software process improvement: A field study'''
{{header}}
{{article
|author= Sandra A. Slaughter,Laurie J. Kirsch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Because of challenges often experienced when deploying software, many firms have embarked on software process improvement (SPI) initiatives. Critical to the success of these initiatives is the transfer of knowledge across individuals who occupy a range of roles in various organizational units involved in software production. Prior research suggests that a portfolio of different mechanisms, employed frequently, can be required for effective knowledge transfer. However, little research exists that examines under what situations differing portfolios of mechanisms are selected. Further, it is not clear how effective different portfolio designs are. In this study, we conceptualize knowledge transfer portfolios in terms of their composition (the types of mechanisms used) and their intensity (the frequency with which the mechanisms are utilized). We hypothesize the influence of organizational design decisions on the composition and intensity of knowledge transfer portfolios for SPI. We then posit how the composition and intensity of knowledge transfer portfolios affect performance improvement. Our findings indicate that a more intense portfolio of knowledge transfer mechanisms is used when the source and recipient are proximate, when they are in a hierarchical relationship, or when they work in different units. Further, a source and recipient select direction-based portfolios when they are farther apart, in a hierarchical relationship, or work in different units. In terms of performance, our results reveal that the fit between the composition and intensity of the knowledge transfer portfolio influences the recipient's performance improvement. At lower levels of intensity direction-based portfolios are more effective, while at higher levels of intensity routine-based portfolios yield the highest performance improvement. We discuss the implications of our findings for researchers and for managers who want to promote knowledge transfer to improve software processes in their organizations.
|keyword = knowledge transfer mechanism portfolios,portfolio intensity,portfolio composition,knowledge transfer,software process improvement,management of information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology, production process outsourcing, and manufacturing plant performance'''
{{header}}
{{article
|author= Indranil Bardhan,Jonathan Whitaker,Sunil Mithas,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = What is the role of information technology (IT) in enabling the outsourcing of manufacturing plant production processes? Do plant strategies influence production outsourcing? Does production process outsourcing influence plant performance? This research addresses these questions by investigating the role of IT and plant strategies as antecedents of production outsourcing, and evaluating the impact of production outsourcing and IT investments on plant cost and quality. We develop a theoretical framework for the antecedents and performance outcomes of production outsourcing at the plant level. We validate this theoretical framework using cross-sectional survey data from U.S. manufacturing plants. Our analysis suggests that plants with greater IT investments are more likely to outsource their production processes, and that IT investments and production outsourcing are associated with lower plant cost of goods sold and higher product quality improvement. Our research provides an integrated model for studying the effects of IT and production outsourcing on plant performance.
|keyword = IT investment,manufacturing performance,outsourcing,plant performance,plant strategy,production process outsourcing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A virtual integration theory of improved supply-chain performance'''
{{header}}
{{article
|author= Eric T. G. Wang,Jeffrey C. F. Tai,Hsiao-Lan Wei,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Organizing and maintaining a competent and flexible supply chain is a major challenge to manufacturers in today's increasingly competitive and uncertain environments. Virtual integration represents the substitution of ownership with partnership by integrating a set of suppliers through information technology (IT) for tighter supply-chain collaboration. From the systems and control perspectives, this study develops a theory of virtual integration with an empirical model to examine the role that virtual integration plays in facilitating manufacturers to achieve greater manufacturing flexibility and comparative cost advantage. Based on a survey of Taiwanese manufacturing firms, our results show that environmental uncertainty tends to motivate manufacturers to increase their manufacturing flexibility, with both virtual integration and supplier responsiveness playing a vital enabling role. The results demonstrate the importance of supplier responsiveness for manufacturers to gain manufacturing flexibility and comparative cost advantage in supply-chain operations. Environmental uncertainty, thus, might first appear as a threat to a manufacturer, but with the help of IT and more responsive suppliers, such a threat could be transformed into a competitive edge, as reflected in the manufacturer's higher levels of manufacturing flexibility and comparative cost advantage.
|keyword = competitive advantage,environmental uncertainty,interorganizational information systems,structural equation modeling,supply-chain management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Systems design, process performance, and economic outcomes in international banking'''
{{header}}
{{article
|author= Prabu Davamanirajan,Robert J. Kauffman,Charles H. Kriebel,Tridas Mukhopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Information technology (IT) value remains a serious concern of management today, especially how it should be measured and how it is created. Although we have made significant progress at the firm and aggregate levels of analysis, process-level analysis is still in its infancy, and there is a need for a systematic basis for identifying IT effects. We provide such an approach by developing two models: a process performance model of how system characteristics enhance process output and quality and an economic performance model linking process performance to the economic performance of the firm. We apply these models to global trade services in international banking. We obtained estimates for key variables in both models and general support for the approach. We interpret our results and discuss the merits of the process-level approach for the assessment of IT-reliant work systems.
|keyword = business process,business value,economic performance,financial services,international banking,IT value,process performance,technology management,trade services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Network effects and technology licensing with fixed fee, royalty, and hybrid contracts'''
{{header}}
{{article
|author= Lihui Lin,Nalin Kulatilaka,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Technology innovators are faced with the question of whether to license an innovation to other firms, and if so, what type of license it should use. This question takes on paramount importance with information technology innovations that lead to new products and services that exhibit network effects. This paper explores the impact of network effects on the licensing choice. The literature suggests that without network effects, a royalty license is preferred by producer-innovators. We find that a fixed-fee license is optimal with strong network effects. For less intense network effects, the optimal license uses a royalty rate, either alone or in combination with a fee. We further derive the terms of the optimal license and discuss the impact of the investment needed to replicate the innovation and the size of the potential market. Our results provide insights for licensing decisions in industries that exhibit network effects.
|keyword = economic analysis,fixed fees,hybrid charging schemes,IT value,licensing policy,licensing royalty,network effects,technology innovations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Analyzing complementarities using software stacks for software industry acquisitions'''
{{header}}
{{article
|author= Lucia Silva Gao,Bala Iyer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The existence of product complementarities is especially relevant in network-type industries, such as information technology and communications, where systems of complementary components made by different manufacturers have to be assembled. Relying on the characteristics of software markets and drawing on the economic theory of complementarities, this paper investigates how complementarities create value in mergers and acquisitions between software companies. We introduce and empirically validate the software stack as a structure to measure complementarities. In a sample of mergers and acquisitions, in which either the acquirer or the target is a software firm, we find values of abnormal returns consistent with previous results. However, when we use the concept of stack, we find an inverse curvilinear relationship between abnormal returns and the distance between acquirers and targets in various layers of the stack.
|keyword = complementarities,empirical methods,event study,mergers and acquisitions,product complementarities,software stack,technology firms,theory development,value creation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When online reviews meet hyperdifferentiation: A study of the craft beer industry'''
{{header}}
{{article
|author= Eric K. Clemons,Guodong Gordon Gao,Lorin M. Hitt,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = We analyze how online reviews are used to evaluate the effectiveness of product differentiation strategies based on the theories of hyperdifferentiation and resonance marketing. Hyperdifferentiation says that firms can now produce almost anything that appeals to consumers and they can manage the complexity of the increasingly diverse product portfolios that result. Resonance marketing says that informed consumers will purchase products that they actually truly want. When consumers become more informed, firms that provide highly differentiated products should experience higher growth rates than firms with less differentiated offerings. We construct measures of product positioning based on online ratings and find supportive evidence using sales data from the craft beer industry. In particular, we find that the variance of ratings and the strength of the most positive quartile of reviews play a significant role in determining which new products grow fastest in the marketplace. This supports our expectations for resonance marketing.
|keyword = differentiation,online reviews,product positioning,product variety,resonance marketing,word of mouth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Accommodating individual preferences in the categorization of documents: A personalized clustering approach'''
{{header}}
{{article
|author= Chih-Ping Wei,Roger H. L. Chiang,Chia-Chen Wu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = As electronic commerce and knowledge economy environments proliferate, both individuals and organizations increasingly generate and consume large amounts of online information, typically available as textual documents. To manage this ever-increasing volume of documents, individuals and organizations frequently organize their documents into categories that facilitate document management and subsequent access and browsing. Document clustering is an intentional act that should reflect individual preferences with regard to the semantic coherency and relevant categorization of documents. Hence, effective document clustering must consider individual preferences and needs to support personalization in document categorization. In this paper, we present an automatic document-clustering approach that incorporates an individual's partial clustering as preferential information. Combining two document representation methods, feature refinement and feature weighting, with two clustering methods, precluster-based hierarchical agglomerative clustering (HAC) and atomic-based HAC, we establish four personalized document-clustering techniques. Using a traditional content-based document-clustering technique as a performance benchmark, we find that the proposed personalized document-clustering techniques improve clustering effectiveness, as measured by cluster precision and cluster recall.
|keyword = cognitive overload,document clustering,hierarchical agglomerative clustering (HAC),personalization,personalized document clustering,supervised document clustering,text mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer perceptions and willingness to pay for intrinsically motivated online content'''
{{header}}
{{article
|author= Alexandre B. Lopes,Dennis F. Galletta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Providing profitable online content has been an elusive goal, challenging many companies such as the New York Times, Disney/ABC/ESPN, and Microsoft/ Slate. Charging for content has been hit-or-miss, attributable to a lack of generally applicable models of information value. Previous studies in the management information systems literature emphasized extrinsically motivated content (addressing tangible gains), while many sites target intrinsic goals such as entertainment or education. This study examines potential factors influencing willingness to pay for intrinsically motivated online content. Data from 392 college students indicate that even when analyzing content whose potential rewards are intangible and nonquantifiable, potential consumers focus on "expected benefits" as the main antecedent for willingness to pay. Other antecedents, such as perceived quality and provider reputation, only affected willingness to pay indirectly through expected benefits. Researchers are offered a baseline model for future study, and practitioners are advised to provide initial visitors a clear message about benefits of use to entice them to pay for content.
|keyword = consumer assessment,e-commerce,information value,online content,service quality,service value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Do I trust you online, and if so, will I buy? An empirical study of two trust-building strategies'''
{{header}}
{{article
|author= Kai H. Lim,Choon Ling Sia,Matthew K. O. Lee,Izak Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = This research investigates the effectiveness of various trust-building strategies to influence actual buying behavior in online shopping environments, particularly for first-time visitors to an Internet store that does not have an established reputation. Drawing from the literature on trust, we developed a model of how trustbuilding strategies could affect trust and the consequences of trust. We investigated two trust-building strategies: portal association (based on reputation categorization and trust transference) and satisfied customer endorsements (based on unit grouping, reputation categorization, and trust transference). A series of two studies was conducted at a large public university in Hong Kong. The first study employed a laboratory experiment to test the model in an online bookstore environment, using a real task that involves actual book purchases. Of the two strategies investigated, satisfied customer endorsement by similar peers, but not portal association, was found to increase consumers' trusting beliefs about the store. This, in turn, positively influenced consumers' attitudes toward the store and their willingness to buy from the store, which ultimately led to actual buying behaviors. To gather further insights on the two Web strategies investigated, a second study was conducted using a questionnaire survey approach. Overall, the findings corroborated those in the first study. Specifically, it shows that endorsements by similar (local, nonforeign) peers, but not by dissimilar (foreign) peers, were effective means of developing trust among first-time visitors to online stores.
|keyword = e-commerce,online shopping,theory of planned behavior,trust,Web site design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information systems success in the context of different corporate cultural types: An empirical investigation'''
{{header}}
{{article
|author= Randy V. Bradley,Jeannie L. Pridmore,Terry Anthony Byrd,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Previous studies surrounding the DeLone and McLean model of information systems (IS) success have called for future research and further examination of its measure in different contexts. We draw from the literature on strategic IS planning and organizational culture to contextualize the DeLone and McLean model. There is some evidence that a high-quality information technology (IT) plan leads to system success; therefore, we empirically examine the inclusion of the IT plan quality construct as an antecedent to IS success. We also empirically examine the relationships among constructs in the model of IS success in the context of different corporate cultural types-entrepreneurial and formal. The results provide strong support for the research model and suggest that variations in IS success are explained by the quality of the IT plan and the corporate culture exhibited by a firm. We discuss implications related to our finding that IT plan quality has a greater impact on IS success in organizations that exhibit an entrepreneurial corporate culture than in those that exhibit a formal corporate culture. Furthermore, we discuss how the relationships in the DeLone and McLean model of IS success differ in diverse corporate cultural types and the meaning of these differences.
|keyword = corporate culture,impact of information systems use,information systems success,information technology plan quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The transformation of open source software'''
{{header}}
{{article
|author= Brian Fitzgerald,
|source= MIS QUARTERLY
|year= 2006
|abstract = A frequent characterization of open source software is the somewhat outdated, mythical one of a collective of supremely talented software hackers freely volunteering their services to produce uniformly high-quality software. I contend that the open source software phenomenon has metamorphosed into a more mainstream and commercially viable form, which I label as OSS 2.0. I illustrate this transformation using a framework of process and product factors, and discuss how the bazaar metaphor, which up to now has been associated with the open source development process, has actually shifted to become a metaphor better suited to the OSS 2.0 product delivery and support process. Overall the OSS 2.0 phenomenon is significantly different from its free software antecedent. Its emergence accentuates the fundamental alteration of the basic ground rules in the software landscape, signifying the end of the proprietary-driven model that has prevailed for the past 20 years or so. Thus, a clear understanding of the characteristics of the emergent OSS 2.0 phenomenon is required to address key challenges for research and practice.
|keyword = open source software,free software,IS development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Academic data collection in electronic environments: Defining acceptable use of internet resources'''
{{header}}
{{article
|author= Gove N. Allen,Dan L. Burk,Gordon B. Davis,
|source= MIS QUARTERLY
|year= 2006
|abstract = Academic researchers access commercial web sites to collect research data. This research practice is likely to increase. Is this appropriate? Is this legal? Such commercial web sites are maintained to achieve business objectives; research access uses site resources for other purposes. Web site administrators may, therefore, deem academic data collection inappropriate. Is there a process to make research access more open and acceptable to web site owners and administrators? These are significant issues. This article clarifies the problems and suggests possible approaches to handle the issues with sensitivity and openness. Research access to commercial web sites may be manual (using a standard web browser) or automated (using automated data collection agents). These approaches have different effects on web sites. Researchers using manual access tend to make a limited number of page requests because manual access is costly to perform. Researchers using automated access methods can request large numbers of pages at a low cost. Therefore, web site administrators tend to view manual access and automated access very differently. Because of the number of accesses and the nonbusiness purpose, automated research requests for data are sometimes blocked by site administration using a variety of means (both technological and legal). This paper details the pertinent legal issues including trespass, copyright violation, and breech of contract. It also explains the nature of express and implied consent by site administration for research access. Based on the issues presented, guidelines for researchers are proposed to reduce objections to research activities, to facilitate communication with web site administration, and to achieve express or implied consent. These include notification to web site administration of intended automated research activity, description of the research project posted as a web page, and clear identification of automated requests for web pages. In order to encourage good research practices with respect to automated data collection, suggestions are made with respect to disclosing methods used in research papers and for self regulation by academic associations.
|keyword = Internet,research,automated data collection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Order lead-time improvement following enterprise information technology implementation: An empirical study'''
{{header}}
{{article
|author= Mark J. Cotteleer,Elliot Bendoly,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper investigates the influence of enterprise systems implementation on operational performance. The work extends the literature on enterprise systems by focusing on changes in process dynamics as a source for ongoing firm-level performance improvement. A case discussion of Tristen Corporation, a firm that implemented ERP and subsequently experienced benefits through gains to its continuous improvement efforts, is examined in light of theorized impacts of such implementations on process dynamics. Analyses of longitudinal data suggest that performance along a key metric motivating the ERP initiative (i.e., order fulfillment lead-time) showed a significant improvement immediately after system deployment. The data further suggest that the system implementation gave rise to an ongoing trend of performance improvement, in contrast to a stable performance trend prior to go-live.
|keyword = enterprise resource planning,visibility variability reduction,continuous improvement,organizational learning,empirical research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Enhancing the design of web navigation systems: The influence of user disorientation on engagement and performance'''
{{header}}
{{article
|author= Jane Webster,Jaspreet S. Ahuja,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper draws on research from a wide literature base to develop a model relating Web navigation systems, disorientation, engagement, user performance, and intentions. The model is tested in an experimental study examining the effects of one simple and two global navigation systems. Although well-accepted design guidelines were followed for the first global navigation system, it was not superior to the simple system. However, the second global navigation system resulted in lower disorientation than the simple system. Based on the study's results, two design guidelines to govern the development of future Web-based systems are suggested.
|keyword = disorientation,lost,engagement,flow,World Wide Web,Internet,design,Web-based learning,electronic commerce,electronic shopping,Web navigation,commercial Web site,performance,efficiency,intentions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic marketplaces and price transparency: Strategy, information technology, and success'''
{{header}}
{{article
|author= Christina Soh,M. Lynne Markus,Kim Huat Goh,
|source= MIS QUARTERLY
|year= 2006
|abstract = Electronic marketplaces (EMPs) are widely assumed to increase price transparency and hence lower product prices. Results of empirical studies have been mixed, with several studies showing that product prices have not decreased and others showing that prices have increased in some cases. One explanation is that sellers prefer not to join EMPs with high price transparency, leading highly price transparent EMPs to fail. Therefore, in order to be successful, EMPs might be expected to avoid high price transparency. But that strategy creates a catch-22 for EMPs on the buy side: Why would buyers want to join EMPs in the absence of price transparency and the benefit of lower prices? We argue that successful EMPs must provide compensatory benefits for sellers in the case of high price transparency and for buyers in the case of low price transparency. To understand how EMPs could succeed, regardless of price transparency, we examined the relationships among EMP strategy, price transparency, and performance by analyzing all 19 EMPs that compete by selling a broad range of standard electronics components. We found that all EMPs pursuing a low cost strategy had high price transparency and performed poorly. All EMPs that performed well pursued strategies of differentiation, but, interestingly, not all successful EMPs avoided price transparency: Some EMPs succeeded despite enabling high price transparency. We therefore examined two differentiated EMPs in greater depth-one with high price transparency, the other with low price transparency-to show how they achieved strategic alignment of activities and resources and provided compensatory benefits for their customers.
|keyword = electronic marketplaces,electronic market theory,electronic commerce,Internet,price transparency,strategic alignment,electronic components industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Professional versus political contexts: Institutional mitigation and the transaction cost heuristic in information systems outsourcing'''
{{header}}
{{article
|author= Shaila M. Miranda,Yong-Mi Kim,
|source= MIS QUARTERLY
|year= 2006
|abstract = IS research has considered the outsourcing decision from the perspective of transaction cost economics (TCE) and institutional theory. In this research, we consider how the appropriation of the logic of transaction cost economics is contingent on decision makers' institutional context. The institutional contexts contrasted are professional versus political contexts. In a survey of 214 city governments in the United States, we substantiate the existence of these two institutional contexts, a distinction that has been noted to extend into the private sector as well. Subsequent analyses of the moderating effects of institutional context on the application of the TCE heuristic to the outsourcing decision revealed the following: The institutional context moderated the impacts of "human frailty" conditions-of opportunism and bounded rationality-and of transaction frequency on outsourcing decisions. In professional contexts, opportunism reduced outsourcing and frequency increased outsourcing; in political contexts, bounded rationality fostered outsourcing and frequency dissuaded outsourcing. However, no institutional moderation was noted for the situational conditions of asset specificity and uncertainty. Instead, situational conditions were found to increase the incidence of outsourcing across both contexts. Findings about the contingent effects of human frailty conditions augment our understanding of the outsourcing phenomenon by emphasizing that decision makers' attentiveness to the logic of transaction costs during outsourcing is shaped by their institutional context. Findings with regard to situational conditions suggest a need for future research to consider the role of another contextual factor-resource munificence-in mitigating the effects of situational conditions on responses to transaction costs.
|keyword = IS outsourcing,transaction costs,institutional environment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and pricing decisions: Price adjustments in online computer markets'''
{{header}}
{{article
|author= Wonseok Oh,Jr. Henry C. Lucas,
|source= MIS QUARTERLY
|year= 2006
|abstract = Determining prices is a key management task for a merchant. IT-enabled electronic markets facilitate price discovery by both buyers and sellers compared to traditional, physical markets. Recent research on electronic markets has revealed that IT has increased market transparency due to increased accessibility and availability of market information. However, what online sellers do in terms of strategic pricing decisions, in particular price adjustment behavior overtime, has not been fully investigated. Due to the ease of making price changes, electronic sellers can execute a number of different pricing strategies, including setting the frequency and amount of price changes. We investigate the "opaque" side of electronic markets by exploring online sellers' price adjustment patterns over time. More specifically, we identify four questions related to pricing decisions, which lead to hypotheses about how managers determine prices in electronic markets. The paper tests the hypotheses with data from the online computer commodity market. We found, through a simulation analysis, that this market exhibits synchronized price changes, not random changes that are frequently found in traditional markets. Interestingly, small price increases occur more frequently than decreases, while the frequency of price adjustment is significantly associated with a product's price dispersion. A ranking analysis suggests that online sellers change their price strategies frequently, which makes it difficult for consumers to respond appropriately. The paper discusses the implications of our findings for management and for future research on market transparency and strategic pricing in electronic markets.
|keyword = market transparency,strategic pricing,price adjustment,information technology,electronic markets,computer simulations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Circuits of power in creating de jure standards: Shaping an international information systems security standard'''
{{header}}
{{article
|author= James Backhouse,Carol W. Hsu,Leiser Silva,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper addresses the role of power and politics in setting standards. It examines the interaction of external contingencies, powerful agents, resources, meaning, and membership of relevant social and institutional groupings in generating succesful political outcomes. To study these interactions, the paper adopts the circuits of power, a theoretical framework taken from the social sciences, and applies it to understanding the creation and development of the first standard in information security management. An informal group of UK, security chiefs sparked off a process which led first to BS7799, the British standard, and later to ISO 17799, the international standard. The case study portrays how the institutionalization of this ad hoc development process results,from the interactions of power among the stakeholders involved. The case study also shows how the different interests and objectives of the stakeholders were influenced by exogenous contingencies and institutional forces. The paper discusses theoretical and practical implications for the future development of such standards.
|keyword = power and politics,institutionalization,information systems security standards,information systems security management,security management code of practice,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Industry-wide information systems standardization as collective action: The case of the US residential mortgage industry'''
{{header}}
{{article
|author= Lynne Markus,Charles W. Steinfield,Rolf T. Wigand,Gabe Minton,
|source= MIS QUARTERLY
|year= 2006
|abstract = Vertical information systems (VIS) standards are technical specifications designed to promote coordination among the organizations within (or across) vertical industry sectors. Examples include the bar code, electronic data interchange (EDI) standards, and RosettaNet business process standards in the electronics industry. This contribution examines VIS standardization through the lens of collective action theory, applied in the literature to information technology product standardization, but not yet to VIS standardization, which is led by heterogeneous groups of user organizations rather than by IT vendors. Through an intensive case analysis of VIS standardization in the U.S. residential mortgage industry, VIS standardization success is shown to be as problematic as IT product standardization success, but for different reasons. VIS standardization involves two linked collective action dilemmas-standards development and standards diffusion characteristics, such that a solution to the first may fail to resolve the second. Whereas prior theoretical and empirical research shows that IT product standardization efforts tend to splinter into rival factions that compete through standards wars in the marketplace, successful VIS standards consortia must encompass heterogeneous groups of user organizations and IT vendors without fragmenting. Some tactics successfully used to solve the collective action dilemma of VIS standardization (e.g., governance mechanisms and policies about intellectual property protection) are also used by IT product standardization efforts, but some are different, and successful VIS standardization requires a package of solutions tailored to fit and jointly resolve the specific dilemmas of particular VIS standards initiatives.
|keyword = vertical IS standards and standardization,collective action,public goods theory,governance,intellectual property rights,technical design issues,institutional support,heterogeneity of resources and interests,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The ecology of standards processes: Insights from Internet standard making'''
{{header}}
{{article
|author= Jeffrey V. Nickerson,Michael zur Muehlen,
|source= MIS QUARTERLY
|year= 2006
|abstract = In order to create Internet standards, people and ideas move across many institutions. By drawing upon the new institutionalism and on organizational ecology, we develop an ecological approach to studying this movement. The approach examines the birth and death of standards bodies and the ideas they cultivate. We apply the approach to the history of Web services choreography standards, in which over 500 participants traversed nine institutions during a 12-year period. We explain critical aspects of this history by analyzing patterns of movement of standardization ideas. We show that standard-making institutions refuse to legitimate standards by utilizing bylaws which reflect the values of the institution; these values reflect the design legacy of the Internet. We formulate conjectures about the dynamics of the birth and death of working groups inside larger institutions that form a population ecology. We discuss plausible explanations for why specific Internet standard-making efforts do not resolve quickly. The theoretical implication of the study, is that an ecological approach will apply well to inventions that have been incubated, such as the Internet. The pragmatic implication is that changes to institutional Internet governance, particularly to the bylaws of standards bodies, can have drastic and unintended effects that will reshape the standard-making ecology.
|keyword = standard making,legitimacy,organizational ecology,institutionalism,Internet standards,Web services choreography,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A unified economic model of standard diffusion: The impact of standardization cost, network effects, and network topology'''
{{header}}
{{article
|author= Tim Weitzel,Daniel Beimborn,Wolfgang Koenig,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper is motivated by the following question: What drives the diffusion of a communication standard and what results can we expect? Past literature provides many instructive but mostly unrelated answers. Frequent findings are startup problems, penguin effects, and tendencies toward monopoly. But substantial problems in applying the models to concrete standardization problems reveal that the dynamics are probably more complex. Not all networks are ultimately conquered by a single standard once it has attracted a certain number of users. And not all diffusion results are either complete or no standardization.
|keyword = standardization,diffusion,network effects,equilibrium,noncooperative games,topology,cornputational analysis,penguin effects,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Migration to open-standard interorganizational systems: Network effects, switching costs, and path dependency'''
{{header}}
{{article
|author= Kevin Zhu,Kenneth L. Kraemer,Vijay Gurbaxani,Sean Xin Xu,
|source= MIS QUARTERLY
|year= 2006
|abstract = As firms seek to improve coordination through the use of electronic interorganizational systems (IOS), open standards are becoming increasingly important. To better understand diffusion, we investigate firms' the process of standards diffusion migration from proprietary or less-open IOS (i.e., electronic data interchange or EDI) to open-standard IOS (i.e., the Internet). Theoretical work in economics suggests that network are a determinant of network adoption, yet the work e extant literature falls short of empirical testing of the theory. We develop a conceptual model that features network effects, expected benefits, and adoption costs as prominent antecedents. We examine the model on a large dataset of 1,394 firms. The empirical results demonstrate the significant impacts on open-standard IOS adoption. impacts of network effects on open-standard IOS adoption. We find that adoption costs are a significant barrier to open-standard IOS adoption, but EDI users and nonusers treat this very differently: EDI users are much more sensitive to the costs of switching to the new, standard. This finding illustrates that experience with older standards may create switching costs and make it difficult to shift to open and switching costs and make it clef potentially better standards, a phenomenon called "excess inertia" in technology change. Further testing the underlying effects and adoption costs, factors that contribute to network effects and adoption costs, we find that trading community influence is a key driver of network effects, while managerial complexity, as opposed to financial costs, is a key determinant of adoption costs. Overall we believe that this study, based on a rigorous empirical analysis of a unique international dataset, provides valuable insights into a set of key factors that influence standards diffusion.
|keyword = open standards,standards diffusion,network effects,switching costs,path dependency,interorganizational systems,Internet,electronic data interchange,economics of standards,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Can vendors influence switching costs and compatibility in an environment with open standards?'''
{{header}}
{{article
|author= Pei-yu Chen,Chris Forman,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper examines the potential social costs of standardization, including possible vendor reactions to standards and their impacts on the adoption of new technology and long-term market structure. Specifically, we study how vendors might react to standards in the market for routers and switches, two of the most important pieces of networking hardware for the information systems infrastructure of modern firms. Using data from over 22,000 establishments surveyed vy Harte Hanks Market Intelligence, we provide evidence that vendors are able to maintain high switching costs in the market for routers and switches despite the presence of open standards in the industry. Several vendor actions are discussed in this paper, including manipulating horizontal compatibility between comparable rival products and vertical compatibility between complementary products, maintaining a broader product line, creating product suites, and targeting specific market segments. Our results further suggest that the presence of switching costs can lead to inefficient adoption of new information technology and that vendors may be able to influence the speed of new information technology adoption.
|keyword = open standards,switching costs,compatibility,switches,routers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reflexive standardization: Side effects and complexity in standard making'''
{{header}}
{{article
|author= Ole Hanseth,Edoardo Jacucci,Miria Grisot,Margunn Aanestad,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper addresses the general question proposed by the call of this special issue.- "What historical or contingent events and factors influence the creation of ICT standards, and in particular, their success or,failure? " Based on a case study) conducted over a period of three Years in a Norwegian hospital on the standardization process of an electronic patient record (EPR), the paper contributes to the current discussion on the conceptualization of standard-making in the field of Information Systems. By drawing upon the Concepts (of logic of ordering adopted from actor network theory; and efffects adopted from upon reflexivity and the unexpected side of reflexive modernization, the paper makes three key contributions: (1) it demonstrates the socio-technical complexity; of IS standards and standardization efforts; (2) it shows how complexity generates reflexive processes that undermine standardization aims; and () it suggests a theoretical interpretation of standardization complexity by using ideas from complexity theory and the theory of reflective modernization. These research questions are addressed by offering an historical and contingent analysis of the complexity, dynamics emerging from the case.
|keyword = standards,reflexive moderation,side effects,socio-technical theory,electronic patient records,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Issues and opinions - Publication opportunities in premier business outlets: How level is the playing field?'''
{{header}}
{{article
|author= JS Valacich,MA Fuller,C Schneider,AR Dennis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = This paper reports an analysis of the proportion of faculty publishing articles in premier business journals (i.e., the ratio of authors of premier business journal articles to total faculty of a discipline) across the disciplines of accounting, finance, management, marketing, and information systems (IS) for the years 1994-2003. This analysis revealed that over this period the management discipline had on average the highest proportion of faculty publishing in premier journals (12.7 authors per 100 management faculty), followed by finance (9.4 authors per 100 faculty), marketing (9.2 authors per 100 faculty), IS (5.5 authors per 100 faculty), and accounting (4.8 authors per 100 faculty). A further analysis examined these ratios for the different disciplines over time, finding that the ratios of authors to faculty have actually decreased for the disciplines of marketing and IS over this time period but have remained stable for the disciplines of accounting, management, and finance. Given steady growth in faculty size of all disciplines, the proportion of faculty publishing articles in premier journals in 2003 for all disciplines is lower than their 10-year averages, with IS having the lowest proportion in 2003. A sensitivity analysis reveals that without substantial changes that would allow more IS faculty to publish in the premier journals (e.g., by increasing publication cycles, number of premier outlets; and so on), IS will continue to lag far below the average of other disciplines. The implications of these findings for IS researchers, for institutions and administrators of IS programs, and for the IS academic discipline are examined. Based on these implications, recommendations for the IS discipline are presented.
|keyword = academic discipline,tenure,academic promotion,research journals,publication,scientometrics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impacts of license choice and organizational sponsorship on user interest and development activity in open source software projects'''
{{header}}
{{article
|author= KJ Stewart,AP Ammeter,LM Maruping,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = What differentiates successful from unsuccessful open source software projects? This paper develops and tests a model of the impacts of license restrictiveness and organizational sponsorship on two indicators of success: user interest in, and development activity on, open source software development projects. Using data gathered from Freshmeat.net and project home pages, the main conclusions derived from the analysis are that (1) license restrictiveness and organizational sponsorship interact to influence user perceptions of the likely utility of open source software in such a way that users are most attracted to projects that are sponsored by nonmarket organizations and that employ nonrestrictive licenses, and (2) licensing and sponsorship address complementary developer motivations such that the influence of licensing on development activity depends on what kind of organizational sponsor a project has. Theoretical and practical implications are discussed, and the paper outlines several avenues for future research.
|keyword = open source,software development,software licensing,success,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ERP investments and the market value of firms: Toward an understanding of influential ERP project variables'''
{{header}}
{{article
|author= C. Ranganathan,Carol V. Brown,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = This study contributes to the growing body of literature on the value of enterprise resource planning (ERP) investments at the firm level. Using an organization integration lens that takes into account investments in complementary resources as well as an options thinking logic about the value of an ERP platform, we argue that not all ERP purchases have the same potential impact at the firm level due to ERP project decisions made at the time of purchase. Based on a sample of 116 investment announcements in United States-based firms between 1997 and 2001, we find support for our hypotheses that ERP projects with greater functional scope (two or more value-chain modules) or greater physical scope (multiple sites) result in positive, higher shareholder returns. Furthermore, the highest increases in returns (3.29%) are found for ERP purchases with greater functional scope and greater physical scope; negative returns are found for projects with lesser functional scope and lesser physical scope. These findings provide empirical support for prior theory about the organizational integration benefits of ERP systems, the contribution of complementary resource investments to the business value of IT investments, and the growth options associated with IT platform investments. The article concludes with implications of our firm-level findings for this first wave of enterprise systems.
|keyword = business value,enterprise resource planning systems,enterprise systems,IT investments,market value,organizational impacts,project scope,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the adoption of multipurpose information appliances: The case of mobile data services'''
{{header}}
{{article
|author= SJ Hong,KY Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = We have come to a stage when information technology (IT) innovations have permeated every walk of life. Many new technologies can be used for many different purposes and in different contexts other than the workplace. The current study attempts to understand individual adoption of IT innovations that are used beyond work settings. We define a new class of IT innovations called multipurpose information appliances, which are personal, universally accessible, and multipurpose. The ubiquitous nature of these appliances has led to a constant permeability between the separate contexts of social life. An adoption model that reflects the unique characteristics and usage contexts of multipurpose information appliances was developed. The model consists of five sets of adoption factors and was tested using data collected on mobile data services adoption. Our findings show that the determinants of multipurpose information appliance adoption decisions are not only different from those in the workplace, but are also dependent on the nature of the target technology and its usage context. Theoretical and practical implications of the findings are discussed.
|keyword = technology adoption and usage,IT innovations in nonwork settings,information appliances,mobile data services,user perceptions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology, contract completeness, and buyer-supplier relationships'''
{{header}}
{{article
|author= RD Banker,J Kalvenes,RA Patterson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = The theory of incomplete contracts has been used to study the relationship between buyers and suppliers following the deployment of modern information technology to facilitate coordination between them. Previous research has sought to explain anecdotal evidence from some industries on the recent reduction in the number of suppliers selected to do business with buyers by appealing to relationship-specific costs that suppliers may incur. In contrast, this paper emphasizes that information technology enables greater completeness of buyer-supplier contracts through more economical monitoring of additional dimensions of supplier performance. The number of terms included in the contract is an imperfect substitute for the number of suppliers. Based on this idea, alternative conditions are identified under which increased use of information technology leads to a reduction in the number of suppliers without invoking relationship-specific costs. We find that a substantial increase in contract completeness due to reduced cost of information technology could increase the cost per supplier even though the cost of coordination and the cost per term monitored decrease. Such an increase in the cost per supplier leads to a reduction in the number of suppliers with whom the buyer chooses to do business. Similarly, we find that if coordination cost is reduced when more information technology is deployed so that the number of suppliers in the buyer's pool increases substantially, the buyer might choose to make the supplier contracts less complete, instead relying on a more market-oriented approach to finding a supplier with good fit.
|keyword = contract theory,transaction cost,interorganizational systems,business-to-business relationships,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding business process change failure: An actor-network perspective'''
{{header}}
{{article
|author= Suprateek Sarker,Saonee Sarker,Anna Sidorova,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = In this paper, we use concepts from actor-network theory (ANT) to interpret the sequence of events that led to business process change (BPC) failure at a telecommunications company in the United States. Through our intensive examination of the BPC initiative, we find that a number of issues suggested by ANT, such as errors in problematization, parallel translation, betrayal, and irreversible inscription of interests, contributed significantly to the failure. We provide nine abstraction statements capturing the essence of our findings in a concrete form. The larger implication of our study is that, for sociotechnical phenomena such as BPC with significant political components, an ANT-informed understanding can enable practitioners to better anticipate and cope with emergent complexities.
|keyword = actor-network theory,business process change,case study,information systems implementation,information systems politics,interpretive research,organizational change,power,reengineering,social construction of technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumer search and retailer strategies in the presence of online music sharing'''
{{header}}
{{article
|author= Sudip Bhattacharjee,Ram D. Gopal,Kaveepan Lertwachara,James R. Marsden,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Advances in online technologies and bandwidth availability have opened new vistas for online distribution of digital goods, but potential benefits for consumers are juxtaposed against challenges for retailers. Here, we investigate one type of digital experience good-music-whose market environment includes the very real presence of online piracy. Although arguments abound for and against online distribution of such digital goods, little research exists in this area. We develop a model of consumer search for such an experience good, and study different emerging market environments for retailers, where consumers can pirate music online. Retailer cost to publishers is modeled using a variety of licensing schemas. Survey results, together with data from online sharing networks, are utilized to validate a key assumption. Finally, computational analysis is used to develop insights that cannot be obtained analytically. Our results indicate that decreasing piracy is not necessarily equivalent to increasing profit, and online selling strategies can provide additional profits for a traditional retailer even in the presence of piracy. We show that leading strategies for business in such goods should include pricing options, provision of efficient search tools, and new licensing structures.
|keyword = digital experience goods,license,music,online channels,piracy,sampling,search,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Market segmentation within consolidated E-markets: A generalized combinatorial auction approach'''
{{header}}
{{article
|author= Joni L. Jones,Robert F. Easley,Gary J. Koehler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = We analyze an e-market design that allows multiple market segments to be served simultaneously with a single generalized combinatorial auction. The mechanism uses rule-based bids designed to accommodate various kinds of bidders, such as those more sensitive to price or those more restricted in their requirements. We demonstrate experimentally-using agent-based simulation of the actual market for television advertising slots-that the rule-based approach effectively handles the wide range of market segments, while maintaining buyer and seller surplus and efficiently allocating goods.
|keyword = combinatorial auction,electronic markets,market segmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How hypertext links influence consumer perceptions to build and degrade trust online'''
{{header}}
{{article
|author= Katherine J. Stewart,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = This paper suggests and empirically supports the propositions that a link between two organizations' Web sites will have simultaneous effects on trust in both the link sender and the link recipient, and that these effects result from interactions among the reputation of the link recipient, trust in the link sender, and the perceived relationship of the linked organizations. The study finds that the perceived relationship caused by a link leads to positive effects for the less reputable of the linked organizations, but negative effects for the more reputable organization. These effects are exaggerated or attenuated depending on the reputation of the organization that sends the link. The effect of presenting the link as an advertisement or a link to a partner was also examined, but no effect was uncovered, raising the question of how organizations may effectively differentiate links on their Web sites.
|keyword = entitativity,hypertext links,reputation,trust,trust transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Buyers' choice of online search strategy and its managerial implications'''
{{header}}
{{article
|author= Ravi Sen,Ruth C. King,Michael J. Shaw,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The Internet offers several tools such as shopping bots and search engines that help potential buyers search for lower prices. This paper defines buyers' online search strategy as using one or more of these tools to search for lower prices, and empirically investigates the validity of economics of information search theory in explaining buyers' choice of a particular online search strategy. We find that buyers' attitudes toward the price offered by their preferred online seller, their perception of online price dispersion, and their awareness of shopping agents have a significant effect on their choice of online search strategy. An understanding of buyers' choice of online search strategies can help an online seller to estimate its expected probability of making an online sale, optimize its online pricing, and improve its online promotional and advertising activities.
|keyword = e-commerce,e-markets,multinomial logit,online pricing,online search,online search tools,price search,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On the valuation of multistage information technology investments embedding nested real options'''
{{header}}
{{article
|author= Michel Benaroch,Sandeep Shah,Mark Jeffery,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = As real options analysis (ROA) is being applied to increasingly complex information technology (IT) investment problems, a concern arises over the use of heuristic ROA models that are simpler to apply but can produce overvaluations. A good example is the application of a heuristic nested variation of the Black-Scholes (BS) model to the evaluation of interrelated IT investments as nested options. This particular heuristic BS model could overvalue by more than 100 percent. Using a binomial model that is custom-tailored to a generic IT investment embedding nested options as the "baseline," we identify conditions under which the degree of overvaluation of this heuristic BS model is severe and unpredictable. Moreover, upon examining the structure of the custom-tailored binomial model, we identify the reason for overvaluation and derive a more accurate nested variation of the BS model. These findings should serve as a cautionary message about the use of untested heuristic ROA models.
|keyword = Black-Scholes model,interdependent investments,IT investment,nested real options,real options,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ERP misfit: Country of origin and organizational factors'''
{{header}}
{{article
|author= Eric T. G. Wang,Gary Klein,James J. Jiang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = There are many benefits of enterprise resource planning (ERP) systems, but their implementation is both complicated and difficult because the product spans functional silos and involves many internal and external entities. An ERP system is the outcome of social processes, and different ERP systems can embody distinct social arrangements when developed in different cultural contexts. Such social arrangements are difficult to change due the closure effect of technology stabilization. This leads to various misfit problems, both during and after ERP implementation, causing adverse effects on delivered ERP quality. With a survey of 85 ERP implementation cases in Taiwan, this study derives and empirically tests the main as well as the interaction effects of the country of origin of the ERP package, consultant quality, top management support, and user support of the ERP system quality as perceived by the client after implementation. The results demonstrate the important role of the country of origin of the ERP package and consultant quality in configuring a high-quality ERP system and alleviating the negative effect of misfit problems.
|keyword = consultant quality,country of origin,enterprise resource planning (ERP),implementation,misfit,social shaping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reliability, mindfulness, and information systems'''
{{header}}
{{article
|author= BS Butler,PH Gray,
|source= MIS QUARTERLY
|year= 2006
|abstract = In a world where information technology is both important and imperfect, organizations and individuals are faced with the ongoing challenge of determining how to use complex, fragile systems in dynamic contexts to achieve reliable outcomes. While reliability is a central concern of information systems practitioners at many levels, there has been limited consideration in information systems scholarship of how firms and individuals create, manage, and use technology to attain reliability. We propose that examining how individuals and organizations use information systems to reliably perform work- will increase both the richness and relevance of IS research. Drawing from studies of individual and organizational cognition, we examine the concept of mindfulness as a theoretical foundation for explaining efforts to achieve individual and organizational reliability in the face of complex technologies and surprising environments. We then consider a variety of implications of mindfulness theories of reliability in the form of alternative interpretations of existing knowledge and new directions for inquiry in the areas of IS operations, design, and management.
|keyword = mindfulness,reliability,IS operations,IS management,IS design,resilience,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical analysis of the value of complete information for eCRM models'''
{{header}}
{{article
|author= B Padmanabhan,ZQ Zheng,SO Kimbrough,
|source= MIS QUARTERLY
|year= 2006
|abstract = Due to the vast amount of user data tracked online, the use of data-based analytical methods is becoming increasingly common for e-businesses. Recently the term analytical eCRM has been used to refer to the use of such methods in the online world. A characteristic of most of the current approaches in eCRM is that they use data collected about users' activities at a single site only and, as we argue in this paper, this can present an incomplete picture of user activity. However, it is possible to obtain a complete picture of user activity from across-site data on users. Such data is expensive, but can be obtained by firms directly from their users or from market data vendors. A critical question is whether such data is worth obtaining, an issue that little prior research has addressed. In this paper, using a data mining approach, we present an empirical analysis of the modeling benefits that can be obtained by having complete information. Our results suggest that the magnitudes of gains that can be obtained from complete data range from a few percentage points to 50 percent, depending on the problem for which it is used and the performance metrics considered. Qualitatively we find that variables related to customer loyalty and browsing intensity are particularly important and these variables are difficult to derive from data collected at a single site. More importantly, we find that a firm has to collect a reasonably large amount of complete data before any benfits can be reaped and caution against acquiring too little data.
|keyword = data mining,incomplete data,information value,eCRM,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of state-based and event-based data representation on user performance in query formulation tasks'''
{{header}}
{{article
|author= GN Allen,ST March,
|source= MIS QUARTERLY
|year= 2006
|abstract = Ad hoc query formulation is an important task in effectively utilizing organizational data resources. To facilitate this task, managers and casual end-users are commonly presented with database views expressly constructed for their use. Differences in the way in which things, states, and events are represented in such views can affect a users ability to understand the database, potentially leading to different levels of performance (i.e., accuracy, confidence, and prediction of the accuracy of their queries). An experiment was conducted over the Internet involving 342 subjects from 6 universities in North America and Europe to investigate these effects. When presented with an event-based view, subjects expressing low or very low comfort levels in reading entity-relationship diagrams expressed confidence that better predicted query accuracy although there were no significant differences in actual query accuracy or level of confidence expressed.
|keyword = query formulation performance,event-based,state-based,artifact-based,data models,database user view,sense-making,E-R diagram,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of ideology on effectiveness in open source software development teams'''
{{header}}
{{article
|author= Katherine J. Stewart,Sanjay Gosain,
|source= MIS QUARTERLY
|year= 2006
|abstract = The emerging work on understanding open source software has questioned what leads to effectiveness in OSS development teams in the absence of formal controls, and it has pointed to the importance of ideology. This paper develops a framework of the OSS community ideology (including specific norms, beliefs, and values) and a theoretical model to show how adherence to components of the ideology impacts effectiveness in OSS teams. The model is based on the idea that the tenets of the OSS ideology motivate behaviors that enhance cognitive trust and communication quality and encourage identification with the project team, which enhances affective trust. Trust and communication in turn impact OSS team effectiveness. The research considers two kinds of effectiveness in OSS teams: the attraction and retention of developer input and the generation of project outputs. Hypotheses regarding antecedents to each are developed. Hypotheses are tested using survey and objective data on OSS projects. Results support the main thesis that OSS team members' adherence to the tenets of the OSS community ideology impacts OSS team effectiveness and reveal that different components impact effectiveness in different ways. Of particular interest is the finding that adherence to some ideological components was beneficial to the effectiveness of the team in terms of attracting and retaining input, but detrimental to the output of the team. Theoretical and practical implications are discussed.
|keyword = open source software,trust,ideology,communication,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Plant information systems, manufacturing capabilities, and plant performance'''
{{header}}
{{article
|author= Rajiv D. Banker,Indranil R. Bardhan,Hsihui Chang,Shu Lin,
|source= MIS QUARTERLY
|year= 2006
|abstract = Firms have been investing over $5 billion a year in recent years on new information technology, and software in their manufacturing plants. In this study, we develop a conceptual model based on the theory of dynamic capabilities to study how manufacturing plants realize improvements in plant performance by leveraging plant information systems to enable implementation of advanced manfacturing capabilities. We develop hypotheses about relationships between information systems, their impact on manufacturing practices, and the overall impact on plant performance. Analysis of survey data from 1, 077 U.S. manufacturing plants provides empirical support for the dynamic capabilities model and suggests that manufacturing capabilities mediate the impact of information systems on plant performance. Our results underscore the importance of manufacturing and organizational capabilities in studying the impact of IT on manufacturing plant productivity, and provide a sharper theoretical lens to evaluate them impact.
|keyword = manufacturing capabilities,information systems,resource planning systems,electronic data interchange,just-in-time manufacturing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing peer-to-peer conflicts in disruptive information technology innovations: The case of software reuse'''
{{header}}
{{article
|author= Karma Sherif,Robert W. Zmud,Glenn J. Browne,
|source= MIS QUARTERLY
|year= 2006
|abstract = We examine the case of software reuse as a disruptive information technology innovation (i.e., one that requires changes in the architecture of work processes) in software development organizations. Using theories of conflict, coordination, and learning, we develop a model to explain peer-to-peer conflicts that are likely to accompany the introduction of disruptive technologies and how appropriately devised managerial interventions (e.g., coordination mechanisins and organizational learning practices) can lessen these conflicts. A study of software reuse programs in four organizations was conducted to assess the validity of the model. Qualitative and quantitative analyses of the data obtained showed that companies that had implemented such managerial interventions experienced greater success with their software reuse programs. Implications for theory and practice are discussed.
|keyword = disruptive IT innovations,software reuse,goal conflict,coordination mechanisms,organizational learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Internet exchanges for used books: An empirical analysis of product cannibalization and welfare impact'''
{{header}}
{{article
|author= A Ghose,MD Smith,R Telang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Information systems and the Internet have facilitated the creation of used-product markets that feature a dramatically wider selection, lower search costs, and lower prices than their brick-and-mortar counterparts do. The increased viability of these used-product markets has caused concern among content creators and distributors, notably the Association of American Publishers and Author's Guild, who believe that used-product markets will significantly cannibalize new product sales. This proposition, while theoretically possible, is based on speculation as opposed to empirical evidence. In this paper, we empirically analyze the degree to which used products cannibalize new-product sales for books-one of the most prominent used-product categories sold online. To do this, we use a unique data set collected from Amazon.com's new and used book marketplaces to measure the degree to which used products cannibalize new-product sales. We then use these estimates to measure the resulting first-order changes in publisher welfare and consumer surplus. Our analysis suggests that used books are poor substitutes for new books for most of Amazon's customers. The cross-price elasticity of new-book demand with respect to used-book prices is only 0.088. As a result, only 16% of used-book sales at Amazon cannibalize new-book purchases. The remaining 84% of used-book sales apparently would not have occurred at Amazon's new-book prices. Further, our estimates suggest that this increase in book readership from Amazon's used-book marketplace increases consumer surplus by approximately $67.21 million annually This increase in consumer surplus, together with an estimated $45.05 million loss in publisher welfare and a $65.76 million increase in Amazon's profits, leads to an increase in total welfare to society of approximately $87.92 million annually from the introduction of used-book markets at Amazon.com.
|keyword = publisher welfare,retailer welfare,consumer surplus,price competition,used-books sales,electronic markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When the wait isn't so bad: The interacting effects of website delay, familiarity, and breadth'''
{{header}}
{{article
|author= DE Galletta,RM Henry,S McCoy,P Polak,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Although its popularity is widespread, the Web is well known for one particular drawback: its frequent delay when moving from one page to another. This experimental study examined whether delay and two other website design variables (site breadth and content familiarity) have interaction effects on user performance, attitudes, and behavioral intentions. The three experimental factors (delay, familiarity, and breadth) collectively impact the cognitive costs and penalties that users incur when making choices in their search for target information. An experiment was conducted with 160 undergraduate business majors in a completely counterbalanced, fully factorial design that exposed them to two websites and asked them to browse the sites for nine pieces of information. Results showed that all three factors have strong direct impacts on performance and user attitudes, in turn affecting behavioral intentions to return to the site, as might be expected. A significant three-way interaction was found between all three factors indicating that these factors not only individually impact a user's experiences with a website, but also act in combination to either increase or decrease the costs a user incurs. Two separate analyses support an assertion that attitudes mediate the relationship of the three factors on behavioral intentions. The implications of these results for both researchers and practitioners are discussed. Additional research is needed to discover other factors that mitigate or accentuate the effects of delay, other effects of delay, and under what amounts of delay these effects occur.
|keyword = electronic commerce,response time,website design,attitudes,performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Conceptualizing systems for understanding: An empirical test of decomposition principles in object-oriented analysis'''
{{header}}
{{article
|author= A Burton-Jones,PN Meso,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = During the early phase of systems development, systems analysts often conceptualize the domain under study and represent it in one or more conceptual models. One of the most important, yet elusive roles of conceptual models is to increase analysts' understanding of a domain. In this paper, we evaluate the ability of the good decomposition model (GDM) (Wand and Weber 1990) to explain the degree to which conceptual models communicate meaning about a domain to analysts. We address the question, "Do unified modeling language (UML) analysis diagrams that manifest better decompositions increase analysts' understanding of a domain?" GDM defines five conditions (minimality, determinism, losslessness, weak coupling, and strong cohesion) deemed necessary to decompose a domain in such a way that the resulting model communicates meaning about the domain effectively. In our evaluation, we operationalized each of these conditions in a set of UML diagrams and tested participants' understanding of those diagrams. Our results lend support to GDM across measures of actual understanding. However, the impact on participants' perceptions of their understanding was equivocal.
|keyword = systems analysis,conceptualization,conceptual model,decomposition,object oriented,unified modeling language,ontology,systems principles,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding conceptual schemas: Exploring the role of application and IS domain knowledge'''
{{header}}
{{article
|author= V Khatri,I Vessey,V Ramesh,P Clay,SJ Park,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2006
|abstract = Although information systems (IS) problem solving involves knowledge of both the IS and application domains, little attention has been paid to the role of application domain knowledge. In this study, which is set in the context of conceptual modeling, we examine the effects of both IS and application domain knowledge on different types of schema understanding tasks: syntactic and semantic comprehension tasks and schema-based problem-solving tasks. Our thesis was that while IS domain knowledge is important in solving all such tasks, the role of application domain knowledge is contingent upon the type of understanding task under investigation. We use the theory of cognitive fit to establish theoretical differences in the role of application domain knowledge among the different types of schema understanding tasks. We hypothesize that application domain knowledge does not influence the solution of syntactic and semantic comprehension tasks for which cognitive fit exists, but does influence the solution of schema-based problem-solving tasks for which cognitive fit does not exist. To assess performance on different types of conceptual schema understanding tasks, we conducted a laboratory experiment in which participants with high- and low-IS domain knowledge responded to two equivalent conceptual schemas that represented high and low levels of application knowledge (familiar and unfamiliar application domains). As expected, we found that IS domain knowledge is important in the solution of all types of conceptual schema understanding tasks in both familiar and unfamiliar applications domains, and that the effect of application domain knowledge is contingent on task type. Our findings for the EER model were similar to those for the ER model. Given the differential effects of application domain knowledge on different types of tasks, this study highlights the importance of considering more than one application domain in designing future studies on conceptual modeling.
|keyword = conceptual modeling,conceptual schema understanding,the theory of cognitive fit,syntactic comprehension tasks,semantic comprehension tasks,schema-based problem-solving tasks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Turning a community into a market: A practice perspective on information technology use in boundary spanning'''
{{header}}
{{article
|author= N Levina,E Vaast,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = This paper examines how information technology (IT) transforrns relations across fields of practice within organizations. Drawing on Bourdieu's practice theory, we argue that the production of any practice involves varying degrees of embodiment (i.e., relying on personal relationships) and objectification (i.e., relying on the exchange of objects). We subsequently characterize boundary-spanning practices according to their relative degrees of embodiment and objectification. We distinguish between "market-like" boundary-spanning practices, which rely primarily on an objectified mode of practice production, from "community-like" practices, which involve mostly the embodied mode of practice production. IT is then conceptualized as a medium for sharing objects in the production of practices. As such, IT use allows for the sharing of objects without relying on embodied relationships. We use data from an in-depth ethnographic case study to investigate how IT was used to transform community-like boundary-spanning practices within an organization into market-like ones. Moreover, we demonstrate how, as IT was used to support the exchange and combination of depersonalized objects, other aspects of the practice (such as the roles of intermediaries and the nature of meetings) also changed. The related changes in these diverse aspects of a boundary-spanning practice supported the trend toward greater objectification. IT use also increased visibility of the terms associated with object exchange. This increased visibility exposed the inequity of the exchange and encouraged the disadvantaged party to renegotiate the relationship.
|keyword = boundary objects,boundary spanners,boundary spanning,communities of practice,coordination mechanisms,information technology use,practice theory,qualitative methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Field experiences with eXtreme programming: Developing an emergency response system'''
{{header}}
{{article
|author= A Fruhling,GJ De Vreede,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = eXtreme Programming (XP) is a well-known agile software development method. While a number of reports have been published on explaining the XP methodology and the perceived benefits when using XP for system development in recent years, less is known about the actual operationalization of the XP principles. This paper presents an action research study reporting on the experiences of implementing the XP methodology in a development project for a Web-based, distributed information system. The goal of this research was to increase the understanding of how to effectively operationalize XP techniques so that the system being developed catered to today's fast-paced technological environment by allowing the developers to respond quickly to innovative and changing requirements. Overall, the research indicates that most of the XP principles could be effectively implemented; however, three of the principles required modification (i.e., testing, pair programming, customer collocation). Several benefits resulted from the usage of XP. The rapid prototyping enabled information technology developers and users to clarify system requirements, communicate openly, quickly build rapport, and create an interface that was easy to use and learn. Further, the research found that where the technology was new or foreign to the development team and the user, the XP process was flexible enough to support several iterations of technology and produce prototypes in a timely manner. Pair programming appeared to work effectively and offer value; however, it is not always practically feasible.
|keyword = action research,agile methodologies,emergency response,eXtreme programming,pair programming,rapid prototyping,software testing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating the adoption of enterprise application integration in health-care organizations'''
{{header}}
{{article
|author= K Khoumbati,M Themistocleous,Z Irani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The integration of heterogeneous information systems has always been problematic in health-care organizations, as it is associated with the delivery of key services and has high operational costs. Therefore, health-care organizations are looking for new means to increase their functional capabilities and reduce integration cost. In addressing this need, enterprise application integration (EAI) technology has emerged to facilitate systems integration, enhance the quality of services, and reduce integration costs. Despite the application of EAI in other sectors, its adoption in health care is slow. In seeking to build on the limited normative research surrounding EAI, the authors of this paper focus on the evaluation of factors that influence EAI adoption in the health-care sector. In doing so, using fuzzy cognitive mapping as a technique to identify causal interrelationships among the EAI adoption factors. This approach will enhance the quality of the evaluation process and emphasizes the importance of each factor and its interrelationship with other factors. The outcomes shown in this paper will support health-care organizations' decision makers in exploring the implications surrounding EAI adoption.
|keyword = enterprise application integration,health-care organization,information system adoption,information system evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An information systems security risk assessment model under the Dempster-Shafer theory of belief functions'''
{{header}}
{{article
|author= LL Sun,RP Srivastava,TJ Mock,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = This study develops an alternative methodology for the risk analysis of information systems security (ISS), an evidential reasoning approach under the Dempster-Shafer theory of belief functions. The approach has the following important dimensions. First, the evidential reasoning approach provides a rigorous, structured manner to incorporate relevant ISS risk factors, related countermeasures, and their interrelationships when estimating ISS risk. Second, the methodology employs the belief function definition of risk-that is, ISS risk is the plausibility of ISS failures. The proposed approach has other appealing features, such as facilitating cost-benefit analyses to help promote efficient ISS risk management. The paper elaborates the theoretical concepts and provides operational guidance for implementing the method. The method is illustrated using a hypothetical example from the perspective of management and a real-world example from the perspective of external assurance providers. Sensitivity analyses are performed to evaluate the impact of important parameters on the model's results.
|keyword = belief function theory,cost-benefit analysis,evidential reasoning,information systems security,risk analysis,sensitivity analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaborative activities in virtual settings: A knowledge management perspective of telemedicine'''
{{header}}
{{article
|author= DL Paul,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Advances in information and communications technology have made possible collaborative activities in virtual settings. Virtual settings can significantly expand the knowledge resources available, yet they also create additional challenges to the already difficult activities of collaborating. The purpose of this research is to provide a better understanding of how collaborative activities in virtual settings enable the different parties to achieve their desired objectives by examining them from a knowledge management perspective. Three aspects of knowledge management-knowledge transfer, knowledge discovery, and knowledge creation-are examined in the context of telemedicine projects. The findings indicate that an association exists between the types of collaborative activities engaged in virtual settings and the effects such projects are perceived as having. While this research focuses only on virtual collaborative activities in health care, it is likely that these findings are applicable to other industries engaged in such activities in virtual settings.
|keyword = collaboration,knowledge management,telemedicine,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Digital inclusiveness - Longitudinal study of Internet adoption by older adults'''
{{header}}
{{article
|author= JCY Lam,MKO Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = In order to build a digital inclusive society, both government and nongovernment organizations in countries such as China, Japan, Korea, Singapore, Taiwan, the United Kingdom, and the United States have been offering training programs to the general public and establishing communitywide public access computer facilities in recent years. However, offering training programs and enabling access to facilities are not sufficient on their own if, due to other reasons, the socially disadvantaged groups do not choose to make use of the facilities. As an exploratory investigation, this study focuses on the voluntary adoption of these facilities (typified by the Internet) by one such disadvantaged group-older adults. In particular, this study investigates the role of Internet self-efficacy and Outcome expectations in older adults' usage of the Internet through a three-part longitudinal study, involving almost 1,000 participants. A theoretical model based on social cognitive theory was developed and empirically tested through both surveys and lab experiments. Behavioral modeling training courses were offered to adults age 55 or older in the study over a one-year period. Questionnaire surveys and cognitive knowledge assessments were conducted. In general, the findings in the longitudinal study (including three repeated measures) validated the affects of Internet self-efficacy and outcome expectations on usage intention, and the important roles of support and encouragement in the formation of self-efficacy and outcome expectations. Limitations and implications are discussed.
|keyword = behavioral modeling,computer training,digital divide,digital inclusiveness,information systems adoption,Internet self-efficacy,social cognitive theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interactions between system evaluation and theory testing: A demonstration of the power of a multifaceted approach to information systems research'''
{{header}}
{{article
|author= JW Cao,JM Crews,M Lin,A Deokar,JK Burgoon,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Historically, information systems (IS) researchers have questioned which research paradigms, activities, and methods IS research should follow. In this paper, we argue that different research methods and activities may interact with each other, different research paradigms may complement each other due to such interactions, and therefore, a multimethodological, cross-paradigm research approach may result in better IS research than a singular approach. Three existing multimethodological IS research frameworks are reviewed and summarized into an integrated approach. Two types of interactions between different research methods across system evaluation and theory testing research activities are identified. A three-year research study about a computer-based training system for deception detection (Agent99 Trainer) provides a concrete example to demonstrate the existence and research benefits of these two types of interactions, as well as the benefits of a multimethodological, cross-paradigm IS research approach.
|keyword = IS research framework,research activity,research method,research paradigm,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Discovering cues to error detection in speech recognition output: A user-centered approach'''
{{header}}
{{article
|author= LN Zhou,YM Shi,DS Zhang,A Sears,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = The great potential of speech recognition systems in freeing users' hands while interacting with computers has inspired a variety of promising applications. However, given the performance of the state-of-the-art speech recognition technology today, widespread acceptance of speech recognition technology would not be realistic without designing and developing new approaches to detecting and correcting recognition errors effectively. In seeking solutions to the above problem, identifying cues to error detection (CERD) is central. Our survey of the extant literature on the detection and correction of speech recognition errors reveals that the system-initiated, data-driven approach is dominant, but that heuristics from human users have been largely overlooked. This may have hindered the advance of speech technology. In this research, we propose a user-centered approach to discovering CERD. User studies are carried out to implement the approach. Content analysis of the collected verbal protocols lends itself to a taxonomy of CERD. The CERD discovered in this study can improve our knowledge on CERD by not only validating CERD from a user's perspective but also suggesting promising new CERD for detecting speech recognition errors. Moreover, the analysis of CERD in relation to error types and other CERD provides new insights into the context where specific CERD are effective. The findings of this study can be used to not only improve speech recognition output but also to provide context-aware support for error detection. This will help break the barrier for mainstream adoption of speech technology in a variety of information systems and applications.
|keyword = cues to error detection,speech recognition,taxonomy,verbal protocol analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The market's perception of the transactional risks of information technology outsourcing announcements'''
{{header}}
{{article
|author= WS Oh,MJ Gallivan,JW Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Despite the fact that several event studies have investigated the market's reaction to information technology (IT) investment announcements, little is known about how specific transactional risks influence the market value of a firm. This study examines stock market data to assess investors' responses to various transactional risks associated with IT outsourcing. More specifically, we develop and test several hypotheses to understand how transactional risks that arise due to a range of factors (i.e., the size of outsourcing contracts, difficulties in performance monitoring, asset specificity of IT resources, vendor capability, and the lack of cultural similarity between client and vendor firms) influence investors' reactions to IT outsourcing announcements. Our results indicate that most of these factors indeed significantly influence investors' perceptions of the risks involved in IT outsourcing. We discuss these findings in a larger organizational context and offer implications for both research and practice. In particular, our study offers a theoretical rationale for why negative reactions to IT outsourcing announcements may occur, while providing practitioners with several means by which they can increase the informational value of outsourcing arrangements.
|keyword = agency theory,asset specificity,event study,industry similarity,information technology outsourcing,resource dependency,transaction cost economics,transactional risks,vendor capability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring attribute correspondences across heterogeneous databases by mutual information'''
{{header}}
{{article
|author= HM Zhao,ES Soofi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Identifying attribute correspondences across heterogeneous databases is a critical and time-consuming step in integrating the databases. Past research has applied correlation analysis techniques to explore correspondences between attributes. These techniques, however, are appropriate for numeric attributes that are linearly related. This paper proposes an information-theoretic approach to exploring correspondences between attributes in heterogeneous databases. The proposed approach is applicable to character attributes, as well as to numeric attributes, regardless whether or not they are linearly related. It overcomes some serious shortcomings of previous approaches based on correlation analysis and has much broader applicability. The proposed procedure samples both matching and nonmatching pairs of records from the databases under consideration, applies matching functions to compare pairs of attributes, and then uses the mutual information to measure the dependency between a matching function as applied to a pair of attributes and the class (i.e., matching or nonmatching) of a pair of records. A high mutual information index implies a potential attribute correspondence, which is presented to the analyst for further evaluation. The paper also presents some empirical results demonstrating the utility of the proposed approach.
|keyword = attribute correspondence,attribute matching,composite information systems,database interoperability,heterogeneous databases,information theory,interorganizational systems,mutual information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The use of cognitive maps and case-based reasoning for B2B negotiation'''
{{header}}
{{article
|author= KC Lee,SJ Kwon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2006
|abstract = Conventional approaches to business-to-business (B2B) negotiation use primary negotiation terms (PNTs) such as price or order quantity for modeling and analysis, but pay little attention to Such secondary negotiation terms (SNTs) as resource availability and corporate culture. This paper argues that SNTs also contribute to good negotiation decisions because PNTs and SNTs are closely interlinked in the form of causal relationships. Moreover, B2B negotiation demands a practical and useful framework that can reuse past negotiation knowledge and perform "what-if" analysis. This paper proposes a framework that consists of formalization, reuse, and problem-solving phases. The framework first formalizes TAKBN (tacit knowledge about B2B negotiation) with both PNTs and SNTs using a cognitive map and case-based reasoning, then stores them in case bases as cases that can be retrieved for later use and problem solving. This framework provides a platform with which decision makers can study past B2B negotiation cases, apply them to current B2B negotiation problems, and simulate different negotiation situations before making decisions. The framework has been tested using two practical scenarios. A structured, 13-item questionnaire was rigorously developed and applied to evaluate the validity of the proposed framework based on 16 B2B negotiation experts' judgments. Statistical tests proved that the proposed framework could improve decision performance significantly in B2B negotiations.
|keyword = B2B negotiation,case-based reasoning,causal relationship,cognitive map,primary negotiation terms,secondary negotiation terms,tacit knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The personalization privacy paradox: An empirical evaluation of information transparency and the willingness to be profiled online for personalization'''
{{header}}
{{article
|author= NF Awad,MS Krishnan,
|source= MIS QUARTERLY
|year= 2006
|abstract = Firms today use information about customers to improve service and design personalized offerings. To do this successfully however, firms must collect consumer information. This study enhances awareness about a central paradox for firms investing in personalization; namely, that consumers who value information transparency are also less likely to participate in personalization. We examine the relationship between information technology features, specifically , information transparency features, and consumer willingness to share information for online personalization. Based on a survey of over 400 online consumers, we examine the question of whether customer perceived information transparency is associated with consumer willingness to be profiled online. Our results indicate that customers who desire greater information transparency are less willing to be profiled. This result poses a dilemma for firms, as the consumers that value information transparency features most are also the consumers who are less willing to be profiled online. In order to manage this dilemma, we suggest that firms adopt a strategy of providing features that address the needs of consumers who are more willing to partake in personalization, therefore accepting that the privacy sensitive minority of consumers are unwilling to participate in personalization, despite additional privacy features.
|keyword = online privacy,information transparency,web site features,online experience,consumer privacy,online personalization,online information sharing,empirical studies of information systems,business value of information systems,information sharing practices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of cognitive fit in the relationship between software comprehension and modification'''
{{header}}
{{article
|author= TM Shaft,I Vessey,
|source= MIS QUARTERLY
|year= 2006
|abstract = Although there is a long tradition of empirical studies of software developers, few studies have focused on software maintenance. Prior work is predicated on the belief that higher levels of software comprehension are associated with higher levels of performance on modification tasks. This study provides a more complete understanding of the relationship between software comprehension and modification. We conceptualize software maintenance as inter-linking comprehension and modification, and argue that the relationship between the two is moderated by cognitive fit. Specifically, cognitive fit exists when the software maintainer's dominant mental representation of the software and their mental representation of the modification task emphasize the same type of knowledge. We hypothesize that when cognitive fit exists, greater improvements in comprehension are associated with higher levels of performance on a modification task. When cognitive fit does not exist, however, the software maintainer's mental representations of the software and of the modification task do not emphasize the same type of knowledge, which may mean that attention is devoted to comprehension at the expense of modification, resulting in lower performance on the modification task. In these circumstances, comprehension and modification tasks may interfere with each other, an effect known as dual-task interference. We therefore hypothesize that performance on a modification task is moderated by the fit between the mental representation of the software and that of the modification task. We tested our theory by varying cognitive fit to create matched and mismatched conditions in a single experiment that used IT professionals as subjects. Our finding, support our theory: cognitive fit moderates the relationship between comprehension and modification. Specifically, changes in software comprehension and modifcation performance are positively related when cognitive fit exists and negatively related when cognitive fit does not exist. Our findings demonstrate the need to examine more complex relationships among the numerous types of tasks involved in software development rather than examining software comprehension alone.
|keyword = theory of cognitive fit,software modification,software comprehension,dual-task interference,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Performance effects of information technology synergies in multibusiness firms'''
{{header}}
{{article
|author= H Tanriverdi,
|source= MIS QUARTERLY
|year= 2006
|abstract = Unlike technologies that are applicable in a few specific industries, information technologies have a wide range of applicability across almost all industries. The fundamental principles of good IT management are also applicable in many industries. Thus,firms whose business units operate in different industries have all opportunity to exploit cross-unit IT synergies by applying their IT resources and management processes across multiple units. This study, examines sources of cross-unit IT synergy and the conditions under which cross-unit IT synergies improve the performance of multibusiness firms. Building on the resource-based view of diversification and the economic theory of complementarities, the study identifies the relatedness and complementarity of IT resources as two major sources of cross-unit IT synergy. It argues that IT relatedness-the use of common IT infrastructure technologies and common IT management processes across business units-creates sub-additive cost synergies, whereas complementarities among IT infrastructure technologies and IT management processes create super-additive value synergies. In a sample of 356 multibusiness Fortune 1000 firms, the study finds that sub-additive cost synergies arising from the use of related IT resources or management processes do not have any effects on corporate performance, whereas the super-additive value synergies arising from the use of a complementary set of IT resources and management processes have significant effects on corporate performance. The diversification level of the firm moderates the relationship between IT synergies and corporate performance. As the diversification level increases, the performance effects of IT synergies remain positive, but they become weaker. The IT governance mode of the firm (centralized decentralized, hybrid) does not make a difference in the performance effects of IT synergies.
|keyword = corporate performance,multibusiness firm,synergy,relatedness,complementarity,diversification,IT governance,coordination,survey,second-order construct,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The differential use and effect of knowledge-based system explanations in novice and expert judgment decisions'''
{{header}}
{{article
|author= V Arnold,N Clark,PA Collier,SA Leech,SG Sutton,
|source= MIS QUARTERLY
|year= 2006
|abstract = Explanation facilities are considered essential in facilitating user interaction with knowledge-based systems (KBS). Research on explanation provision and the impact on KBS users has shown that the domain expertise affects the type of explanations selected by the user and the basis for seeking such explanations. The prior literature has been limited, however, by the use of simulated KBS that generally provide only feedback explanations (i.e., ex post to the recommendation of the KBS being presented to the user). The purpose of this study is to examine the way users with varying levels of expertise use alternative types of KBS explanations and the impact of that use on decision making. A total of 64 partner/manager-level and 82 senior/staff-level insolvency professionals participated in an experiment involving the use of a fully functioning KBS to complete a complex judgment task. In addition to feedback explanations, the KBS also provided feedforward explanations (i. e., general explanations during user input about the relationships between information cues in the KBS) and included definition type explanations (i.e., declarative-level knowledge). The results show that users were more likely to adhere to recommendations of the KBS when an explanation facility was available. Choice patterns in using explanations indicated that novices used feedforward explanations more than experts did, while experts were more likely than 170 vices to use feedback explanations. Novices also used more declarative knowledge and initial problem solving type explanations, while experts used more procedural knowledge explanations. Finally, use of feedback explanations led to greater adherence to the KBS recommendation by experts-a condition that was even more prevalent as the use of feedback explanations increased The results have several implications for the design and use of KBS in a professional decision-making environment.
|keyword = explanations,explanation use,knowledge-based systems,expert systems,intelligent systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Unraveling the temporal fabric of knowledge conversion: A model of media selection and use'''
{{header}}
{{article
|author= AP Massey,MM Montoya-Weiss,
|source= MIS QUARTERLY
|year= 2006
|abstract = We draw from and extend Nonaka's (1994) theory of knowledge creation to develop a model of media selection and use in the knowledge conversion (KC) process. KC is a process wherein an individual is affected by the experiences of another. The out comes of KC-transferred and transformed knowledge-hinge on the development Of understanding. The KC process is enabled via various communicative and noncommunicate media. Because the KC process occur over time, it possesses a temporal fabric or structure. We explore the practical realities of KC as a dynamic, time- and experience-dependent process. We consider how the temporal fabric of KC creates an evolving reciprocal relationship among perceived media utility, selection, and use of media, as well as switching and/or combining media. We propose and discuss two key factors as determinants of perceived media utility use in the KC process: (1) the temporal behavior of individuals engaged in the KC process and (2) individual and joint experience-based factors. We also discuss the role of contextual factors as antecedents. Finally, we offer and illustrate two primary temporal structures for KC media selection and use: (1) monophasic, wherein KC participants use a single medium at a time, and (2) polyphasic, wherein KC participants deploy multiple media simultaneously. We conclude with a discussion of the implications for the design of KC-enabling systems and directions for future research.
|keyword = knowledge management,knowledge conversion,individual temporal behavior,media utility,monophasic and polyphasic temporal structures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding and predicting electronic commerce adoption: An extension of the theory of planned behavior'''
{{header}}
{{article
|author= PA Pavlou,M Fygenson,
|source= MIS QUARTERLY
|year= 2006
|abstract = This paper extends Ajzen's (1991) theory of planned behavior (TPB) to explain and predict the process of e-commerce adoption by consumers. The process is captured through two online consumer behaviors: (1) getting information and (2) purchasing a product from a Web vendor. First, we simultaneously, model the association between these two contingent online behaviors and their respective intentions by appealing to consumer behavior theories and the theory of implementation intentions, respectively. Second following TPB, we derive for each behavior its intention, attitude, subjective norm, and perceived behavioral control (PBC). Third we elicit and test a comprehensive set of salient beliefs for each behavior. A longitudinal study with online consumers supports the proposed e-commerce adoption model, validating the predictive power of TPB and the proposed conceptualization of PBC as a higher-order factor formed by self-efficacy and controllability. Our findings stress the importance of trust and technology adoption variables (perceived usefulness and ease of use) as salient beliefs for predicting e-commerce adoption, justifying the integration of trust and technology adoption variables within the TPB framework. In addition, technological characteristics(downloaded delay, Website navigability and information protection), consumer skills, time and monetary resources, and product characteristics (product diagnosticity and product value) add to the explanatory and predictive power of our model. Implications for information Systems, e-commerce, TPB, and the study of trust are discussed.
|keyword = theory of planned behavior,perceived behavioral control,self-efficacy,controllability,technology adoption,technology acceptance model,trust,electronic commerce,consumer behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Incorporating software agents into supply chains: Experimental investigation with a procurement task'''
{{header}}
{{article
|author= ME Nissen,K Sengupta,
|source= MIS QUARTERLY
|year= 2006
|abstract = Recently, researchers have begun investigating an emerging, technology-enabled innovation that involves the use of intelligent software agents in enterprise supply chains. Software agents combine and integrate capabilities of several information technology classes in a novel manner that enables supply chain management and decision making in modes not supported previously by IT and not reported previously in the information systems literature. Indeed, federations and swarms of software agents today are moving the boundaries of computer-aided decision making more generally. Such moving boundaries highlight promising new opportunities for competitive advantage in business, in addition to novel theoretical insights. Bill they also call for shifting research thrusts in information systems. The stream of research associated with this article is taking some first steps to address such issues by examining experimentally the capabilities, limitations, and boundaries of agent technology for computer-based decision support and automation in the procurement domain. Procurement represents an area of particular potential for agent-based process innovation, as well as reflecting some of the greatest technological advances in terms of agents emerging from the laboratory. Procurement is imbued with considerable ambiguity in its task environment, ambiguity that presents a fundamental limitation to IT-based automation Of decision making and knowledge work. By investigating the comparative performance of human and software agents across varying levels of ambiguity in the procurement domain, the experimentation described in this article helps to elucidate some new boundaries of computer-based decision making quite broadly. We seek in particular to learn from this domain and to help inform computer-based decision making, agent technological design, and IS research more generally.
|keyword = agents,artificial intelligence,behavioral decision theory,computer-aided decision making,human performance,procurement,supply chain management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ethical decision making in software piracy: Initial development and test of a four-component model'''
{{header}}
{{article
|author= TT Moores,JCJ Chang,
|source= MIS QUARTERLY
|year= 2006
|abstract = Software piracy costs the software industry billions of dollars each year. To better understand piracy, we propose a model of ethical decision making that is an adaptation of the four-component model of morality. This model defines four internal processes that result in external moral behavior: recognition, judgment, intention, and behavior. We test our model with a sample of Information Systems students in Hong Kong who provided measures of self-reported behavior regarding levels of buying and using pirated software. Using partial least squares, we investigated the causal pathways of the model and the effects of age and gender. We find that use is determined by buying, buying is determined by intention, and intention is determined by judgment. Although respondents recognized software piracy as an infringement of intellectual property rights, this fact did not affect their judgment of the morality of the act. Significant differences are also found in the ethical decision-making process based on age but only limited differences based on gender. The implications of these results, including the development of a professional ethics program, are discussed.
|keyword = ethics,morality,ethical decision making,moral reasoning,ethical scenarios,intellectual property rights,software piracy,age,gender,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web and wireless site usability: Understanding differences and modeling use'''
{{header}}
{{article
|author= V Venkatesh,V Ramesh,
|source= MIS QUARTERLY
|year= 2006
|abstract = Recent research has presented a conceptualization, metric, and instrument based on Microsoft Usability Guidelines (MUG; see Agarwal and Venkatesh 2002). In this paper, we use MUG to further or understanding of web and wireless site use. We conducted two empirical studies among over 1,000 participants. In study 1, conducted in both the United States and Finland, we establish the generalizability of the MUG conceptualization, metric, and associated instrument from the United States to Finland. In study 2, which involved longitudinal data collection in Finland, we delved into an examination of differences in factors important in determining web versus wireless site usability. Also, in study 2, based on a follow-up survey about site use conducted 3 months after the initial survey, we found support for a model of site use that employs the MUG categories and subcategories as predictors. The MUG-based model outperformed the widely employed technology acceptance model both in terms of richness and variance explained (about 70 pet-cent compared to 50 percent).
|keyword = usability guidelines,modeling,wireless,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A two-stage model of the promotional performance of pure online firms'''
{{header}}
{{article
|author= JN Wu,VJ Cook,EC Strong,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Internet firms frequently employ a two-stage approach to promotional activities. In Stage 1, they attract customers to their websites through advertising. In Stage 2, firms generate sales transactions or sales leads through their website. Comprehensive assessment of the promotional performance of pure online firms requires the study of Stage I and of Stage 2 jointly. In this paper we develop a joint two-stage conceptual and econometric model for assessing website promotion on three important dimensions: (1) how advertising response can be measured by linking media schedules to website log files; (2) how advertising and website characteristics jointly affect the desired system outcome of the promotion; and (3) whether the joint investigation of advertising response and desired system outcomes is essential to assess the results of website promotion. Three general findings follow from application of our model to a pure online firm's campaign to generate sales leads through print advertising. First, advertising and website characteristics affect sales leads in different ways. A characteristic may influence sales leads directly, or indirectly, or both. Second, assessing advertising effectiveness in an online environment may not require costly survey research data. Instead, secondary data available from website log files may be used for such assessment. Third, the interaction between the first and second stages of our two-stage model can lead to misspecifications that produce misleading inferences. This occurs because the unobserved characteristics in generating website visits and sales leads may be correlated.
|keyword = e-commerce,website traffic,sales leads,two-stage model,sample selection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of e-commerce on competition in the retail brokerage industry'''
{{header}}
{{article
|author= Y Bakos,HC Lucas,W Oh,G Simon,S Viswanathan,BW Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = This paper analyzes the impact of e-commerce on markets where established firms face competition from Internet-based entrants with focused offerings. In particular, we study the retail brokerage sector where the growth of online brokerages and the availability of alternate sources of information and research services have challenged the dominance of traditional brokerages. We develop a stylized game-theoretic model to analyze the impact of competition between an incumbent full-service brokerage firm with a bundled offering of research services and trade execution and an online entrant offering just trade execution. We find that as consumers' willingness to pay for research declines, the incumbent finds it optimal to unbundle its offering when competing with the online entrant. We also find that the online entrant chooses a lower quality of trade execution when faced with direct competition from the incumbent's unbundled offering. The analytical model motivates a unique field experiment placing actual simultaneous trades with traditional full-service and online brokers, to compare order handling practices and the quality of trade execution. In keeping with our analytical results, our empirical findings show a significant difference in the quality of execution between online brokerages and their full-service counterparts. We discuss the relevance of our findings for quality differentiation, price convergence, and profit decline in a variety of markets where traditional incumbents are faced with changes in the competitive landscape as a result of e-commerce.
|keyword = e-commerce,impact of information technology,unbundling,execution quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing piracy: Pricing and sampling strategies for digital experience goods in vertically segmented markets'''
{{header}}
{{article
|author= RK Chellappa,S Shivendu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Digital goods lend themselves to versioning but also suffer from piracy losses. This paper develops a pricing model for digital experience goods in a segmented market and explores the optimality of sampling as a piracy-mitigating strategy. Consumers are aware of the true fit of an experience good to their tastes only after consumption, and as piracy offers an additional (albeit illegal) consumption opportunity traditional segmentation findings from economics and sampling recommendations from marketing, need to be revisited. We develop a two-stage model of piracy for a market where consumers are heterogeneous in their marginal valuation for quality and their moral costs. In our model, some consumers pirate the product in the first stage allowing them to update their fit-perception that may result in re-evaluation of their buying/pirating decision in the second stage. We recommend distinct pricing and sampling strategies for underestimated and overestimated products and suggest that any potential benefits of piracy can be internalized through product sampling. Two counter-intuitive results stand out. First, piracy losses are more severe for products that do not live up to their hype rather than for those that have been undervalued in the market, thus requiring a greater deterrence investment for the former, and second, unlike physical goods where sampling is always beneficial for underestimated products, sampling for digital goods is optimal only under narrowly defined circumstances due to the price boundaries created by both piracy and segmentation.
|keyword = digital products,experience goods,vertical segmentation,pricing,piracy,sampling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Two competing perspectives on automatic use: A theoretical and empirical comparison'''
{{header}}
{{article
|author= SS Kim,NK Malhotra,S Narasimhan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Although much research has examined conscious use, which involves deliberate evaluation and decision making, we know less about automatic use, which occurs spontaneously with little conscious effort. The objective of this study is to compare two contrasting views in the literature on the nature of automatic use, namely, the habit/automaticity perspective (HAP) and the instant activation perspective (JAP). According to HAP, automatic use occurs because of the force of habit/automaticity without the formation of evaluations and intention; thus, past use-which is a proxy for habit/automaticity-is believed to weaken the evaluations-intention-usage relationship. In contrast, JAP posits that automatic use is simply an expedited form of conscious use; accordingly, as with conscious use, automatic use is still a function of evaluations/intention, so past use will not weaken the evaluations-intention-usage relationship. We tested the competing hypotheses using 2,075 cross-sectional and 990 longitudinal responses from actual users of two online news sites. Our results show that the evaluations-intention-usage relationship is generally weaker among heavier users than among lighter users. These findings suggest that with an increase in past use, user behavior becomes less evaluative and less intentional, in support of the argument that automatic use is driven more by habit/automaticity than by instant activation of cognitions. Overall, this research shows an initial piece of evidence of the moderating role of past use in postadoption phenomena, and it is expected to help the information systems community systematically investigate the important yet underexplored subject of habit/automaticity.
|keyword = user evaluation,user behavior,habit,automaticity,structural equation modeling,longitudinal study,cross-validation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Involvement and decision-making performance with a decision aid: The influence of social multimedia, gender, and playfulness'''
{{header}}
{{article
|author= TJ Hess,MA Fuller,J Mathew,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This research explores how multimedia vividness and the use of computer-based social cues can influence involvement with technology and decision-making outcomes. An experiment is conducted that examines the effect that increased levels of vividness (text, voice, and animation) and decision aid personality have on decision-making involvement. In addition, the influence of two individual differences, gender and computer playfulness, on decision aid involvement are investigated. The cost-benefit framework of decision making and related research on consumer information processing provide the theoretical foundation for the study and suggest how increased involvement may influence decision making. Several decision-making outcomes are measured, including decision effort, decision quality, satisfaction with the decision aid, and understanding of the decision aid. Findings indicate that personality similarity (between the user and the decision aid) and computer playfulness result in increased involvement with the decision aid. In addition, women report higher levels of involvement with the decision aid. Increased levels of multimedia vividness are found to have a contradictory effect, with animation actually reducing involvement with the decision aid. The findings are discussed in terms of theoretical contributions and practical interface design implications.
|keyword = computer-based social cues,computer playfulness,decision aids,decision making,decision performance,multimedia in computing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How presentation flaws affect perceived site quality, trust, and intention to purchase from an online store'''
{{header}}
{{article
|author= A Everard,DF Galletta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Although there has been a great deal of research on impression formation, little application of that research has been made to electronic commerce. A research model was constructed that hypothesized errors, poor style, and incompleteness to be inversely related to the users' level of perceived quality of an online store. Further, this perceived quality of the online store's Web site would be directly related to users' trust in the store and, ultimately, to users' intentions to purchase from the store. An experimental study with 272 undergraduate and graduate student volunteers supported all the hypotheses. In addition, it was found that the relationship between the factors and perceived quality was mediated by the perception of the flaws. The perception of flaws rather than the actual flaws influenced users' perception of quality. Supplemental analysis also seemed to indicate a pattern of diminishing effects with each subsequent flaw.
|keyword = intention to purchase,trust in e-commerce,Web site credibility,Web site presentation flaws,Web site quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating coherence and multimedia effects of a technology-mediated collaborative environment'''
{{header}}
{{article
|author= A Gemino,D Parker,AO Kutzschan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper presents an experiment investigating the impact of context-relevant graphics on a knowledge sharing task in a technology-mediated collaborative (TMC) environment. The Cognitive Theory of Multimedia Learning (CTML) is introduced as the theoretical base for the hypotheses. The principles of multimedia and coherence from the CTML are used to hypothesize about the effectiveness of graphics embedded in TMC environments. Comprehension and transfer are used as dependent measures. Three TMC interface treatments were considered (no graphic, irrelevant graphic, relevant graphic). Hierarchical analysis of covariance (HANCOVA) comparing TMC treatments indicated no significant differences in comprehension; however, transfer scores for the TMC teams with context-relevant graphics were significantly higher than the other TMC teams. Although adding graphics to the collaborative interface improves the level of understanding developed within a group, the graphics need to be context relevant to be effective. These findings support the coherence and multimedia principles and provide guidance for designers of TMC environments.
|keyword = cognitive theory,collaborative learning,computer-mediated communication,human-computer interaction,knowledge sharing,technology-mediated collaborative environments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Moderating effects of task type on wireless technology acceptance'''
{{header}}
{{article
|author= XW Fang,S Chan,J Brzezinski,S Xu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The technology acceptance model (TAM) is one of the most widely used models of information technology (IT) adoption. According to TAM, IT adoption is influenced by two perceptions: usefulness and ease of use. In this study, we extend TAM to the mobile commerce context. We categorize the tasks performed on wireless handheld devices into three categories: (1) general tasks that do not involve transactions and gaming, (2) gaming tasks, and (3) transactional tasks. We propose a unified conceptual model for wireless technology adoption. In this model, task type moderates the effects of four possible determinants: perceived usefulness, perceived ease of use, perceived playfulness, and perceived security. We postulate that, under the mobile context, user intention to perform general tasks that do not involve transactions and gaming is influenced by perceived usefulness and perceived ease of use, user intention to play games is affected by perceived playfulness, and user intention transact is influenced by perceived usefulness and perceived security. A survey was conducted to collect data about user perception of 12 tasks that could be performed on wireless handheld devices and user intention to use wireless technology. Multiple regression analyses supported the proposed research model.
|keyword = mobile commerce,perceived ease of use,perceived playfulness,perceived security,perceived usefulness,TAM,task performance,task type,user intention,wireless handheld devices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of knowledge repositories in technical support environments: Speed versus learning in user performance'''
{{header}}
{{article
|author= PH Gray,A Durcikova,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Knowledge repositories are commonly used by technical Support analysts in call center environments as a way of capturing and reusing solutions to common problems, and are generally expected to improve service quality, reduce costs, and enhance analyst learning. This study investigates why technical support analysts seek out and access knowledge from these repositories, as opposed to more traditional sources of such knowledge-col leagues and manuals. Focusing on the demand for-rather than supply of-knowledge in organizations, our research elaborates the role played by analysts' learning orientation, perceived work demands, and risk aversion in predicting their knowledge sourcing behavior. Our results include several counterintuitive findings that suggest there is not very much learning going on via technical support knowledge repositories. Analysts seem to be focused on finding recipes for solving customers' problems rather than building a better understanding of the products they support. Implications for research and practice highlight the need for more effective technologies to speed searches, the utility of a formal and visible mechanism for validating knowledge, and the inherent tension between efficiency and learning in these environments.
|keyword = knowledge management systems,knowledge repositories,knowledge sourcing,learning orientation,risk aversion,technical support help desks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical examination of the influence of organizational culture on knowledge management practices'''
{{header}}
{{article
|author= M Alavi,TR Kayworth,DE Leidner,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Knowledge management to facilitate the creation, storage, transfer, and application of knowledge in organizations has received wide attention in practice and research in the past several years. Often cited as a significant challenge in knowledge management practices is the issue of organizational culture. Although many studies raise the issue of organizational culture's influence on knowledge management success, few investigate the way in which this influence manifests itself. This paper aims to explore how organizational culture influences knowledge management practices. Using a case study method, we examine the cultural values and knowledge management approaches within a large global information services company and one of its knowledge communities. The findings highlight the influence of culture on the use of knowledge management technologies and the outcomes of such use.
|keyword = impact of culture on knowledge management,knowledge management,knowledge management tools,organizational culture,values in organizations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dynamic knowledge patterns to inform design: A field study of knowledge stocks and flows in an extreme organization'''
{{header}}
{{article
|author= ME Nissen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Knowledge represents a critical resource in the modern enterprise. But it is dynamic and distributed unevenly. Capitalizing on this dynamic resource for enterprise performance depends upon its rapid and reliable flows across people, organizations, locations, and times of application. From a technological perspective, this points immediately to the design of information systems to enhance knowledge flows. The problem is, the design of information systems to enhance knowledge flows requires new understanding. The research described in this paper concentrates on understanding the dynamics of knowledge phenomenologically and on developing and applying techniques for modeling and visualizing dynamic knowledge flows and stocks. We draw key, theoretical concepts from multiple literatures, and we build upon integrative modeling work that composes a parsimonious, multidimensional, analytical framework for representing and visualizing dynamic knowledge. We then conduct field research to learn how this theoretical framework may be used to model knowledge flows in practice. By focusing this empirical work on an extreme organization and processes that involve and rely upon tacit knowledge, we illustrate how dynamic knowledge patterns can inform design in new ways. New chunks of kernel theory deriving from this fieldwork are articulated in terms of a propositional model, which provides a basis for the development of testable design theory hypotheses.
|keyword = case study,dynamics,information systems,knowledge design,knowledge flow,knowledge management,knowledge transfer,military,organizational learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coauthorship dynamics and knowledge capital: The patterns of cross-disciplinary collaboration in information systems research'''
{{header}}
{{article
|author= W Oh,JN Choi,K Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = From the social network perspective, this study explores the ontological structure of knowledge sharing activities engaged in by researchers in the field of information systems (IS) over the past three decades. We construct a knowledge network based on coauthorship patterns extracted from four major journals in the IS field in order to analyze the distinctive characteristics of each subfield and to assess the amount of internal and external knowledge exchange that has taken place among IS researchers. This study also tests the role of different types of social capital that influence the academic impact of researchers. Our results indicate that the proportion of coauthored IS articles in the four journals has doubled over the past 25 years, from merely 40 percent in 1978 to over 80 percent in 2002. However, a significant variation exists in terms of the shape, density, and centralization of knowledge exchange networks across the four subfields of IS - namely, behavioral science, organizational science, computer science, and economic science. For example, the behavioral science subgroup, in terms of internal cohesion among researchers, tends to develop the most dense collaborative relationships, whereas the computer science subgroup is the most fragmented. Moreover, external collaboration across these subfields appears to be limited and severely unbalanced. Across the four subfields, on average, less than 20 percent of the research collaboration ties involved researchers from different subdisciplines. Finally, the regression analysis reveals that knowledge capital derived from a network rich in structural holes has a positive influence on an individual researcher's academic performance.
|keyword = coauthorship patterns,knowledge capital,ontology of IS research,research impact,social networks,SSIC index,structural holes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An assessment of electronic information transfer in B2B supply-channel relationships'''
{{header}}
{{article
|author= KK Kim,NS Umanath,BH Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The basic premise of the extant literature related to electronic integration has been that the higher the integration, the higher will be the organizational performance. However, excessive electronic integration can be dysfunctional too. We make a conceptual argument that more is not always better and that the fit between contextual factors and electronic information sharing should be achieved to seek optimal channel performance. We empirically examine the fit between electronic information transfer (EIT) and contextual factors of a supply channel, our specific contribution being the assessment of fit in terms of multivariate congruence. The data required for this field study was collected from 124 managers/buyers responsible for supplier relationships in six multinational enterprises in two different industries (automobile and heavy shipbuilding) headquartered in Korea. The results ratify our hypothesis that multivariate congruence between EIT components and supply-channel contextual factors indeed exists. Follow-up drill-down analysis indicates that the monitoring component of EIT has a significant influence on demand uncertainty, and complexity-in-use is influenced by the coordination aspect of EIT. However, both the coordination and monitoring aspects of EIT are significantly relevant to interdependence of partners in a supply channel. A post hoc exploratory analysis suggests that the supply-channel performance is influenced by the fit between the contextual factors and the channel design factors. An inference of practical value that emerges from our findings is that more or less electronic integration is not the real issue. What is critical is the fit between supply-channel context and the level of electronic integration.
|keyword = B2B procurement,electronic information transfer,empirical research,organizational information processing theory,supply-chain management,survey methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology investment strategies under declining technology cost'''
{{header}}
{{article
|author= D Demirhan,VS Jacob,S Raghunathan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Investments in information technology (IT) have become crucial for firms to improve the quality of their products and services. Typically, IT cost for the same performance level declines over time. In a competitive market, a decline in IT cost over time provides a cost advantage to the later entrant, making the early entrant's investment decision problem challenging. In this paper, we study the problem of strategic IT investments in the declining cost scenario using a sequential duopoly model. Our results show that declining IT cost intensifies or relaxes competition between firms depending on whether they are serving quality- or price-sensitive markets. In both cases, the average price per unit quality decreases when the IT cost declines, which benefits consumers. We also show that if the first entrant is uncertain about the extent of its cost disadvantage, the first entrant overinvests (underinvests) in a price-sensitive (quality-sensitive) market as the degree of uncertainty increases.
|keyword = cost advantage,declining IT cost,economic analysis,game theory,IT investment,IT strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Negotiation support systems in budget negotiations: An experimental analysis'''
{{header}}
{{article
|author= CJ Wolfe,US Murthy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper reports the results of an experiment investigating the differences between budget negotiations conducted on an electronic negotiation support system (NSS) and those conducted face-to-face. The negotiation setting consisted of a supervisor and a subordinate negotiating a performance budget for the subordinate. Results revealed that when supervisor performance expectations were incongruent with subordinate capability, face-to-face negotiations hit impasse at a significantly higher rate than NSS negotiations. These results held regardless of the amount of concession needed to reach consensus, and they support the contention that single-issue distributive negotiations, such as budget negotiations, can benefit from the use of an NSS. In a secondary analysis of subordinate performance after the budget negotiation, we found that NSS subordinates perceived more task conflict, which positively influenced postnegotiation performance, whereas face-to-face subordinates perceived less relational conflict, which worked through satisfaction to positively influence postnegotiation performance. This result adds to the literature by clarifying the roles that communication mode plays in a negotiation and a negotiation's aftermath.
|keyword = budget negotiation,distributive negotiations,negotiation impasse,negotiation support systems (NSS),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Taking industry seriously in information systems research'''
{{header}}
{{article
|author= MW Chiasson,E Davidson,
|source= MIS QUARTERLY
|year= 2005
|abstract = In this essay, we argue that industry receives little attention in information systems research and theory, despite its increasingly important influence on IS activities. This is evident both in the narrow range of industries examined in IS research and the infrequent consideration of industry in theory. We base these observations on an analysis of IS publications in two top-tier journals (MIS Quarterly and Information Systems Research) over eight years. Drawing from institutional theory, we consider various ways industry can be addressed and assess how industry influences IS activities. We conclude that industry provides an important contextual "space" to build new IS theory and to evaluate the boundaries of existing IS theory. We outline a range of strategies for incorporating industry into IS research.
|keyword = information systems theory,institutional theory,industry context,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Zones of tolerance: Alternative scales for measuring information systems service quality'''
{{header}}
{{article
|author= WJ Kettinger,CC Lee,
|source= MIS QUARTERLY
|year= 2005
|abstract = The expectation norm of Information Systems SERVQUAL has been challenged on both conceptual and empirical grounds, drawing into question the instrument's practical value. To address the criticism that the original IS SERVQUAL's expectation measure is ambiguous, we test a new set of scales that posits that service expectations exist at two levels that IS customers use as a basis to assess IS service quality: (1) desired service: the level of IS service desired, and (2) adequate service: the minimum level of IS service customers are willing to accept. Defining these two levels is a "zone of tolerance" (ZOT) that represents the range of IS service performance a customer would consider satisfactory. In other words, IS customer service expectations are characterized by a range of levels, rather than a single expectation point This research note adapts the ZOT and the generic operational definition from marketing to the IS field, assessing its psychometric properties. Our findings conclude that the instrument shows validity of a four-dimension IS ZOT SERVQUAL measure for desired, adequate, and perceived service quality levels, identifying 18 commonly applicable question items. This measure addresses past criticism while offering a practical diagnostic tool.
|keyword = IS service quality,zones of tolerance,IS management,SERVQUAL,evaluation,user expectations,information services function,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and the performance of the customer service process: A resource-based analysis'''
{{header}}
{{article
|author= G Ray,WA Muhanna,JB Barney,
|source= MIS QUARTERLY
|year= 2005
|abstract = Delivering quality customer service has emerged as a strategic imperative, one that is increasingly tied to a firm's information technology resources and capabilities. This paper presents an empirical study that examines the extent to which IT impacts customer service. More specifically, this study investigates the differential effects of various IT resources and capabilities on the performance of the customer service process across firms that compete in the North American life and health insurance industty. The paper builds on (1) information systems work that suggests that the effects of IT are best documented at the level of processes within a firm, (2) information systems work that suggests that the performance effects of IT are likely to be contingent in nature, and (3) developments in the resource-based view, which describes the kinds of IT resources and capabilities that are likely to enable a process in one firm to outperform the same process in competing firms. The findings suggest that tacit, socially complex, firm-specific resources explain variation in process performance across firms and that IT resources and capabilities without these attributes do not. Of particular interest to IS scholars, it is found that shared knowledge between IT and customerservice units-an important driver of how IT is implemented and used in the customer service process-is a key IT capability that affects customer service process performance and moderates the impacts of explicit IT resources such as the generic information technologies used in the process and IT spending, which-consistent with resource-based predictions-were not found to be directly and positively associated with relative process performance. The implications of the findings for research and practice are discussed.
|keyword = IT resources and capabilities,shared knowledge,resource-based theory,business processes,process performance,business value of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing client dialogues during information systems design to facilitate client learning'''
{{header}}
{{article
|author= A Majchrzak,CM Beath,RA Lim,WW Chin,
|source= MIS QUARTERLY
|year= 2005
|abstract = It has long been recognized that client learning is an important factor in the successful development of information systems. While there is little question that clients should learn, there is less clarity about how best to facilitate client learning during developer-client meetings. In this study, we suggest that a cooperative learning strategy called collaborative elaboration developed by educational psychologists provides a theoretical and practical basis for stimulating client learning during an IS design process. The problem with assessing the effects of collaborative elaboration, however, is in controlling for the many other factors that might affect client learning and outcomes of an IS design phase. In a unique research opportunity, we were able to measure the use of collaborative elaboration among 85 developers and clients involved in 17 projects over a semester-long IS design process. The projects were homogeneous with respect to key contextual variables. Our PLS analysis suggested that teams using more collaborative elaboration had more client learning and teams with more client learning achieved better IS design-phase outcomes. This suggests that theories about collaborative elaboration have significant potential for helping IS researchers identify new approaches for stimulating client learning early in the IS design process.
|keyword = information systems development,requirements elicitation,learning,client-developer dialogue,cognitive elaboration,user participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of virtual reality on consumer learning: An empirical investigation'''
{{header}}
{{article
|author= KS Suh,YE Lee,
|source= MIS QUARTERLY
|year= 2005
|abstract = As competition in business-to-consumer e-commerce becomes fiercer, Web-based stores are attempting to attract consumers' attention by exploiting state-of-the-art technologies. Vitrual-reality (VR) on the Internet has been gaining prominence recently because it enables consumers to experience products realistically over the Internet, there- by mitigating the problems associated with consumers' lack of physical contact with products. However, while the employment of VR has increased in B2C e-commerce, its impact has not been explored extensively by research in the IS field. This study investigates whether and under what circumstances VR enhances consumer learning about products. In general, VR enables consumers to learn about products thoroughly by providing high-quality three-dimensional images of products, interactivity with the products, and increased telepresence. In addition, congruent with the theory of cognitive fit, the effects of VR are more pronounced when it exhibits products whose salient attributes are completely apparent through visual and auditory cues (because most VR on desktop computers uses only those two sensory modalities to deliver information). Based on these attributes, we distinguish between two types of products-namely, virtually high experiential (VHE) and virtually low experiential (VLE) products-in terms of the sensory modalities that are used and required for product inspection. Hypotheses arising from the distinctions expressed by these terms were tested via a laboratory experiment. The results support the predictions that VR interfaces increase overall consumer learning about products and that these effects extend to VHE products more significantly than to VLE products.
|keyword = virtual reality,consumer learning,interface design in e-commerce,cognitive fit,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Intellectual property rights and cannibalization in information technology outsourcing contracts'''
{{header}}
{{article
|author= EA Walden,
|source= MIS QUARTERLY
|year= 2005
|abstract = This paper examines the question of how intellectual property rights in the software created during information technology outsourcing relationships should be divided. This paper expands on the property rights approach developed by Grossman, Hart, and Moore by recognizing that, with respect to software, it is possible to separate excludability rights from usability rights. These rights are modeled and the contractually optimal distribution is determined. This model is then modified to account for the possibility of cannibalization of the client's benefit when multiple others are allowed use of the software. The results show that the best contractual structure depends strongly on the environment.
|keyword = contracts,outsourcing,economic models,interorganizational relationships,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The history of Texaco's corporate information technology function: A general systems theoretical interpretation'''
{{header}}
{{article
|author= J Porra,R Hirschheim,MS Parks,
|source= MIS QUARTERLY
|year= 2005
|abstract = We attempt to use general systems theory (GST) to understand why the resources of Texaco's corporate information technology function consistently did not match its task during its 40-year lifetime. Our interpretation uses mechanistic, organic, and colonial systems metaphors, each with three components. The first is an analysis of a management action system made up of organizational indicators such as Texaco's revenues, profits, employee numbers, IT budgets, and IT personnel numbers. The second is a narrative of performance versus resource needs, which shows a gap between the resources and expanding responsibilities of Texaco's IT function. The third is a management perception system, which offers reasons why top management continually misinterpreted IT's performance as inferior. Our results show that the mechanistic, organic, and colonial interpretations converge. In addition, our GST-based interpretations show how top management might have remedied the situation.
|keyword = systems theory,mechanistic systems,organic systems,colonial systems,historical research,interpretive research,IT function success factors,IT function failure,longitudinal study,punctuated equilibrium,radical change,organizational change,organizational alignment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT human resource management configurations and IT turnover: Theoretical synthesis and empirical analysis'''
{{header}}
{{article
|author= TW Ferratt,R Agarwal,CV Brown,JE Moore,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Increasingly, scholars and practitioners acknowledge that information technology (IT) human capital is a strategic resource and that its effective management represents a significant organizational capability. We use configurational theory to examine organizational practices related to the management of IT human capital. In contrast to much prior work in IT human resource management (HRM) that is focused at the individual level, our inquiry is focused at the organizational level of analysis. Building on strategic human resource management (SHRM) research in general and research on the management of IT professionals in particular, we examine the broad question: Are different configurations of IT HRM practices associated with different IT staff turnover rates? A multidimensional view of IT HRM practices is presented, based on prior IT and SHRM literature. We formalize hypotheses regarding the relationship of turnover with configurations of IT HRM practices grounded in prior theory and empirical research. Based on survey responses from 106 organizations, IT HRM dimensions and configurations are derived and the hypotheses are tested. A five-configuration solution, obtained via cluster analysis, includes two contrasting configurations consistent with two archetypes found in the prior literature. Specifically, the configuration with a human capital focus has lower turnover than the task-focused configuration, providing support for our first hypothesis. Although the hypothesis on intermediate configurations and their relationship with turnover is not supported, we discover and interpret three additional configurations that embody patterns of practices with unique emphases. Theoretical and practical implications of the findings are discussed.
|keyword = IT HR strategy,information technology professionals,staffing,information systems personnel management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Maximizing accuracy of shared databases when concealing sensitive patterns'''
{{header}}
{{article
|author= S Menon,S Sarkar,S Mukherjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = The sharing of databases either within or across organizations raises the possibility of unintentionally revealing sensitive relationships contained in them. Recent advances in data-mining technology have increased the chances of such disclosure. Consequently, firms that share their databases might choose to hide these sensitive relationships prior to sharing. Ideally, the approach used to hide relationships should be impervious to as many data-mining techniques as possible, while minimizing the resulting distortion to the database. This paper focuses on frequent item sets, the identification of which forms a critical initial step in a variety of data-mining tasks. it presents an optimal approach for hiding sensitive item sets, while keeping the number of modified transactions to a minimum. The approach is particularly attractive as it easily handles databases with millions of transactions. Results from extensive tests, conducted on publicly available real data and data generated using IBM's synthetic data generator indicate that the approach presented is very effective, optimally solving problems involving millions of transactions in a few seconds.
|keyword = data quality,privacy,item set mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web personalization as a persuasion strategy: An elaboration likelihood model perspective'''
{{header}}
{{article
|author= KY Tam,SY Ho,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = With advances in tracking and database technologies, firms are increasingly able to understand their customers and translate this understanding into products and services that appeal to them. Technologies such as collaborative filtering, data mining, and click-stream analysis enable firms to customize their offerings at the individual level. While there has been a lot of hype about web personalization recently, our understanding of its effectiveness is far from conclusive. Drawing on the elaboration likelihood model (ELM) literature, this research takes the view that the interaction between a firm and its customers is one of communicating a persuasive message to the customers driven by business objectives. In particular, we examine three major elements of a web personalization strategy: level of preference matching, recommendation set size, and sorting cue. These elements can be manipulated by a firm in implementing its personalization strategy. This research also investigates a personal disposition, need for cognition, which plays a role in assessing the effectiveness of web personalization. Research hypotheses are tested using 1,000 subjects in three field experiments based on a ring-tone download website. Our findings indicate the saliency of these variables in different stages of the persuasion process. Theoretical and practical implications of the findings are discussed.
|keyword = web personalization,elaboration likelihood model,persuasion,preference matching,human computer interaction,recommendation set size,sorting cue,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal software development: A control theoretic approach'''
{{header}}
{{article
|author= YH Ji,VS Mookerjee,SP Sethi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = We study the problem of optimally allocating effort between software construction and debugging. As construction proceeds, new errors are introduced into the system. The objective is to deliver a system of the highest possible quality (fewest number of errors) subject to the constraint that N system modules are constructed in a specified duration T. If errors are not corrected during construction, then further construction can produce errors at a faster rate. To curb the growth of errors, some of the effort must be taken away from construction and assigned to testing and debugging. A key finding of this model is that the practice of alternating between pure construction and pure debugging is suboptimal. Instead, it is desirable to concurrently construct and debug the system. We extend the above model to integrate decisions traditionally considered "external" such as the time to release the product to the market with those that are typically treated as "internal" such as the division of effort between construction and debugging. Results show that integrating these decisions can yield significant reduction in the overall cost. Also, when competitive forces are strong, it may be better to release a product early (with more errors) than late (with fewer errors). Thus, underestimating the cost of errors in the product may be better than overestimating the cost.
|keyword = optimal software development,concurrent development and debugging,optimal control theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On data reliability assessment in accounting information systems'''
{{header}}
{{article
|author= R Krishnan,J Peters,R Padman,D Kaplan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = The need to ensure reliability of data in information systems has long been recognized. However, recent accounting scandals and the subsequent requirements enacted in the Sarbanes-Oxley Act have made data reliability assessment of critical importance to organizations, particularly for accounting data. Using the accounting functions of management information systems as a context, this paper develops an interdisciplinary approach to data reliability assessment. Our work builds on the literature in accounting and auditing, where reliability assessment has been a topic of study for a number of years. While formal probabilistic approaches have been developed in this literature, they are rarely used in practice. The research reported in this paper attempts to strike a balance between the informal, heuristic-based approaches used by auditors and formal, probabilistic reliability assessment methods. We develop a formal, process-oriented ontology of an accounting information system that defines its components and semantic constraints. We use the ontology to specify data reliability assessment requirements and develop mathematical-model-based decision support methods to implement these requirements. We provide preliminary empirical evidence that the use of our approach improves the efficiency and effectiveness of reliability assessments. Finally, given the recent trend toward specifying information systems using executable business process models (e.g., business process execution language), we discuss opportunities for integrating our process-oriented data reliability assessment approach-developed in the accounting context-in other IS application contexts.
|keyword = workflow and process management,accounting information systems,mathematical modeling,internal control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Offshore outsourcing: A dynamic causal model of counteracting forces'''
{{header}}
{{article
|author= A Dutta,R Roy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Many argue that offshoring is an inexorable trend, since a variety of information technology (IT) skills have become global commodities and they are vastly cheaper in other parts of the world. According to this view, most IT work would be drained from the United States to overseas locations. However, the loss of jobs to offshoring has increased pressure to impose restrictions. On the supply side, as IT salaries in outsourcing vendor nations increase, they become less attractive for offshoring. The literature identifies multiple factors-some enhancing, others inhibiting-that affect the growth of offshoring. In this paper, we attempt to add to that knowledge by asking, "What are the mechanics by which these factors interact to produce the observed growth in IT offshoring?" We use the system dynamics methodology to build a two-country simulation model of offshoring growth that captures individual cause-effect relationships generated by its supply and demand drivers. Examined as a whole, these individual relationships reveal larger feedback loops that constitute the mechanism underlying offshoring growth between the two countries. Simulation experiments show how the dynamic behavior of offshoring is likely to evolve beyond the current high-growth period. The model contributes to our understanding of offshoring by offering a causal foundation for its growth pattern. It can also be used to computationally study different scenarios of offshoring growth.
|keyword = business policy,growth modeling,offshoring,outsourcing,simulation,system dynamics modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Just right outsourcing: Understanding and managing risk'''
{{header}}
{{article
|author= R Aron,EK Clemons,S Reddi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross-border outsourcing. In addition to technological improvements in risk management, it is possible to reduce the risk of opportunistic behavior faced by the buyer by redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing. We provide a taxonomy of risks associated with the outsourcing of business processes. We focus on strategic risks and identify the components of this risk and the means by which it can be mitigated.
|keyword = holdup problem,interorganizational work flows,outsourcing,process design,strategic risks,transaction-cost economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Beyond the hype of frictionless markets: Evidence of heterogeneity in price rigidity on the Internet'''
{{header}}
{{article
|author= ME Bergen,RJ Kauffman,D Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = We explore daily patterns of Internet pricing for the two major retailers, Amazon.com and Barnes and Noble (BN), using data on 377 books collected over a 449-day period in 2003-4. We frame this investigation in terms of a key question: How rigid are prices on the Internet? Are there reasons to suggest that prior predictions of more flexible prices on the Internet may not have been founded on the appropriate theoretical knowledge? We find that Internet retailers, in contrast with traditional firms, adjust prices any day of the week throughout the year. Yet firms' price adjustments for books occur much less frequently than daily-every 90 days on average. For most observers of Internet-based selling, this is surprising, because most expect more frequent price adjustments-based on the quality of technological environment that supports price-setting. In fact, our results show that price-change activity appears to vary by book category, from a high of one change, on average, every 61 days for best sellers to a low of one change every 184 days, on average, for steady sellers. In addition, we learned that individual firms exhibited different patterns for their price changes: Amazon changed book prices every 222 days, whereas BN changed its book prices more frequently, every 56 days on average.
|keyword = bookselling,e-commerce,economic analysis,empirical research,empirical regularities,Internet retailing,price rigidity,strategic pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effect of electronic secondary markets on the supply chain'''
{{header}}
{{article
|author= A Ghose,R Telang,R Krishnan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = We present a model to investigate the competitive implications of electronic secondary markets that promote concurrent selling of new and used goods on a supply chain. In secondary markets where suppliers cannot directly utilize used goods for practicing intertemporal price discrimination and where transaction costs of resales are negligible, the threat of cannibalization of new goods by used goods becomes significant. We examine conditions under which it is optimal for suppliers to operate in such markets, explaining why these markets may not always be detrimental for them. Intuitively, secondary markets provide an active outlet for some high-valuation consumers to sell their used goods. The potential for such resales leads to an increase in consumers' valuation for a new good, leading them to buy an additional new good. Given sufficient heterogeneity in consumers' affinity across multiple suppliers' products, the "market expansion effect" accruing from consumers' cross-product purchase affinity can mitigate the losses incurred by suppliers from the direct "cannibalization effect." We also highlight the strategic role that the used goods commission set by the retailer plays in determining profits for suppliers. We conclude the paper by empirically testing some implications of our model using a unique data set from the online book industry, which has a flourishing secondary market.
|keyword = electronic markets,information goods,market segmentation,quality degradation,supply chain,used goods markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A pricing mechanism for digital content distribution over computer networks'''
{{header}}
{{article
|author= KR Lang,R Vragov,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper uses modified economic growth theory to compare and contrast two currently available ways of digital content distribution: the client-server model and the peer-to-peer (P2P) model. We describe a monopolistic pricing scheme for distributing digital content over P2P networks that rewards peer users who actively participate in the distribution process. Our results show that digital distribution through a P2P network is more profitable and more efficient than in the corresponding client-server setting, if the pricing mechanism used provides strong incentives to users to share content. The basic results hold when the model is extended to include time-variant preferences across generations of consumers, and when the monopolist performs price discrimination based on generations. Some practical implications from the theoretical analysis are also discussed.
|keyword = client-server networks,content sharing,digital content distribution,information goods,monopolistic seller,peer-to-peer networks,pricing,user participation incentives,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Buyer's efficient E-sourcing structure: Centralize or decentralize?'''
{{header}}
{{article
|author= R Dai,S Narasimhan,DJ Wu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Strategic sourcing, defined as a firm's key business process to identify, evaluate, configure, and negotiate purchases in important spend categories while managing long-term supplier relationships, is playing a significant role in sourcing strategies. The adoption of e-sourcing, defined as the use of business software (for example, using application service providers to conduct online procurement auctions) to automate or augment the aforementioned key business process, has been growing rapidly in recent years. One often-cited benefit of e-sourcing is the predicted savings, which is appealing, given the increasing pressure on cost competitiveness faced by firms. Using queuing techniques, this paper develops an economic model that captures fundamental trade-offs in a firm's e-sourcing business process as characterized by communication complexity, frequency of use, and cost of delay. This allows comparisons of two widely adopted structures for e-sourcing: the centralized structure versus the decentralized structure. Conditions under which the centralized structure is favored over the decentralized structure and vice versa are identified and illustrated with numerical examples and case evidence. These findings are robust in other settings. The paper concludes with a discussion of managerial implications.
|keyword = communication complexity,economic analysis,e-sourcing,organization structure,procurement,queuing,reverse auction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology standards choices and industry structure outcomes: The case of the US home mortgage industry'''
{{header}}
{{article
|author= RT Wigand,CW Steinfield,ML Markus,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Vertical IS standards prescribe data structures and definitions, document formats, and business processes for particular industries, in contrast to generic information technology (IT) standards, which concern IT characteristics applicable to many industries. This paper explores the potential industry structure effects of vertical information systems (IS) standards through a case study of the U.S. home mortgage industry. We review theoretical arguments about the potential industry structure effects of standards for interorganizational coordination, and we compare the characteristics of XML-based vertical IS standards with those of electronic data interchange (EDI) to gauge the applicability of prior literature. We argue that the lower costs and wider accessibility of XML-based standards that use the Internet can result in significant changes to the structure of the mortgage industry. However, the nature of industry change will depend on the specific ways in which standards are implemented by organizations in the industry-there are many patterns of implementation with potentially different effects at the industry level of analysis. We illustrate these theoretical arguments with data from our case.
|keyword = adoption,effects of standards,implementation,industry-level effects,industry structure,IS standards,IT choices,vertical standards,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effect of communication frequency and channel richness on the convergence between chief executive and chief information officers'''
{{header}}
{{article
|author= AM Johnson,AL Lederer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Convergence (i.e., mutual understanding) between an organization's CEO and CIO is critical to its efforts to successfully exploit information technology. Communication theory predicts that greater communication frequency and channel richness lead to more such convergence. A postal survey of 202 pairs of CEOs and CIOs investigated the effect of communication frequency and channel richness on CEO/CIO convergence, as well as the effect of convergence on the financial contribution of information systems (IS) to the organization. Convergence was operationalized in terms of the current and future roles of information technology (IT) as defined by the strategic grid. Rigorous validation confirmed the current role as composed of one factor and the future role as composed of three factors (i.e., managerial support, differentiation, and enhancement). More frequent communication predicted convergence about the current role, differentiation future role, and enhancement future role. The use of richer channels predicted convergence about the differentiation future role. Convergence about the current role predicted IS financial contribution. From a research perspective, the study extended theory about communication frequency, media richness, convergence, and the role of IT in organizations. From a managerial perspective, it provided direction for CEOs and CIOs interested in increasing their mutual understanding of the role of IT.
|keyword = CEO/CIO communication,channel richness,communication frequency,managerial communication,media richness,strategic grid,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Types of information technology capabilities and their role in competitive advantage: An empirical study'''
{{header}}
{{article
|author= GD Bhatt,V Grover,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = During the past two decades, both business managers and academic researchers have shown considerable interest in understanding how information technologies (IT) help to create competitive advantage for a firm. While recently the idea of competitive differentiation through IT has been challenged, this study contrasts the traditional thinking about competitive advantage with the resource-based view. Specifically, it is argued that by demarcating specific types of capabilities, we can contribute to better understanding of the sources of IT-based competitive advantage. Conceptually, we distinguish here between value, competitive, and dynamic capabilities as three distinct types of capabilities. Within each type, we identify specific capabilities, such as quality of the IT infrastructure, IT business experience, relationship infrastructure, and intensity of organizational learning, and present a model that describes relationships between these capabilities and competitive advantage. We then empirically test the model using data collected via a national mail survey from chief IT executives from 202 manufacturing firms. While the quality of the IT infrastructure is hypothesized as a value capability and expectedly did not have any significant effect on competitive advantage, the quality of IT business expertise and the relationship infrastructure (competitive capabilities) did. The results of the study also indicate that the intensity of organizational learning (dynamic capability) was significantly related to all of the capabilities. These results point to the importance of delineating capabilities such as relationship infrastructure that can facilitate differentiation in the marketplace, and dynamic capabilities such as organizational learning as an important antecedent to IT capability building.
|keyword = competitive advantage,competitive capabilities,dynamic capabilities,firm performance,IT business experience,IT capabilities,IT infrastructure,organizational learning,relationship infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology as an enabler of growth in firms: An empirical assessment'''
{{header}}
{{article
|author= S Mitra,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This research provides empirical evidence of the role of information technology (IT) as an enabler of growth in firms. We postulate that as a firm grows larger, a superior IT infrastructure increases the productivity of other inputs by controlling the complexity-related costs that the firm incurs as it increases in size. In the empirical analysis to support this view, we find that firms with high-growth expectations increase their IT spending as their free cash flow increases, whereas low-growth firms maintain a constant level of IT spending, irrespective of their free cash flow. Further, we analyze the effect of IT investments in one period on growth-related metrics in subsequent periods. We find that a superior IT infrastructure significantly reduces the cost of operations for high-growth firms in subsequent periods. In summary, the theory and empirical evidence presented in this paper point to an indirect but important contribution of the IT infrastructure to a firm's growth.
|keyword = business value of IT,firm growth,free cash flow,Tobin's q,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The information systems identity crisis: Focusing on high-visibility and high-impact research'''
{{header}}
{{article
|author= R Agarwal,HC Lucas,
|source= MIS QUARTERLY
|year= 2005
|abstract = This paper presents an alternative view of the Information Systems identity crisis described recently by Benbasat and Zmud (2003). We agree with many of their observations, but we are concerned with their prescription for IS research. We critique their discussion of errors of inclusion and exclusion in IS research and highlight the potential misinterpretations that are possible from a literal reading of their comments. Our conclusion is that following Benbasat and Zmud's nomological net will result in a micro focus for IS research. The results of such a focus are potentially dangerous for the field. They could result in the elimination of IS from many academic programs. We present an alternative set of heuristics that can be used to assess what lies within the domain of IS scholarship. We argue that the IS community has a powerful story to tell about the transformational impact of information technology. We believe that a significant portion of our research should be macro studies of the impact of IT. It is important for academic colleagues, deans, and managers to understand the transformational power of the technology. As IS researchers with deep knowledge of the underlying artifact, we are best positioned to do such research.
|keyword = IS identity crisis,transformational impact of IT,macro studies of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Model of adoption of technology in households: A baseline model test and extension incorporating household life cycle'''
{{header}}
{{article
|author= SA Brown,V Venkatesh,
|source= MIS QUARTERLY
|year= 2005
|abstract = Individual adoption of technology has been studied extensively in the workplace. Far less attention has been paid to adoption of technology in the household. In this paper, we performed the first quantitative test of the recently developed model of adoption of technology in households (MATH). Further, we proposed and tested a theoretical extension of MATH by arguing that key demographic characteristics that vary across different life cycle stages would play moderating roles. Survey responses were collected from 746 U.S. households that had not yet adopted a personal computer. The results showed that the integrated model, including MATH constructs and life cycle characteristics, explained 74 percent of the variance in intention to adopt a PC for home use, a significant increase over baseline MATH that explained 50 percent of the variance. Finally, we compared the importance of various factors across household life cycle stages and gained a more refined understanding of the moderating role of household life cycle stage.
|keyword = adoption,technology adoption,household,personal computers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A multilevel model of resistance to information technology implementation'''
{{header}}
{{article
|author= L Lapointe,S Rivard,
|source= MIS QUARTERLY
|year= 2005
|abstract = To better explain resistance to information technology implementation, we used a multilevel, longitudinal approach. We first assessed extant models of resistance to IT Using semantic analysis, we identified five basic components of resistance: behaviors, object, subject, threats, and initial conditions. We further examined extant models to (1) carry out a preliminary specification of the nature of the relationships between these components and (2) refine our understanding of the multilevel nature of the phenomenon. Using analytic induction, we examined data from three case studies of clinical information systems implementations in hospital settings, focusing on physicians' resistance behaviors. The resulting mixed determinants model suggests that group resistance behaviors vary during implementation. When a system is introduced, users in a group will first assess it in terms of the interplay between its features and individual and/or organizational-level initial conditions. They then make projections about the consequences of its use. If expected consequences are threatening, resistance behaviors will result During implementation, should some trigger occur to either modify or activate an initial condition involving the balance of power between the group and other user groups, it will also Modify the object of resistance, from system to system significance. If the relevant initial conditions pertain, to the power of the resisting group vis-a-Vis the system advocates, the object of resistance will also be modified, from system significance to system advocates. Resistance behaviors will follow if threats are perceived from the interaction between the object of resistance and initial conditions. We also found that the bottom-up process by which group resistance behaviors emerge from individual behaviors is not the same in early versus late implementation. In early implementation, the emergence process is one of compilation, described as a combination of independent, individual behaviors. In later stages of implementation, if group level initial conditions have become active, the emergence process is one of composition, described as the convergence of individual behaviors.
|keyword = user resistance,information technology,implementation,information system implementation,longitudinal perspective,multilevel approach,resistance behaviors,semantic analysis,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding user responses to information technology: A coping model of user adaptation'''
{{header}}
{{article
|author= A Beaudry,A Pinsonneault,
|source= MIS QUARTERLY
|year= 2005
|abstract = This paper defines user adaptation as the cognitive and behavioral efforts performed by users to cope with significant information technology events that occur in their work environment. Drawing on coping theory, we posit that users choose different adaptation strategies based on a combination of primary appraisal (i.e., a user's assessment of the expected consequences of an IT event) and secondary appraisal (i.e., a user's assessment of his/her control over the situation). On that basis, we identify four adaptation strategies (benefits maximizing, benefits satisficing, disturbance handling, and self-preservation) which are hypothesized to result in three different individual-level outcomes: restoring emotional stability, minimizing the perceived threats of the technology, and improving user effectiveness and efficiency. A study of the adaptation behaviors of six account managers in two large North American banks provides preliminary support for our model. By explaining adaptation patterns based an users' initial appraisal and subsequent responses to an IT event, our model offers predictive power while retaining an agency view of user adaptation. Also, by focusing on user cognitive and behavioral adaptation responses related to the technology, the work system, and the self, our model accounts for a wide range of user behaviors such as technology appropriation, avoidance, and resistance.
|keyword = coping theory,user adaptation,IT appropriation,individual performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What happens after ERP implementation: Understanding the impact of interdependence and differentiation on plant-level outcomes'''
{{header}}
{{article
|author= TF Gattiker,DL Goodhue,
|source= MIS QUARTERLY
|year= 2005
|abstract = We present a model of the organizational impacts of enterprise resource planning (ERP) systems once the system has gone live and the "shake-out" phase has occurred. Organizational information processing theory states that performance is influenced by the level of fit between information processing mechanisms and organizational context. Two important elements of this context are interdependence and differentiation among subunits of the organization. Because ERP systems include data and process integration, the theory suggests that ERP will be a relatively better fit when interdependence is high and differentiation is low. Our model focuses at the subunit level of the organization (business function or location, such as a manufacturing plant) and includes intermediate benefits through which ERPs overall subunit impact occurs (in our case at the plant level). ERP customization and the amount of time since ERP implementation are also included in the model. The resulting causal model is tested using a questionnaire survey of 111 manufacturing plants. The data support the key assertions in the model.
|keyword = organizational information processing theory,enterprise systems,ERP,data integration,interdependence,differentiation,manufacturing planning and control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaborating on multiparty information systems development projects: A collective reflection-in-action view'''
{{header}}
{{article
|author= N Levina,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Growth of Web-based applications has drawn a great number of diverse stakeholders and specialists into the information systems development (ISD) practice. Marketing, strategy, and graphic design professionals have joined technical developers, business managers, and users in the development of Web-based applications. Often, these specialists work for different organizations with distinct histories and cultures. A longitudinal, qualitative field study of a Web-based application development project was undertaken to develop an in-depth understanding of the collaborative practices that unfold among diverse professionals on ISD projects. The paper proposes that multiparty collaborative practice can be understood as constituting a "collective reflection-in-action" cycle through which an information systems (IS) design emerges as a result of agents producing, sharing, and reflecting on explicit objects. Depending on their control over the various economic and cultural (intellectual) resources brought to the project and developed on the project, agents influence the design in distinctive ways. They use this control to either "add to," "ignore," or "challenge" the work produced by others. Which of these modes of collective reflection-in-action are enacted on the project influences whose expertise will be reflected in the final design. Implications for the study of boundary objects, multiparty collaboration, and organizational learning in contemporary ISD are drawn.
|keyword = system design and implementation,outsourcing,management of IS projects,critical perspectives on IT,interpretive research,ethnographic research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Lying on the Web: Implications for expert systems redesign'''
{{header}}
{{article
|author= ZR Jiang,VS Mookerjee,S Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = We consider a new variety of sequential information gathering problems that are applicable for Web-based applications in which data provided as input may be distorted by the system user, such as an applicant for a credit card. We propose two methods to compensate for input distortion. The first method, termed knowledge base modification, considers redesigning the knowledge base of an expert system to best account for distortion in the input provided by the user. The second method, termed input modification, modifies the input directly to account for distortion and uses the modified input in the existing (unmodified) knowledge base of the system. These methods are compared with an approach where input noise is ignored. Experimental results indicate that both types of modification substantially improve the accuracy of recommendations, with knowledge base modification outperforming input modification in most cases. Knowledge base modification is, however, more computationally intensive than input modification. Therefore, when computational resources are adequate, the knowledge base modification approach is preferred; when such resources are very limited, input modification may be the only viable alternative.
|keyword = sequential information gathering,expert systems,input distortion,noise handling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Is out of sight, out of mind? An empirical study of social loafing in technology-supported groups'''
{{header}}
{{article
|author= L Chidambaram,LL Tung,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Research on group behavior has identified social loafing, i.e., the tendency of members to do less than their potential, as a particularly serious problem plaguing groups. Social Impact Theory (SIT) helps explain social loafing in terms of two theoretical dimensions - the dilution effect (where an individual feels submerged in the group) and the immediacy gap (where an individual feels isolated from the group). In this study, which employed a controlled experiment, we investigated these dimensions of social loafing in the context of group decision making, using collocated and distributed teams of varying sizes. Our results - in line with SIT - indicate that small groups, signifying a small dilution effect, had increased individual contributions and better group outcomes compared to their larger counterparts. However, support for SIT's arguments about the immediacy gap was mixed: Members contributed visibly more when they were collocated, but no significant differences in group outcomes were evident. Regardless of dimension, the quality of the input (ideas generated) determined the quality of the output (decisions made). Also, contrary to the literature on brainstorming, having more ideas to work with resulted in poorer-quality decisions. This apparent paradox is explained using the notion of integrative complexity, which challenges conventional wisdom regarding the relationship between individual inputs and group outputs. The implications of these results for practice and research are examined.
|keyword = computer-supported group work,distributed group decision making,group size,social loafing,integrative complexity,collaborative technologies,virtual teams,group performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward comprehensive real-time bidder support in iterative combinatorial auctions'''
{{header}}
{{article
|author= G Adomavicius,A Gupta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Many auctions involve selling several distinct items simultaneously, where bidders can bid on the whole or any part of the lot. Such auctions are referred to as combinatorial auctions. Examples of such auctions include truck delivery routes, industrial procurement, and FCC spectrum. Determining winners in such auctions is an NP-hard problem, and significant research is being conducted in this area. However, multiple-round (iterative) combinatorial auctions present significant challenges in bid formulations as well. Because the combinatorial dynamics in iterative auctions can make a given bid part of a winning and nonwinning set of bids without any changes in the bid, bidders are usually not able to evaluate whether they should revise their bid at a given point in time or not. Therefore, in this paper we address various computational problems that are relevant from the bidder's perspective. In particular, we introduce two bid evaluation metrics that can be used by bidders to determine whether any given bid can be a part of the winning allocation and explore their theoretical properties. Based on these metrics, we also develop efficient data structures and algorithms that provide comprehensive information about the current state of the auction at any time, which can help bidders in evaluating their bids and bidding strategies. Our approach uses exponential memory storage but provides fast incremental update for new bids, thereby facilitating bidder support for real-time iterative combinatorial auctions.
|keyword = iterative combinatorial auctions,real-time bidder support,bid evaluation metrics,computational techniques for combinatorial auctions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The economic incentives for sharing security information'''
{{header}}
{{article
|author= E Gal-Or,A Ghose,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Given that information technology (IT) security has emerged as an important issue in the last few years, the subject of security information sharing among firms, as a tool to minimize security breaches, has gained the interest of practitioners and academics. To promote the disclosure and sharing of cyber security information among firms, the U.S. federal government has encouraged the establishment of many industry-based Information Sharing and Analysis Centers (ISACs) under Presidential Decision Directive (PDD) 63. Sharing security vulnerabilities and technological solutions related to methods for preventing, detecting, and correcting security breaches is the fundamental goal of the ISACs. However, there are a number of interesting economic issues that will affect the achievement of this goal. Using game theory, we develop an analytical framework to investigate the competitive implications of sharing security information and investments in security technologies. We find that security technology investments and security information sharing act as "strategic complements" in equilibrium. Our results suggest that information sharing is more valuable when product substitutability is higher, implying that such sharing alliances yield greater benefits in more competitive industries. We also highlight that the benefits from such information-sharing alliances increase with the size of the firm. We compare the levels of information sharing and technology investments obtained when firms behave independently (Bertrand-Nash) to those selected by an ISAC, which maximizes social welfare or joint industry profits. Our results help us predict the consequences of establishing organizations such as ISACs, Computer Emergency Response Team (CERT), or InfraGard by the federal government.
|keyword = security technology investment,information sharing,security breaches,externality benefit,social welfare,spillover effect,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reputation mechanism design in Online trading environments with pure moral hazard'''
{{header}}
{{article
|author= C Dellarocas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = This paper offers a systematic exploration of reputation mechanism design in trading environments with opportunistic sellers of commonly known cost and ability parameters, imperfect monitoring of a seller's actions, and two possible seller effort levels, one of which has no value to buyers. The objective of reputation mechanisms in such pure moral hazard settings is to induce sellers to exert high effort as often as possible. I study the impact of various mechanism parameters (such as the granularity of solicited feedback, the format of the public reputation profile, the policy regarding missing feedback, and the rules for admitting new sellers) on the resulting market efficiency. I find that maximum efficiency is bounded away from the hypothetical first-best case where sellers can credibly precommit to full cooperation by a factor that is related to the probability that cooperating sellers may receive "unfair" bad ratings. Furthermore, maximum efficiency is independent of the length of past history summarized in a seller's public reputation profile. I apply my framework to a simplified model of eBay's feedback mechanism and conclude that, in pure moral hazard settings, eBay's simple mechanism is capable of inducing the maximum theoretical efficiency independently of the number of recent ratings that are being summarized in a seller's profile. I derive optimal policies for dealing with missing feedback and easy online identity changes. Finally, I show that if the number of buyers is large, the results obtained in the monopoly case are also approximately valid in settings where multiple sellers of different reputations simultaneously offer auctions for identical goods.
|keyword = reputation mechanisms,moral hazard,online auctions,eBay,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Expertise integration and creativity in information systems development'''
{{header}}
{{article
|author= A Tiwana,ER Mclean,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper addresses the understudied issue of how individually held expertise in information systems development (ISD) teams results in creativity at the team level during the development process. We develop the idea that team creativity results primarily from integration of individually held expertise of team members at the team level. We further propose the quality of intrateam relationships and knowledge complementarities that align the work of individual team members at the project level influence creativity primarily through the process of expertise integration. We use data from a field study of 142 participants in 42 ISD projects to test the proposed model. The paper makes three new contributions to the IS literature. Its key contribution lies in developing an expertise integration view of team creativity. We demonstrate the centrality of integrating individually held tacit and explicit knowledge about the problem domain and the technology at the team level in achieving team creativity. The use of a process-focused conceptualization of team creativity is especially noteworthy here. The second contribution of the paper lies in conceptually developing and operationalizing the concept of expertise integration, a mechanism by which individually held knowledge is integratively applied at the project level. Although the importance of knowledge in the ISD process is widely recognized in prior research, this is the first study to develop the concept in a operationally meaningful way. The third key contribution lies in showing that the compositional and relational attributes of ISD project teams--diverse specialized knowledge in a team, the quality of intratearn working relationships, and members' cross-domain absorptive capacity--do not engender creativity by themselves; they do so primarily because they enhance integration of individual knowledge at the project level. We offer empirical evidence for such full mediation. These findings have important theoretical and practical implications, which are discussed in the paper.
|keyword = absorptive capacity,creativity,expertise integration,information systems development,information systems innovation,integration,knowledge integration,knowledge management,knowledge transfer,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Complexity of information systems development projects: Conceptualization and measurement development'''
{{header}}
{{article
|author= WD Xia,GH Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper conceptualizes and develops valid measurements of the key dimensions of information systems development project (ISDP) complexity. A conceptual framework is proposed to define four components of ISDP complexity: structural organizational complexity, structural IT complexity, dynamic organizational complexity, and dynamic IT complexity. Measures of ISDP complexity are generated based on literature review, field interviews, and focus group discussions. The measures are then refined through a systematic process and are tested using confirmatory data analyses with survey responses from 541 ISDP managers. Results support the final measurement model that consists of a second-order factor of ISDP complexity, four distinct first-order factors, and 15 measurement items. The measurement adequately satisfies the criteria for unidimensionality, convergent validity, discriminant validity, reliability, factorial invariance across different types of ISDPs, and nomological validity. Implications of the study results to research and practice as well as limitations of the study and directions for future research are discussed.
|keyword = complexity,conceptual framework,confirmatory factor analysis,information systems development project,scale development,scale validation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A multidimensional commitment model of volitional systems adoption and usage behavior'''
{{header}}
{{article
|author= Y Malhotra,D Galletta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = In recent years, several organizations have implemented nonmandatory information and communication systems that escape the conventional behavioral logic of understanding acceptance and usage from a normative perspective of compliance with the beliefs of others. Because voluntary systems require users' volitional behavior, researchers have traced recent implementation failures to a lack of user commitment. However, gaps in our understanding of volitional usage behavior and user commitment have made it difficult to advance theory, research, and practice on this issue. To validate a proposed research model, cross-sectional, between-subjects, and within-subjects field data were collected from 714 users at the time of initial adoption and after six months of extended use. The model explained between 44.1 percent and 58.5 percent of the variance in adoption and usage behavior based upon direct effects of user commitment. Findings suggest that user commitment plays a critical role in the volitional acceptance and usage of such systems. Affective commitment-that is, internalization and identification based upon personal norms--exhibits a sustained positive influence on usage behavior. In contrast, continuance commitment-that is, compliance based upon social norms-shows a sustained negative influence from initial adoption to extended use. Theory development based upon Kelman's social influence framework offers new empirical insights about system users' commitment and how it affects volitional usage behavior.
|keyword = affective processes,cognitive processes,information systems,acceptance and use,multidimensional commitment model,personal norms,psychological attachment,social influence theory,social norms,systems implementation,user commitment,volitional usage behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does information technology training really matter? A social information processing analysis of coworkers' influence on IT usage in the Workplace'''
{{header}}
{{article
|author= MJ Gallivan,VK Spitler,M Koufaris,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This paper develops a conceptual framework to explain employees' technology usage within organizations. Much of the prior information systems literature has assumed an underlying relationship between "facilitating conditions" for information technology (IT) adoption (e.g., user training, technical support, resource availability) and employees' technology use. Although these facilitating conditions are important, they do not provide a complete explanation of employees' IT usage. The reality of working in organizational settings suggests a different model of IT adoption and usage. Drawing from research on social information processing theory, and acknowledging the role of other individuals within the work context that shapes employees' learning, values, and behavior, we propose a framework to explain employees' adoption of IT and their level of usage within organizations, featuring both individual level factors and factors related to the social information processing influence of coworkers. Our results show that an employee's coworkers exert an important influence on IT usage, whereas individual-level factors exhibit more modest effects.
|keyword = community of practice,end-user computing,information systems usage,situated learning,social influence,social information processing,survey research/design,user training,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Financial performance, CEO compensation, and large-scale information technology Outsourcing decisions'''
{{header}}
{{article
|author= JA Hall,SL Liedtka,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The inherent riskiness of large-scale information technology (IT) outsourcing led us to investigate what motivates large-scale IT outsourcing decisions. We employed fixed-effects logistical regressions to examine publicly available data for 51 firms that announced their decisions to outsource all or a large portion of their IT function during the 1993-2001 period. Results suggest that incentives created by CEO stock options and overall compensation mix significantly influence decisions to outsource. We thus provide the first evidence of a relationship between managerial self-interest and IT outsourcing. Additional results suggest that poor overall firm performance, poor cost control, and short-term cash needs also drive large-scale IT outsourcing, but provide no evidence that firms outsource IT to reduce leverage. Overall, we conclude that CEOs consider several personal and firm-level financial factors, including factors unrelated to IT cost and performance, when making large scale IT outsourcing decisions. Our conclusion has numerous implications regarding long-term IT performance, long-term firm performance, CEO contracting and financial reporting.
|keyword = CEO compensation,incentive compensation,information systems outsourcing,information technology outsourcing,stock options,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The relationship of E-commerce competence to customer value and firm performance: An empirical investigation'''
{{header}}
{{article
|author= KA Saeed,V Grover,YJ Hwang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The advent of electronic commerce has induced many organizations to develop a Web presence and exploit the opportunities offered by the Internet. In an environment that commoditizes products and allows for easy imitative behavior through instant access to information on competitor's offerings, it is not clear how to build a sustainable competitive advantage. This study endeavors to facilitate an understanding of this complex issue. Electronic commerce competence is posited as a key driver of organizational performance, and it is argued that this effect is mediated by the generation of "customer value" through Web site functionality. By empirically analyzing primary and secondary data from over 100 companies, the relationship between electronic commerce competence, customer value, and both short- and long-term firm performance is examined. The results show that firms with high electronic commerce competence exhibit superior performance and that customer value generated through Web site functionality partially mediates this relationship. In addition, the results show that companies can enhance short-term performance by providing value to the customer in prepurchase situations. But in order to build customer loyalty and thus long-term performance, companies need to enhance the product ownership experience of customers.
|keyword = competitive advantage,customer value,electronic commerce,electronic commerce competence,financial measures,organizational performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information processing view of organizations: An exploratory examination of fit in the context of interorganizational relationships'''
{{header}}
{{article
|author= G Premkumar,K Ramamurthy,CS Saunders,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = This study uses Galbraith's information processing theory to examine the fit between information processing needs and information processing capability in an interorganizational supply chain context and to examine its effect on performance. Information processing needs are assessed based on various characteristics of the product and procurement environment and information processing capabilities are assessed by the level of information technology support for various activities in the procurement life cycle. A taxonomy of information processing needs and information processing capabilities is developed. The effect of the fit between information processing needs and capabilities on procurement performance is examined. The study collected data on 142 products through personal interviews and surveys, used cluster analytic techniques to develop taxonomies, and analysis of variance (ANOVA) to test the fit between needs and capability, modeled as an interaction effect. The results reveal two clusters for information processing needs and three clusters for information processing capability. ANOVA results show that the interactive effect of information needs and capability has a significant effect on performance, supporting our fit theory.
|keyword = B2B,cluster analysis,information processing capability,information processing needs,information processing theory,interorganizational systems,strategic fit,supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A game-theoretic model of E-marketplace participation growth'''
{{header}}
{{article
|author= MR Galbreth,ST March,GD Scudder,M Shor,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Despite their potential to significantly reduce transaction costs for both buyers and sellers, e-marketplaces have struggled. Recent literature has examined the value propositions of e-marketplaces and proposed conceptual frameworks for their analysis. In this research, we move beyond conceptual analysis by developing a game theoretic model of return-on-investment (ROI)-driven e-marketplace participation growth. This model provides insights into expected e-marketplace growth and participation, and can be used to determine both the viability and expected long-run size of a given e-marketplace. Our results indicate that the pricing policy of the e-marketplace intermediary can affect the rate at which participation grows and, therefore, sentiment about its prospects. We focus on e-marketplaces that add value to buyers and sellers by increasing the efficiency of administrative tasks but also simultaneously add value to buyers and reduce value to sellers by lowering prices for goods purchased. Value to participants in these e-marketplaces is determined by the volume of transactions that can be conducted using the e-marketplace, resulting in a two-sided network effect-buyers reacting to sellers and sellers reacting to buyers. The game theoretic model identifies an e-marketplace equilibrium at which participation growth is predicted to stop.
|keyword = business-to-business e-commerce,e-marketplace,game theory,network effect,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of relational factors and channel climate on EDI usage in the customer-supplier relationship'''
{{header}}
{{article
|author= JY Son,S Narasimhan,FJ Riggins,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Managing electronic trading partner relationships is a key to successful development of an interorganizational systems (IOS) network. Firms often exercise their power and offer reciprocal investments to their trading partners in developing an IOS network. However, limited effort has been made to empirically validate their effects on increasing IOS usage between trading partners. This paper gauges the effects of these two relational factors-power and reciprocal investments-within the context of an electronic data interchange (EDI) network development. Moreover, the role of channel climate in increasing EDI usage is explicated with a particular focus on its determinants and impacts. With insights obtained from social exchange and transaction cost theories, a research model is developed and tested with data collected from 233 suppliers with electronic linkages via EDI with a nationally recognized retailer of home improvement supplies and materials in the United States. The customer's reciprocal investments in the form of EDI-related support are proven to be effective in increasing EDI volume and diversity. However, power exercised is found to be not effective. Suppliers' cooperation with the customer, which is influenced by perceived uncertainty, trust, and transaction-specific investments, is found to have strong effects on EDI volume and diversity. Finally, the reciprocal investments are found to be an even more effective strategy when suppliers desire to keep a more cooperative relationship with the customer.
|keyword = buyer-seller relationships,electronic data interchange,information technology use,interorganizational relations and cooperation,interorganizational systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the role of "Free" product-augmenting Online services in pricing and customer retention strategies'''
{{header}}
{{article
|author= RK Chellappa,KR Kumar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = As products on the Web are continually enhanced through "free" Web-based services that add to the product purchase experience, it is important to understand how these free services may affect pricing and customer retention strategies of an online vendor. This paper argues that product competition on the Web is not for generic products but, rather, for expected and augmented product bundles. Our findings point out that even in the absence of price premiums, variance in the ability to offer online services can affect pricing strategies and possibly contribute to online price dispersion. We then go on to suggest that online services affect a vendor's customer retention strategy as they influence the design of the augmented product. We characterize an online vendor's selection of augmenting services as a knapsack problem, and recommend that the online vendor should not only periodically reevaluate the set of services offered to satisfy the expected product requirements, but also assess the customer retention ability of his augmented product. A service does not contribute to customer retention when it has either lost its value to the customer or become required as a part of the expected product. Our solution recommends that a vendor should include new services based on the cost-to-value ratio of each service so as to remain above the loyalty threshold of a consumer. The results from our model partially explain the variety in product offerings of many online vendors, whose competency in providing Web-based services allows them to vary the generic product.
|keyword = consumer loyalty,customer retention,knapsack problem,online pricing,online services,product augmentation,switching behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A sender-receiver framework for knowledge transfer'''
{{header}}
{{article
|author= LH Lin,XJ Geng,AB Whinston,
|source= MIS QUARTERLY
|year= 2005
|abstract = The shift to more distributed forms of organizations and the prevalence of interorganizational relationships have led to an increase in the transfer of knowledge between patties with asymmetric and incomplete information about each other. Because of this asymmetry and incompleteness, parties seeking knowledge may not be able to identify qualified knowledge providers, and the appropriate experts may fail to be motivated to engage in knowledge transfer. We propose a sender-receiver framework for studying knowledge transfer under asymmetric and/or incomplete information. We outline four types of information structures for knowledge transfer, and focus on the sender-advantage asymmetric information structure and the symmetric incomplete information structure. We develop formal game-theoretical models, show how information incompleteness and asymmetry may negatively influence knowledge transfer, and propose solutions to alleviate these negative impacts. Implications for knowledge transfer research and practice are also discussed.
|keyword = knowledge transfer,knowledge management,incomplete information,asymmetric information,sender-receiver game,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effective use of knowledge management systems: A process model of content ratings and credibility indicators'''
{{header}}
{{article
|author= RS Poston,C Speier,
|source= MIS QUARTERLY
|year= 2005
|abstract = Knowledge management systems (KMSs) facilitate the efficient and effective sharing of a firm's intellectual resources. However, sifting through the myriad of content available through KMSs can be challenging, and knowledge workers may be overwhelmed when trying to find the content most relevant for completing a new task. To address this problem, KMS designers often include content rating schemes (i.e., users of the KMS submit ratings to indicate the quality of specific content used) and credibility indicators (indicators describing the validity of the content and/or the ratings) to improve users' search and evaluation of KMS content. This study examines how content ratings and credibility indicators affect KMS users' search and evaluation processes and decision performance (how well and how quickly users selected alternatives offered by the KMS). Four interrelated laboratory experiments provide evidence that ratings have a strong influence on KMS search and evaluation processes, which in turn affects decision performance. Finally, this study demonstrates that certain credibility indicators can moderate the relationship between rating validity and KMS content search and evaluation processes.
|keyword = knowledge management systems,knowledge usage,decision making,content ratings,credibility indicators,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge acquisition via three learning processes in enterprise information portals: Learning-by-investment, learning-by-doing, and learning-from-others'''
{{header}}
{{article
|author= C Ryu,YJ Kim,A Chaudhury,HR Rao,
|source= MIS QUARTERLY
|year= 2005
|abstract = An enterprise information portal (EIP) is viewed as a knowledge community. Activity theory provides a framework to study such a community: members of an EIP conduct specific tasks that are assigned through a division of labor. Each member of an enterprise information portal can undergo three distinct types of learning processes: learning-by-investment, learning-by-doing, and learning-from-others. Through these three types of learning processes, each member achieves specialized knowledge that is related to his or her own task. Cumulative knowledge resulting from the learning processes is considered in terms of two distinct attributes: depth and breadth of knowledge. This paper formulates a mathematical model and defines the goal of an EIP member as maximizing the net benefits of knowledge resulting from individual investment and effort. Numerical examples are provided to analyze patterns of optimal investment and effort plans as well as the resulting accumulated knowledge. The results provide useful managerial implications. In business conditions characterized by high interest rates or high internal rate of returns, it is preferable for members to delay spending their resources for learning. Intensive investment and efforts to obtain knowledge are preferable when the discount rate of costs is high, when knowledge is durable, when the value of knowledge is high, when the initial level of knowledge is high, when the productivity of the learning process is high, and when sufficient knowledge is transferred from other members. On the other hand, the size of the EIP has a positive or negative effect depending on the attribute of knowledge and the productivity of learning processes. Further properties of the optimal decisions and learning processes are analyzed and discussed.
|keyword = knowledge management,enterprise information portals,learning,activity theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing value in organizational knowledge creation: Considerations for knowledge workers'''
{{header}}
{{article
|author= ANK Chen,TM Edgington,
|source= MIS QUARTERLY
|year= 2005
|abstract = To maintain competitive advantage, a firm's investment decisions related to knowledge creation are likely to be strategic in nature. However, strategic investments usually have an element of risk linked to uncertain and deferred investment benefits. To date, such-investment decisions relating to knowledge workers have not been extensively researched. In this paper, we explore the following research question: How do we strategically assess knowledge creation over time giving consideration to complex decision criteria in order to improve organizational value? We develop a model based on economic and organization theory for assessing organizational value with regard to knowledge creation investments. Our model prototype provides managers with a learning tool relating to the timing and selection of knowledge creation investments. Our own use of the tool in simulation experiments yielded several insights which suggest that the decisions typically made by managers may dilute knowledge creation investments. Our results demonstrate that the organizational benefit of knowledge creation processes should be well aligned with near-term tasks. Under instances of high knowledge depreciation, however, it is unlikely that individual workers can optimize knowledge creation process decisions without organizational involvement in matching skills to task complexities. The organizational benefits of consistent and frequent knowledge creation process participation increase over time as the match of skills and task complexities improve.
|keyword = knowledge management,knowledge creation,organizational dynamics,task characteristics,organizational theory,economic theory,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology relatedness, knowledge management capability, and performance of multibusiness firms'''
{{header}}
{{article
|author= H Tanriverdi,
|source= MIS QUARTERLY
|year= 2005
|abstract = Business value of information technology is an enduring research question. The elusive link between IT and financial firm performance calls for further research into intermediate organizational variables through which IT may influence firm performance. This study proposes that knowledge management (KM) is a critical organizational capability through which IT influences firm performance. In the context of multibusiness firms, the study examines how the IT resources of a firm should be organized and managed to enhance the firm's KM capability, and whether and how KM capability influences firm performance. The study develops two hypothesizes: (1) IT relatedness of the firm's business units enhances cross-unit KM capability; (2) KM capability, in turn, leads to superior firm performance. Data from 250 Fortune 1000 firms provide empirical support for these hypotheses. IT relatedness of business units enhances the cross-unit KM capability of the firm. The KM capability creates and exploits cross-unit synergies from the product, customer, and managerial knowledge resources of the firm. These synergies increase the financial performance of the firm. IT relatedness also has significant indirect effects on firm performance through the mediation of KM capability.
|keyword = IT relatedness,knowledge management capability,complementarity,corporate performance,multibusiness firms,diversification,coordination,synergy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The emergence of boundary spanning competence in practice. implications for implementation and use of information systems'''
{{header}}
{{article
|author= N Levina,E Vaast,
|source= MIS QUARTERLY
|year= 2005
|abstract = This paper investigates how an organizational competence in boundary spanning emerges in practice by drawing on the concepts of boundary spanner and boundary object Using data from two qualitative field studies, we argue that in order for boundary spanning to emerge a new joint field of practice must be produced. Our data illustrate that some agents partially transform their practices in local settings so as to accommodate the interests of their counterparts. While negotiating the new joint field, these agents become what we call boundary spanners-in-practice who produce and use objects which become locally useful and which acquire a common identity-hence, boundary objects-in-use. Moreover, we show how boundary spanners-in-practice use various organizational and professional resources including the influence that comes with being nominated to boundary spanners' roles to create the new joint field. The conditions necessary for boundary spanners-in-practice to emerge are outlined and discussed, as are important implications for IS implementation and use.
|keyword = boundary spanning,boundary objects,boundary spanners,boundaries,practice theory,Bourdieu,knowledge management,organizational learning,IS implementation,IS use,client-consultant relationship,intranet,roles,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Running in packs to develop knowledge-intensive technologies'''
{{header}}
{{article
|author= AHV de Ven,
|source= MIS QUARTERLY
|year= 2005
|abstract = Advances in information technologies and the growth of a knowledge-based service economy are transforming the basis of technological innovation and corporate competition. This transformation requires taking a broader, institutional and political view of information technology and knowledge management To succeed, firms are advised to focus on building their distinctive competencies, outsource the rest, and become nodes in value chain networks. This shifts the level of competition from between individual firms to between networks of firms. In these networks, individual firms or entrepreneurs seldom have the resources, power, or legitimacy to produce change alone. As a result, "running in packs" is often more successful than "going it alone" to develop and commercialize knowledge-intensive technologies. Many different actors in public and private sectors make important contributions. These actors do not play impartial roles; instead, they are active participants who become embroiled in diverse, partisan, and embedded issues of innovation development. In this setting, success requires not only technical and rational competence, but also political savvy to understand and mobilize the interests of other players with stakes in an emerging industry.
|keyword = knowledge management,competence,infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Perceived individual collaboration know-how development through information technology-enabled contextualization: Evidence from distributed teams'''
{{header}}
{{article
|author= A Majchrzak,A Malhotra,R John,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = In today's global market environment, enterprises are increasingly turning to use of distributed teams to leverage their resources and address diverse markets. Individual members of structurally diverse distributed teams need to develop their collaboration know-how to work effectively with others on their team. The lack of face-to-face cues creates challenges in developing the collaboration know-how-challenges that can be overcome by communicating not just content, but also context. We derive a theoretical model from Te'eni's (2001) cognitive-affective model of communication to elaborate how information technology (IT) can support an individual's communication of context to develop collaboration know-how. Two hundred and sixty-three individuals working in structurally diverse distributed teams using a variety of virtual workspace technologies to support their communication needs were surveyed to test the model. Results indicate that when individuals perceive their task as nonroutine, there is a positive relationship between an individual's perceived degree of IT support for communicating context information and his collaboration know-how development. However, when individuals perceive their task as routine, partial IT support for contextualization is associated with lower levels of collaboration know-how development. This finding is attributed to individuals' misunderstanding of the conveyed context, or their struggling to utilize the context conveyed with partial IT support, making a routine task more prone to misunderstanding and leaving the user worse than if she had no IT support for contextualization. We end the paper by drawing theoretical and practical implications based on these findings.
|keyword = knowledge management,collaboration,virtual teams,distributed teams,knowledge sharing,group support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The value of intrusion detection systems in information technology security architecture'''
{{header}}
{{article
|author= H Cavusoglu,B Mishra,S Raghunathan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = The increasing significance of information technology (IT) security to firms is evident from their growing IT security budgets. Firms rely on security technologies such as firewalls and intrusion detection systems (IDSs) to manage IT security risks. Although the literature on the technical aspects of IT security is proliferating, a debate exists in the IT security community about the value of these technologies. In this paper, we seek to assess the value of IDSs in a firm's IT security architecture. We find that the IDS configuration, represented by detection (true positive) and false alarm (false positive) rates, determines whether a firm realizes a positive or negative value from the IDS. Specifically, we show that a firm realizes a positive value from an IDS only when the detection rate is higher than a critical value, which is determined by the hacker's benefit and cost parameters. When the firm realizes a positive (negative) value, the IDS deters (sustains) hackers, However, irrespective of whether the firm realizes a positive or negative value from the IDS, the IDS enables the firm to better target its investigation of users, while keeping the detection rate the same. Our results suggest that the positive value of an IDS results not from improved detection per se, but from an increased deterrence enabled by improved detection. Finally, we show that the firm realizes a strictly nonnegative value if the firm configures the IDS optimally based on the hacking environment.
|keyword = economics of IT security,intrusion detection systems (IDSs),ROC curves,security configuration,IT security management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Competition among sellers in online exchanges'''
{{header}}
{{article
|author= S Bandyopadhyay,JM Barron,AR Chaturvedi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = With the advent of the Internet, and the minimal information technology requirements of a trading partner to join an exchange, the number of sellers who can qualify and participate in online exchanges is greatly increased. We model the competition between two sellers with different unit costs and production capacities responding to a buyer demand. The resulting mixed-strategy equilibrium shows that one of the sellers has a normal high price with random sales, while the other seller continuously randomizes its prices. It also brings out the inherent advantages that sellers with lower marginal costs or higher capacities have in joining these exchanges, and provides a theoretical basis for understanding the relative advantages of various types of sellers in such exchanges.
|keyword = online exchanges,reverse auctions,pricing power,mixed-strategy equilibria,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Post-adoption variations in usage and value of e-business by organizations: Cross-country evidence from the retail industry'''
{{header}}
{{article
|author= K Zhu,KL Kraemer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = Grounded in the innovation diffusion literature and the resource-based theory, this paper develops an integrative research model for assessing the diffusion and consequence of e-business at the firm level. Unlike the typical focus on adoption as found in the literature, we focus on postadoption stages, that is, actual usage and value creation. The model thus moves beyond dichotomous "adoption versus nonadoption" and accounts for the "missing link"-actual usage-as a critical stage of value creation. The model links technological, organizational, and environmental factors to e-business use and value, based on which a series of hypotheses are developed. The theoretical model is tested by using structural equation modeling on a dataset of 624 firms across 10 countries in the retail industry. To probe deeper into whether e-business use and value are influenced by economic environments, two subsamples from developed and developing countries are compared. The study finds that technology competence, firm size, financial commitment, competitive pressure, and regulatory support are important antecedents of e-business use. In addition, the study finds that, while both front-end and back-end capabilities contribute to e-business value, back-end integration has a much stronger impact. While front-end functionalities are becoming commodities, e-businesses are more differentiated by back-end integration. This is consistent with the resource-based theory because back-end integration possesses the value-creating characteristics of resources (e.g., firm specific, difficult to imitate), which are strengthened by the Internet-enabled connectivity. Our study also adds an international dimension to the innovation diffusion literature, showing that careful attention must be paid to the economic and regulatory factors that may affect technology diffusion across different countries.
|keyword = technology diffusion,innovation,e-business,IT investment,usage,value,back-end integration,firm performance,resource-based view,international perspective,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A theoretical integration of user satisfaction and technology acceptance'''
{{header}}
{{article
|author= BH Wixom,PA Todd,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2005
|abstract = In general, perceptions of information systems (IS) success have been investigated within two primary research streams-the user satisfaction literature and the technology acceptance literature. These two approaches have been developed in parallel and have not been reconciled or integrated. This paper develops an integrated research model that distinguishes beliefs and attitudes about the system (i.e., object-based beliefs and attitudes) from beliefs and attitudes about using the system (i.e., behavioral beliefs and attitudes) to build the theoretical logic that links the user satisfaction and technology acceptance literature. The model is then tested using a sample of 465 users from seven different organizations who completed a survey regarding their use of data warehousing software. The proposed model was supported, providing preliminary evidence that the two perspectives can and should be integrated. The integrated model helps build the bridge from design and implementation decisions to system characteristics (a core strength of the user satisfaction literature) to the prediction of usage (a core strength of the technology acceptance literature).
|keyword = user satisfaction,technology acceptance model,information systems success,theory of reasoned action,system quality,information quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing the Digital Rosetta Stone model for long-term access to digital documents'''
{{header}}
{{article
|author= AR Heminger,DM Kelley,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Information that is stored digitally is at risk for being irretrievably lost if either the methods used to retrieve the bitstream or the methods used to interpret the bitstream are lost. The Digital Rosetta Stone (DRS) model was developed as a conceptual framework for capturing and maintaining the methods necessary to retrieve digital information stored on obsolete media and to properly interpret it, though the software used to create it may also be obsolete. However, the community of those professionals concerned with this issue had not yet assessed this conceptual model. This study used the Delphi method to explore these issues with those responsible for maintaining access to digital data. Overall, the Delphi group expressed concerns about the practicality of developing the DRS, but agreed that it is an important concept that should be explored further. If found to be technologically feasible and economically desirable, the DRS could well lead to a long-term solution for recovering information that would otherwise be impossible to recover.
|keyword = access to digital documents,digital storage,permanence of stored information,technological obsolescence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Genetic programming-based discovery of ranking functions for effective Web search'''
{{header}}
{{article
|author= WG Fan,MD Gordon,P Pathak,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Web search engines have become an integral part of the daily life of a knowledge worker, who depends on these search engines to retrieve relevant information from the Web or from the company's vast document databases. Current search engines are very fast in terms of their response time to a user query. But their usefulness to the user in terms of retrieval performance leaves a lot to be desired. Typically, the user has to sift through a lot of nonrelevant documents to get only a few relevant ones for the user's information needs. Ranking functions play a very important role in the search engine retrieval performance. In this paper, we describe a methodology using genetic programming to discover new ranking functions for the Web-based information-seeking task. We exploit the content as well as structural information in the Web documents in the discovery process. The discovery process is carried out for both the ad hoc task and the routing task in retrieval. For either of the retrieval tasks, the retrieval performance of these newly discovered ranking functions has been found to be superior to the performance obtained by well-known ranking strategies in the information retrieval literature.
|keyword = business intelligence,genetic programming,information retrieval,machine learning,ranking function,search engine,text mining,Web mining,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A visual framework for knowledge discovery on the Web: An empirical study of business intelligence exploration'''
{{header}}
{{article
|author= W Chung,H Chen,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Information overload often hinders knowledge discovery on the Web. Existing tools lack analysis and visualization capabilities. Search engine displays often overwhelm users with irrelevant information. This research proposes a visual framework for knowledge discovery on the Web. The framework incorporates Web mining, clustering, and visualization techniques to support effective exploration of knowledge. Two new browsing methods were developed and applied to the business intelligence domain: Web community uses a genetic algorithm to organize Web sites into a tree format; knowledge map uses a multidimensional scaling algorithm to place Web sites as points on a screen. Experimental results show that knowledge map outperformed Kartoo, a commercial search engine with graphical display, in terms of effectiveness and efficiency. Web community was found to be more effective, efficient, and usable than result list. Our visual framework thus helps to alleviate information overload on the Web and offers practical implications for search engine developers.
|keyword = business intelligence,genetic algorithm,knowledge map,multidimensional scaling,visualization,Web browsing,Web community,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Data mining with combined use of optimization techniques and self-organizing maps for improving risk grouping rules: Application to prostate cancer patients'''
{{header}}
{{article
|author= L Churilov,A Bagirov,D Schwartz,K Smith,M Dally,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Data mining techniques provide a popular and powerful tool set to generate various data-driven classification systems. In this paper, we investigate the combined use of self-organizing maps (SOM) and nonsmooth nonconvex optimization techniques in order to produce a working case of a data-driven risk classification system. The optimization approach strengthens the validity of SOM results, and the improved classification system increases both the quality of prediction and the homogeneity within the risk groups. Accurate classification of prostate cancer patients into risk groups is important to assist in the identification of appropriate treatment paths. We start with the existing rules and aim to improve classification accuracy by identifying inconsistencies utilizing self-organizing maps as a data visualization tool. Then, we progress to the study of assigning prostate cancer patients into homogenous groups with the aim to support future clinical treatment decisions. Using the case of prostate cancer patients grouping, we demonstrate strong potential of data-driven risk classification schemes for addressing the risk grouping issues in more general organizational settings.
|keyword = classification,data mining,optimization,prostate cancer,risk grouping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design, implementation, and evaluation of trust-supporting components in virtual communities for patients'''
{{header}}
{{article
|author= JM Leimeister,W Ebner,H Krcmar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Trust provides the foundation for the successful implementation and operation of a virtual community (VC). Trust is an especially relevant success factor in online health-care communities. A look at existing communities leads to the conclusion that many VCs fail to meet requirements upon which trust is established. Based on the findings in the literature and the researchers' experience, this paper describes how trust-enabling functionalities can be systematically designed and implemented in a VC for cancer patients. Consequently, the outcomes of these design measures are evaluated. The evaluation results show that supporting trust can be achieved following a two-step model. The presented components support the perceived competence and perceived goodwill of the operators and the other members. Perceived goodwill and competence then support the process of creating and sustaining trust between members as well as between members and the operators of the VC and contribute to the successful implementation and maintenance of the community. The paper concludes with a discussion on further trust-supporting components yet to be implemented and gives recommendations for further research in this area.
|keyword = access rights,anonymity,health care,online community,perceived competence,perceived goodwill,quality-assured content,transparency criteria,trust,virtual community,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Negotiation in technology landscapes: An actor-issue analysis'''
{{header}}
{{article
|author= S Bendahan,G Camponovo,JS Monzani,Y Pigneur,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = In large-scale negotiation problems and in assessments of complex and uncertain environments, it is vital to analyze the different stakeholders involved and to evaluate their positions in the negotiations. This paper extends a model, which merges previous multi-issue and actor-focused methods, based on power relationships between stakeholders and their ability to bargain in order to increase their utility. The model has already used for assessing a public WLAN landscape. The paper emphasizes the dynamic application of the model we developed for experimenting the negotiation evolution, shifting positions on some issues, and exchanging positions between actors. We also claim that such forecasting analyses of negotiation landscapes can be significantly improved using more appropriate visualization support. We propose new visualization tools for analyzing negotiation outcomes, representing negotiation landscapes, and applying what-if simulations, using passive influence, expected outcome and dissatisfaction, power distribution, proximity, and negotiation maps.
|keyword = actor-issue analysis,collective decision-making,forecasting,negotiation,scenario planning,visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Choice of transaction channels: The effects of product characteristics on market evolution'''
{{header}}
{{article
|author= S Ba,J Stallaert,AB Whinston,H Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = The capabilities of network technologies have facilitated the growth of electronic commerce. Major issues-notably, security and product quality uncertainty-still pose serious challenges to the further adoption of electronic commerce. Traditional market transactions have a long history and well-understood protections for buyers and sellers. In the electronic markets, formal and informal mechanisms such as trusted third parties (TTP) have emerged trying to ensure safe transactions. In this paper, we investigate under what conditions people will stick to the traditional market and face-to-face transactions, and under what conditions electronic transactions will be the convention of the future. Of particular interest is the role of TTPs in facilitating online transactions. Using evolutionary game theory, we present an analytical model of buyer and seller choices and examine which patterns of transactions can be sustained. We further study how the traders' adaptive behavior may influence the outcome of the market evolution. Through this analysis, we demonstrate that the market will show divergence: for commodity products, electronic transactions through TTPs will get established as the convention for market transactions when traders use historical information about other traders' past strategies. For "look and feel" products, the market evolution depends on the initial distribution of the transaction strategies in the population.
|keyword = electronic commerce,electronic markets,evolutionarily stable equilibrium,evolutionary game theory,market evolution,product characteristics,stochastically stable equilibrium,transaction channel,trusted third party,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents of information and system quality: An empirical examination within the context of data warehousing'''
{{header}}
{{article
|author= RR Nelson,PA Todd,BH Wixom,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2005
|abstract = Understanding the successful adoption of information technology is largely based upon understanding the linkages among quality, Satisfaction, and usage. Although the satisfaction and usage constructs have been well studied in the information systems literature, there has been only limited attention to information and system quality over the past decade. To address this shortcoming, we developed a model consisting of nine fundamental determinants of quality in an information technology contexts four under the rubric of information quality (the output of an information system) and five that describe system quality (the information processing system required to produce the output). We then empirically examined the aptness of our model using a sample of 465 data warehouse users from seven different organizations that employed report-based, query based, and analytical business intelligence tools. The results suggest that our determinants are indeed predictive of overall information and system quality in data warehouse environments, and that our model strikes a balance between comprehensiveness and parsimony. We conclude with a discussion of the implications for both theory and the development and implementation of information technology applications in practice.
|keyword = business intelligence software,data warehousing,information quality,information systems success,system quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why should I share? Examining social capital and knowledge contribution in electronic networks of practice'''
{{header}}
{{article
|author= MM Wasko,S Faraj,
|source= MIS QUARTERLY
|year= 2005
|abstract = Electronic networks of practice are computer-mediated discussion forums focused on problems of practice that enable individuals to exchange advice and ideas with others based on common interests. However, why individuals help strangers in these electronic networks is not well understood: there is no immediate benefit to the contributor, and free-riders are able to acquire the same knowledge as everyone else. To understand this paradox, we apply theories of collective action to examine how individual motivations and social capital influence knowledge contribution in electronic networks. This study reports on the activities of one electronic network supporting a professional legal association. Using archival, network, survey, and content analysis data, we empirically test a model of knowledge contribution. We find that people contribute their knowledge when they perceive that it enhances their professional reputations, when they have the experience to share, and when they are structurally embedded in the network. Surprisingly, contributions occur without regard to expectations of reciprocity from others or high levels of commitment to the network.
|keyword = electronic networks of practice,knowledge management,online communities,social capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents of knowledge transfer from consultants to clients in enterprise system implementations'''
{{header}}
{{article
|author= DG Ko,LJ Kirsch,WR King,
|source= MIS QUARTERLY
|year= 2005
|abstract = Enterprise resource planning (ERP) systems and other complex information systems represent critical organizational resources. For such systems, firms typically use consultants to aid in the implementation process. Client firms expect consultants to transfer their implementation knowledge to their employees so that they can contribute to successful implementations and learn to maintain the systems independent of the consultants. This study examines the antecedents of knowledge transfer in the context of such an interfirm complex information systems implementation environment. Drawing from the knowledge transfer, information systems, and communication literatures, an integrated theoretical model is developed that posits that knowledge transfer is influenced by knowledge-related, motivational, and communication-related factors. Data were collected from consultant-and-client matched-pair samples from 96 ERP implementation projects. Unlike most prior studies, a behavioral measure of knowledge transfer that incorporates the application of knowledge was used. The analysis suggests that all three groups of factors influence knowledge transfer, and provides support for 9 of the 13 hypotheses. The analysis also confirms two mediating relationships. These results (1) adapt prior research, primarily done in non-IS contexts, to the ERP implementation context, (2) enhance prior findings by confirming the significance of an antecedent that has previously shown mixed results, and (3) incorporate new IS-related constructs and measures in developing an integrated model that should be broadly applicable to the interfirm IS implementation context and other IS situations. Managerial and research implications are discussed.
|keyword = knowledge transfer,enterprise systems,ERP,implementation,consultants,structural equation modeling,partial least squares,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contributing knowledge to electronic knowledge repositories: An empirical investigation'''
{{header}}
{{article
|author= A Kankanhalli,BCY Tan,KK Wei,
|source= MIS QUARTERLY
|year= 2005
|abstract = Organizations are attempting to leverage their knowledge resources by employing knowledge management (KM) systems, a key form of which are electronic knowledge repositories (EKRs). A large number of KM initiatives fail due to the reluctance of employees to share knowledge through these systems. Motivated by such concerns, this study formulates and tests a theoretical model to explain EKR usage by knowledge contributors. The model employs social exchange theory to identify cost and benefit factors affecting EKR usage, and social capital theory to account for the moderating influence of contextual factors. The model is validated through a large-scale survey of public sector organizations. The results reveal that knowledge self-efficacy and enjoyment in helping others significantly impact EKR usage by knowledge contributors. Contextual factors (generalized trust, pro-sharing norms, and identification) moderate the impact of codification effort, reciprocity, and organizational reward on EKR usage, respectively. It can be seen that extrinsic benefits (reciprocity and organizational reward) impact EKR usage contingent on particular contextual factors whereas the effects of intrinsic benefits (knowledge self-efficacy and enjoyment in helping others) on EKR usage are not moderated by contextual factors. The loss of knowledge power and image do not appear to impact EKR usage by knowledge contributors. Besides contributing to theory building in KM, the results of this study inform KM practice.
|keyword = knowledge management,electronic knowledge repositories,knowledge contribution,social exchange,social capital,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward an integration of agent- and activity-centric approaches in organizational process modeling: Incorporating incentive mechanisms'''
{{header}}
{{article
|author= TS Raghu,B Jayaraman,HR Rao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = This paper presents an approach to organizational modeling that combines both agent-centric and activity-centric approaches. Activity-centric approaches to process modeling capture the mechanistic components of a process (including aspects of workflow, decision, and information), but agent-centric approaches capture specific aspects of the human component. In this paper, we explore an integrative viewpoint in which the transactional aspects of agent-centric concerns-for example, economic incentives for agents to perform-are integrated with decision and informational aspects of a process. To illustrate issues in this approach, we focus on modeling incentive mechanisms in a specific sales process and present results from an extensive simulation experiment. Our results highlight the importance of considering the effects of incentives when decision and informational aspects of a process undergo changes.
|keyword = organizational processes,activity-centric modeling,agent-centric modeling,simulation,incentive mechanisms,workflow,information structure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Internet users' information privacy concerns (IUIPC): Tthe construct, the scale, and a causal model'''
{{header}}
{{article
|author= NK Malhotra,SS Kim,J Agarwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = The lack of consumer confidence in information privacy has been identified as a major problem hampering the growth of e-commerce. Despite the importance of understanding the nature of online consumers' concerns for information privacy, this topic has received little attention in the information systems community. To fill the gap in the literature, this article focuses on three distinct, yet closely related, issues. First, drawing on social contract theory, we offer a theoretical framework on the dimensionality of Internet users' information privacy concerns (IUIPC). Second, we attempt to operationalize the multidimensional notion of IUIPC using a second-order construct, and we develop a scale for it. Third, we propose and test a causal model on the relationship between IUIPC and behavioral intention toward releasing personal information at the request of a marketer. We conducted two separate field surveys and collected data from 742 household respondents in one-on-one, face-to-face interviews. The results of this study indicate that the second-order IUIPC factor, which consists of three first-order dimensions-namely, collection, control, and awareness-exhibited desirable psychometric properties in the context of online privacy. In addition, we found that the causal model centering on IUIPC fits the data satisfactorily and explains a large amount of variance in behavioral intention, suggesting that the proposed model will serve as a useful tool for analyzing online consumers' reactions to various privacy threats on the Internet.
|keyword = information privacy,concerns for information privacy,Internet users' information privacy concerns,structural equation modeling,nomological network,causal model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT outsourcing success: A psychological contract perspective'''
{{header}}
{{article
|author= C Koh,S Ang,DW Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Information technology (IT) outsourcing success requires careful management of customer-supplier relationships. However, there are few published studies on the ongoing relationships, and most of these adopt a customer perspective, de-emphasizing suppliers. In this study, we look at both customer and supplier perspectives, by means of the psychological contract of customer and supplier project managers. We apply the concept of psychological contract to perceived mutual obligations, and to how such fulfillment of obligations can predict success. Our research questions are (1) What are the critical customer-supplier obligations in an IT outsourcing relationship? and (2) What is the impact of fulfilling these obligations on success? We use a sequential, qualitative-quantitative approach to develop and test our model. In the qualitative study, we probe the nature of customer-supplier obligations using in-depth interviews. Content analysis of interview transcripts show that both customers and suppliers identify six obligations that are critical to success. Customers perceive supplier obligations to be accurate project scoping, clear authority structures, taking charge, effective human capital management, effective knowledge transfer, and effective interorganizational teams. Suppliers perceive customer obligations as clear specifications, prompt payment, close project monitoring, dedicated project staffing, knowledge sharing, and project ownership. In the second quantitative study, we assess the impact of fulfilling these obligations on success through a field study of 370 managers. Results show that fulfilled obligations predict success over and above the effects of contract type, duration, and size.
|keyword = IT outsourcing success,postcontractual issues,contracts,psychological contracting theory,outsourcing relationship management,mixed methods approach,hierarchical regression,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Deploying common systems globally: The dynamics of control'''
{{header}}
{{article
|author= LJ Kirsch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = In today's competitive environment, an increasing number of firms are building common information systems, which will be deployed globally, to support their strategic globalization initiatives. These systems are designed to meet the requirements of a diverse set of stakeholders with different business needs, priorities, and objectives. One managerial tool for addressing and reconciling such differences is control, which encompasses all attempts to motivate individuals to act in a manner that is consistent with organizational objectives. This paper examines two research questions. How do stakeholders exercise control during different phases of large IS projects? Why do control choices change across project phases? Results of two case studies suggest control is exercised differently for each phase. During the initial phase of a project, control is exercised as "collective sensemaking," in which both IS and business stakeholders utilize mostly informal mechanisms of control. During development, "technical winnowing" of mechanisms occurs such that control is vested primarily in IS managers, who structure hierarchical relationships with subordinates and who rely extensively on formal control mechanisms. Both IS and business stakeholders employ formal and informal mechanisms during implementation to exercise control as "collaborative coordinating." The results also suggest that changes in control choices from one project phase to another are triggered by factors in the project, stakeholder, and global contexts. As factors change across phases, so too do control choices. Further, problems that surface in one project phase trigger changes to controls in subsequent phases. These findings are integrated into a model of the dynamics of control. Implications of these results are drawn, and directions for future research are suggested.
|keyword = organizational control,IS project management,information systems development,common information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A decision model for software maintenance'''
{{header}}
{{article
|author= MS Krishnan,T Mukhopadhyay,CH Kriebel,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = In this paper we address the problem of increasing software maintenance costs in a custom software development environment, and develop a stochastic decision model for the maintenance of information systems. Based on this modeling framework, we derive an optimal decision rule for software systems maintenance, and present sensitivity analysis of the optimal policy. We illustrate an application of this model to a large telecommunications switching software system, and present sensitivity analysis of the optimal state for major upgrade derived from our model. Our modeling framework also allows for computing the expected time to perform major upgrade to software systems.
|keyword = software maintenance,system replacement,stochastic model,legacy systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coordinating for flexibility in e-business supply chains'''
{{header}}
{{article
|author= S Gosain,A Malhotra,OA El Sawy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The widespread use of information technology (IT) to create electronic linkages among supply chain partners with the objective of reducing transaction costs may have unintended adverse effects on supply chain flexibility. Increasing business dynamics, changing customer preferences, and disruptive technological shifts pose the need for two kinds of flexibility that interenterprise information systems must address-the ability of interenterprise linkages to support changes in offering characteristics (offering flexibility) and the ability to alter linkages to partner with different supply chain players (partnering flexibility). This study explores how enterprises in supply chains may forge supply chain linkages that enable both types of flexibility jointly, and allow them to deal with ubiquitous change. Drawing on March and Simon's coordination theory, we propose two design principles: (1) advance structuring of interorganizational processes and information exchange that allows partnering organizations to be loosely coupled, and (2) IT-supported dynamic adjustment that allows enterprises to quickly sense change and adapt their supply chain linkages. This study reports on a survey of 41 supply chain relationships in the IT industry. For design principle, our empirical investigation of factors shows (1) that modular design of interconnected processes and structured data connectivity are associated with higher supply chain flexibility, and (2) that deep coordination-related knowledge is critical for supply chain flexibility. Also, sharing a broad range of information with partners is detrimental to supply chain flexibility, and organizations should instead focus on improving the quality of information shared. For industry managers, the study provides clear insights for information infrastructure design. To manage their interdependencies, enterprises need to encapsulate their interconnected processes in modular chunks, and support these with IT platforms for information exchange in structured formats. Enterprises also need to nurture their execution capabilities by putting in place the information systems to process information exchanged with partners, augmenting their understanding of factors such as how partner actions need to trigger adaptive responses. For researchers, the study initiates a new stream of theorizing that focuses on the role of the information infrastructure in managing the tension between competing goals of offering flexibility and partnering flexibility.
|keyword = coordination theory,information technology infrastructure,interorganizational systems,supply chain flexibility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Health of electronic communities: An evolutionary game approach'''
{{header}}
{{article
|author= XJ Geng,AB Whinston,H Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Creating electronic communities is a critical venture in the digital economy. However, fraud and misrepresentation have led to widespread skepticism and distrust of electronic communities. We develop an evolutionary model to explore the issue of trust within an electronic community from a dynamic process perspective. This model emphasizes large populations, continuous change in community memberships, and imperfect information and memory. As the term trust is often used in the context of individual interaction, at a group level we propose using the term health to measure the sustained competitive advantages of honest members over cheaters throughout the evolution of a community. We find conditions under which an electronic community is healthy and attracts outside population. We find that many factors. such as information dissemination speed, honest players' payoffs and possible losses. new community members' initial trust status, and the replacement rate of community members, all affect the health of an electronic community, and that some of them also affect a community's size. We then discuss the implications of our research for e-community practices.
|keyword = dynamic process,electronic community,evolutionary game theory,trust in e-commerce,trust status,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Virtual product experience: Effects of visual and functional control of products on perceived diagnosticity and flow in electronic shopping'''
{{header}}
{{article
|author= ZH Jiang,I Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The development of electronic commerce has been constrained by the, inability of online consumers to feel, touch, and sample products through Web interfaces, as they are able to do in conventional in-store shopping. Previous academic studies have argued that this limitation could be partly alleviated by providing consumers with virtual product experience (VPE), to enable potential customers to experience products virtually. This paper discusses virtual control, a specific type of VPE implementation, and identifies its two dimensions: visual control and functional control. Visual control enables consumers to manipulate Web product images, to view products from various angles and distances; functional control enables consumers to explore and experience different features and functions of products. The individual and joint effects of visual and functional control were investigated in a laboratory experiment, the results of which indicated that visual and functional control increased the perceived diagnosticity (i.e., the extent to which a consumer believes the shopping experience is helpful to evaluate a product) of their corresponding attribute factors, and that both visual and functional control increased consumer overall perceived diagnosticity and flow.
|keyword = B2C e-commerce,direct manipulation,e-commerce,e-tailing,flow,multimedia,perceived diagnosticity,virtual control,virtual product experience,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of information format and shopping task on consumers' online shopping behavior: A cognitive fit perspective'''
{{header}}
{{article
|author= W Hong,JYL Thong,KY Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = A feature central to the success of e-commerce Web sites is the design of an effective interface to present product information. However, the suitability of the prevalent information formats in supporting various online shopping tasks is not known. Using the cognitive fit theory as the theoretical framework, we developed a research model to investigate the fit between information format and shopping task, and examine its influence on consumers' online shopping performance and perceptions of shopping ping experience. The competition for attention theory from the marketing literature and the scanpath theory from vision research were employed to support the analyses. An experiment was conducted to examine the effects of two types of information formats (list versus matrix) in the context of two types of shopping tasks (searching versus browsing). The results show that when there is a match between the information format and the shopping task, consumers can search the information space more efficiently and have better recall of product information. Specifically, the list format better supports browsing tasks, and the matrix format facilitates searching tasks. However, a match between the information format and the shopping task has no effect on cognitive effort or attitude toward using the Web site. Overall, this research supports the application of the cognitive fit theory to the study of Web interface design. It also demonstrates the value in integrating findings from cognitive science and vision research to understand the processes involved. As the information format has been shown to affect consumers' online shopping behavior, even when the information content is held constant, the practical implications for Web site designers include providing both types of information format on their Web sites and matching the appropriate information format to the individual consumer's task.
|keyword = B2C e-commerce,cognitive fit theory,competition for attention theory,e-commerce,e-tailing,information format,interface design,online shopping,scanpath theory,shopping task,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical investigation of collaborative conflict management style in group support system-based global virtual teams'''
{{header}}
{{article
|author= S Paul,IM Samarah,P Seetharaman,PP Mykytyn,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Virtual teams cut across national, organizational, and functional boundaries, often resulting in diversity in team composition. This paper presents the results of a laboratory study involving groupware-supported, culturally homogeneous, and heterogeneous virtual teams where collaborative conflict management style, a team's cultural orientation as measured by the degree of individualism-collectivism, and group diversity affect several group performance variables. Collaborative conflict management style was positively related to performance, group diversity was found to have a moderating influence between collaborative style and group performance, and collaborative style was influenced by the individualistic-collectivistic orientations. Consistent with prior research, we found that collectivistic orientations help enhance the level of collaborative conflict management style prevailing in teams. Our research also indicates that the process to motivate team members may differ depending on their orientation.
|keyword = collaborative conflict management,collectivism,group support systems,individualism,perceived decision quality,perceived participation in decision-making,satisfaction with decision process,team diversity,virtual team,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating and tuning predictive data mining models using receiver operating characteristic curves'''
{{header}}
{{article
|author= AP Sinha,JH May,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = In this study, we conduct an empirical analysis of the performance of five popular data mining methods-neural networks, logistic rearession, linear discriminant analysis, decision trees, and nearest neighbor-on two binary classification problems from the credit evaluation domain. Whereas most studies comparing data mining methods have employed accuracy as a performance measure, we argue that, for problems such as credit evaluation, the focus should be on minimizing misclassification cost. We first generate receiver operating characteristic (ROC) curves for the classifiers and use the area under the curve (AUC) measure to compare aggregate performance of the five methods over the spectrum of decision thresholds. Next. using the ROC results, we propose a method for tuning the classifiers by identifying optimal decision thresholds. We compare the methods based on expected costs across a range of cost-probability ratios. In addition to expected cost and AUC, we evaluate the models on the basis of their generalizability to unseen data, their scalability to other problems in the domain, and their robustness against changes in class distributions. We found that the performance of logistic regression and neural network models was superior under most conditions. In contrast. decision tree and nearest neighbor models yielded higher costs, and were much less generalizable and robust than the other models. An important finding, of this research is that the models can be effectively tuned post hoc to make them cost sensitive, even though they were built without incorporating misclassification costs.
|keyword = binary classification,credit evaluation,data mining,decision analysis,misclassification costs,performance evaluation and tuning,predictive models,ROC curves,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CABSYDD: Case-based system for database design'''
{{header}}
{{article
|author= J Choobineh,AW Lo,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The majority of the reported research and development efforts on automated techniques and tools for conceptual database design have focused on design from first principles. Very few have used case-based reasoning, where cases of conceptual design are stored, indexed, and used for future designs. Furthermore, there is a general lack of reported research on validating and verifying such systems. In this paper, we describe our approach in using case-based reasoning for conceptual database design. To test and demonstrate the feasibility of our approach and its theoretical foundation, two prototype systems were constructed. In the absence of existing matching conceptual design constructs, the first system uses first principles of conceptual design to assist a human designer in arriving at a design for a new problem. In contrast, the second system uses constructs from previously stored design cases. The two are tightly integrated. A novel approach in structuring the case base was developed. Unique aspects of the case-base architecture and its learning mechanism are described. In order to measure user preference, an experiment was designed and conducted. Findings indicate that reuse of schemata not only is preferred by the users over the design from the first principles, but also results in fewer errors.
|keyword = automated database design,case-based reasoning,conceptual database design,database design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reach and grasp'''
{{header}}
{{article
|author= JL King,K Lyytinen,
|source= MIS QUARTERLY
|year= 2004
|abstract = The short history of Information Systems suggests persistent anxiety about the field's purported lack of academic legitimacy. A common refrain in the anxiety discourse is that legitimacy can be obtained only by creating a strong theoretic core for the field. This essay takes exception with this view, attributing the anxiety to the field's relative youth, its focus on technology in a technophobic institutional environment, and academic ethnocentrism within and without the field. While developing stronger theory might be helpful, it is more important that the IS field pushes back against the hegemony of IS critics outside the field whose arguments masquerade as concerns about academic quality. The anxiety discourse should be replaced by the IS field's aggressive pursuit of new instructional and research opportunities that cross traditional institutional barriers and the pursuit of excellence on academic criteria deemed important by the field itself.
|keyword = information systems,identity,legitimacy,theoretic core,discipline,disciplinary,academic politics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of individual memory and attention processes during electronic brainstorming'''
{{header}}
{{article
|author= RE Potter,P Balthazard,
|source= MIS QUARTERLY
|year= 2004
|abstract = Electronic brainstorming (EBS) applications and their methodologies may have achieved the benchmark of enabling interactive users to perform as well as nominal groups. The current challenge is to view this as a plateau and not an endpoint, and to seek ways of improving EBS performance. In this study, we apply theory from cognitive psychology and adopt the individual, rather than the group, as the unit of analysis. We present a model of idea generation cognition based on Hintzman's MINERVA2 global matching model of memory cognition and additional literature from cognitive psychology on cueing and categorization. Based on this model, we present a technique called cause cueing, which directs subjects' attention to the causes of the target problem that they themselves have identified, and hypothesize that this will increase the number of ideas that an individual generates. Also based on the model- and consistent with current views on production blocking-we hypothesize that receiving input from others during brainstorming will reduce the number of ideas that an individual generates. Following is an EBS-based study that offers an empirical examination of (1) the effects of cueing attention to natural categories (causes) during idea generation and (2) the effects of cueing attention to ad hoc categories, represented by input from others, during idea generation. A total of 82 subjects were randomly assigned to one of four conditions in a 2 x 2 factorial design ANOVA experiment. Results indicate strong support for our model of idea generation as memory cognition. Cueing participant attention to natural self-generated search categories via the cause cueing technique greatly increased the generation of ideas and high quality ideas, whether or not participants were also cued to attend to ad hoc categories (input from others). Cueing attention to input from others was detrimental to the generation of ideas and the number of high quality ideas, clearly diminishing the positive effects of cueing to natural categories. We explain how our theorizing and results are consistent with and extend earlier production blocking orientations in EBS research. We also examine limitations of current EBS designs and suggest how prevailing methodologies can be modified to better support idea generation cognition.
|keyword = ideation,brainstorming,memory,cognitive load,EMS,GSS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Bridging user organizations: Knowledge brokering and the work of information technology professionals'''
{{header}}
{{article
|author= SD Pawlowski,D Robey,
|source= MIS QUARTERLY
|year= 2004
|abstract = This interpretive case study examines knowledge brokering as an aspect of the work of information technology professionals. The purpose of this exploratory study is to understand knowledge brokering from the perspective of IT professionals as they reflect upon their work practice. As knowledge brokers, IT professionals see themselves as facilitating the flow of knowledge about both IT and business practices across the boundaries that separate work units within organizations. A qualitative analysis of interviews conducted with 23 IT professionals and business users in a large manufacturing and distribution company is summarized in a conceptual framework showing the conditions, practices, and consequences of knowledge brokering by IT professionals. The framework suggests that brokering practices are conditioned by structural conditions, including decentralization and a federated IT management organization, and by technical conditions, specifically shared IT systems that serve as boundary objects. Brokering practices include gaining permission to cross organizational boundaries, surfacing and challenging assumptions made by IT users, translation and interpretation, and relinquishing ownership of knowledge. Consequences of brokering are the transfer of both business and IT knowledge across units in the organization.
|keyword = boundary spanning,organizational,communication,organizational learning,IS skill requirements,IT professionals,knowledge broker,internal knowledge transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business competence of information technology professionals: Conceptual development and influence on it-business partnerships'''
{{header}}
{{article
|author= G Bassellier,I Benbasat,
|source= MIS QUARTERLY
|year= 2004
|abstract = This research aims at improving our understanding of the concept of business competence of information technology professionals and at exploring the contribution of this competence to the development of partnerships between IT professionals and their business clients. Business competence focuses on the areas of knowledge that are not specifically IT-related. At a broad level, it comprises the organization-specific knowledge and the interpersonal and management knowledge possessed by IT professionals. Each of these categories is in turn inclusive of more specific areas of knowledge. Organizational overview, organizational unit, organizational responsibility, and IT-business integration form the organization-specific knowledge, while interpersonal communication, leadership, and knowledge networking form the interpersonal and management knowledge. Such competence is hypothesized to be instrumental in increasing the intentions of IT professionals to develop and strengthen the relationship with their clients. The first step in the study was to develop a scale to measure business competence of IT professionals. The scale was validated, and then used to test the model that relates competence to intentions to form IT-business partnerships. The results support the suggested structure for business competence and indicate that business competence significantly influences the intentions of IT professionals to develop partnerships with their business clients.
|keyword = business competence,business,knowledge,IT professionals,measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User acceptance of hedonic information systems'''
{{header}}
{{article
|author= H van der Heijden,
|source= MIS QUARTERLY
|year= 2004
|abstract = This paper studies the differences in user acceptance models for productivity-oriented (or utilitarian) and pleasure-oriented (or hedonic) information systems. Hedonic information systems aim to provide self-fulfilling rather than instrumental value to the user are strongly connected to home and leisure activities, focus on the fun-aspect of using information systems, and encourage prolonged rather than productive use. The paper reports a cross-sectional survey on the usage intentions for one hedonic information system. Analysis of this sample supports the hypotheses that perceived enjoyment and perceived ease of use are stronger determinants of intentions to use than perceived usefulness. The paper concludes that the hedonic nature of an information system is an important boundary condition to the validity of the technology acceptance model. Specifically, perceived usefulness loses its dominant predictive value in favor of ease of use and enjoyment.
|keyword = user acceptance,technology acceptance model,hedonic information systems,perceived enjoyment,perceived ease of use,perceived usefulness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DSS effectiveness in marketing resource allocation decisions: Reality vs. perception'''
{{header}}
{{article
|author= GL Lilien,A Rangaswamy,GH Van Bruggen,K Starke,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = W e study the process by which model-based decision support systems (DSSs) influence managerial decision making in the context of marketing budgeting and resource allocation. We focus on identifying whether and how DSSs influence the decision process (e.g., cognitive effort deployed, discussion quality; and decision alternatives considered) and, as a result, how these DSSs influence decision outcomes (e.g., profit and satisfaction both with the decision process and the outcome). We study two specific marketing resource allocation decisions in a laboratory context: sales effort allocation and customer targeting. We find that decision makers who use high-quality, model-based DSSs make objectively better decisions than do decision makers who only have access to a generic decision tool (Microsoft Excel). However, their subjective evaluations (perceptions) of both their decisions and the processes that lead to those decisions do not necessarily improve as a result of DSS use. And expert judges, serving as surrogates for top management, have a difficult time assessing the objective quality of those decisions. Our results suggest that what managers get from a high-quality DSS may be substantially better than what they see. To increase the inclination for managerial adoption and use of DSS, we must get users to "see" the benefits of using a DSS. Our results also suggest two ways to bridge the perception-reality gap: (1) improve the perceived value of the decision process by designing DSSs both to encourage discussion (e.g., by providing explanation and support for alternative recommendations) as well as to reduce the perceived complexity of the problem so that managers invest more cognitive effort in exploring additional options and (2) provide feedback on the likely market/business outcomes of various decision options.
|keyword = DSS,marketing models,decision quality,decision process,resource allocation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Hope or hype: On the viability of escrow services as trusted third parties in online auction environments'''
{{header}}
{{article
|author= XR Hu,ZX Lin,AB Whinston,H Zhang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Internet fraud has been on the rise in online consumer-to-consumer (C2C) auction markets, posing serious challenges to people's trust in electronic markets. Among various remedies to promote trust and reduce trader's risk, online escrow service has been proposed as a trusted third party to protect online transactions from Internet fraud. However, whether an escrow service constitutes a viable business model for a trusted third party to effectively block Internet fraud remains an open question. This research proposes a dynamic game model for online traders and a profit maximization model for the escrow service provider. Through the investigation of the optimal strategies of online traders, we explore the relationships among traders' decision making, escrow service fee rates, and adoption rates. We reveal the demand for escrow services and establish the optimal pricing rule for the escrow service provider. A numerical study based on the theoretical analysis is conducted to provide detailed guidelines of the model application for an escrow service provider and to explore if the escrow service is a viable business model in C2C auction markets.
|keyword = escrow service,trusted third party,online auction,fraud,dynamic game,numerical study,optimum pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward contextualized theories of trust: The role of trust in global virtual teams'''
{{header}}
{{article
|author= SL Jarvenpaa,TR Shaw,DS Staples,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Although trust has received much attention in many streams of information systems research, there has been little theorizing to explain how trust evokes sentiments and affects task performance in IT-enabled relationships. Many studies unquestionably assume that trust is intrinsically beneficial, and dismiss the possibility that the effects of trust may be dependent on the situation (or conditions) at present. This paper theoretically and empirically examines outcomes of an individual's trust in global virtual teams under differing situations (or conditions). In Study 1, we find that early in a team's existence, a member's trusting beliefs have a direct positive effect on his or her trust in the team and perceptions of team cohesiveness. Later on, however, a member's trust in his team operates as a moderator, indirectly affecting the relationships between team communication and perceptual outcomes. Study 2 similarly suggests that trust effects are sensitive to the particular situation or condition. Combined, the studies find that trust affects virtual teams differently in different situations. Future studies on trust will need to consider situational contingencies. This paper contributes to the literature on IT-enabled relationships by theorizing and empirically testing how trust affects attitudes and behaviors.
|keyword = global virtual teams,trust,trust development,strength of situational structure,moderation effects,longitudinal study,team communication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An economic model of product quality and IT value'''
{{header}}
{{article
|author= ME Thatcher,DE Pingry,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = We use an economic model to formalize the complex relationships among IT investments, intermediate performance measures (e.g., product quality and output levels), and economic performance (e.g., productivity, profits, and consumer surplus). We demonstrate that a profit-maximizing monopolist invests in IT (modeled as changes in parametric characteristics of the firm) to design a better-quality product and charge a higher price. While this profit-maximizing adjustment generates more consumer surplus, it also increases production costs in a way that adversely affects productivity. In contrast, a simple model extension shows that when a firm is unwilling or unable to improve product quality, then IT investments result in suboptimal improvements in profits, an increase in consumer surplus, and an increase in productivity. Together, these models highlight the way in which product quality moderates the relationship between IT investments and economic performance. We also demonstrate that these relationships are robust to the socially optimal case in which a social planner chooses price and quality to maximize social welfare. In addition, we demonstrate that the results of the monopoly model hold when considering the design and development of products offered free of charge (e.g., free online content), but that provide indirect benefits to the firm (e.g., more advertising revenues).
|keyword = economic value,IT value,IT investments,consumer welfare,productivity,quality,economic modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing digital piracy: Pricing and protection'''
{{header}}
{{article
|author= A Sundararajan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = T his paper analyzes the optimal choice of pricing schedules and technological deterrence levels in a market with digital piracy where sellers can influence the degree of piracy by implementing digital rights management (DRM) systems. It is shown that a monopolist's optimal pricing schedule can be characterized as a simple combination of the zero-piracy pricing schedule and a piracy-indifferent pricing schedule that makes all customers indifferent between legal usage and piracy. An increase in the quality of pirated goods, while lowering prices and profits, increases total surplus by expanding both the fraction of legal users and the volume of legal usage. In the absence of price discrimination, a seller's optimal level of technology-based protection against piracy is shown to be at the technologically maximal level, which maximizes the difference between the quality of the legal and pirated goods. However, when a seller can price discriminate, its optimal choice is always a strictly lower level of technology-based protection. These results are based on the following digital rights conjecture: that granting digital rights increases the incidence of digital piracy, and that managing digital rights therefore involves restricting the rights of usage that contribute to customer value. Moreover, if a digital rights management system weakens over time due to the underlying technology being progressively hacked, a seller's optimal strategic response may involve either increasing or decreasing its level of technology-based protection. This direction of change is related to whether the DRM technology implementing each marginal reduction in piracy is increasingly less or more vulnerable to hacking. Pricing and technology choice guidelines are presented, and some welfare implications are discussed.
|keyword = piracy,digital piracy,piracy deterrence,copyright,digital rights management,DRM,nonlinear pricing,price discrimination,screening,intellectual property,enforcement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A framework for assessing the business value of information technology infrastructures'''
{{header}}
{{article
|author= RL Kumar,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Information technology (IT) infrastructure investments are an extremely important part of e-business and constitute a major portion of IT investments in many organizations. IT infrastructure investments include investments in connectivity, systems integration, and data storage that may be used by multiple applications. Prior research has recognized the importance of a flexible IT infrastructure as a source of competitive advantage. Evidence regarding the value of IT infrastructures is anecdotal, and there is a realization that large investments in IT infrastructures are often difficult to justify. This paper expands on the idea that the value of an IT infrastructure depends on its use in an organizational context, and presents a relatively simple approach to understanding and assessing the value of IT infrastructure investments. This approach is based on the asset valuation literature in finance. An example is provided to illustrate the proposed approach, and managerial implications are discussed.
|keyword = asset valuation,business value,economic analysis,IT infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Prioritizing a portfolio of information technology investment projects'''
{{header}}
{{article
|author= I Bardhan,S Bagchi,R Sougstad,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Although the use of real options for valuation of information technology (IT) investments has been documented, little research has been conducted to examine its relevance for valuing and prioritizing a portfolio of projects. Complexities of IT projects along with the effect of project interdependencies raise several challenges in applying real options for prioritization of IT investments. We examine a large U.S.based energy utility firm in a deregulated environment that is considering investment in a portfolio of 31 projects to provide a range of Internet-enabled energy services to customers. Using real data on expected project benefits and costs for different competitive scenarios, we develop a nested options model that extends prior research by incorporating the impact of project interdependencies to calculate the option value of all projects. Our nested options model provides a better understanding of project interdependencies on valuation and prioritization decisions, and provides insights into the business value of IT infrastructure projects that provide the managerial flexibility to launch future projects. We present a real options portfolio optimization algorithm for dynamic multiperiod portfolio optimization by incorporating the project values based on real options analysis in a portfolio management model with budget constraints.
|keyword = business value,information technology,investment evaluation,net present value,portfolio optimization,real options analysis,sequential investment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding the business value of information technology investments: Theoretical evidence from alternative market and cost structures'''
{{header}}
{{article
|author= ME Thatcher,DE Pingry,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = This paper develops a series of two-stage duopoly models of quality-price competition and a series of monopoly models of quality-price choice in order to examine the impact of information technology (IT) investments on firm profit, firm productivity, and consumer welfare. We solve the duopoly and monopoly models for four cost functions, where each function makes a different assumption about the form of the marginal cost of production. These models are used to conduct a two-by-four comparison [(monopoly, duopoly) x (four cost functions)] of the impact of IT investments on economic performance. The analysis reveals that together market structure and cost structure play a critical role in determining the form of the relationship between IT investment and economic measures. Specifically, moving from monopoly to duopoly and moving from zero marginal cost to marginal cost as a function of quality increase the number of economic measures for which the directional effects of IT investment are ambiguous, or depend on model parameter values.
|keyword = business value,consumer welfare,economic analysis,information technology investments,information technology value,productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Poaching and the misappropriation of information: Transaction risks of information exchange'''
{{header}}
{{article
|author= EK Clemons,LM Hitt,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = We address the concept of poaching, the risk that in any transactional relationship, information that is transferred between parties for purposes specified in the contract will deliberately be used by the receiving party for purposes outside the contract, to its own economic benefit, and to the detriment of the party that provided the information. We argue that this form of transactional risk, a component of transaction costs, is increasingly important in our service-centered, information-driven, postindustrial economy. Using case examples and a discussion of the related literature, we demonstrate and discuss the conditions under which shared information creates the potential for poaching, examine the impact and efficacy of traditional remedies for contractual problems in managing poaching, and identify additional mechanisms for managing poaching risk. Our analysis suggests that these risks and their remedies are fundamentally different in nature from those considered in previous theories of supplier relations and contractual governance.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information exploitation and interorganizational systems ownership'''
{{header}}
{{article
|author= K Han,RJ Kauffman,BR Nault,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = We develop a model based on the theory of incomplete contracts for how ownership structure of interorganizational systems (IOS) can affect information exploitation and information technology adoption. Our model yields several propositions that suggest the appropriate strategic actions that a firm may take when there is potential for IOS adopters to question whether adopting the IOS will be value-maximizing. We analyze and illustrate the related strategic thinking in a real-world context involving a financial risk management IOS. We present a case study of the ownership and spin-off of RiskMetrics, developed by New York City-based investment bank, J.P. Morgan, in the late 1980s. The firm first gave RiskMetrics to its correspondent banking, treasury, and investment clients for free, in the context of its clearing account relationship services. Later, the bank spun off the product to an independent company that offered fee-based services. We model the bank's clients in terms of their heterogeneous portfolio risks, and their effects on the value a client can gain from adopting the technology. We also examine the value they may lose if their private portfolio risk information is exploited. A key roadblock to the adoption of the free service may have been the potential for strategic information exploitation by the service provider. When Morgan spun off RiskMetrics with multiparty ownership, wider adoption occurred. Our theory interprets this strategic move as an appropriate means to maximize long-term profits when information exploitation may occur.
|keyword = economic theory,financial risk management,incomplete contracts,information sharing,information systems,interorganizational systems,ownership,value-at-risk,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The market structure for Internet search engines'''
{{header}}
{{article
|author= R Telang,U Rajan,T Mukhopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The Internet search engine market has seen a proliferation of entrants over the past few years. Whereas Yahoo was the early market leader, there has been entry by both lower-quality engines and higher-quality ones (such as Google). Prior work on quality differentiation requires that low-quality products have low prices in order to survive in a market with high-quality products. However, the price charged to users of search engines is typically zero. Therefore, consumers do not face a tradeoff between quality and price. Why do lower-quality products survive in such a market? We develop a vertical differentiation model that explains this phenomenon. The quality of the results provided by a search engine is inherently stochastic, and there is no charge for using an engine. Therefore, users who try out one engine may consult a lower-quality engine in the same session. This "residual demand" allows lower-quality products to survive in equilibrium. We then extend our model to incorporate horizontal differentiation as well and show that residual demand leads to higher quality and less differentiation in this market. Engines want to attract competitors' customers and therefore have a strong incentive to be "similar" to each other.
|keyword = e-commerce,market structure,product differentiation,residual demand,search engines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A multichannel model of separating equilibrium in the face of the digital divide'''
{{header}}
{{article
|author= FJ Riggins,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = We develop a multichannel model of separating equilibrium where a seller markets a durable good to high- and low-type consumers in two different channels-an online Internet storefront and an offline bricks-and-mortar store. We show how the digital divide, where high-type consumers dominate the online channel and low-type consumers dominate the offline channel, artificially segments the marketplace, thereby mitigating the classic cannibalization problem. This allows the seller to more efficiently market its goods to each consumer segment. We show conditions under which low-type consumers are initially served in the offline channel, but subsequently bridging the divide results in their not being served in either channel. We also examine the implications of bridging the digital divide when the seller uses delay by engaging in intertemporal price discrimination.
|keyword = digital divide,electronic commerce,market segmentation,multichannel marketing,pricing strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web portals: Evidence and analysis of media concentration'''
{{header}}
{{article
|author= RM Dewan,ML Freimer,A Seidmann,J Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Although the Web has grown to several billion pages over the past few years, just a few of the Web sites get most of the visits. Such sites, called portals, attract visitors and advertisers and provide a lot of valuable content at no charge to the visitors. The portals attract a disproportionate amount of the Internet advertising dollars and have the ability to influence the success of new electronic commerce ventures. Using monthly audience data, we examine relative market shares of Web sites in search engines, travel, financial, news, and other categories. We find clear evidence of increasing disparity in page views, with the top Web sites getting an increasing share of the market. Using economic modeling, we show that this disparity is a result of a development externality that exists in this industry: the sites that have more viewers get more revenues; this in turn allows them to develop more content and attract an even greater number of viewers.
|keyword = advertisement policy,electronic commerce,media concentration,page views,portals,value index,World Wide Web,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computing as utility: Managing availability, commitment, and pricing through contingent bid auctions'''
{{header}}
{{article
|author= HK Bhargava,S Sundaresan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Enabled by advances in grid and network computing architectures for the delivery of on-demand computing services, the vision of an e-services economy in which computing will be as ubiquitous as a utility is becoming a possibility in business computing. Major firms in the computing industry such as IBM, Hewlett-Packard, and Sun Microsystems are focusing on agility and flexibility of computing resources and gearing up for their own versions of on-demand computing and information technology (IT) outsourcing solutions. The successful introduction of these new computing models requires the development of appropriate pricing mechanisms that are consistent with the enabling technologies. Our paper introduces the notion of contingent auctions to address this lacuna. In contingent auctions, users bid for computing resources in an auction, but are relieved from the contract (paying a penalty) if demand is not realized. We study different mechanisms-ranging from an advance commitment (capacity reservation) to no commitment (pay-as-you-go)-under demand uncertainty. We consider markets in which the demand for computing is uncertain and, moreover, users' value of computing and demand realization may be related. We show how the different levels of commitment affect prices, revenues, and resource utilization under different market conditions. Our results reiterate the need to address the availability-commitment dichotomy in the design of business models for on-demand computing and IT outsourcing.
|keyword = computing marketplace,electronic auctions,grid computing,on-demand computing,utility computing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Networks of action: Sustainable health information systems across developing countries'''
{{header}}
{{article
|author= J Braa,E Monteiro,S Sahay,
|source= MIS QUARTERLY
|year= 2004
|abstract = Our paper is motivated by one simple question: Why do so many action research efforts fail to persist overtime? We approach this question, the problem of sustainability, building on a perspective on action research identifying the pivotal importance of networks. More precisely, local action research interventions need to be conceptualized and approached as but one element in a larger network of action in order to ensure sustainability. A vital aspect of our perspective is that local interventions depend heavily on the support of similar action research efforts in other locations. This is essential for the necessary processes of learning and experience sharing. We suggest that the scaling (i.e., spreading) of intervention is a prerequisite, not a luxury, for sustainable action research. Empirically, we base our analysis on an ongoing, large-scale action research project within the health care sector (called HISP) in a number of developing countries. HISP provides a fruitful occasion to investigate key criteria for our approach to action research, namely sustainability, scalability, and capacity to be politically relevant to the participants. We contribute to three discourses: (1) models of action research, (2) lessons for health information systems in developing countries, and (3) more generally, IS implementations that are dispersed, large-scale, and have scarce resources.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Informating the clan: Controlling physicians' costs and outcomes'''
{{header}}
{{article
|author= R Kohli,WJ Kettinger,
|source= MIS QUARTERLY
|year= 2004
|abstract = Past literature recognizes the power of information technology (IT) to establish greater transparency and in turn the potential for greater control. Theoretical perspectives such as informating and agency theory describe situations whereby legitimized management authority can control goal divergence by implementing information systems to better monitor agents' behavior and outcomes. But what happens when the principal does not possess legitimacy to impose an agent's use of information and/or behavioral conformance? This study investigates this situation. Through an action research project a physicians' profiling system (PPS) was used to monitor and benchmark physicians' clinical practices and outcomes resulting in changed practice behaviors in closer congruence with management's goals. The PPS project represents a successful attempt of a hospital's management (principal) to "informate the clan" of physicians (agents) to reduce clinical procedural costs and adopt practices benchmarked to produce better outcomes. This research moves beyond directly controlling informated workers through legitimized managerial authority to a better understanding of how to informate autonomous professionals. Emerging insights suggest that a clan can be informated if the principal can improve the perceived legitimacy of the information (the message), legitimize the technical messenger (customized user interface), legitimize the human messenger (boundary spanners and influential clan members), and facilitate an environment where clan-based discussion, using the information provided by the principal, is incorporated into the process of concertive control.
|keyword = action research,informating,clan,control and IT implementation,IT-based performance monitoring,agency theory,concertive control,health care information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing risk in software process improvement: An action research approach'''
{{header}}
{{article
|author= JH Iversen,L Mathiassen,PA Nielsen,
|source= MIS QUARTERLY
|year= 2004
|abstract = Many software organizations engage in software process improvement (SPI) initiatives to increase their capability to develop quality solutions at a competitive level. Such efforts, however, are complex and very demanding. A variety of risks makes it difficult to develop and implement new processes. We studied SPI in its organizational context through collaborative practice research (CPR), a particular form of action research. The CPR program involved close collaboration between practitioners and researchers over a three-year period to understand and improve SPI initiatives in four Danish software organizations. The problem of understanding and managing risks in SPI teams emerged in one of the participating organizations and led to this research. We draw upon insights from the literature on SPI and software risk management as well as practical lessons learned from managing SPI risks in the participating software organizations. Our research offers two contributions. First, we contribute to knowledge on SPI by proposing an approach to understand and manage risks in SPI teams. This risk management approach consists of a framework for understanding risk areas and risk resolution strategies within SPI and a related process for managing SPI risks. Second, we contribute to knowledge on risk management within the information systems and software engineering disciplines. We propose an approach to tailor risk management to specific contexts. This approach consists of a framework for understanding and choosing between different forms of risk management and a process to tailor risk management to specific contexts.
|keyword = risk management,software process improvement,action research,collaborative practice research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Design principles for competence management systems: A synthesis of an action research study'''
{{header}}
{{article
|author= R Lindgren,O Henfridsson,U Schultze,
|source= MIS QUARTERLY
|year= 2004
|abstract = Even though the literature on competence in organizations recognizes the need to align organization level core competence with individual level job competence, it does not consider the role of information technology in managing competence across the macro and micro levels. To address this shortcoming, we embarked on an action research study that develops and tests design principles for competence management systems. This research develops an integrative model of competence that not only outlines the interaction between organizational and individual level competence and the role of technology in this process, but also incorporates a typology of competence (competence-in-stock, competence-in-use, and competence-in-the-making). Six Swedish organizations participated in our research project, which took 30 months and consisted of two action research cycles involving numerous data collection strategies and interventions such as prototypes. In addition to developing a set of design principles and considering their implications for both research and practice, this article includes a self-assessment of the study by evaluating it according to the criteria for canonical action research.
|keyword = canonical action research,competence management systems,core competence,design principles,HR management,prototypes,skill-based approach,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Small business growth and internal transparency: The role of information systems'''
{{header}}
{{article
|author= CT Street,DB Meister,
|source= MIS QUARTERLY
|year= 2004
|abstract = While many large businesses start out as a small enterprise, remarkably little is known about how an organization actually changes internally during the periods of growth. Small business growth is known to strain internal communication processes, for example, which likely limits growth opportunities. Information systems are often called upon to remedy such deficiencies. Through a participatory action research project, we investigated the ways in which a small business management team developed an IS-enabled solution to address their growth needs. During the progression of the project, a new outcome of organizational effectiveness, internal transparency, was identified and developed. Adopting a punctuated equilibrium perspective, a theoretical process model is proposed that sheds light on a relationship between internal transparency, small business growth, and IS. The paper concludes with observations that internal transparency may well be a concept that offers significant potential for MIS research as well as a discussion about the applicability and credibility of participatory action research for this project.
|keyword = business growth,IT investment,small business,internal transparency,process models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dialogical action research at omega corporation'''
{{header}}
{{article
|author= P Martensson,AS Lee,
|source= MIS QUARTERLY
|year= 2004
|abstract = In dialogical action research, the scientific researcher does not "speak science" or otherwise attempt to teach scientific theory to the real-world practitioner, but instead attempts to speak the language of the practitioner and accepts him as the expert on his organization and its problems. Recognizing the difficulty that a practitioner and a scientific researcher can have in communicating across the world of science and the world of practice, dialogical action research offers, as its centerpiece, reflective one-on-one dialogues between the practitioner and the scientific researcher, taking place periodically in a setting removed from the practitioner's organization. The dialogue itself serves as the interface between the world of science, marked by theoria and the scientific attitude, and the world of the practitioner, marked by praxis and the natural attitude of everyday life. The dialogue attempts to address knowledge heterogeneity, which refers to the different forms that knowledge takes in the world of science and the world of practice, and knowledge contextuality, which refers to the dependence of the meaning of knowledge, such as a scientific theory or professional expertise, on its context. In successive dialogues, the scientific researcher and the practitioner build a mutual understanding, including an understanding of the organization and its problems. The scientific researcher, based on one or more of the scientific theories in her discipline, formulates and suggests one or more actions for the practitioner to take in order to solve or remedy a problem in his organization. Dialogical action research recognizes that the practitioner's experience, expertise, and tacit knowledge, or praxis, largely shapes how he understands the suggested actions and appropriates them as his own. Upon returning to his organization, he takes one or more of the suggested actions, depending,on his reading of the situation at hand. The reactions or responses of the problem to the actions or stimuli of the practitioner would embody, in the practitioner's eyes, success or failure in solving or remedying the problem and, in the scientific researcher's eyes, evidence confirming or disconfirming the theory on which the action was based. The scientific researcher may then suggest, based on her theories, additional actions, hence initiating another cycle of action and learning. To illustrate dialogical action research, this paper reconstructs some dialogues between an information systems researcher and a managing director at a European company called Omega Corporation.
|keyword = action research,qualitative research,research methods,case studies,phenomenology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT Outsourcing strategies: Universalistic, contingency, and configurational explanations of success'''
{{header}}
{{article
|author= JN Lee,SM Miranda,YM Kim,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Focus on individual outsourcing decisions in IT research has often yielded contradictory findings and recommendations. To address these contradictions, we investigate a holistic, configurational approach with the prevailing universalistic or contingency perspectives in exploring the effects of IT outsourcing strategies on outsourcing success. Based on residual rights theory, we begin by identifying three dimensions of IT outsourcing strategies: degree of integration, allocation of control, and performance period. We then develop a model of fit-as-gestalt, drawing from literatures on strategy, governance, interorganizational relationships, and outsourcing. Next, based on data from 311 firms in South Korea, we test universalistic and contingency perspectives in explaining the relationship between IT outsourcing strategies and outsourcing success. We then identify three congruent patterns, or gestalts, of IT outsourcing strategies. We term these strategies independent, arm's-length, and embedded strategies. To establish the predictive validity of these gestalts and the viability of a configurational perspective, we then explore the effects of these congruent gestalts vis-a-vis noncongruent patterns on three dimensions of outsourcing success: strategic competence, cost efficiency, and technology catalysis. We also contrast the effects of each of the three gestalts on each of the three dimensions of outsourcing success. Our findings indicate the superiority of the configurational approach over universalistic and contingency perspectives in explaining outsourcing success.
|keyword = IT outsourcing,outsourcing success,fit,congruence,strategy,knowledge sharing,arm's-length,embeddedness,partnership,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical analysis of network externalities in peer-to-peer music-sharing networks'''
{{header}}
{{article
|author= A Asvanund,K Clay,R Krishnan,MD Smith,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Peer-to-peer (P2P) file sharing networks are an important medium for the distribution of information goods. However, there is little empirical research into the optimal design of these networks under real-world conditions. Early speculation about the behavior of P2P networks has focused on the role that positive network externalities play in improving performance as the network grows. However, negative network externalities also arise in P2P networks because of the consumption of scarce network resources or an increased propensity of users to free ride in larger networks, and the impact of these negative network externalities-while potentially important-has received far less attention. Our research addresses this gap in understanding by measuring the impact of both positive and negative network externalities on the optimal size of P2P networks. Our research uses a unique dataset collected from the six most popular OpenNap P2P networks between December 19, 2000, and April 22, 2001. We find that users contribute additional value to the network at a decreasing rate and impose costs on the network at an increasing rate, while the network increases in size. Our results also suggest that users are less likely to contribute resources to the network as the network size increases. Together, these results suggest that the optimal size of these centralized P2P networks is bounded-At some point the costs that a marginal user imposes on the network will exceed the value they provide to the network. This finding is in contrast to early predictions that larger P2P networks would always provide more value to users than smaller networks. Finally, these results also highlight the importance of considering user incentives-an important determinant of resource sharing in P2P networks-in network design.
|keyword = peer-to-peer networks,Napster,network externalites,empirical,incentives,size limitation,efficiency,network design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of environmental uncertainty and task characteristics on user satisfaction with data'''
{{header}}
{{article
|author= J Karimi,TM Somers,YP Gupta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Today, more than ever before, organizations are faced with the task of processing volumes of information under more uncertain and more competitive environments. This study investigates the impact of environmental uncertainty and task characteristics on user satisfaction with data by using IS and organizational theories. Responses were matched from 77 CEOs and 166 senior managers, who were end users of IS. The partial least squares technique indicated that environmental uncertainty has a positive impact on task characteristics. Task characteristics have a direct and mediating impact on user satisfaction with data. Our findings also demonstrated that user satisfaction with data could be better understood by overlapping IS and organizational theories, rather than by treating the subject matter in disjoint fields. The paper concludes with discussions and implications for researchers and practitioners.
|keyword = environmental uncertainty,task characteristics,task technology fit,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information overload and the message dynamics of online interaction spaces: A theoretical model and empirical exploration'''
{{header}}
{{article
|author= Q Jones,G Ravid,S Rafaeli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Online spaces that enable shared public interpersonal communications are of significant social, organizational, and economic importance. In this paper, a theoretical model and associated unobtrusive method are proposed for researching the relationship between online spaces and the behavior they host. The model focuses on the collective impact that individual information-overload coping strategies have on the dynamics of open, interactive public online group discourse. Empirical research was undertaken to assess the validity of both the method and the model, based on the analysis of over 2.65 million postings to 600 Usenet newsgroups over a 6-month period. Our findings support the assertion that individual strategies for coping with "information overload" have an observable impact on large-scale online group discourse. Evidence was found for the hypotheses that: (1) users are more likely to respond to simpler messages in overloaded mass interaction; (2) users are more likely to end active participation as the overloading of mass interaction increases; and (3) users are more likely to generate simpler responses as the overloading of mass interaction grows. The theoretical model outlined offers insight into aspects of computer-mediated communication tool usability, technology design, and provides a road map for future empirical research.
|keyword = human-computer interaction,computer-mediated communication,computer supported cooperative work,virtual community,information overload,interaction coping strategies,message dynamics,online group discourse,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology payoff in e-business environments: An international perspective on value creation of e-business in the financial services industry'''
{{header}}
{{article
|author= K Zhu,KL Kraemer,S Xu,J Dedrick,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Grounded in the technology-organization-environment (TOE) framework, we develop a research model for assessing the value of e-business at the firm level. Based on this framework, we formulate six hypotheses and identify six factors (technology readiness, firm size, global scope, financial resources, competition intensity, and regulatory environment) that may affect value creation of e-business. Survey data from 612 firms across 10 countries in the financial services industry were collected and used to test the theoretical model. To examine how e-business value is influenced by economic environments, we compare two subsamples from developed and developing countries. Based on structural equation modeling, our empirical analysis demonstrates several key findings: (1) Within the TOE framework, technology readiness emerges as the strongest factor for e-business value, while financial resources, global scope, and regulatory environment also significantly contribute to e-business value. (2) Firm size is negatively related to e-business value, suggesting that structural inertia associated with large firms tends to retard e-business value. (3) Competitive pressure often drives firms to adopt e-business, but e-business value is associated more with internal organizational resources (e.g., technological readiness) than with external pressure to adopt. (4) While financial resources are an important factor in developing countries, technological capabilities become far more important in developed countries. This suggests that as firms move into deeper stages of e-business transformation, the key determinant of e-business value shifts from monetary spending to higher dimensions of organizational capabilities. (5) Government regulation plays a much more important role in developing countries than in developed countries. These findings indicate the usefulness of the proposed research model and theoretical framework for studying e-business value. They also provide insights for both business managers and policy-makers.
|keyword = business value,cross-country comparison,electronic business,electronic commerce,financial services industry,firm performance,information technology investment,technology diffusion,technology-organization-environment framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reexamining the value relevance of e-commerce initiatives'''
{{header}}
{{article
|author= B Dehning,VJ Richardson,A Urbaczewski,JD Wells,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = This study reexamines the value relevance of e-commerce announcements using an event study methodology. Event studies have become an increasingly popular technique for information systems research by giving researchers a tool to measure the notoriously elusive value of information technology. We find evidence that the traditional event study methodology may not provide an accurate measure of abnormal returns during periods of high market volatility, and propose an alternative methodology. The alternative methodology does not use an estimation period, and takes into account extreme or unusual market movements in the period in which the e-commerce announcement was made. Using the alternative methodology, we find evidence of positive abnormal returns for e-commerce announcements made in the fourth quarter of 1998, but no abnormal returns to e-commerce announcements made in the fourth quarter of 2000. We also find significant differences in value depending on the type of e-commerce initiative. In 2000, e-commerce initiatives with a digital product were valued significantly more than e-commerce initiatives with a tangible product, while in 1998 no such difference existed. In 1998, business-to-business e-commerce initiatives, e-commerce initiatives with a tangible product, and e-commerce initiatives by pure-play Internet firms were valued more than similar initiatives in 2000. The study makes a significant contribution for understanding the value of e-commerce initiatives in highly volatile markets and demonstrates how market values of e-commerce changed from 1998 to 2000. Furthermore, this study shows the importance of carefully considering both the time frame examined and the methodology used when assessing the value relevance of e-commerce initiatives as to avoid inflating the magnitude of any observed effects.
|keyword = business value,e-commerce announcements,electronic commerce,event study,market value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring firm performance at the network level: A nomology of the business impact of digital supply networks'''
{{header}}
{{article
|author= D Straub,A Rai,R Klein,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = For decades, information technology has been posited to have a major impact on firm performance. Investigations into this line of inquiry have almost always used constructs related to individual firm performance as their dependent measures, an approach that made sense under historical economic conditions. In recent years, however, value chains are giving way to digital supply networks with electronic interactions between tiers in the flow of goods and services. Such an environment makes it imperative to develop sophisticated measures of the performance of entire networks of firms, as opposed to individual firm performance. Using game-theoretic concepts, this paper explores several dimensions of networked organizational performance as a construct, as a set of measures, and as a construct within a nomology. It describes a program of research in which some empirical validation has already been completed and other work is now underway. We first validate measures for a dyadic view of network performance, followed by an n-firm perspective.
|keyword = business networks,e-business,e-commerce,firm performance,supply chains,supply networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding determinants of online consumer satisfaction: A decision process perspective'''
{{header}}
{{article
|author= R Kohli,S Devaraj,MA Mahmood,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = As business-to-consumer online shopping grows, e-commerce channel providers will need to explore ways to anticipate consumers' needs to deliver an efficient shopping experience. Yet the consumers' decision-making process and its relationship to the selection of the online channel are not well understood. Utilizing Simon's decision-making model, we examined support for decision-making phases using 134 online consumers. We also extended the model to include consumers' cost savings and time savings, as well as their satisfaction with the e-commerce channel. Structural equation modeling results indicate that the online shopping channel supported the overall decision-making process. In particular, we found strong support for the design and choice phases of online consumers' decision-making process. Our results also indicate that support for the decision-making process was mediated by the cost savings and time savings gained by the online consumers and led to their greater channel satisfaction.
|keyword = B2C e-commerce,business-to-consumer commerce,consumer decision-making,e-commerce,online selling channel,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Proprietary and open systems adoption in e-procurement: A risk-augmented transaction cost perspective'''
{{header}}
{{article
|author= RJ Kauffman,H Mohtadi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = We present an economic model that enables the study of incentives for business-to-business (B2B) e-procurement systems investments that permit inventory coordination and improved operational control. We focus on the information technology adoption behavior of firms in the presence of transaction costs, agency costs and information uncertainty. We conclude that it is appropriate to rethink the prior theory and develop an extended transaction-cost theory perspective that incorporates the possibility of shocks. We distinguish among three kinds of B2B e-procurement systems platforms. Proprietary platform procurement systems involve traditional electronic data interchange (EDI) technologies. Open platform procurement systems are associated with e-market Web technologies. Hybrid platforms involve elements of both. We specify an analytical model that captures the key elements of our perspective, including the conditions under which strong conclusions can be made about the likely observed equilibrium e-procurement solutions of the firms. Our results explain the coexistence of both proprietary and open platforms, showing that larger firms tend to adopt costlier procurement technology solutions, such as proprietary EDI, which provides greater supply certainty. Smaller firms adopt less costly procurement technologies that entail greater supply uncertainties, such as open platform procurement systems. Two guidelines emerge for practitioners: (1) adoption of standard e-procurement platforms needs to be understood in terms of the controllable risk tradeoffs that are offered to small and large firms, and (2) gauging the business value impacts of exogenous shocks is critical to decision-making.
|keyword = e-procurement,information system economics,information technology adoption,information technology infrastructure,open platforms,proprietary platforms,supply chain management,transaction costs,uncertainty handling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The complementarity of information technology infrastructure and e-commerce capability: A resource-based assessment of their business value'''
{{header}}
{{article
|author= K Zhu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = This study seeks to assess the business value of e-commerce capability and information technology (IT) infrastructure in the context of electronic business at the firm level. Grounded in the IT business-value literature and enhanced by the resource-based theory of the firm, we developed a research framework in which both the main effects and the interaction effects of e-commerce and IT on firm performance were tested. Within this theoretical framework, we formulated several hypotheses. We then developed a multidimensional e-commerce capability construct, and after establishing its validity and reliability, tested the hypotheses with empirical data from 114 companies in the retail industry. Controlling for variations of firm size and subindustry effects, our empirical analysis found a strong positive interaction effect between IT infrastructure and e-commerce capability. This suggests that their complementarity positively contributes to firm performance in terms of sales per employee, inventory turnover, and cost reduction. The results are consistent with the resource-based theory, and provide empirical evidence to the complementary synergy between front-end e-commerce capability and back-end IT infrastructure. Combined together, they become more effective in producing business value. Yet the value of this synergy has not been recognized in the IT payoff literature. The "productivity paradox" observed in various studies has been attributed to variation in methods and measures, yet we offer an additional explanation: ignoring complementarities in business value measurement implies that the impact of IT was seriously underestimated. Our results emphasized the integration of resources as a feasible path to e-commerce value-companies need to enhance the integration between front-end e-commerce capability and back-end IT infrastructure in order to reap the benefits of e-commerce investments.
|keyword = electronic commerce,firm performance,information technology business value,resource-based theory,resource complementarity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Stopping behavior of systems analysts during information requirements elicitation'''
{{header}}
{{article
|author= MG Pitts,GJ Browne,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Understanding the cognitive activities of analysts during information requirements determination (IRD) has been recognized as a key indicator of IRD success. The research presented here examines one such cognitive activity: analysts' determination of the sufficiency of information gathered during the elicitation of requirements. Research in behavioral decision-making has identified various heuristics, or stopping rules, that are used to gauge the sufficiency of the information obtained and to terminate information acquisition. Despite the fact that analysts undoubtedly employ such stopping rules in requirements elicitation, no research has studied this phenomenon. In the present research, we present a classification of stopping rules appropriate for information gathering problems. Stopping-rule use was identified for 54 practicing systems analysts participating in a requirements determination problem in a laboratory setting. Results indicated that analyst experience influences the application of specific cognitive stopping rules, and that the use of these stopping rules has an impact on requirements determination outcomes. In addition, the use of certain stopping rules resulted in greater quantity and completeness of requirements elicited from users. Theoretical implications for the elicitation of information and practical implications for the training of systems analysts are discussed.
|keyword = cognitive stopping rules,information gathering,knowledge elicitation,requirements determination,systems analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The meaning and measurement of user satisfaction: A multigroup invariance analysis of the end-user computing satisfaction instrument'''
{{header}}
{{article
|author= WJ Doll,XD Deng,TS Raghunathan,G Torkzadeh,WD Xia,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Although user satisfaction is widely used by researchers and practitioners to evaluate information system success, important issues related to its meaning and measurement across population subgroups have not been adequately resolved. To be most useful in decision-making, instruments like end-user computing satisfaction (EUCS), which are designed to evaluate system success, should be robust. That is, they should enable comparisons by providing equivalent measurement across diverse samples that represent the variety of conditions or population subgroups present in organizations. Using a sample of 1,166 responses, the EUCS instrument is tested for measurement invariance across four dimensions-respondent positions, types of application, hardware platforms, and modes of development. While the results suggest that the meaning of user satisfaction is context sensitive and differs across population subgroups, the 12 measurement items are invariant across all four dimensions. The 12-item summed scale enables researchers or practitioners to compare EUCS scores across the instrument's originally intended universe of applicability.
|keyword = confirmatory factor analysis,end-user computing satisfaction,factorial invariance,instrument validation,research methods,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What makes an ERP implementation relationship worthwhile: Linking trust mechanisms and ERP usefulness'''
{{header}}
{{article
|author= D Gefen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = To a large extent, trust determines expected utility derived from business transactions where the trusting party is dependent upon others, but lacks control over them. In many instances, this typifies the relationship between clients and an enterprise resource planning (ERP) customization vendor. This exploratory study examines how trust is built during an ERP implementation, and the relative weight of this trust compared with the perceived qualities of the implemented ERP itself in determining clients' assessment that the business relationship with the vendor is worthwhile. The data, collected from companies that were involved in the process of implementing a new ERP with the on-site assistance of a certain customization vendor, show that all three trust antecedents suggested by Zucker's seminal study of trust-process-based, characteristic-based, and institution-based mechanisms-contribute significantly to client trust. The data also show that client trust in this customization vendor and the perceived usefulness of the ERP both contribute to client assessment that their business relationship with the vendor is worthwhile, showing that both getting the job done and creating a trust-based relationship contribute to this assessment. The implications of the importance of creating trust in ERP implementation and the means of doing how to do so are discussed.
|keyword = enterprise resource planning,ERP,ERP implementation,technology acceptance model,trust,trust antecedents,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Operationalizing the essential role of the information technology artifact in information systems research: Gray area, pitfalls, and the importance of strategic ambiguity'''
{{header}}
{{article
|author= AB Whinston,XJ Geng,
|source= MIS QUARTERLY
|year= 2004
|abstract = In this paper we argue that a large gray area of information systems research exists, whose relevance to the information technology artifact is subject to significant debate even among IS scholars who support the essential role of the IT artifact. As we explain, not explicitly addressing this gray area can have negative, although often inadvertent, effects on the innovative nature of IS research; we explore this danger through three pitfalls. We then propose a stance of strategic ambiguity to deal with the gray area. Strategic ambiguity calls for deliberately withholding judgment on the relevance of research in the gray area and acceptance of gray-area research provided it meets the excellence required by professional journals. We believe that strategic ambiguity benefits innovative IS research without harming the essential role of the IT artifact.
|keyword = IS discipline,IT artifact,gray area,strategic ambiguity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GIST: A model for design and management of content and interactivity of customer-centric Web sites'''
{{header}}
{{article
|author= TC Albert,PB Goes,A Gupta,
|source= MIS QUARTERLY
|year= 2004
|abstract = Customer-centric Web-based systems, such as e-commerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frameworks that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company's Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new business opportunities.
|keyword = Web site analysis and design,customer segmentation,personalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding changes in belief and attitude toward information technology usage: A theoretical model and longitudinal test'''
{{header}}
{{article
|author= A Bhattacherjee,G Premkumar,
|source= MIS QUARTERLY
|year= 2004
|abstract = User beliefs and attitudes are key perceptions driving information technology usage. These perceptions, however, may change with time as users gain first-hand experience with IT usage, which, in turn, may change their subsequent IT usage behavior. This paper elaborates how users' beliefs and attitudes change during the course of their IT usage, defines emergent constructs driving such change, and proposes a temporal model of belief and attitude change by drawing on expectation-disconfirmation theory and the extant IT usage literature. Student data from two longitudinal studies in end-user computing (computer-based training system usage) and system development (rapid application development software usage) contexts provided empirical support for the hypothesized model, demonstrated its generalizability across technologies and usage contexts, and allowed us to probe context-specific differences. Content analysis of qualitative data validated some of our quantitative results. We report that emergent factors such as disconfirmation and satisfaction are critical to understanding changes in IT users' beliefs and attitudes and recommend that they be included in future process models of IT usage.
|keyword = information systems,usage,acceptance,attitude,belief,perceived usefulness,expectation disconfirmation theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effect of relationship encoding, task type, and complexity on information representation: An empirical evaluation of 2D and 3D line graphs'''
{{header}}
{{article
|author= N Kumar,I Benbasat,
|source= MIS QUARTERLY
|year= 2004
|abstract = Most of the recent research in data visualization has focused on technical and aesthetic issues involved in the manipulation of graphs, specifically on features that facilitate data exploration to make graphs interactive and dynamic. The present research identifies a gap in the existing knowledge of graph construction, namely potential problems in both 3D and 2D graphs that will impede comprehension of information when three or more variables are used in a graphical representation. Based on theories regarding perceptual issues of graph construction (Bertin 1981; Pinker 1991), we evaluate specific cases where 3D graphs may outperform 2D graphs, and vice-versa. Two experiments have been conducted to test these hypotheses, and 3D graphs have been found to consistently outperform 2D graphs in all of our experimental scenarios. A third experiment has been conducted to identify situations where 2D graphs might perform at least as well as 3D graphs, but its results suggest that 3D graphs outperform 2D graphs even for simple tasks, thus leading to the conclusion that 3D graphs perform better than 2D graphs under all task conditions with more than two variables.
|keyword = computer graphics,3-D graphics,human information processing,information presentation,information retrieval,information characteristics,information processing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A fault threshold policy to manage software development projects'''
{{header}}
{{article
|author= IR Chiang,VS Mookerjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = T his paper presents a project management policy in which the appearance of software faults during system construction is used to determine the timing of system integration activities (e.g., team meetings, analyzing modules for interface inconsistencies, system fault correction, and so on). System integration is performed only if a threshold fault count has been exceeded; otherwise, module development is allowed to continue. We derive an expression for calculating fault thresholds and analyze the policy to reveal the presence of three operating regions: (1) a region in which development should continue with no system integration, (2) a region in which system integration occurs if a threshold fault count has been exceeded, and (3) a region in which system integration should always take place. Analytical and numerical results demonstrate how the fault thresholds change with system complexity, team skill, development environment, and project schedule. We also show how learning that occurs during each round of system integration leads to less frequent integration in the future, and lower total construction effort. Simulation experiments reveal that the fault threshold policy can be applied even if several homogeneity assumptions in the model are relaxed, allowing for differences in the propensity among modules to accumulate faults and the effort needed to correct these faults. Finally, the fault threshold policy outperforms a fixed-release policy in which system integration occurs whenever a fixed number of modules has been released.
|keyword = software project management,quality-driven integration policy,incremental development,team coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Economics of an information intermediary with aggregation benefits'''
{{header}}
{{article
|author= HK Bhargava,V Choudhary,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = T he widespread use of the Internet has led to the emergence of numerous information intermediaries that bring buyers and sellers together and leverage their knowledge of the marketplace to provide value-added services. Infomediaries offer matching services that facilitate establishment of a buyer-seller agreement, and value-added services that either provide a standalone benefit or enhance benefits from matching services. This paper develops and analyzes economic models of intermediaries to examine their pricing and product line design strategies. Intermediaries provide aggregation benefits: Buyers find an intermediary's service more valuable if it provides access to more sellers, and sellers value it more if it provides access to more buyers, but also when they compete with fewer sellers. Due to this unique combination of network effects, we find that an intermediary has stronger incentives to provide quality-differentiated versions of its service relative to other information goods sellers. When buyers have constant marginal valuations for service quality, the intermediary should offer only two levels of service. While it is optimal for the intermediary to offer two levels of service, increasing the quality of the low-level service reduces the intermediary's profits due to increased cannibalization of the premium service. Hence, the optimal menu consists of a basic matching service and a premium service that includes matching and value-added services. The intermediary's profits are larger when positive network effects are stronger, and lower when negative network effects are stronger.
|keyword = infomediary,two-sided markets,online marketplace,product differentiation,network externalities,information goods versioning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Building effective online marketplaces with institution-based trust'''
{{header}}
{{article
|author= PA Pavlou,D Gefen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Institution-based trust is a buyer's perception that effective third-party institutional mechanisms are in place to facilitate transaction success. This paper integrates sociological and economic theories about institution-based trust to propose that the perceived effectiveness of three IT-enabled institutional mechanisms-specifically feedback mechanisms, third-party escrow services, and credit card guarantees-engender buyer trust in the community of online auction sellers. Trust in the marketplace intermediary that provides the overarching institutional context also builds buyer's trust in the community of sellers. In addition, buyers' trust in the community of sellers (as a group) facilitates online transactions by reducing perceived risk. Data collected from 274 buyers in Amazon's online auction marketplace provide support for the proposed structural model. Longitudinal data collected a year later show that transaction intentions are correlated with actual and self-reported buyer behavior. The study shows that the perceived effectiveness of institutional mechanisms encompasses both "weak" (market-driven) and "strong" (legally binding) mechanisms. These mechanisms engender trust, not only in a few reputable sellers, but also in the entire community of sellers, which contributes to an effective online marketplace. The results thus help explain why, despite the inherent uncertainty that arises when buyers and sellers are separated in time and in space, online marketplaces are proliferating. Implications for theory are discussed, and suggestions for future research on improving IT-enabled trust-building mechanisms are suggested.
|keyword = institution-based trust,online auction marketplaces,institutional structures,feedback mechanisms,escrows,third-party guarantees,reputation systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Does animation attract online users' attention? The effects of flash on information search performance and perceptions'''
{{header}}
{{article
|author= W Hong,JYL Thong,KY Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = T he proliferation of information on the Internet poses a significant challenge on humans' limited attentional resources. To attract online users' attention, various kinds of animation are widely used on websites. Despite the ubiquitous use of animation, there is an inadequate understanding of its effect on attention. Focusing on flash animation, this study examines its effects on online users' performance and perceptions in both task-relevant and task-irrelevant information search contexts by drawing on the visual search literature and two theories from cognitive psychology. In the task-relevant context, flash is applied on the search target; while in the task-irrelevant context, flash is applied on a nontarget item. The results of this study confirm that flash does attract users' attention and facilitates quicker location of the flashed target item in tightly packed screen displays. However, there is no evidence that attracting attention increases recall of the flashed item, as is generally presumed in practice, and may even decrease the overall recall. One explanation is that when users have to use their limited attentional resources on suppressing the distraction of flash, they will have less mental resources to process information. Moreover, the results suggest that processing information about an item depends not only on the attention it attracts per se, but also on the attention that other items on the same screen attract. While flashing an item may not increase the recall of that item, it can reduce the recall of other items (especially the nontarget items) on the screen. Finally, flash has negative effects on users' focused attention and attitude towards using the website. These results have implications for website interface design, online product promotion, online advertising, and multimedia training systems, among others.
|keyword = flash animation,attention,online information search,visual search,central capacity theory,associative network model,laboratory experiment,website interface design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A practice perspective on technology-mediated network relations: The use of Internet-based self-serve technologies'''
{{header}}
{{article
|author= U Schultze,WJ Orlikowski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2004
|abstract = Embedded relationships with customers have been key in generating repeat business and economic advantage, especially in business-to-business settings. Such relationships are typically maintained through interpersonal interactions between customers and their providers. Lately, however, firms have been seeking to make their service operations more scalable by offering customers access to Internet-based, self-serve technology. This raises questions about the implications of inserting self-serve technology into embedded relationships. Recent research on the role of information technology (IT) within interfirm network relations suggests that relationships and the use of IT are complementary. However, most of this research focuses on the organizational level and fails to consider the instantiation of these interfirm relations by the actions and interactions of individual actors (e.g., customers and salespeople) representing their respective firms. In this paper, we explore the implications of using IT within interfirm relations through an analysis of customers' and sales representatives' (reps) work activities and interpersonal relationships. We apply a practice perspective that highlights how macrolevel phenomena such as interfirm relations are created and recreated through the microlevel actions taken by firm members. This analysis reveals that managing the complementarity between relationships and IT in practice is fraught with considerable tension. This study of WebGA, a bricks-and-clicks dotcom, highlights how the use of the self-serve technology made it more difficult for sales reps to build and maintain embedded relationships with their customers. The use of IT altered the nature and quality of information shared by the participants, undermined the ability of sales reps to provide consulting services to customers, reduced the frequency of their interaction, and prompted sales reps to expend social capital to promote customers' technology adoption. These changes produced intended and unintended shifts in the network relations enacted by WebGA and its customers, and raised serious challenges to the viability of WebGA's business model.
|keyword = arm's length relationships,electronic brokering,embedded relationships,service strategies,social capital,work practices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Integrating collaborative processes and quality assurance techniques: Experiences from requirements negotiation'''
{{header}}
{{article
|author= P Grunbacher,M Halling,S Biffl,H Kitapci,BW Boehm,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Collaboration is essential in many mission-critical activities. Consequently, numerous methods and tools are available supporting collaborative processes such as strategic planning, risk management, requirements definition, and so on. These methods typically emphasize the collaborative, value-creating activities, but there is often less emphasis on quality aspects. Quality assurance (QA) techniques have been well-known in engineering for a long time, and their effectiveness and efficiency has been empirically evaluated in many domains. In this paper, we propose to integrate repeatable QA techniques and collaborative processes. We evaluate our idea in the context of a collaborative process for requirements negotiation. We propose pre-process techniques to be used before the actual negotiation, in-process techniques for checking quality during a negotiation, as well as post-process inspection techniques. These techniques help a project team reduce unnecessary complexity and to mitigate risks stemming from defects in requirements negotiation results. We present the results of a feasibility study we conducted to test our approach.
|keyword = collaboration engineering,group support systems,quality assurance techniques,requirements negotiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Foundation for the study of computer-supported collaborative learning requiring immersive presence'''
{{header}}
{{article
|author= R Sharda,NC Romano,JA Lucca,M Weiser,G Scheets,JM Chung,CM Sleezer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The dramatic increase in distance learning (DL) enrollments in higher education is likely to continue. However, research on DL, which includes psychomotor, cognitive, and affective skills, is virtually nonexistent. Indeed, DL for psychomotor skills has been viewed as impossible. Laboratory coursework, which we define as including the acquisition of psychomotor, cognitive, and affective skills, has become a limiting factor in the growth of DL. What is needed is a synergistic integration of technologies and human-computer interface (HCI) principles from computer-supported collaborative learning (CSCL), collaborative learning systems, and immersive presence technologies to enable achievement of psychomotor learning objectives. This paper defines the cornputer-supported collaborative learning requiring immersive presence (CSCLIP) research area, provides a theoretical foundation for CSCLIP, and develops an agenda for research in CSCLIP to establish a foundation for the study of this emerging area. It also briefly describes a CSCLIP-based telecommunications lab Currently under development. CSCLIP is presented as a major research opportunity for information systems researchers interested in empirical research as well as technical development.
|keyword = computer-supported collaborative learning requiring immersive presence (CSCLIP),distance learning (DL),immersive presence (IP),psychomotor learning objectives,situated learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A unified model of requirements elicitation'''
{{header}}
{{article
|author= AM Hickey,AM Davis,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Effective requirements elicitation is essential to the success of software development projects. Many papers have been written that promulgate specific elicitation methods. A few model elicitation in general. However, none have yet modeled elicitation in a way that makes clear the critical role played by situational knowledge. This paper presents a unified model of the requirements elicitation process that emphasizes the iterative nature of elicitation as it transforms the current state of the requirements and the situation to an improved understanding of the requirements and, potentially, a modified situation. One meta-process of requirements elicitation, selection of an appropriate elicitation technique, is also captured in the model. The values of this model are: (1) an improved understanding of elicitation helps analysts improve their elicitation efforts and (2) as we improve our ability to perform elicitation, we improve the likelihood that systems we create will meet their intended customers' needs.
|keyword = requirements analysis,requirements elicitation,systems analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaborative business engineering: A decade of lessons from the field'''
{{header}}
{{article
|author= M Den Hengst,GJ De Vreede,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Business process reengineering (BPR) projects have been carried out for many years, with varying degrees of success. Two key reasons for failure are distinguished from the literature: insufficient stakeholder involvement and poor analyses of the business processes. The purpose of this paper is to present a decade of field experiences with collaborative BPR. Nine BPR projects were executed and analyzed in detail, leading to the identification of 87 themes regarding the efficiency and effectiveness of the project. These themes were organized into 12 categories of lessons learned that provide insight into the "best practices" and together informed the evolution of the collaborative business engineering (CBE) approach to BPR. The CBE approach combines a BPR process with collaboration and simulation modeling support to address the above-mentioned reasons for failure in BPR. The field experiences show that the CBE approach can be successfully applied for BPR projects in real life.
|keyword = action research,business process reengineering,collaborative business engineering,group support systems,groupware,participative design,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interpersonal traits, complementarity, and trust in virtual collaboration'''
{{header}}
{{article
|author= HG Brown,MS Poole,TL Rodgers,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Trust has been a focus of research on virtual collaboration in distributed teams, e-commerce, e-learning, and telemedicine. Central to several models of trust and virtual collaboration is user's disposition to trust. This construct, however, has generally been conceptualized in as a stand-alone trait without a substantive theoretical background in personality theory. This paper advances the interpersonal circumplex model (ICM) as a theoretical framework for understanding the role of personal traits in collaboration in virtual contexts. The ICM posits that tendencies in interpersonal interaction stem from personal dispositions that can be understood in terms of dimensions of power and affiliation, fundamental constituents of user's personality. We develop a model that proposes that interpersonal traits, specifically, personality type as defined by the circumplex, affect the individual's disposition to trust, perceived trustworthiness, communication, and thereby affects willingness to collaborate and the sustainability and productivity of the collaboration. The model enables us to unpack the black box concepts of disposition to trust, faith in others, and trusting stance that are currently incorporated in theories of trust in information systems. The theory also enables explanation of trust dynamics at the dyadic and group levels. We develop propositions positing that individual's traits and dyadic complementarity are mediating factors in interpersonal trust and willingness to use new technologies and significantly affect the initiation, duration, and productivity of computer-mediated collaboration.
|keyword = collaboration,computer-mediated collaboration,interpersonal traits,technology acceptance,telemedicine,trust,virtual collaboration,virtual teams,virtual trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A comparison of classification methods for predicting deception in computer-mediated communication'''
{{header}}
{{article
|author= L Zhou,JK Burgoon,DP Twitchell,TT Qin,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = The increased chance of deception in computer-mediated communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception. To achieve the ultimate goal of automatic prediction of deception, we selected four common classification methods and empirically compared their performance in predicting deception. The deception and truth data were collected during two experimental studies. The results suggest that all of the four methods were promising for predicting deception with cues to deception. Among them, neural networks exhibited consistent performance and were robust across test settings. The comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods. The selected cues offer both methodological and theoretical contributions to the body of deception and information systems research.
|keyword = classification methods,deception,deception detection,linguistic cues,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Causal relationships in creative problem solving: Comparing facilitation interventions for ideation'''
{{header}}
{{article
|author= EL Santanen,RO Briggs,GJ De Vreede,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Organizations must be creative continuously to Survive and thrive in today's highly competitive, rapidly changing environment. A century of creativity research has produced several descriptive models creativity, and hundreds of prescriptions for interventions that demonstrably improve creativity. This paper presents the cognitive network model (CNM) as a causal model of the cognitive mechanisms that give rise to creative solutions in the human mind. The model may explain why creativity prescriptions work as they do. The model may also provide a basis for deriving new techniques to further enhance creativity. The paper tests the model in an experiment where 61 four-person groups used either free-brainstorming or one of three variations on directed-brainstorming to generate solutions for one of two unstructured tasks. In both tasks, people using directed-brainstorming produced more solutions with high creativity ratings, produced solutions with higher average creativity ratings, and produced higher concentrations of creative solutions than did people using free-brainstorming. Significant differences in creativity were also found among the three variations on directed-brainstorming. The findings were consistent with the CNM.
|keyword = brainstorming,cognitive models,creativity,facilitation,group problem solving,group support systems,idea generation,ideation,thinkLets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Supporting collaboration in heterogeneous environments'''
{{header}}
{{article
|author= AM Krebs,B Dorohonceanu,I Marsic,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2004
|abstract = Heterogeneous sharing in synchronous collaboration is important with the proliferation of diverse computing environments, such as wearable computers and handheld devices. We present here a data-centric design for synchronous collaboration of users with heterogeneous computing platforms. Our approach allows clients with different capabilities to share different subsets of data in order to conserve communication bandwidth. We have built a robust middleware consisting of a distributed repository of shared data objects and a client-server-based infrastructure. Using the middleware, we have developed a framework for building collaborative applications for clients with different display and processing capabilities. We discuss the design and implementation of our middleware and framework and evaluate them by building four complex sample applications that demonstrate scalability, good performance, and high degree of code reusability.
|keyword = collaboration,heterogeneity,information systems,middleware,mobile devices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Beta versus VHS and the acceptance of electronic brainstorming technology'''
{{header}}
{{article
|author= AR Dennis,BA Reinicke,
|source= MIS QUARTERLY
|year= 2004
|abstract = This paper argues that much of the past research on electronic brainstorming has been somewhat myopic. Much as Sony focused on the quality of the picture on its Beta format, we as IS researchers have focused on the number of ideas generated as the dominant measure of electronic brainstorming effectiveness. When VHS killed Beta, Sony discovered that image quality was a secondary consideration for most VCR users. Despite the compelling research on its performance benefits, electronic brainstorming has not yet displaced-or even joined-verbal brainstorming as a widely used idea generation technique. This paper presents arguments that users may not be primarily concerned with the number of ideas generated when planning a brainstorming session, but rather may equally desire group well being and member support. We present theoretical arguments and empirical evidence suggesting that electronic brainstorming is not as effective as verbal brainstorming at providing group well being and member support. We believe that these arguments may also apply to other group and individual research areas and may also call for a reevaluation of the technology acceptance model (TAM). Finally, we suggest further research that may help electronic brainstorming avoid the fate of the Beta format.
|keyword = electronic brainstorming,group support systems,technology acceptance model (TAM),technology adoption,brainstorming,idea generation,nominal group technique,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User heterogeneity and its impact on electronic auction market design: An empirical exploration'''
{{header}}
{{article
|author= R Bapna,P Goes,A Gupta,YW Jin,
|source= MIS QUARTERLY
|year= 2004
|abstract = While traditional information systems research emphasizes understanding of end users from perspectives such as cognitive fit and technology acceptance, it fails to consider the economic dimensions of their interactions with a system. When viewed as economic agents who participate in electronic markets, it is easy to see that users' preferences, behaviors, personalities, and ultimately their economic welfare are intricately linked to the design of information systems. We use a data-driven, inductive approach to develop a taxonomy of bidding behavior in online auctions. Our analysis indicates significant heterogeneity exists in the user base of these representative electronic markets. Using online auction data from 1999 and 2000, we find a stable taxonomy of bidder behavior containing five types of bidding strategies. Bidders pursue different bidding strategies that, in aggregate, realize different winning likelihoods and consumer surplus. We find that technological evolution has an impact on bidders' strategies. We demonstrate how the taxonomy of bidder behavior can be used to enhance the design of some types of information systems. These enhancements include developing user-centric bidding agents, inferring bidders' underlying valuations to facilitate real-time auction calibration, and creating low-risk computational platforms for decision making.
|keyword = electronic markets,online auctions,bidding strategies,user behavior taxonomy,smart agents,valuation discovery,calibration,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How do suppliers benefit from information technology use in supply chain relationships?'''
{{header}}
{{article
|author= M Subramani,
|source= MIS QUARTERLY
|year= 2004
|abstract = Supply chain management systems (SCMS) championed by network leaders in their supplier networks are now ubiquitous. While prior studies have examined the benefits to network leaders from these systems, little attention has been paid to the benefits to supplier firms. This study draws from organizational theories of learning and action and transaction cost theory to propose a model relating suppliers' use of SCMS to benefits. It proposes that two patterns of SCMS use by suppliers-exploitation and exploration-create contexts for suppliers to make relationship-specific investments in business processes and domain knowledge. These, in turn, enable suppliers to both create value and retain a portion of the value created by the use of these systems in interfirm relationships. Data from 131 suppliers using an SCMS implemented by one large retailer support hypotheses that relationship-specific intangible investments play a mediating role linking SCMS use to benefits. Evidence that patterns of information technology use are significant determinants of relationship-specific investments in business processes and domain expertise provides a finer-grained explanation of the logic of IT-enabled electronic integration. The results support the vendors-to-partners thesis that IT deployments in supply chains lead to closer buyer-supplier relationships (Bakos and Brynjyolfsson 1993). The results also suggest the complementarity of the transaction-cost and resource-based views, elaborating the logic by which specialized assets can also be strategic assets.
|keyword = buyer-supplier relationships,interorganizational systems (IOS),EDI,supply chain management systems (SCMS),transaction cost economics,intangible asset specificity,IT use,exploration,exploitation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The influence of business managers' IT competence on championing IT'''
{{header}}
{{article
|author= G Bassellier,I Benbasat,BH Reich,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = With the increased importance of IT in organizations, business managers are now expected to show stronger leadership in regard to its deployment of IT in organizations. This requires greater focus on their capability to understand and use IT resources effectively. This paper explores the concept of IT competence of business managers as a contributor to their intention to champion IT within their organizations. Based on the knowledge literature, IT competence is defined as "the set of IT-related knowledge and experience that a business manager possesses." The relationship between IT knowledge, IT experience, and championing IT is tested empirically using Structural Equation Modeling with LISREL. Four hundred and four business managers from two large insurance organizations were surveyed. Specific areas of IT knowledge and IT experience were first identified and the first half of the data set was utilized to assess the measurement properties of the instrument in a confirmatory analysis. The contribution of IT knowledge and IT experience to their intention to champion IT was assessed using the second half of the data set. The results show that IT knowledge and IT experience together explain 34% of the variance in managers' intentions to champion IT. Recommendations are given as to how organizations can enhance their business managers IT knowledge and experience to achieve stronger IT leadership from line people.
|keyword = IT competence,IT knowledge,IT experience,championing IT,measuring IT competence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Synthesis and decomposition of processes in organizations'''
{{header}}
{{article
|author= A Basu,RW Blanning,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Organizations today face increasing pressures to integrate their processes across disparate divisions and functional units, in order to remove inefficiencies as well as to enhance manageability. Process integration involves two major types of changes to process structure: (1) synthesizing processes from separate but interdependent subprocesses, and (2) decomposing aggregate processes into distinct subprocesses that are more manageable. We present an approach to facilitate this type of synthesis and decomposition through formal analysis of process structure using a mathematical structure called a metagraph.
|keyword = process integration,distributed processes,metagraphs,graph theory,synthesis and decomposition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A research note regarding the development of the consensus on appropriation scale'''
{{header}}
{{article
|author= CD Allport,WA Kerler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Measurement is perhaps the most difficult aspect of behavioral research. In a recent edition of ISR, a scale for consensus on appropriation was developed. Consensus on appropriation is one of three global constructs incorporated in adaptive structuration theory (Poole and DeSanctis 1990). The principal components analysis on the initial questionnaire revealed two factors with eigenvalues greater than one. While the methods used to develop the scale were thorough, the weaker factor was excluded from the rest of the analysis with little justification. We suggest that this finding has two possible explanations, multidimensionality or response bias. This research note suggests that in addition to the convergent and discriminant validity that Salisbury et al. (2002) provided for the consensus on appropriation scale, we may have an opportunity to further refine the measurement of this construct. By further exploring this principal component finding, consensus on appropriation may be better understood and measured.
|keyword = consensus on appropriation,scale development,framing,multidimensionality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowing-why about data processes and data quality'''
{{header}}
{{article
|author= YW Lee,DM Strong,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Knowledge about work processes is a prerequisite for performing work. We investigate whether a certain mode of knowledge, knowing-why, affects work performance and whether the knowledge held by different work roles matters for work performance. We operationalize these questions in the specific domain of data production processes and data quality. We analyze responses from three roles within data production processes, data collectors, data custodians, and data consumers, to investigate the effects of different knowledge modes held by different work roles on data quality. We find that work roles and the mode of knowledge do matter. Specifically, data collectors with why-knowledge about the data production process contribute to producing better quality data. Overall, knowledge of data collectors is more critical than that of data custodians.
|keyword = data processes,data quality,knowing-why,knowledge,knowledge modes,work roles,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The design and implementation of a corporate householding knowledge processor to improve data quality'''
{{header}}
{{article
|author= S Madnick,R Wang,X Xian,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Advances in corporate householding are needed to address certain categories of data quality problems caused by data misinterpretation. In this paper, we first summarize some of these data quality problems and our more recent results from studying corporate householding applications and knowledge exploration. Then we outline a technical approach to a corporate householding knowledge processor (CHKP) to solve a particularly important type of corporate householding problem-entity aggregation. We illustrate the operation of the CHKP by using a motivational example in account consolidation. Our CHKP design and implementation uses and expands on the COntext INterchange (COIN) technology to manage and process corporate householding knowledge.
|keyword = context mediation,corporate household,corporate householding,database interoperability,data quality,enterprise knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Time-related factors of data quality in multichannel information systems'''
{{header}}
{{article
|author= C Cappiello,C Francalanci,B Pernici,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Modem organizations offer services through multiple channels, such as branches, ATMs, telephones, and Internet sites, and are supported by multifunctional software architectures. Different functional modules share data, which are typically stored in multiple local databases. Functional modules are usually not integrated across channels, as channels are implemented at different times within independent software projects and are subject to varying requirements of availability and performance. This lack of channel and functional integration raises data quality problems that can impact the quality of the products and services of an organization. In particular, in complex systems in which data are managed in multiple databases, timeliness is critical. This paper focuses on time-related factors of data quality and provides a model that can help companies to evaluate data currency, accuracy, and completeness in software architectures with different degrees of integration across channels and functionalities. The model is validated through simulation based on empirical data on financial information systems. Results indicate how architectural choices on the degree of data integration have a varying impact on currency, accuracy, and completeness depending on the type of financial institution and on customer profiles.
|keyword = data accuracy,data completeness,data currency,data quality,financial information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Crafting rules: Context-reflective data quality problem solving'''
{{header}}
{{article
|author= YW Lee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Motivated by the growing importance of data quality in data-intensive, global business environments and by burgeoning data quality activities, this study builds a conceptual model of data quality problem solving. The study analyzes data quality activities at five organizations via a five-year longitudinal study. The study finds that experienced practitioners solve data quality problems by reflecting on and explicating knowledge about contexts embedded in, or missing from, data. Specifically, these individuals investigate how data problems are framed, analyzed, and resolved throughout the entire information discourse. Their discourse on contexts of data, therefore, connects otherwise separately managed data processes, that is, collection, storage, and use. Practitioners' context-reflective mode of problem solving plays a pivotal role in crafting data quality rules. These practitioners break old rules and revise actionable dominant logic embedded in work routines as a strategy for crafting rules in data quality problem solving.
|keyword = context-reflective problem solving,data quality,data quality rules,information quality,problem solving,reflection-in-action,situated practice,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology investments and firms' performance - A duopoly perspective'''
{{header}}
{{article
|author= JJ Quan,Q Hu,PJ Hart,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Over the past two decades, numerous empirical studies have been conducted on the contribution of information technology (IT) to productivity and other measures of firm performance. However, few theoretical studies have attempted to explain the contingencies under which IT investments may or may not be valuable to a firm in a competitive market. This research proposes a duopoly competition model to study the impacts of IT investments on firm performance and productivity. We show that the extent to which a profit-maximizing firm benefits from IT investments is a function of, among other things, market sensitivities to the price and quality of the products and services offered by the firm and its competitor. We demonstrate that, under duopolistic competition, the effects of IT investments are not as deterministic as under monopolistic competition. We further show that the effect of IT investments on productivity, in a duopoly market, are contingent on market sensitivities to changes in the price and quality of products and services offered by the firm and its competitor, as well as on fixed and overhead costs being sufficiently large in relation to market size-an important condition in a monopoly market. Especially, the price sensitivity has a positive effect on the impact of IT investments on productivity and quality sensitivity has a negative effect. We submit that firms are better off making efficiency-enhancing IT investments if the market in which they operate is more price sensitive than quality sensitive.
|keyword = competitive strategies,duopoly,game theory,information technology impacts,information technology investments,information technology productivity paradox,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The matrix of control: Combining process and structure approaches to managing software development'''
{{header}}
{{article
|author= SR Nidumolu,MR Subramani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = The performance of firms in the software industry depends considerably on the quality of their software development processes. Managing software development is a challenging task, as management controls need to impose discipline and coordinate action to ensure goals are met while simultaneously incorporating autonomy to motivate software developers to be innovative and produce quality work. How should such firms manage software development projects so that their development processes are flexible and predictable-resulting in products that meet quality goals and that are delivered within budget and on time? The management literature suggests two approaches to control: the process approach and the structure approach. The process approach recommends control of activities through specifying methods (behavior control) and through specifying performance criteria (outcome control). In contrast, the structure approach recommends control through centrally devised standards for activities (standardization) and by the delegation of authority for decision-making (decentralization). This study synthesizes these two approaches to suggest that formal managerial control is exerted through a matrix of control comprising four modes: standardization of methods, standardization of performance criteria, decentralization of methods, and decentralization of performance criteria. We test the association of the modes of control with performance in a sample of 56 firms in the software industry in the United States. The results suggest that performance is enhanced by establishing uniform performance criteria across projects (standardization of performance criteria) while giving each project team the authority to make decisions with respect to methods (decentralization of methods). However, standardization of methods across all projects and decentralization of performance criteria by delegating the authority to make decisions about performance criteria to project teams were both not significantly related to performance. The matrix of control and its relationship to performance has theoretical and practical implications for managing software development. This model of control is also likely to be useful in other knowledge-work-intensive settings.
|keyword = decentralization,organizational control,process performance,software development,software project management,standardization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of system trust in business-to-consumer transactions'''
{{header}}
{{article
|author= R Pennington,HD Wilcox,V Grover,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = It has been argued that the buyer's trust of the vendor is a critical precursor to a transactional relationship in an e-commerce environment. This study uses an experimental survey to test a model that includes a number of factors such as trust mechanisms, "system trust," and vendor reputation. The results suggest that one trust mechanism, vendor guarantees, has a direct influence on system trust. Further, within e-commerce situations, system trust plays an important role in the nomological network by directly affecting trust in vendors and indirectly affecting attitudes and intentions to purchase. These results held in the case of both firms with and without an established reputation. The results demonstrate the importance of interventions such as self-reported vendor guarantees that affect system trust in enabling successful e-commerce outcomes.
|keyword = e-commerce,intention to purchase,reputation in e-commerce,system trust,trust in vendor,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An inductively derived model of leader-initiated relationship building with virtual team members'''
{{header}}
{{article
|author= DJ Pauleen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = This paper seeks to add to the nascent research literature on virtual teams and virtual team leadership by investigating the issues facing virtual team leaders as they implement and lead virtual teams. In particular, the way in which leaders develop relationships with their virtual team members is explored. A research framework involving action learning was instituted, with data collection and analysis based on grounded theory approaches. In all, seven virtual team leaders from a variety of New Zealand organizations took part in the study. The data showed very clearly that the leaders considered it essential to build some level of personal relationship with their virtual team members before commencing a virtual working relationship. A unifying framework of three interrelated theoretical steps, which illustrates how a virtual leader builds relationships with virtual team members, is introduced. These three steps are assessing conditions, targeting level of relationship, and creating strategies. This study is the first to identify the steps a virtual team leader undertakes when building relationships with virtual team members. The implications for virtual team practice and research are discussed.
|keyword = action learning,grounded theory,information technology in team building,leadership,relationship building,trust,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A longitudinal field study of training practices in a collaborative application environment'''
{{header}}
{{article
|author= D Kang,R Santhanam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Researchers have emphasized that existing training strategies must be modified in order to adequately prepare users to employ collaborative applications. We utilize findings from the vast amount of training research conducted thus far and point to some problems that might occur when existing strategies are applied to train users of collaborative applications. We test our ideas by conducting a longitudinal field study of a collaborative work flow application. As proposed in a recent knowledge-level framework, our findings indicate that training programs must not solely focus on developing users' system proficiency skills but must also educate users about the business processes that the collaborative application will support. This additional knowledge will enable users to deal with technology-induced changes in the business processes due to the deployment of the collaborative application. Furthermore, we find that training programs should sensitize users to the interdependencies that exist among their tasks and make them aware of the collective consequences of their individual actions. We also found that users have to engage in collective problem solving efforts and continuously learn new knowledge during the process of appropriation of the collaborative application. We propose a training framework that integrates these ideas to prepare users to make effective use of collaborative applications. The proposed framework calls for trainers to be continuously engaged with users and help refine their knowledge during the process of appropriation. We suggest that theoretical foundations rooted in collective learning be adopted to guide training research in collaborative applications.
|keyword = collaborative applications,end-user computing,end-user training,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dealing with plagiarism in the information systems research community: A look at factors that drive plagiarism and ways to address them'''
{{header}}
{{article
|author= N Kock,R Davison,
|source= MIS QUARTERLY
|year= 2003
|abstract = Imagine yourself spending years conducting a research project and having it published as an article in a refereed journal, only to see a plagiarized copy of the article later published in another journal. Then imagine yourself being left to fight for your rights alone, and eventually finding out that it would be very difficult to hold the plagiarist accountable for what he or she did. The recent decision by the Association of Information Systems to create a standing committee on member misconduct suggests that while this type of situation may sound outrageous, it is likely to become uncomfortably frequent in the information systems research community if proper measures are not taken by a community-backed organization. In this article, we discuss factors that can drive plagiarism, as well as potential measures to prevent it. Our goal is to discuss alternative ways in which plagiarism can be prevented and dealt with when it arises. We hope to start a debate that provides the basis on which broader mechanisms to deal with plagiarism can be established, which we envision as being associated with and complementary to the committee created by the Association for Information Systems.
|keyword = ethics,committees,community,plagiarism,information systems research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The contingent effects of management support and task interdependence on successful information systems implementation'''
{{header}}
{{article
|author= R Sharma,P Yetton,
|source= MIS QUARTERLY
|year= 2003
|abstract = Management support is considered to be a critical factor in the successful implementation of information systems innovations. The literature suggests a complex relationship between management support and implementation success. However, the empirical literature typically hypothesizes and tests a simple main-effects model. Drawing upon the role of the institutional context and metastructuration actions, we propose a contingent model in which task interdependence moderates the effect of management support on implementation success. A meta-analysis of the empirical literature provides strong support for the model and begins to explain the wide variance in empirical findings. Implications for theory and practice are discussed.
|keyword = IS success,IS utilization,IS implementation,management support,task interdependence,meta-analysis,contingency models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The disruptive nature of information technology innovations: The case of Internet computing in systems development organizations'''
{{header}}
{{article
|author= K Lyytinen,GM Rose,
|source= MIS QUARTERLY
|year= 2003
|abstract = Information technology (IT) innovation can be defined as the creation and new organizational application of digital computer and communication technologies. The paper suggests that IT innovation theory needs to be expanded to analyze IT innovations in kind that exhibit atypical discontinuities in IT innovation behaviors by studying two questions. First, can a model of disruptive IT innovations be created to understand qualitative changes in IT development processes and their outcomes so that they can be related to architectural discontinuities in computing capability? Second, to what extent can the observed turmoil among systems development organizations that has been spawned by Internet computing be understood as a disruptive IT innovation? To address the first question, a model of disruptive IT innovation is developed. The model defines a disruptive IT innovation as an architectural innovation originating in the information technology base that has subsequent pervasive and radical impacts on development processes and their outcomes. These base innovations establish necessary but not sufficient conditions for subsequent innovation behaviors. To address the second question, the impact of Internet computing on eight leading-edge systems development organizations in the United States and Finland is investigated. The study shows that the adoption of Internet computing in these firms has radically impacted their IT innovation both in development processes and services.
|keyword = Internet computing,innovation theory,disruptive IT innovation,IT innovation cores,system development,software management,IT applications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The value relevance of announcements of transformational information technology investments'''
{{header}}
{{article
|author= B Dehning,VJ Richardson,RW Zmud,
|source= MIS QUARTERLY
|year= 2003
|abstract = In this paper, we examine the influence of IT strategic role to extend the findings of Im et al. (2001), Chatterjee et al. (2002) and Dos Santos et al. (1993). Specifically, we demonstrate that IT strategic role can explain how IT investments in each of the IT strategic roles might affect the firm's competitive position and ultimately firm value. We find positive, abnormal returns to announcements of IT investments by firms making transformative IT investments, and with membership in industries with transform IT strategic roles. The results of previous research are not found to be significant when IT strategic role is included as an explanatory variable. These results provide support for the value of capturing the IT strategic role of a firm's IT-related competitive maneuvering in studies striving to understand the conditions under which IT investments are likely to produce out-of-the-ordinary, positive returns.
|keyword = IT investment,event study,IT strategic role,stock market reaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sources of influence on beliefs about information technology use: An empirical study of knowledge workers'''
{{header}}
{{article
|author= W Lewis,R Agarwal,V Sambamurthy,
|source= MIS QUARTERLY
|year= 2003
|abstract = Individual beliefs about technology use have been shown to have a profound impact on subsequent behaviors toward information technology (IT). This research note builds upon and extends prior research examining factors that influence key individual beliefs about technology use. It is argued that individuals form beliefs about their use of information technologies within a broad milieu of influences emanating from the individual, institutional, and social contexts in which they interact with IT. We examine the simultaneous effects of these three sets of influences on beliefs about usefulness and ease of use in the context of a contemporary technology targeted at autonomous knowledge workers. Our findings suggest that beliefs about technology use can be influenced by top management commitment to new technology and the individual factors of personal innovativeness and self-efficacy. Surprisingly, social influences from multiple sources exhibited no significant effects. Theoretical and practical implications are offered.
|keyword = technology adoption,technology beliefs,belief antecedents,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Generalizing generalizability in information systems research'''
{{header}}
{{article
|author= AS Lee,RL Baskerville,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Generalizability is a major concern to those who do, and use, research. Statistical, sampling-based generalizability is well known, but methodologists have long been aware of conceptions of generalizability beyond the statistical. The purpose of this essay is to clarify the concept of generalizability by critically examining its nature, illustrating its use and misuse, and presenting a framework for classifying its different forms. The framework organizes the different forms into four types, which are defined by the distinction between empirical and theoretical kinds of statements. On the one hand, the framework affirms the bounds within which statistical, sampling-based generalizability is legitimate. On the other hand, the framework indicates ways in which researchers in information systems and other fields may properly lay claim to generalizability, and thereby broader relevance, even when their inquiry falls outside the bounds of sampling-based research.
|keyword = research methodology,positivist research,interpretive research,quantitative research,qualitative research,case studies,research design,generalizability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Replicating online Yankee auctions to analyze auctioneers' and bidders' strategies'''
{{header}}
{{article
|author= R Bapna,P Goes,A Gupta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = We present a simulation approach that provides a relatively risk-free and cost-effective environment to examine the decision space for both bid takers and bid makers in web-based dynamic price setting processes. The applicability of the simulation platform is demonstrated for Yankee auctions in particular. We focus on the optimization of bid takers' revenue, as well as on examining the welfare implications of a range of consumer-bidding strategies-some observed, some hypothetical. While these progressive open discriminatory multiunit auctions with discrete bid increments are made feasible by Internet technologies, little is known about their structural characteristics, or their allocative efficiency. The multiunit and discrete nature of these mechanisms renders the traditional analytic framework of gametheory intractable (Nautz and Wolfstetter 1997). The simulation is based on theoretical revenue generating properties of these auctions. We use empirical data from real online auctions to instantiate the simulation's parameters. For example, the bidding strategies of the bidders are specified based on three broad bidding strategies observed in real online auctions. The validity of the simulation model is established and subsequently the simulation model is configured to change the values of key control factors, such as the bid increment. Our analysis indicates that the auctioneers are, most of the time, far away from the optimal choice of bid increment, resulting in substantial losses in a market with already tight margins. The simulation tool provides a test bed for jointly exploring the combinatorial space of design choices made by the auctioneer's and the bidding strategies adopted by the bidders. For instance, a multinornial logit model reveals that endogenous factors, such as the bid increment and the absolute magnitude of the auction have a statistically significant impact on consumer-bidding strategies. This endogeniety is subsequently modeled into the simulation to investigate whether the effects are significant enough to alter the optimal bid increments or auctioneer revenues. Additionally, we investigate hybrid-bidding strategies, derived as a combination of three broad strategies, such as jump bidding and strategic-at-margin (SAM) bidding. We find that hybrid strategies have the potential of significantly altering bidders' likelihood of winning, as well as their surplus.
|keyword = dynamic pricing,online auctions,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Improving analysis pattern reuse in conceptual design: Augmenting automated processes with supervised learning'''
{{header}}
{{article
|author= S Purao,VC Storey,TD Han,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Conceptual design is an important, but difficult, phase of systems development. Analysis patterns can greatly benefit this phase because they capture abstractions of situations that occur frequently in conceptual modeling. Naive approaches to automate conceptual design with reuse of analysis patterns have had limited success because they do not emulate the learning that occurs over time. This research develops learning mechanisms for improving analysis pattern reuse in conceptual design. The learning mechanisms employ supervised learning techniques to support the generic reuse tasks of retrieval, adaptation, and integration, and emulate expert behaviors of analogy making and designing by assembly. They are added to a naive approach and the augmented methodology implemented as an intelligent assistant to a designer for generating an initial conceptual design that a developer may refine. To assess the potential of the methodology to benefit practice, empirical testing is carried out on multiple domains and tasks of different sizes. The results suggest that the methodology has the potential to benefit practice.
|keyword = reuse,design automation,object-oriented systems,learning mechanisms,conceptual design,analysis patterns,APSARA,software development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Portfolios of control in outsourced software development projects'''
{{header}}
{{article
|author= V Choudhury,R Sabherwal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = This paper examines the evolution of portfolio of controls over the duration of outsourced information systems development (ISD) projects. Drawing on five cases, it concludes that many findings from research on control of internal ISD projects apply to the outsourced context as well, but with some interesting differences. The portfolios of control in outsourced projects are dominated by outcome controls, especially at the start of the project; although the precision and frequency of these controls varies across projects. Behavior controls are often added later in the project, as are controls aimed to encourage and enable vendor self-control. Clan controls were used in only two of the cases-when the client and vendor had shared goals, and when frequent interactions led to shared values. In general, the outsourced projects we studied began with relatively simple controls but often required significant additional controls after experiencing performance problems. Factors influencing the choice and evolution of controls are also examined.
|keyword = systems development,control,outsourcing,project management,case studies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Justifying contingent information technology investments: Balancing the need for speed of action with certainty before action'''
{{header}}
{{article
|author= EK Clemons,B Gu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Executives need to master different mechanisms for analyzing their firms' investment opportunities in uncertain, difficult times. Rapidly changing business conditions require firms to move quickly, with total commitment and the rapid deployment of capital, resources, and management attention, often in several directions at the same time. However, high levels of strategic uncertainty and environmental risk, combined with limits on available funding, require firms to limit their commitment. In brief, we require high levels of strategic commitment to numerous projects, while simultaneously preserving our flexibility and withholding commitment. Whereas achieving both is clearly impossible, techniques exist that enable executives (1) to identify and to delimit their range of investment alternatives that must be considered, and to do so rapidly and reliably, (2) to divide investments into discrete stages that can be implemented sequentially, (3) to determine which chunks can safely and profitably be developed as strategic options, with value that can be captured when subsequent stage investments are made later; and (4) to quantify and to estimate the value of these strategic options with a significant degree of accuracy, so that selections can be made from a portfolio of investment alternatives. This paper also avoids restrictions of common option valuation models by providing a technique that is general enough to be used when the data required by common models are not available or the assumptions are not satisfied.
|keyword = information technology investments,option valuation,strategic investments,strategic options,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''What do you know? Rational expectations in information technology adoption and investment'''
{{header}}
{{article
|author= YA Au,RJ Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = This study examines the potential applications of the rational expectations hypothesis (REH) and adaptive learning theory in IT investment and adoption decision-making. Despite the fact that rationality is commonly assumed in economic analyses, the REH's assumptions make it a unique theory and allow us to offer new perspectives on IS/IT adoption and investment decision-making. Our application of these theoretical perspectives to the IT adoption context-the first time in the IS literature to our knowledge that REH has been used to examine the mechanism for business value expectations formation-will allow us to treat the investment and adoption issues using a perspective that is based on a longer time horizon. Such settings require managers, as economic agents, to form a set of expectations about the values of various variables related to the business value of IT. Rational expectations and adaptive learning assume that decision-makers are able to utilize all available decision-relevant information efficiently and can learn the true value of a prospective investment over time. We present a number of propositions that characterize this perspective, and discuss some illustrative examples that demonstrate the efficacy of the theoretical perspective that we present to characterize the business value expectations formation process in IT adoption.
|keyword = adaptive learning,business value,herd behavior,informational cascades,information technology investments,rational expectations hypothesis,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Codifiability, relationship-specific information technology investment, and optimal contracting'''
{{header}}
{{article
|author= M Levi,PR Kleindorfer,DJ Wu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = The past few years have seen an explosion in the number of e-marketplaces, including a variety of electronic exchanges in the B2B arena, but many of these have also collapsed (e.g., ChemdexNentro). The question addressed in this paper is what are the underlying factors that affect which transactions are likely to be supportable by B2B exchanges. In particular, we identify and study three factors: supplier management, idiosyncratic investments in information systems, and codifiability (i.e., digitalizability) of product and order-fulfillment specifications underlying transactions. We show that transaction codifiability plays a fundamental role in influencing the nature of sustainable contracting and IT investments in e-markets. Hypotheses are derived from an analytical model of codifiability in e-marketplaces; these hypotheses are supported by several case studies by the authors and others on the key success factors underlying B2B exchanges.
|keyword = B2B exchanges,codifiability,electronic marketplaces,relationship-specific investments,supplier management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Consumers prefer bundled add-ins'''
{{header}}
{{article
|author= RM Dewan,ML Freimer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Software such as operating systems, word processing, spreadsheets, graphics, and others often serves as a base for a number of third-party add-in products or plug-ins. These add-ins enhance the functionality of the base product. Unless protected by patents, these add-ins can potentially be bundled into the base software. The impact of this bundling on the profits of the base software producer and the consumer depends on the proportion of consumers that value the add-in and the penalty that some consumers incur from finding only a bundled product available when they do not desire the add-in. Using a model of the market, we show that the price of the bundle will be less than the sum of the prices of the base and add-in software when they are sold separately. We also show that the total consumer surplus and the social welfare increase if the base software producer's profit increases with bundling.
|keyword = bundled software,consumer welfare,information goods,software add-ins,supplemental goods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contingency pricing for information goods and services under industrywide performance standard'''
{{header}}
{{article
|author= HK Bhargava,S Sundaresan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = This paper demonstrates that quality-contingent pricing is a useful mechanism for mitigating the negative effects of quality uncertainty in e-commerce and information technology services. Under contingency pricing of an information good or service, the firm preannounces a rebate for poor performance. Consumers determine performance probabilities using publicly available historical performance data, and the firm may have additional private information with respect to its future probability distribution. Examining the monopoly case, we explicate the critical role of private information and differences in belief between the firm and market in the choice of pricing scheme. Contingent pricing is useful when the market underestimates the firm's performance; then it is optimal for the firm to offer a full-price rebate for mis-performance, with a correspondingly higher price for meeting the performance standard. We study the competitive value of contingency pricing in a duopoly setting where the firms differ in their probabilities of meeting the performance standard, but are identical in other respects. Contingency pricing is a dominant strategy for a firm when the market underestimates the firm's performance. Whereas both firms would earn equal profits if they were constrained to standard pricing, the superior firm earns greater profits under contingency pricing by setting lower expected prices. We show that contingency pricing is efficient as well, and consumer surplus increases because more consumers buy from the superior firm.
|keyword = contingency pricing,information goods,information technology outsourcing,money-back guarantees,pricing of IT services,quality uncertainty,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Economic implications of variable technology standards for movie piracy in a global context'''
{{header}}
{{article
|author= RK Chellappa,S Shivendu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Even if bandwidth on the Internet is limited, compression technologies have made online music piracy a foremost problem in intellectual copyright protection. However, due to significantly larger sizes of video files, movies are still largely pirated by duplicating DVDs, VCDs, and other physical media. In the case of DVDs, movie studios have historically maintained different technology codes or formats across various regions of the world, primarily to control the timing of theatrical releases in these parts of the world. This paper formulates an analytical model to study the implications of maintaining different or incompatible technology standards in DVD and other optical disc players on global pricing and piracy of movie discs. Our formulation develops two distinct piracy types, namely, regional and global piracy, signifying if consumers will pirate movies released for their own region or those meant for other regions. Our results find that maintaining separate technology standards is very critical when there is piracy, as losses from global piracy can be higher than when only regional piracy exists. Further, we observe that piracy is not a victimless crime, in that not only do producers suffer losses but consumers in regions with high willingness to pay for quality also stand to lose. In addition, we find that increasing homogeneity in consumer preferences for quality across regions may not be beneficial to digital product vendors unless there is also uniformity in copyright protection laws. We conclude with recommendations for research and practice for movie studios as well as producers for other goods that are dependent on copyright protection such as books and pharmaceuticals.
|keyword = contract theory,digital products,information goods,moral rent,movie piracy,pricing,technology standards,vertical segmentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A taxonomy of antecedents of information systems success: Variable analysis studies'''
{{header}}
{{article
|author= KRT Larsen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Research in the information systems (IS) field has often been characterized as fragmented. This paper builds on a belief that for the field to move forward and have an impact on practitioners and other academic fields, the existing work must be examined and systematized. It is particularly important to systematize research on the factors that underlie success of organizational IS. The goal here is to conceptualize the IS success antecedents (ISSA) area of research through surveying, synthesizing, and explicating the work in the domain. Using a combination of qualitative and quantitative research methods, a taxonomy of 12 general categories is created, and existing research within each category is examined. Important lacunae in the direction of work have been determined. It is found that little work has been conducted on the macro-level independent variables, the most difficult variables to assess, although these variables may be the most important to understanding the ultimate value of IS to organizations. Similarly, ISSA research on success variables of consequence to organizations was found severely lacking. Variable analysis research on organizational-level success variables was found to be literally nonexistent in the IS field, whereas research in the organizational studies field was found to provide useful directions for IS researchers. The specifics of the 12 taxonomy areas are analyzed and directions for research in each of them provided. Thus, researchers and practitioners are directed toward available research and receive suggestions for future work to bring ISSA research toward an organized and cohesive future.
|keyword = antecedents of success,information systems implementation,quantitative research,taxonomies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''From the vendor's perspective: Exploring the value proposition in information technology outsourcing'''
{{header}}
{{article
|author= N Levina,JW Ross,
|source= MIS QUARTERLY
|year= 2003
|abstract = To date, most research on information technology (IT) outsourcing concludes that firms decide to outsource IT services because they believe that outside vendors possess production cost advantages. Yet it is not clear whether vendors can provide production cost advantages, particularly to large firms who may be able to replicate vendors' production cost advantages in-house. Mixed outsourcing success in the past decade calls for a closer examination of the IT outsourcing vendor's value proposition. While the client's sourcing decisions and the client-vendor relationship have been examined in IT outsourcing literature, the vendor's perspective has hardly been explored. In this paper, we conduct a close examination of vendor strategy and practices in one long-term successful applications management outsourcing engagement. Our analysis indicates that the vendor's efficiency was based on the economic benefits derived from the ability to develop a complementary set of core competencies. This ability, in turn, was based on the centralization of decision rights from a variety and multitude of IT projects controlled by the vendor. The vendor was enticed to share the value with the client through formal and informal relationship management structures. We use the economic concept of complementarity in organizational design, along with prior findings from studies of client-vendor relationships, to explain the IT vendors' value proposition. We further explain how vendors can offer benefits that cannot be readily replicated internally by client firms.
|keyword = outsourcing of IS,case study,complementarity in organizational design,IS core competencies,management of computing and IS,systems maintenance,IS staffing issues,IS project management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trust and the unintended effects of behavior control in virtual teams'''
{{header}}
{{article
|author= G Piccoli,B Ives,
|source= MIS QUARTERLY
|year= 2003
|abstract = Our analysis shows that the behavior control mechanisms typically used in traditional teams have a significant negative effect on trust in virtual teams. In-depth analysis of the communication logs of selected teams reveals that trust decline in virtual teams is rooted in instances of reneging and incongruence. Behavior control mechanisms increase vigilance and make instances when individuals perceive team members to have failed to uphold their obligations (i.e., reneging and incongruence) salient. Heightened vigilance and salience increase the likelihood that team members' failure to fulfill their obligations will be detected, thus contributing to trust decline.
|keyword = virtual teams,trust,behavior control,control theory,psychological contract,teamwork,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The influence of query interface design on decision-making performance'''
{{header}}
{{article
|author= C Speier,MG Morris,
|source= MIS QUARTERLY
|year= 2003
|abstract = Managers in modern organizations are confronted with ever-increasing volumes of information that they must evaluate when making a decision. Data warehousing and data mining technologies have given managers a number of valuable tools that can help them store, retrieve, and analyze information contained in large databases; however, maximizing user performance with these tools remains a challenge for information systems professionals. One important and under-explored aspect of the effectiveness of these tools is the design of the query interface. In this study, we compared the use of visual and text-based interfaces on both low and high complexity tasks. Results demonstrated that decision maker performance was more accurate using the text-based interface when task complexity was low; however, decision makers using the visual interface performed better when task complexity was high. In addition, decision makers' subjective mental workload was significantly lower when using the visual interface, regardless of task complexity. In contrast to expectations, less time was needed to make a decision on low complexity tasks when using the visual interface, but those results were reversed under conditions of high task complexity. These results have important implications for the design of managerial decision-making systems, particularly in complex decision-making environments.
|keyword = database,computer interface,decision-making,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User acceptance of information technology: Toward a unified view'''
{{header}}
{{article
|author= V Venkatesh,MG Morris,GB Davis,FD Davis,
|source= MIS QUARTERLY
|year= 2003
|abstract = Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R-2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding,the organizational outcomes associated with new technology use.
|keyword = theory of planned behavior,innovation characteristics,technology acceptance model,social cognitive theory,unified model,integrated model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''In pursuit of moderation: Nine common errors and their solutions'''
{{header}}
{{article
|author= TA Carte,CJ Russell,
|source= MIS QUARTERLY
|year= 2003
|abstract = One result of the increasing sophistication and complexity of MIS theory and research is the number of studies hypothesizing and testing for moderation effects. A review of the MIS and broader management literatures suggests researchers investigating moderated relationships often commit one or more errors falling into three broad categories: inappropriate use or interpretation of statistics, misalignment of research design with phenomena of interest, and measurement or scaling issues. Examples of nine common errors are presented. Commission of these errors is expected to yield literatures characterized by mixed results at best, and thoroughly erroneous results at worse. Procedures representing examples of best practice and reporting guidelines are provided to help MIS investigators avoid or minimize these errors.
|keyword = tests of moderation,contingency models,PLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Developing and validating an observational learning model of computer software training and skill acquisition'''
{{header}}
{{article
|author= MY Yi,FD Davis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Computer skills are key to organizational performance, and past research indicates that behavior modeling is a highly effective form of computer skill training. The present research develops and tests a new theoretical model of the underlying observational learning processes by which modeling-based training interventions influence computer task performance. Observational learning processes are represented as a second-order construct with four dimensions (attention, retention, production, and motivation). New measures for these dimensions were developed and shown to have strong psychometric properties. The proposed model controls for two pretraining individual differences (motivation to learn and self-efficacy) and specifies the relationships among three training outcomes (declarative knowledge, post-training self-efficacy, and task performance). The model was tested using PLS on data from an experiment (N = 95) on computer spreadsheet training. As hypothesized, observational learning processes significantly influenced training outcomes. A representative modeling-based training intervention (retention enhancement) significantly improved task performance through its specific effects on the retention processes dimension of observational learning. The new model provides a more complete theoretical account of the mechanisms by which modeling-based interventions affect training outcomes, which should enable future research to systematically evaluate the effectiveness of a wide range of modeling-based training interventions. Further, the new instruments can be used by practitioners to refine ongoing training programs.
|keyword = observational learning,modeling-based training,retention enhancement,behavior modeling,computer training,skill aquisition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of experience and time on the use of Data Quality Information in decision making'''
{{header}}
{{article
|author= CW Fisher,I Chengalur-Smith,DP Ballou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Data Quality Information (DQI) is metadata that can be included with data to provide the user with information regarding the quality of that data. As users are increasingly removed from any personal experience with data, knowledge that would be beneficial in judging the appropriateness of the data for the decision to be made has been lost. Data tags could provide this missing information. However, it would be expensive in general to generate and maintain such information. Doing so would be worthwhile only if DQI is used and affects the decision made. This work focuses on how the experience of the decision maker and the available processing time influence the use of DQI in decision making. It also explores other potential issues regarding use of DQI, such as task complexity and demographic characteristics. Our results indicate increasing use of DQI when experience levels progress through the stages from novice to professional. The overall conclusion is that DQI should be made available to managers without domain-specific experience. From this it would follow that DQI should be incorporated into data warehouses used on an ad hoc basis by managers.
|keyword = data quality,information quality,Data Quality Information (DQI),decision making,data quality tags,data warehouse,metadata,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A partial least squares latent variable modeling approach for measuring interaction effects: Results from a Monte Carlo simulation study and an electronic-mail emotion/adoption study'''
{{header}}
{{article
|author= WW Chin,BL Marcolin,PR Newsted,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = he ability to detect and accurately estimate the strength of interaction effects are critical. issues that are fundamental to social science research in general and IS research in particular. Within the IS discipline, a significant percentage of research has been devoted to examining the conditions and contexts under which relationships may vary, often under the general umbrella of contingency theory (cf. McKeen et al. 1994, Weill and Olson 1989). In our survey of such studies, the majority failed to either detect or provide an estimate of the effect size. In cases where effect sizes are estimated, the numbers are generally small. These results have led some researchers to question both the usefulness of contingency theory and the need to detect interaction effects (e.g., Weill and Olson 1989). This paper addresses this issue by providing a new latent variable modeling approach that can give more accurate estimates of interaction effects by accounting for the measurement error that attenuates the estimated relationships. The capacity of this approach at recovering true effects in comparison to summated regression is demonstrated in a Monte Carlo study that creates a simulated data set in which the underlying true effects are known. Analysis of a second, empirical data set is included to demonstrate the technique's use within IS theory. In this second analysis, substantial direct and interaction effects of enjoyment on electronic-mail adoption are shown to exist.
|keyword = PLS,moderators,interaction effects,structural equation m,odeling,measurement error,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizing visions for information technology and the information systems executive response'''
{{header}}
{{article
|author= NC Ramiller,EB Swanson,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Making sense of new information technology (IT) and the many buzzwords associated with it is by no means an easy task for executives. Yet doing so is crucial to making good innovation decisions. This paper examines how information systems (IS) executives respond to what has been termed organizing visions for IT, grand ideas for applying IT, the presence of which is typically announced by much "buzz" and hyperbole. Developed and promulgated in the wider interorganizational community, organizing visions play a central role in driving the innovation adoption and diffusion process. Familiar and recent examples include electronic commerce, data warehousing, and enterprise systems. A key aspect of an organizing vision is that it has a career That is, even as it helps shape how IS managers think about the future of application and practice in their field, the organizing vision undertakes its own struggle to achieve ascendancy in the community. The present research explores this struggle, specifically probing how IS executives respond to visions that are in different career stages. Employing field interviews and a survey, the study identifies four dimensions of executive response focusing on a vision's interpretability, plausibility, importance, and discontinuity. Taking a comparative approach, the study offers several grounded conjectures concerning the career dynamics of organizing visions. For the IS executive, the findings help point the way to a more proactive, systematic, and critical stance toward innovations that can place the executive in a better position to make informed adoption decisions.
|keyword = information systems management,information tecbnology innovation,innovation diffusion,institutional theory,sense-making,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Extending critical success factors methodology to facilitate broadly participative information systems planning'''
{{header}}
{{article
|author= K Peffers,CE Gengler,T Tuunanen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = We extend critical success factors (CSF) methodology to facilitate participation by many people within and around the organization for information systems (IS) planning. The resulting new methodology, called "critical success chains" (CSC), extends CSF to explicitly model the relationships between IS attributes, CSF, and organizational goals. Its use is expected to help managers to (1) consider a wider range of development ideas, (2) better balance important strategic, tactical, and operational systems in the development portfolio, (3) consider the full range of options to accomplish desired objectives, and (4) better optimize the allocation of resources for maintenance and small systems. We trace the development of CSF and make the case for extending it. In two case studies, one at Rutgers University and another at Digia, Inc., we demonstrate the use of CSC in planning. At Rutgers, we use CSC to observe employees' preferences for new systems features, to model the reasons why they think that the features are important to the firm, and to generate strategic IS project proposal ideas. At Digia, we use CSC to generate ideas for new financial services applications based on mobile communications technology for which Digia would be a part of the value chain. From our experience in the case studies, we define a practical procedure for data gathering and analysis to uncover and model CSC in the firm and to generate ideas for important IS projects.
|keyword = critical success chains,critical success factors,information systems development portfolio,information systems planning,information systems project selection,information systems strategic planning,laddering,personal construct theory,strategic grid,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Nearing the threshold: An economics approach to pressure on information systems professionals to separate from their employer'''
{{header}}
{{article
|author= RA Josefek,RJ Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Organizational approaches to managing information systems (IS) professionals have been making headlines as technology-intensive businesses search for ways to cope with an ever-changing economic landscape. Consequently, understanding and predicting employee quitting or separation behavior is crucial. We incorporate human capital theory from economics to form an alternative theoretical perspective for understanding IS professionals' separation and retention. This shift allows us to focus on precursors to observed separation, rather than attitudinal precursors of intentions. We introduce three new constructs: pressure to separate, retention frontiers, and separation thresholds. These constructs provide a basis for identifying when an employee is close to leaving the firm and for a new approach to analyze the potential effectiveness of action taken by a firm to change separation behavior: pre-implementation retention intervention assessment (PRI-assessment). We illustrate the application of the new approach using data on the observed separation behavior of 661 IS professionals at a large multidivision firm.
|keyword = binomial logit model,economic analysis,information systems professionals,information technology human capital,personnel separation,workforce retention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating determinants of software developers' intentions to follow methodologies'''
{{header}}
{{article
|author= BC Hardgrave,FD Davis,CK Riemenschneider,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Seeking to improve software development, many organizations attempt to deploy formalized methodologies. This typically entails substantial behavioral change by software developers away from previous informal practices toward conformance with the methodology. Developers' resistance to such change often results in failure to fully deploy and realize the benefits of the methodology. The present research draws upon theories of intention formation and innovation diffusion to advance knowledge about why developers accept or resist following methodologies. Results from a field study within a large organization indicate that developers' intentions are directly influenced by their perceptions of usefulness, social pressure, compatibility, and organizational mandate. This pattern of intention determinants is quite different from that typically observed in studies of information technology tool adoption, revealing several key differences between the domains of tool versus methodology adoption. Specifically, although organizational mandate had a significant effect on intentions, the strength of its direct influence was the lowest among the four significant constructs, and usefulness, compatibility, and social pressure all influenced intentions directly, above and beyond the effects of organizational mandate. The findings suggest, contrary to popular belief, that an organizational mandate is not sufficient to guarantee use of the methodology in a sustained manner.
|keyword = diffusion of innovations,innovation adoption,software development methodologies,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software piracy in the workplace: A model and empirical test'''
{{header}}
{{article
|author= AG Peace,DF Galletta,JYL Thong,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Theft of software and other intellectual property has become one of the most visible problems in computing today. This paper details the development and empirical validation of a model of software piracy by individuals in the workplace. The model was developed from the results of prior research into software piracy, and the reference disciplines of the theory of planned behavior, expected utility theory, and deterrence theory. A survey of 201 respondents was used to test the model. The results indicate that individual attitudes, subjective norms, and perceived behavioral control are significant precursors to the intention to illegally copy software. In addition, punishment severity, punishment certainty, and software cost have direct effects on the individual's attitude toward software piracy, whereas punishment certainty has a significant effect on perceived behavioral control. Consequently, strategies to reduce software piracy should focus on these factors. The results add to a growing stream of information systems research into illegal software copying behavior and have significant implications for organizations and industry groups aiming to reduce software piracy.
|keyword = computer ethics,deterrence theory,expected utility theory,software piracy,theory of planned behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Product-based workflow design'''
{{header}}
{{article
|author= HA Reijers,S Limam,WMP van der Aalst,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = In manufacturing, the interaction between the design of a product and the process to manufacture this product is studied in detail. Consider, for example, material requirements planning (MRP) as part of current enterprise resource planning (ERP) systems, which is mainly driven by the bill of material (BOM). For information-intensive products such as insurances, and many other services, the workflow process typically evolves or is redesigned without careful consideration of the structure and characteristics of the product. In this paper, we present a method named product-based workflow design (PBWD). PBWD takes the product specification and three design criteria as a starting point, after which formal models and techniques are used to derive a favorable new design of the workflow process. The ExSpect tool is used to support PBWD. Finally, using a real case study, we demonstrate that a full evaluation of the search space for a workflow design may be feasible depending on the chosen design criteria and the specific nature of the product specifications.
|keyword = business process redesign,formal methods,process design,workflows,workflow systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Identification of comment authorship in anonymous group support systems'''
{{header}}
{{article
|author= SC Hayne,CE Pollard,RE Rice,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = This study examines whether technically "anonymous" comments entered by participants during group support system (GSS) brainstorming sessions are, in fact, unidentifiable. Hypotheses are developed and tested about the influences of comment length, comment evaluative tone, duration of group membership, and prior communication among group members on the accuracy of attributions they made about the identity of the authors of these technically anonymous comments. Data on prior communication and group history about each of the 32 small groups was collected before participants began using a GSS for brainstorming. Immediately after the session, each member was asked to attribute authorship to a sample of the session's anonymous comments (comment authorship was known to the researchers). The study's participants made attributions that were significantly more accurate than chance guessing. Factors that had a positive influence on attribution accuracy include evaluative tone of comments (especially humorous comments) and amount of prior communication received from other group members. Vividness of comment tone and comment length was not significantly correlated with attribution accuracy. Although the attributions of anonymous comments were more accurate than expected by chance, most of the attributions were incorrect. Implications and consequences of both accurate and inaccurate attribution are discussed along with suggestions for future research.
|keyword = anonymity,computer-mediated communication,group support systems,social networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The identity crisis within the is discipline: Defining and communicating the discipline's core properties'''
{{header}}
{{article
|author= I Benbasat,RW Zmud,
|source= MIS QUARTERLY
|year= 2003
|abstract = We are concerned that the IS research community is making the discipline's central identity ambiguous by, all too frequently, under-investigating phenomena intimately associated with IT-based systems and over-investigating phenomena distantly associated with IT-based systems. In this commentary, we begin by discussing why establishing an identity for the IS field is important. We then describe what such an identity may look like by proposing a core set of properties, i.e., concepts and phenomena, that define the IS field. Next, we discuss research by IS scholars that either fails to address this core set of properties (labeled as error of exclusion) or that addresses concepts/phenomena falling outside this core set (labeled as error of inclusion). We conclude by offering suggestions for redirecting IS scholarship toward the concepts and phenomena that we argue define the core of the IS discipline.
|keyword = IS discipline,IT artifact,IT nomological net,errors of exclusion,errors of inclusion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reconceptualizing users as social actors in information systems research'''
{{header}}
{{article
|author= R Lamb,R Kling,
|source= MIS QUARTERLY
|year= 2003
|abstract = A concept of the user is fundamental to much of the research and practice of information systems design, development, and evaluation. User-centered information studies have relied on individualistic cognitive models to carefully examine the criteria that influence the selection of information and communication technologies (ICTs) that people make. In many ways, these studies have improved our understanding of how a good information resource fits the people who use it. However research approaches based on an individualistic user concept are limited. In this paper, we examine the theoretical constructs that shape this user concept and contrast these with alternative views that help to reconcep-tualize the user as a social actor. Despite pervasive ICT use, social actors are not primarily users of ICTs. Most people who use ICT applications utilize multiple applications, in various roles, and as part of their efforts to produce goods and interacting with a variety of other services while in people, and often in multiple social contexts. Moreover, the socially thin user construct limits our understanding of information selection, manipulation, communication, and exchange within complex social contexts. Using analyses from a recent study of online information service use, we develop an institutionalist concept of a social actor whose everyday interactions are infused with ICT use. We then encourage a shift from the user concept to a concept of the social actor in IS research. We suggest that such a shift will sharpen perceptions of how organizational contexts shape ICT-related practices, and at the same time. will help researchers more accurately portray the complex and multiple roles that people fulfill while adopting, adapting, and using information systems.
|keyword = social actor model,IS research methodology,socio-technical theory,social interaction,online use,IS users,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Virtualness and knowledge in teams: Managing the love triangle of organizations, individuals, and information technology'''
{{header}}
{{article
|author= TL Griffith,JE Sawyer,MA Neale,
|source= MIS QUARTERLY
|year= 2003
|abstract = Information technology can facilitate the dissemination of knowledge across the organization even to the point of making virtual teams a viable alternative to face-to-face, work. However, unless managed, the combination of information technology and virtual work may serve to change the distribution of different types of knowledge across individuals, teams, and the organization. Implications include the possibility that information technology plays the role of a jealous mistress when it comes to the development and ownership of valuable knowledge in organizations; that is, information technology may destabilize the relationship between organizations and their employees when it comes to the transfer of knowledge. The paper advances theory and informs practice by illustrating the dynamics of knowledge development and transfer in more and less virtual teams.
|keyword = group dynamics,organizational learning,knowledge acquisition,knowledge utilization,distributed work arrangements,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decentralized mechanism design for supply chain organizations using an auction market'''
{{header}}
{{article
|author= M Fan,J Stallaert,AB Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Traditional development of large-scale information systems is based on centralized information processing and decision making. With increasing competition, shorter product life-cycle, and growing uncertainties in the marketplace, centralized systems are inadequate in processing information that grows at an explosive rate and are unable to make quick responses to real-world situations. Introducing a decentralized information system in an organization is a challenging task. It is often intertwined with other organizational processes. The goal of this research is to outline a new approach in developing a supply chain information system with a decentralized decision making process. Particularly, we study the incentive structure in the decentralized organization and design a market-based coordination system that is incentive aligned, i.e., it gives the participants the incentives to act in a manner that is beneficial to the overall system. We also prove that the system monotonically improves the overall organizational performance and is goal congruent.
|keyword = decentralized information system,coordination mechanism,incentive,auction,complementarity,supply chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''XML-based schema definition for support of interorganizational workflow'''
{{header}}
{{article
|author= WMP van der Aalst,A Kumar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = The full potential of the Web as a medium for electronic commerce can be realized only when multiple partners in a supply chain can route information among themselves in a seamless way. Commerce on the Internet is still far from being "friction free," because business partners cannot exchange information about their business processes in an automated manner. In this paper, we propose the design for an eXchangeable Routing Language (XRL) using eXtensible Markup Language (XML) syntax. XML is a means for trading partners to exchange business data electronically. The novel contribution of our work is to show how XML can also be used to describe workflow process schemas to support flexible routing of documents in the Internet environment. The design of XRL is grounded in Petri nets, which is a well-known formalism. By using this formalism, it is possible to analyze correctness and performance of workflows described in XRL. Architectures to facilitate interoperation through loose and tight integration are also discussed. Examples illustrate how this approach can be used for implementing interorganizational electronic commerce applications.' As a proof of concept, we have also developed XRL/flower, a prototype implementation of a workflow management system based on XRL.
|keyword = workflow systems,XML,interorganizational workflows,e-business,Petri nets,workflow nets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Informational influence in organizations: An integrated approach to knowledge adoption'''
{{header}}
{{article
|author= SW Sussman,WS Siegal,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = This research investigates how knowledge workers are influenced to adopt the advice that they receive in mediated contexts. The research integrates the Technology Acceptance Model (Davis 1989) with dual-process models of informational influence (e.g., Petty and Cacioppo 1986, Chaiken and Eagly 1976) to build a theoretical model of information adoption. This model highlights the assessment of information usefulness as a mediator of the information adoption process. Importantly, the model draws on the dual-process models to make predictions about the antecedents of informational usefulness under different processing conditions. The model is investigated qualitatively first, using interviews of a sample of 40 consultants, and then quantitatively on another sample of 63 consultants from the same international consulting organization. Data reflect participants' perceptions of actual e-mails they received from colleagues consisting of advice or recommendations. Results support the model, suggesting that the process models used to understand information adoption can be generalized to the field of knowledge management, and that usefulness serves a mediating role between influence processes and information adoption. Organizational knowledge work is becoming increasingly global. This research offers a model for understanding knowledge transfer using computer-mediated communication.
|keyword = computer-mediated communication,field study,informational influence,e-mail,usefulness,information adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When subordinates become IT contractors: Persistent managerial expectations in IT outsourcing'''
{{header}}
{{article
|author= VT Ho,S Ang,D Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = This paper investigates the persistence of managerial expectations in an IT outsourcing context where the traditional relationship between supervisor and subordinate changes to one of client-manager and contractor. A mixed-method approach was used, in which a qualitative methodology preceded a large-scale quantitative survey. Data were collected from 147 survivors of a government IT organization which had undergone IT outsourcing in the previous year. Findings show that role overload, the presence of strong ties between manager and contractor, and the lack of prior outsourcing experience increased the persistence of managerial expectations. In turn, persistence of expectations had a distinct influence on managerial perceptions of contractor performance.
|keyword = IT outsourcing,persistent expectations,role overload,strength of ties,contractor performance,contract workers,changing employment status,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The social construction of meaning: An alternative perspective on information sharing'''
{{header}}
{{article
|author= SM Miranda,CS Saunders,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = Research on information sharing has viewed this activity as essential for informing groups on content relevant to a decision. We propose and examine an alternate function of information sharing, i.e., the social construction of meaning. To accomplish this goal, we turn to social construction, social presence, and task closure theories. Drawing from these theories, we hypothesize relationships among the meeting environment, breadth and depth of information shared during a meeting, and decision quality. We explore these relationships in terms of the effects of both the media environment in which the group is situated and the medium that group members choose to utilize for their communication. Our study of 32, 5- and 6-person groups supports our belief that interpretation underlies information sharing and is necessary for favorable decision outcomes. It also supports the proposed negative effect of low social presence media on interpretation in terms of depth of information sharing; a low social presence medium, however, promotes information sharing breadth. Finally, the findings indicate that when in multimedia environments and faced with a relatively complex task, choosing to utilize an electronic medium facilitates closure and, therefore, favorable outcomes.
|keyword = communication media,group support systems,social construction of meaning,intersubjective interpretation,social presence,information sharing,decision quality,task closure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information goods pricing and copyright enforcement: Welfare analysis'''
{{header}}
{{article
|author= YN Chen,I Png,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2003
|abstract = We consider how the government should set the fine for copying, tax on copying medium, and subsidy on legitimate purchases, whereas a monopoly publisher sets price and spending on detection. There are two segments of potential software users-ethical users who will not copy, and unethical users who would copy if the benefit outweighs the cost. In deciding on policy, the government must consider how the publisher adjusts price and detection to changes in the fine, tax, and subsidy. Our key welfare result is that increases in detection affect welfare more negatively than price cuts. We also show that the tax is welfare superior to the fine, and that a subsidy is optimal. Generally, government policies that focus on penalties alone will miss the social welfare optimum.
|keyword = copyright,pricing,enforcement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The DeLone and McLean model of information systems success: a ten-year update'''
{{header}}
{{article
|author= WH DeLone,ER McLean,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Ten years ago, we presented the DeLone and McLean Information Systems (IS) Success Model as a framework and model for measuring the complex-dependent variable in IS research. In this paper, we discuss many of the important IS success research contributions of the last decade, focusing especially on research efforts that apply, validate, challenge, and propose enhancements to our original model. Based on our evaluation of those contributions, we propose minor refinements to the model and propose an updated DeLone and McLean IS Success Model. We discuss the utility of the updated model for measuring e-commerce system success. Finally, we make a series of recommendations regarding current and future measurement of IS success.
|keyword = evaluation of information systems,impact of information technology,information quality,information systems success,service quality,systems quality,use of information systems,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaboration engineering with ThinkLets to pursue sustained success with group support systems'''
{{header}}
{{article
|author= RO Briggs,GJ De Vreede,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Field research and laboratory experiments suggest that, under certain circumstances, people using group support systems (GSS) can be significantly more productive than people who do not use them. Yet, despite their demonstrated potential, GSS have been slow to diffuse across organizations. Drawing on the Technology Transition Model, the paper argues that the high conceptual load of GSS (i.e., understanding of the intended effect of GSS functionality) encourages organizations to employ expert facilitators to wield the technology on behalf of others. Economic and political factors mitigate against facilitators remaining long term in GSS facilities that focus on supporting nonroutine, ad hoc projects. This especially hampers scaling GSS technology to support distributed collaboration. An alternative and sustainable way for organizations to derive value from GSS lies in an approach called "collaboration engineering": the development of repeatable collaborative processes that are conducted by practitioners themselves. To enable the development of such processes, this paper proposes the thinkLet concept, a codified packet of facilitation skill that can be applied by practitioners to achieve predictable, repeatable patterns of collaboration, such as divergence or convergence. A thinkLet specifies the facilitator's choices and actions in terms of the GSS tool used, the configuration of this tool, and scripted prompts to accomplish a pattern of collaboration in a group. Using thinkLets as building blocks, facilitators can develop and transfer repeatable collaborative processes to practitioners. Given the limited availability of expert facilitators, collaboration engineering with thinkLets may become a sine qua non for organizations to effectively support virtual work teams.
|keyword = collaboration engineering,collaboration technology,group support systems,technology acceptance model (TAM),technology adoption,technology transfer,technology transition model (TTM),thinkLets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward an understanding of satisfaction with the process and outcomes of teamwork'''
{{header}}
{{article
|author= BA Reinig,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Collaborative technologies such as group support systems (GSS) are often developed to improve the effectiveness and efficiency of teams; however, the satisfaction users have with the processes and outcomes of the teamwork itself often determines the ultimate adoption and sustained use of collaborative technologies. Much of the research on teamwork has focused on meetings in particular and, consequently, satisfaction with the process and outcomes of meetings, referred to collectively as meeting satisfaction. Research on meeting satisfaction in GSS-supported groups has been equivocal, indicating the need for advancement in our theoretical understanding of the construct. To that end, this paper presents a causal model of meeting satisfaction derived from goal setting theory. The model is tested with an empirical study consisting of 15 GSS groups and 11 face-to-face (FTF) groups engaged in the "lost at sea" task. The results of analysis using structural equation modeling indicate support for the model's integrity across both GSS and FTF groups. Implications for researchers and practitioners are discussed, including how the model can be used to improve future research on the use of collaborative technology to support teamwork.
|keyword = collaborative technology,goal setting theory,group support systems,meeting satisfaction,teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modeling and simulation for mission operations work system design'''
{{header}}
{{article
|author= M Sierhuis,WJ Clancey,C Seah,JP Trimble,MH Sims,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Work system analysis and design is complex and nondeterministic. In this paper we describe Brahms, a multiagent modeling and simulation environment for designing complex interactions in human-machine systems. Brahms was originally conceived as a business process design tool that simulates work practices, including social systems of work. We describe our modeling and simulation method for mission operations work systems design, based on a research case study in which we used Brahms to design mission operations for a proposed discovery mission to the Moon. We then describe the results of an actual method application project-the Brahms Mars Exploration Rover. Space mission operations are similar to operations of traditional organizations; we show that the application of Brahms for space mission operations design is relevant and transferable to other types of business processes in organizations.
|keyword = agent languages,business process modeling,mission operations design,multiagent simulation,work practices,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Because time matters: Temporal coordination in global virtual project teams'''
{{header}}
{{article
|author= AP Massey,MM Montoya-Weiss,YT Hung,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = In this study, we explore the nature of team interaction and the role of temporal coordination in asynchronously communicating global virtual project teams (GVPT). Drawing on Time, Interaction, and Performance (TIP) theory, we consider how and why virtual team behavior is temporally patterned in complex ways. We report on the results of an experiment consisting of 35 virtual project teams comprised of 175 members residing in the United States and Japan. Through content and cluster analysis, we identify distinct patterns of interaction and examine how these patterns are associated with differential levels of GVPT performance. We also explore the role of temporal coordination mechanisms as a means to synchronize temporal patterns in GVPTs. Our results suggest that successful enactment of temporal coordination mechanisms is associated with higher performance. However, we found that temporal coordination per se is not the driver of performance; rather, it is the influence of coordination on interaction behaviors that affects performance.
|keyword = cluster analysis,computer-mediated communication,temporal coordination,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Team boundary issues across multiple global firms'''
{{header}}
{{article
|author= JA Espinosa,JN Cummings,JM Wilson,BM Pearce,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Numerous methodological issues arise when studying teams that span multiple boundaries. The main purpose of this paper is to raise awareness about the challenges of conducting field research on teams in global firms. Based on field research across multiple firms (software development, product development, financial services, and high technology), we outline five types of boundaries that we encountered in our field research (geographical, functional, temporal, identity, and organizational) and discuss methodological issues in distinguishing the effects of one boundary where multiple boundaries exist. We suggest that it is important to: (1) appropriately measure the boundary of interest to the study, (2) assess and control for other influential boundaries within and across teams, and (3) distinguish the effects of each boundary on each team outcome of interest. Only through careful attention to methodology can we properly assess the effects of team boundaries and appreciate their research and practical implications for designing and using information systems to support collaborative work.
|keyword = cross-functional teams,distributed teams,global teams,interorganizational teams multiple boundaries,organizational forms,research methods,virtual teams,work groups,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Generating and browsing multiple taxonomies over a document collection'''
{{header}}
{{article
|author= S Spangler,JT Kreulen,J Lessler,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = We present a novel system and methodology for generating and then browsing multiple taxonomies over a document collection. Taxonomies are generated using a broad set of capabilities, including meta data, key word queries, and automated clustering techniques that serve as a seed taxonomy. The taxonomy editor, eClassifier, provides powerful tools to visualize and edit each taxonomy to make it reflective of the desired theme. Cluster validation tools allow the editor to verify that documents received in the future can be automatically classified into each taxonomy with sufficiently high accuracy. In general, those seeking knowledge from a document collection may have only a vague notion of exactly what they are attempting to understand, and would like to explore related topics and concepts rather than simply being given a set of documents. For this purpose, we have developed MindMap, an interface utilizing multiple taxonomies and the ability to interact with a document collection.
|keyword = data mining,document classification,document clustering techniques,knowledge management,navigation,taxonomy,text mining,visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A methodology for analyzing web-based qualitative data'''
{{header}}
{{article
|author= NC Romano,C Donovan,HC Chen,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = The volume of qualitative data (QD) available via the Internet is growing at an increasing pace and firms are anxious to extract and understand users' thought processes, wants and needs, attitudes, and purchase intentions contained therein. An information systems (IS) methodology to meaningfully analyze this vast resource of QD could provide useful information, knowledge, or wisdom firms could use for a number of purposes including new product development and quality improvement, target marketing, accurate "user-focused" profiling, and future sales prediction. In this paper, we present an IS methodology for analysis of Internet-based QD consisting of three steps: elicitation; reduction through IS-facilitated selection, coding, and clustering; and visualization to provide at-a-glance understanding. Outcomes include information (relationships), knowledge (patterns), and wisdom (principles) explained through visualizations and drill-down capabilities. First we present the generic methodology and then discuss an example employing it to analyze free-form comments from potential consumers who viewed soon-to-be-released film trailers provided that illustrates how the methodology and tools can provide rich and meaningful affective, cognitive, contextual, and evaluative information, knowledge, and wisdom. The example revealed that qualitative data analysis (QDA) accurately reflected film popularity. A finding is that QDA also provided a predictive measure of relative magnitude of film popularity between the most popular film and the least popular one, based on actual first week box office sales. The methodology and tools used in this preliminary study illustrate that value can be derived from analysis of Internet-based QD and suggest that further research in this area is warranted.
|keyword = attitudes and purchase intentions,clustering,coding,elicitation,future sales predictions,information systems (IS),qualitative data analysis (QDA) methodology,reduction,selection,visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relating collaborative technology use to teamwork quality and performance: An empirical analysis'''
{{header}}
{{article
|author= RF Easley,S Devaraj,JM Crant,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2003
|abstract = Although team-based work systems are pervasive in the workplace, the use of collaborative systems designed to facilitate and support ongoing teamwork is a relatively recent development. An understanding of how teams embrace and use such collaborative systems-and the relationship of that usage to teamwork quality and team performance-is critical for organizational success. We present a theoretical model in which usage of a collaborative system intervenes between teamwork quality and team performance for tasks that are supported by the system. We empirically validate the model in a setting where established teams voluntarily used a collaborative system over a four-month period to perform tasks with measurable outcomes. Our principal finding is that collaborative system use intervenes between teamwork quality and performance for tasks supported by the system but not for unsupported tasks.
|keyword = collaboration technology,collaborative systems,computer self-efficacy (CSE),group support systems (GSS),groupware,team performance,teamwork,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting intention to adopt interorganizational linkages: An institutional perspective'''
{{header}}
{{article
|author= HH Teo,KK Wei,I Benbasat,
|source= MIS QUARTERLY
|year= 2003
|abstract = This study used institutional theory as a lens to understand the factors that enable the adoption of interorganizational systems. It posits that mimetic, coercive, and normative pressures existing in an institutionalized environment could influence organizational predisposition toward an information technology-based interorganizational linkage. Survey-based research was carried out to test this theory. Following questionnaire development, validation, and pretest with a pilot study, data were collected from the CEO, the CFO, and the CIO to measure the institutional pressures they faced and their intentions to adopt financial electronic data interchange (FEDI). A firm-level structural model was developed based on the CEO's, the CFO's, and the CIO's data. LISREL and PLS were used for testing the measurement and structural models respectively. Results showed that all three institutional pressures-mimetic pressures, coercive pressures, and normative pressures-had a significant influence on organizational intention to adopt FEDI. Except for perceived extent of adoption among suppliers, all other subconstructs were significant in the model. These results provide strong support for institutional-based variables as predictors of adoption intention for interorganizational linkages. These findings indicate that organizations are embedded in institutional networks and call for greater attention to be directed at understanding institutional pressures when investigating information technology innovations adoption.
|keyword = financial electronic data interchange,interorganizational systems,institutional influences,mimetic pressures,coercive pressures,normative pressures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Issues in linking information technology capability to firm performance'''
{{header}}
{{article
|author= R Santhanam,E Hartono,
|source= MIS QUARTERLY
|year= 2003
|abstract = The resource-based view has been proposed to investigate the impact of information technology (IT) investments on firm performance. Researchers have shown that a firm's ability to effectively leverage its IT investments by developing a strong IT capability can result in improved firm performance. We test the robustness of this approach and examine several related issues. Our results indicate that firms with superior IT capability indeed exhibit superior current and sustained firm performance when compared to average industry performance, even after adjusting for effects of prior firm performance. However, the differences in the results from various analyses suggest that the impact of "halo effects" and prior financial performance of firms must be taken into consideration in future tests of IT capability, Further, it is critical to develop theoretically derived multidimensional measures of IT capability in order to continue to apply the RBV approach to assess the impact of IT investments on firm performance.
|keyword = resource-based view,IT capability,firm performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CIO lateral influence behaviors: Gaining peers' commitment to strategic information systems'''
{{header}}
{{article
|author= HG Enns,SL Huff,CA Higgins,
|source= MIS QUARTERLY
|year= 2003
|abstract = In order to develop and bring to fruition strategic information systems (SIS) projects, chief information officers (CIOs) must be able to effectively influence their peers. This research examines the relationship between CIO influence behaviors and the successfulness of influence outcomes, utilizing a revised model initially developed by Yukl (1994). Focused interviews were first conducted with CIOs and their peers to gain insights into the phenomenon. A survey instrument was then developed and distributed to a sample of CIO and peer executive pairs to gather data with which to test a research model. A total of 69 pairs of surveys were eventually used for data analysis. The research model was found to be generally meaningful in the CIO-top management context. Furthermore, the influence behaviors rational persuasion and personal appeal exhibited significant relationships with peer commitment, whereas exchange and pressure were significantly related to peer resistance. These results provide useful guidance to CIOs who wish to propose strategic information systems to peers.
|keyword = chief information officers,influence,influence behaviors,information systems,PLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Information systems and conceptual modeling - A research agenda'''
{{header}}
{{article
|author= Y Wand,R Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Within the information systems field, the task of conceptual modeling involves building a representation of selected phenomena in some domain. High-quality conceptual-modeling work is important because it facilitates early detection and correction of system development errors. It also plays an increasingly important role in activities like business process reengineering and documentation of best-practice data and process models in enterprise resource planning systems. Yet little research has been undertaken on many aspects of conceptual modeling. In this paper, we propose a framework to motivate research that addresses the following fundamental question: How can we model the world to better facilitate our developing, implementing, using, and maintaining more valuable information systems? The framework comprises four elements: conceptual-modeling grammars, conceptual-modeling methods, conceptual-modeling scripts, and conceptual-modeling contexts. We provide examples of the types of research that have already been undertaken on each element and illustrate research opportunities that exist.
|keyword = conceptual modeling,information systems development,ontology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: The next wave of Nomadic computing'''
{{header}}
{{article
|author= K Lyytinen,Y Yoo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = A nomadic information environment is a heterogeneous assemblage of interconnected technological, and social, and organizational elements that enable the physical and social mobility of computing and communication services between organizational actors both within and across organizational borders. We analyze such environments based on their prevalent features of mobility, digital convergence, and mass scale, along with their mutual interdependencies. By using a framework that organizes research topics in nomadic information environments at the individual, team, organizational, and interorganizational levels and is comprised of both service and infrastructure development, we assess the opportunities and challenges for IS research. These deal with the design, use, adoption, and impacts of nomadic information environments. We conclude by discussing research challenges posed by nomadic information environments for information systems research skills and methods. These deal with the need to invent novel research methods and shift our research focus, the necessity to question the divide between the technical and the social, and the need to better integrate developmental and behavioral (empirical) research modes.
|keyword = mobile computing,IS research,information environments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The security of confidential numerical data in databases'''
{{header}}
{{article
|author= R Sarathy,K Muralidhar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Organizations are storing large amounts of data in databases for data mining and other types of analysis. Some of this data is considered confidential and has to be protected from disclosure. When access to individual values of confidential numerical data in the database is prevented, disclosure may occur when a snooper uses linear models to predict individual values of confidential attributes using nonconfidential numerical and categorical attributes. Hence, it is important for the database administrator to have the ability to evaluate security for snoopers using linear models. In this study we provide a methodology based on Canonical Correlation Analysis that is both appropriate and adequate for evaluating security. The methodology can also be used to evaluate the security provided by different security mechanisms such as query restrictions and data perturbation. In situations where the level of security is inadequate, the methodology provided in this study can also be used to select appropriate inference control mechanisms. The application of the methodology is illustrated using a simulated database.
|keyword = confidentiality,data perturbation,database security,inferential disclosure,inferential security,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A comparative study of distributed learning environments on learning outcomes'''
{{header}}
{{article
|author= M Alavi,GM Marakas,Y Yoo,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Advances in information and communication technologies have fueled rapid growth in the popularity of technology-supported distributed learning (DL). Many educational institutions, both academic and corporate, have undertaken initiatives that leverage the myriad of available DL technologies. Despite their rapid growth in popularity, however, alternative technologies for DL are seldom systematically evaluated for learning efficacy. Considering the increasing range of information and communication technologies available for the development of DL environments, we believe it is paramount for studies to compare the relative learning outcomes of various technologies. In this research, we employed a quasi-experimental field study approach to investigate the relative learning effectiveness of two collaborative DL environments in the context of an executive development program. We also adopted a framework of hierarchical characteristics of group support system (GSS) technologies, outlined by DeSanctis and Gallupe (1987), as the basis for characterizing the two DL environments. One DL environment employed a simple e-mail and listserv capability while the other used a sophisticated GSS (herein referred to as Beta system). Interestingly, the learning outcome of the e-mail environment was higher than the learning outcome of the more sophisticated GSS environment. The post-hoc analysis of the electronic messages indicated that the students in groups using the e-mail system exchanged a higher percentage of messages related to the learning task. The Beta system users exchanged a higher level of technology sense-making messages. No significant difference was observed in the students' satisfaction with the learning process under the two DL environments.
|keyword = technology-supported learning,distributed learning,learning assessment,learning models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Potential research space in MIS: A framework for envisioning and evaluating research replication, extension, and generation'''
{{header}}
{{article
|author= P Berthon,L Pitt,M Ewing,CL Carr,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Replications are an important component of scientific method in that they convert tentative belief to accepted knowledge. Given the espoused importance of replications to the extraction of knowledge from research, there is surprisingly little evidence of its practice or discussion of its importance in the management information systems literature. In this article we develop a framework within which to systematize the conceptualization of replications; we review and illustrate how some key information systems research fits into the framework and examine the factors that influence the selection of a research strategy. Our framework includes a conceptualization of the relationship among replication, extension, and generation in IS research. The concept of "research space" is defined and a framework is developed that delineates eight possible research strategies. Finally, the benefits of our framework to salient stakeholders in the research process are outlined.
|keyword = research issues,research methodology,research models,research status,research,theoretical evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Intrafirm resource allocation with asymmetric information and negative externalities'''
{{header}}
{{article
|author= R Nadiminti,T Mukhopadhyay,CH Kriebel,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = We examine the intrafirm resource allocation problem with the following characteristics. The resource exhibits negative externalities, and the benefit of using the resource is known only to the user department and not to top management or other user departments. In addition, the consumption of the resource depends upon the choice of the mechanism for allocating the resource. For this problem, we derive a two-stage mechanism, and show that this proposed mechanism leads to optimal allocation.
|keyword = resource,allocation,asymmetric information,negative externalities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Newly vulnerable markets in an age of pure information products: An analysis of Online music and Online news'''
{{header}}
{{article
|author= EK Clemons,B Gu,KR Lang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = We describe the emerging competition between music companies and their star acts and the role of online distribution in this industry. We then contrast this with the lack of competition newspapers will face from their reporters, writers, and photographers, but identify other possible competitors for newspaper publishers. We examine what resources have previously enabled record companies to lock in their star acts and ways in which technology has altered artists' abilities to reach the market independently and thus their dependency upon record companies. We examine which resources have seen their value eroded in the newspaper industry and the remaining value that the newspaper company still creates, other than bundling stories, adding advertising, and printing and selling the papers. We consider what part of the newspaper business is vulnerable, if any, and where threats may arise. We combine the resource-based view of competitive advantage to examine which industry may have become newly easy to enter, and the theory of newly vulnerable markets to assess which industry may actually have become vulnerable as a result. Our analyses are then used to create a computer simulation model to make the implications more explicit under a range of assumptions.
|keyword = electronic commerce,music industry,newly vulnerable markets,newspaper industry,resource-based competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A model of neutral B2B'''
{{header}}
{{article
|author= B Yoo,V Choudhary,T Mukhopadhyay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Business-to-business (B2B) electronic commerce has become an important issue in the debate about electronic commerce. How should the intermediary charge suppliers and buyers to maximize profits from such a marketplace? We analyze a monopolistic B2B marketplace owned by an independent intermediary. The marketplace exhibits two-sided network effects where the value of the marketplace, to buyers is dependent on the number of suppliers, and the value to suppliers is dependent on the number of buyers and suppliers. When these two-sided network effects exist, we find that the optimal price for buyers and the fraction of buyers in the electronic market are dependent on the, switching cost and the strength of the network effect of both types: buyers and suppliers,. The same is,true for the optimal price for suppliers and the fraction of suppliers in the electronic market. In other words, the parameters that define the buyers also affect the optimal,price for suppliers and the fraction of suppliers in the electronic market, and vice-versa. Our results. also point to some counterintuitive optimal pricing strategies that depend on the nature of the industry served by the marketplace.
|keyword = business-to-business e-commerce,intermediation,network effect,pricing strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Market segmentation and information development costs in a two-tiered fee-based and sponsorship-based web site'''
{{header}}
{{article
|author= FJ Riggins,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = We develop an analytical model of a separating equilibrium for a two-tier fee-based and sponsorship-based information Web site. We examine the monopolist's choice of content quality and price for a fee-based site targeted at high-type consumers and the content quality level for a sponsored site offered free to all consumers. We show how a reduction in the potential for advertising revenues results in lower content quality on the free site, but permits the seller to raise the fee charged to high-type consumers. We also show how differences in consumer tolerances to ads affects content quality, banner ad volume, and usage fees. In particular, the seller can increase profits by making ads more attractive to either high- or low-type consumers, but rarely both at the same time. We show the conditions that determine which consumer segment the seller should seek to improve ad relevancy.
|keyword = information Web sites,online market segmentation,product cannibalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Management of valuation of advertisement-supported web sites'''
{{header}}
{{article
|author= RM Dewan,ML Freimer,H Zhang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = A key decision by the manager of an advertisement-supported Web site is the balance between content and advertising. Content is costly but attracts viewers, whereas advertisement generates revenues but repels viewers. The period-by-period balancing decision is further complicated by the growth and diffusion. nature of Web site viewership This decision problem is modeled as a control problem that captures the essence of the business model of such Web sites. Using,this model we show that it may be optimal for the Web site to initially have negative cash flows fewer advertisement's and more content. This is more than compensated for by future profits from the Web site. We use the solution to the control, problem to also, develop p a forward-looking measure of Web traffic called the, "discounted total traffic." We empirically examine this new measure and find that it better predicts capitalization than backward-looking measures like page views.
|keyword = advertising policy,content management,market capitalization,Web traffic,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evolution of prices in electronic markets under diffusion of price-comparison shopping'''
{{header}}
{{article
|author= C Kocas,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Price-comparison engines allow customers to compare product offerings of online sellers and reveal almost complete information on the alternatives, and hence create erosion in store loyalty. Consequently, the competitive dynamics of online sales are affected in markets where price-comparison shopping is diffusing rapidly. We develop a dynamic competitive pricing model that-deals with an asymmetric, duopolistic market where the segment sizes are determined through a diffusion process. Our diffusion-of-innovations approach allows us to dynamically capture, the proportion of informed and uninformed customers in-a homogenous' goods market. We use this model to analyze how strategic profit maximization behavior evolves,over time. this analysis shows that the increasing numbers of price-comparison shoppers pull prices down, and the rate at which prices decrease is shaped by the diffusion. curve and brand preference. Our analysis shows that stores with loyal customers, or with a preference for their brands, can attain higher profits I further into the-diffusion process. The direct implication is that firms should use their information technology;. operations, and marketing capabilities to create, enhance, and cultivate stronger preferences ford and loyalty to, their brand names to survive the inevitable information-rich markets of tomorrow.
|keyword = diffusion,of innovations,Internet economics,price-comparison engines,price-comparison shopping,price competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A query-driven approach to the design and management of flexible database systems'''
{{header}}
{{article
|author= ANK Chen,PB Goes,JR Marsden,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = The need for timely information in the e-business world provides the impetus to develop a flexible database system with the capability to adapt and maintain performance levels under changing queries and changing business environments. Recognizing the importance of providing fast access to a variety of read-only applications in today's e-business world, we introduce the systems architecture for developing and implementing a flexible database system to achieve considerable gains in processing times of read queries. The key component of a flexible database system is query mining, the concept of determining relationships among query properties, alternative database structures, and query processing times. We validate the flexible database system concept through extensive laboratory experiments, where we embed learning tools to demonstrate the implementation of query mining.
|keyword = database management,database querying,data mining,inductive learning,information retrieval,neural networks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of local versus global schema diagrams on verification and communication conceptual data modeling'''
{{header}}
{{article
|author= J Parsons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Much research in conceptual data modeling has focused on developing techniques for view integration, or combining local conceptual schemas into a global schema. Local schemas are argued to be important in verifying conceptual data. requirements before proceeding to database design. View integration is claimed to fulfill two purposes. First, a global conceptual schema is a prerequisite to logical design and implementation.. Second, global schemas are thought,to be useful in improving organizational, communication among diverse user, groups, with different perspectives and information needs. However, performing view integration is difficult. Moreover, there i no empirical evidence that global schemas either impede local verification or support communication. Drawing. on classification research, this paper develops and tests claims about the impact of L schema structure (local versus. global) on verification and communication. Local schemas are hypothesized to better support verify cation than global-schemas. When different local views contain conflicting structure, local schemas are expected to be superior in supporting communication. However, when local views contain complementary structure, global schemas. are expected to be superior in supporting communication. A laboratory experiment was conducted to test these predictions. The results-support the hypotheses. Implications for the practice of database design and for further research are considered.
|keyword = classification theory,conceptual database models,data-base schemas,empirical data modeling research,view integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of group support systems and content facilitation on knowledge acquisition'''
{{header}}
{{article
|author= RCW Kwok,R Ma,DR Vogel,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = This paper investigates the effects of group support systems (GSS) and content facilitation on individual knowledge acquisition in general, and on changes in an individual's knowledge structures in particular as indicated through concept mapping development. We present a model explaining the enabling effects of GSS and content facilitation on group processes (group participation, quality of feedback, domination, and communication barrier), cooperation in learning, and individual knowledge structures (knowledge complexity, integration, and commonality). An experiment that employed a 2X2 factorial design was used to explore the main and interaction effects of GSS and content facilitation on knowledge acquisition. Experimental subjects were randomly assigned to one of the four treatment groups; that is, nonfacilitation and non-GSS, nonfacilitation and GSS, facilitation and non-GSS, facilitation and GSS. Results of the experiment indicated that both GSS and content facilitation positively affect certain aspects of individual knowledge acquisition. Content facilitation particularly enhanced learners' knowledge commonality, whereas GSS enhanced the quality of feedback and cooperation in learning, and reduced domination and communication barrier. However, the results also indicated that GSS and content facilitation have crossover interaction effects on group participation and knowledge commonality. The effects of combining GSS and content facilitation were not additive in this study. Explanations are presented.
|keyword = collaborative learning,facilitation,group support system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Turnover of information technology workers: Examining empirically the influence of attitudes, job characteristics, and external markets'''
{{header}}
{{article
|author= JB Thatcher,LP Stepina,RJ Boyle,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = This paper presents and tests a conceptual model, linking perceptions of the internal work environment and external markets to information technology (IT) worker turnover. The model focuses on organizational commitment (OC) as the primary predictor of turnover intention. We suggest that OC mediates perceptions of the workplace and external environment on turnover intention Specifically, we hypothesize that OC mediates the influence of (1) job satisfaction, (2) perceived job characteristics , (3) perceived competitiveness of pay, aind (4) perceived job alternatives on turnover intention. Also, perceived job alternatives are modeled as having a direct effect on turnover intention. Analysis provides moderate empirical support for the research model. OC and perceived job alternatives demonstrated distinct effects on turnover intention. In addition, OC mediated the influence of job satisfaction, perceived job characteristics, and perceived competitiveness of pay on turnover intention. Findings suggest that through cultivating positive beliefs about the job and attitudes toward the employer, managers may counter the influence of external markets on IT workers' turnover intention.
|keyword = information-technology worker turnover,job characteristics,job satisfaction,organizational commitment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding network effects in software markets: Evidence from Web server pricing'''
{{header}}
{{article
|author= JM Gallaugher,YM Wang,
|source= MIS QUARTERLY
|year= 2002
|abstract = Prior theoretical research has established that many software products are subject to network effects and exhibit the characteristics of two-sided markets. However, despite the importance of the software industry to the world economy, few studies have-attempted to empirically examine these characteristics, or several others which theory suggests impact software price. This study develops and tests a research-grounded model of two-sided software markets that accounts for several key factors influencing software pricing, including network externalities, cross-market complementarities, standards, mindshare, and trialability. Applying the model to the context of the market for Web server software, several key findings are offered. First, a positive market share to price relationship is identified, offering support for the network externalities hypothesis even though the market examined is based on open standards. Second, the results suggest that the market under study behaves as a two-sided market in-that firms able to capture market share for one product enjoy benefits in terms of both market share and price for the complement Third, the positive price benefits of securing consumer mindshare, of supporting dominant standards, and from offering a trial product are demonstrated. Last, a negative price shock is also identified in the period after a well-known, free-pricing rival has entered the market. Nonetheless, network effects' continued to remain significant during the period. Theses findings enhance our understanding of software markets, offer new techniques for examining such markets, and suggest the wisdom of allocating resources to develop advantages in the factors studied.
|keyword = Hedonic pricing,network effects,network externalities,composite goods,two-sided markets,open standards,mindshare,trialability,World Wide Web server market,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cross-cultural software production and use: A structurational analysis'''
{{header}}
{{article
|author= G Walsham,
|source= MIS QUARTERLY
|year= 2002
|abstract = This paper focuses on cross-cultural software production and use, which is increasingly common in today's more globalized world. A theoretical basis for analysis is developed, using concepts drawn from structuration theory. The theory is illustrated using two cross-cultural case studies. It is argued that structurational analysis provides a deeper examination of cross-cultural working and IS than is found in the current literature, which is dominated by Hofstede-type studies. In particular, the theoretical approach can be used to analyze cross-cultural conflict and contradiction, cultural heterogeneity, detailed work patterns, and the dynamic nature of culture. The paper contributes to the growing body of literature that emphasizes the essential role of cross-cultural understanding in contemporary society.
|keyword = globalization,cross-cultural work,structuration theory,software development,technology transfer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical examination of individual traits as antecedents to computer anxiety and computer self-efficacy'''
{{header}}
{{article
|author= JB Thatcher,PL Perrewe,
|source= MIS QUARTERLY
|year= 2002
|abstract = To better understand how individual differences influence the use of information technology (IT), this study models and tests relationships among dynamic, IT-specific individual differences (i.e., computer self-efficacy and computer anxiety), stable, situation-specific traits (i.e., personal innovativeness in IT) and stable, broad traits (i.e., trait anxiety and negative affectivity). When compared to broad traits, the model suggests that situation-specific traits exert a more pervasive influence on IT situation-specific individual differences. Further, the model suggests that computer anxiety mediates the influence of situation-specific traits (i.e., personal innovativeness) on computer self-efficacy. Results provide support for many of the hypothesized relationships. From a theoretical perspective, the findings help to further our understanding of the nomological network among individual differences that lead to computer self-efficacy. From a practical perspective, the findings may help IT managers design training programs that more effectively increase the computer self-efficacy of users with different dispositional characteristics.
|keyword = self-efficacy,anxiety,personality,negative affectivity,personal innovativeness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward new metrics for net-enhanced organizations'''
{{header}}
{{article
|author= DW Straub,DL Hoffman,BW Weber,C Steinfield,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Metrics are sine qua non for solid research, and scientific metrics have now been advanced with new approaches in the arena of Net-enablement (NE), otherwise known as e-commerce. Questions that likely require additional attention include: (1) Where/what is the real value in substituting information for physical processes?, (2) which NE systems effectively support end-to-end fulfillment?, and (3) when should a Net-enabled organization share information? With respect to extant studies in Net-enhancement, the field has been advanced in three methodological dimensions. Multiple methods have been used to validate measures. Approaches to metrics using archival/secondary data have also been initiated. Finally, strong external validity has been established through large scale data gathering.
|keyword = metrics,measurement,research constructs,e-commerce,net-enablement,net-enhancement,frameworks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Businesses as buildings: Metrics for the architectural quality of Internet businesses'''
{{header}}
{{article
|author= J Kim,L Jung,K Han,M Lee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Metrics for the architectural quality of Internet businesses are essential in gauging the success and failure of e-commerce. This study proposes six dimensions of architectural metrics for Internet businesses: internal stability, external security, information gathering, order processing, system interface, and communication interface. The metrics are based on the three constructs that have been used to evaluate buildings in the real world. The structural construct indicates that Internet businesses need to be stable internally and secure externally. The functional construct implies that Internet businesses should provide convenient functions in the information-gathering and order-processing phases. Finally, the representational construct indicates that they need to provide a pleasant interface both to the system and to those using it. For each of the six metrics, we have constructed questionnaires to measure the perceived level of architectural quality and identified feature lists that may be closely related to the perceived quality level. Large-scale empirical studies were conducted both to validate the proposed metrics and to explore their relevance across four Internet business domains. The validity of the metrics has been obtained in three ways. First, the content validity of the metrics was assured by pretests and pilot survey. Second, the results from the confirmatory factor analysis showed that the metrics had high convergent and discriminant validities. Finally, the reliability coefficients were found to be high enough to establish the reliability of the proposed metrics. The relevance of the metrics has been explored in two ways. Structural equation models were used to test the causal relations between the three constructs and user satisfaction, as well as customer loyalty, in four domains. Correlation analyses were used to explore the relations between the perceived architectural quality and objective design features in four domains. This paper ends with the implications and limitations of the study results.
|keyword = Internet business,architectural quality,structural firmness,functional convenience,representational delight,subjective questions,objective feature lists,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring switching costs and the determinants of customer retention in Internet-enabled businesses: A study of the Online brokerage industry'''
{{header}}
{{article
|author= PY Chen,LM Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = The ability to retain and lock in customers in the face of competition is a major concern for online businesses, especially those that invest heavily in advertising and customer acquistion. In this paper, we develop and implement an approach for measuring the magnitudes of switching costs and brand loyalty for online service providers based on the random utility modeling framework. We then examine how systems usage, service design, and other firm and individual-level factors affect switching and retention. Using data on the online brokerage industry, we find significant variation (as much as a factor of two) in measured switching costs. We find that customer demographic characteristics have little effect on switching, but that systems usage measures and systems quality are associated with reduced switching. We also find that firm characteristics such as product line breadth and quality reduce switching and may also reduce customer attrition. Overall, we conclude that online brokerage firms appear to have different abilities in retaining customers and have considerable control over their switching costs.
|keyword = switching cost,electronic markets,customer retention,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''e-Commerce metrics for net-enhanced organizations: Assessing the value of e-commerce to firm performance in the manufacturing sector'''
{{header}}
{{article
|author= K Zhu,KL Kraemer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = In this study, we developed a set of constructs to measure e-commerce capability in Internet-enhanced organizations. The e-commerce capability metrics consist of four dimensions: information, transaction, customization, and supplier connection. These measures were empirically validated for reliability, content, and construct validity. Then we examined the nomological validity of these e-commerce metrics in terms of their relationships to firm performance, with data from 260 manufacturing companies divided into high IT-intensity and low IT-intensity sectors. Grounded in the dynamic capabilities perspective and the resource-based theory of the firm, a series of hypotheses were developed. After controlling for variations of industry effects and firm size, our empirical analysis found a significant relationship between e-commerce capability and some measures of firm performance (e.g., inventory turnover), indicating that the proposed metrics have demonstrated value for capturing e-commerce effects. However, our analysis showed that e-commerce tends to be associated with the increased cost of goods sold for traditional manufacturing companies, but there is an opposite relationship for technology companies. This result seems to highlight the role of resource complementarity for the business value of e-commerce-traditional companies need enhanced alignment between e-commerce capability and their existing IT infrastructure to reap the benefits of e-commerce.
|keyword = electronic commerce,IT intensity,e-commerce metrics,measurement,validation,firm performance,net-enhanced organizations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The measurement of web-customer satisfaction: An expectation and disconfirmation approach'''
{{header}}
{{article
|author= V McKinney,K Yoon,F Zahedi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Online shopping provides convenience to Web shoppers, yet its electronic format changes information-gathering methods traditionally used by customers. This change raises questions concerning customer satisfaction with the online purchasing process. Web shopping involves a number of phases, including the information phase, in which customers search for information regarding their intended purchases. The purpose of this paper is to develop theoretically justifiable constructs for measuring Web-customer satisfaction during the information phase. By synthesizing the expectation-disconfirmation paradigm with empirical theories in user satisfaction, we separate Web site quality into information quality (IQ) and system quality (SQ), and propose nine key constructs for Web-customer satisfaction. The measurements for these constructs are developed and tested in a two-phase study. In the first phase, the IQ and SQ dimensions are identified, and instruments for measuring them are developed and tested. In the second phase, using the salient dimensions of Web-IQ and Web-SQ as the basis for formulating first-order factors, we develop and empirically test instruments for measuring IQ-and SQ-satisfaction. Moreover, this phase involves the design and test of second-order factors for measuring Web-customer expectations, disconfirmation, and perceived performance regarding IQ and SQ. The analysis of the measurement model indicates that the proposed metrics have a relatively high degree of validity and reliability. The results of the study provide reliable instruments for operationalizing the key constructs in the analysis of Web-customer satisfaction within the expectation-disconfirmation paradigm.
|keyword = web customer,satisfaction,information quality,system quality,web-information satisfaction,web-system satisfaction,construct validity,MTMM analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Antecedents of B2C channel satisfaction and preference: Validating e-commerce metrics'''
{{header}}
{{article
|author= S Devaraj,M Fan,R Kohli,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Although electronic commerce (EC) has created new opportunities for businesses as well as consumers, questions about consumer attitudes toward business-to-consumer (B2C) e-commerce vis-A-vis the conventional shopping channels continue to persist. This paper reports results of a study that measured consumer satisfaction with the EC channel through constructs prescribed by three established frameworks, namely the Technology Acceptance Model (TAM), Transaction Cost Analysis (TCA), and Service Quality (SERVQUAL). Subjects purchased similar products through conventional as well as EC channels and reported their experiences in a survey after each transaction. Using constructs from the three frameworks, a model was constructed and tested to examine the determinants of the EC channel satisfaction and preference using the survey data. Structural equation model analyses indicate that metrics. tested through each model provide a statistically significant explanation of the variation in the EC consumers' satisfaction and channel preference. The study found that TAM components-perceived ease of use and usefulness-are important in forming consumer attitudes and satisfaction with the EC channel. Ease of use also was found to be a significant determinant of satisfaction in TCA. The study found empirical support for the assurance dimension of SERVQUAL as determinant in EC channel satisfaction. Further, the study also found general support for consumer satisfaction as a determinant of channel preference.
|keyword = electronic commerce,Online shopping,channel preference,satisfaction,Technology Acceptance Model (TAM),Transaction Cost Analysis (TCA),Service Quality (SERVQUAL)),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The shareholder-wealth and trading-volume effects of information-technology infrastructure investments'''
{{header}}
{{article
|author= D Chatterjee,C Pacini,V Sambamurthy,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Despite the rising tide of investments in information technologies (IT) infrastructures, empirical evidence about the effects of such investment moves is scarce. Stock market investors provide one appropriate perspective on the value-creation and growth potential of IT infrastructure investments through their reactions to specific IT infrastructure investment moves by business firms. This research utilizes the event-study analysis approach to examine if IT infrastructure investments are associated with significantly positive abnormal stock market returns and rises in trading volume when firms announce such investments. Drawing upon a sample of IT infrastructure announcements in the early 1990s, this research finds significant evidence that positive abnormal returns and increased trading volume are associated with IT infrastructure investment announcements. Further, when such investments are contrasted with investments in IT applications, evidence exists that infrastructure investments generate greater excess returns and a larger increase in trading volume than applications investments do. The evidence provides empirical support for the potential of IT infrastructure investments to be perceived as a platform for growth and revenue generation opportunities in contemporary business firms.
|keyword = event study,investment announcements,information technology infrastructure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing information technology investment risk: A real options perspective'''
{{header}}
{{article
|author= M Benaroch,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Past information systems research on real options has focused mainly on evaluating information technology (IT) investments that embed a single, a priori known option (such as, deferral option, prototype option). In other words, only once a specific isolated option is identified as being embedded in a target IT investment, does this research call upon using real options analysis to evaluate the option. In effect, however, because real options are not inherent in any IT investment, they usually must be planned and intentionally embedded in a target IT investment in order to control various investment-specific risks, just like financial risk management uses carefully chosen options to actively manage investment risks. Moreover, when an IT investment involves multiple risks, there could be numerous ways to reconfigure the investment using different series of cascading (compound) options. In this light, we present an approach for managing IT investment risk that helps to rationally choose which options to deliberately embed in an investment so as to optimally control the balance between risk and reward. We also illustrate how the approach is applied to an IT investment entailing the establishment of an Internet sales channel.
|keyword = information technology investment evaluation,information technology investment management,information technology investment risk,information technology investments,option-pricing models,real options,risk management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research in information systems: An empirical study of diversity in the discipline and its journals'''
{{header}}
{{article
|author= I Vessey,V Ramesh,RL Glass,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Throughout its history, the information systems (IS) discipline has engaged in extensive self-examination, particularly with regard to its apparent diversity. Our overall objective in this study is to better understand the diversity in IS research, and the extent to which diversity is universal across journals that publish IS research. We developed a classification system that comprises five key characteristics of diversity (reference discipline, level of analysis, topic, research approach, and research method) based on a review of prior literature. We then examined articles over a five-year period, from 1995 to 1999, in five journals acknowledged as the top journals of the field, at least in North America. Analyses reveal considerable diversity in each of the key characteristics. Perhaps not surprisingly, the research approach used is more focused with most studies being conducted using hypothetico-deductive approaches, whereas reference discipline is perhaps the most diverse of the characteristics examined. An interesting finding is that IS itself emerged as a key reference discipline in the late 1990s. The Journal of Management Information Systems and Information Systems Research publish articles displaying the greatest diversity, and MIS Quarterly and Decision Sciences publish articles that focus on subsets of the field. Our research provides a foundation for addressing the direction that diversity in the IS discipline takes over time. In the shorter term, researchers can use our classification system as a guide to writing abstracts and selecting key words, and the findings of our journal analyses to determine the best outlet for their type of research.
|keyword = information systems journals,information systems research,information systems research diversity,level of analysis,reference discipline,research methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Development of a measure for the organizational learning construct'''
{{header}}
{{article
|author= GF Templeton,BR Lewis,CA Snyder,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = The concept of organizational learning (OL) is receiving an increasing amount of attention in the research and practice of management information systems (MIS) due to its potential for affecting organizational outcomes, including control and intelligence, competitive advantage, and the exploitation of knowledge and technology. As such, further development of the salient issues related to OL is warranted, especially measurement of the construct. Based on a domain definition grounded in the literature, this research represents the initial work in developing an empirically reliable and valid measure of organizational learning. The rigorous method utilized in the derivation of this measure, which integrates two methodological frameworks for instrument development, is the main strength of this work. The result is an eight-factor, 28-item instrument for assessing OL, derived from a sample of 119 knowledge-based firms. The empirically derived factors are awareness, communication, performance assessment, intellectual cultivation, environmental adaptability, social learning, intellectual capital management, and organizational grafting. MIS function managers can use these factors to gauge organizational or subunit success in the creation and diffusion of new applications of information technology.
|keyword = innovation,organizational change,organizational intelligence,organizational learning,scale development,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of similarity in the reuse of object-oriented analysis models'''
{{header}}
{{article
|author= G Irwin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Software reuse-the application of existing software artifacts in the development of a new system-has been claimed to dramatically improve systems development productivity and quality. These claims have been particularly pronounced with respect to the reuse of object-oriented (OO) software artifacts. However, the empirical evaluations of these claims are relatively sparse and often inconsistent. This paper begins to address the gap in the literature. A verbal-protocol study was conducted in which analysts created a model for a problem (the target) and were given an example problem and solution (the source) to reuse. The results show little support for reuse in OO analysis. First, reuse had no effect on the quality of the OO analysis models. Subjects given a highly reusable example produced solutions that were no better than those of subjects in the control group. Second, the degree of similarity between the source and target problems did have an effect on the reuse process, although it did not impact the reuse outcome. Subjects given the example with the most similarity to the target problem quickly recognized the reuse potential, attempted a fair amount of reuse, but made several errors stemming from lazy copying. Subjects given an example with a lesser (but still significant) degree of similarity were often unable to recognize the reuse potential, and thus engaged in less reuse activity. Thus, the characteristics of the source-target comparison that facilitate noticing the reuse potential of the source do not necessarily help in applying the source solution to the target problem. These results suggest that the claims associated with reuse should be treated with a healthy dose of skepticism.
|keyword = analogical reasoning,object-oriented analysis,software reuse,verbal protocol,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A discrepancy model of information system personnel turnover'''
{{header}}
{{article
|author= JJ Jiang,G Klein,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Turnover of information system (IS) personnel is a critical problem for organizations. To gain a better understanding of turnover, researchers have explored career orientations that characterize an employee's internal motivations and desires. The inability of an organization to match career desires is often related to measures indicative of turnover in IS employees, including intent to leave and career dissatisfaction, though empirical evidence is indirect and inconclusive. Using career orientations, this study explicitly models the impact of the discrepancy between the wants of employees and employee perceptions of how their organization satisfies those wants. The model is based on discrepancy theory and predicts the gap is closely related to the turnover indicators. Model predictions hold true for a sample of 153 IS personnel. These results indicate the importance of developing career plans that employees perceive as matching their wants.
|keyword = career orientations,career satisfaction,discrepancy theory,employee turnover,information systems personnel,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The dynamic effects of group support systems on group meetings'''
{{header}}
{{article
|author= BA Reinig,B Shin,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = A number of theoretical models have been presented in group support systems (GSS) literature, which suggest that various GSS structures such as anonymity and simultaneity, influence group interaction, which in turn influences group productivity and meeting outcomes. Examples of such theories include the adaptive structuration theory and the balance of forces model and they could generally be described as dynamic or procedural in nature. Much of the empirical research that tests such theories, however, is deterministic in that it often compares final outcomes between various levels of technological support without measuring and testing (1) the influence that the technological structures have on group interaction and group dynamics, and (2) the corresponding influence that group interaction has on meeting outcomes. This paper reports a study that examines the validity of such dynamic theories by examining the relationships between GSS structures, group dynamics, and meeting outcomes over time. Four process constructs (production blocking, free riding, sucker effect, and evaluation apprehension) and three meeting outcome constructs (group cohesion, affective reward, and self-reported learning) were initially selected for the study. Structural equation modeling was used to analyze longitudinal survey data gathered from an experiment conducted with naturally occurring groups. The model tested was found to be valid and GSS was found to be effective in reducing process losses. However, the findings also revealed that process losses vary in the degree to which they influence meeting outcomes and certain meeting outcomes, such as affective reward, were found to be heavily influenced by other meeting outcomes, such as group cohesion and self-reported learning. Theoretical implications of the study and methodology are discussed.
|keyword = adaptive structuration theory,computer-mediated communication,group support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A design theory for systems that support emergent knowledge processes'''
{{header}}
{{article
|author= ML Markus,A Majchrzak,L Gasser,
|source= MIS QUARTERLY
|year= 2002
|abstract = This paper addresses the design problem of providing IT support for emerging knowledge processes (EKPs). EKPs are organizational activity patterns that exhibit three characteristics in combination: an emergent process of deliberations with no best structure or sequence; requirements for knowledge that are complex (both general and situational), distributed across people, and evolving dynamically; and an actor set that is unpredictable in terms of job roles or prior knowledge. Examples of EKPs include basic research, new product development, strategic business planning, and organization design. EKPs differ qualitatively from semi-structured decision making processes; therefore, they have unique requirements that are not all thoroughly supported by familiar classes of systems, such as executive information systems, expert systems, electronic communication systems, organizational memory systems, or repositories. Further, the development literature on familiar classes of systems does not provide adequate guidance on how to build systems that support EKPs. Consequently, EKPs require a new IS design theory, as explicated by Walls et al. (1992).
|keyword = IS design theory,IS development,emergent knowledge process,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evidence of the effect of trust building technology in electronic markets: Price premiums and buyer behavior'''
{{header}}
{{article
|author= SL Ba,PA Pavlou,
|source= MIS QUARTERLY
|year= 2002
|abstract = Despite the wide use of reputational mechanisms such as eBay's Feedback Forum to promote trust, empirical studies have shown conflicting results as to whether online feedback mechanisms induce trust and lead to higher auction prices. This study examines the extent to which trust can be induced by proper feedback mechanisms in electronic markets, and how some risk factors play a role in trust formation. Drawing from economic, sociological, and marketing theories and using data from both an online experiment and an online auction market, we demonstrate that appropriate feedback mechanisms can induce calculus-based credibility trust without repeated interactions between two transacting parties. Trust can mitigate information asymmetry by reducing transaction-specific risks, therefore generating price premiums for reputable sellers. In addition, the research also examines the role that trust plays in mitigating the risks inherent in transactions that involve very expensive products.
|keyword = trust,credibility,reputation,information asymmetry,price premiums,feedback mechanisms,electronic markets,online risks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge management in pursuit of performance: Insights from Nortel Networks'''
{{header}}
{{article
|author= AP Massey,MM Montoya-Weiss,TM O'Driscoll,
|source= MIS QUARTERLY
|year= 2002
|abstract = From 1994 through 2000, Nortel Networks transformed itself from a technology-focused to an opportunity/customer-focused company. By 2000, Nortel was a profitable, innovative leader in the telecommunications industry. The change was the result of an ambitious effort to redesign its entire new product development (NPD) process such that time-to-market was significantly reduced. NPD is highly knowledge-intensive work based on the individual and collective expertise of employees. The primary focus of this case study is on Nortel's efforts to reengineer the front-end of its NPD process and capitalize on knowledge assets. This effort was built around a process-oriented knowledge management (KM) strategy, involving a tripartite and systematic focus on process, people, and technology. Through our case analysis we develop a model of KM success by exploring Nortel's KM strategy and the managerial, resource, and environmental factors that influenced Nortel's success. Nortel's experiences suggest lessons for other firms attempting to manage knowledge assets in core business processes.
|keyword = knowledge management,knowledge management systems,new product development,organizational knowledge management,corporate strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NEBIC: A dynamic capabilities theory for assessing net-enablement'''
{{header}}
{{article
|author= BC Wheeler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = We propose the Net-Enabled Business Innovation Cycle (NEBIC) as an applied dynamic capabilities theory for measuring, predicting, and understanding a firm's ability to create customer value through the business use of digital networks. The theory incorporates both a variance and process view of net-enabled business innovation. It identifies four sequenced constructs: Choosing new IT, Matching Economic Opportunities with technology, Executing Business Innovation for Growth, and Assessing Customer Value, along with the processes and events that interrelate them as a cycle. The sequence of these theorized relationships for net-enablement (NE)(1) asserts that choosing IT precedes rather than aligns with corporate strategy. The theory offers a logically consistent and falsifiable basis for grounding research programs on metrics of net-enabled business innovation.
|keyword = theory building,IS research frameworks,net-enabled organizations (NEOs),innovation,digital business,e-commerce,e-business,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The net-enabled business innovation cycle and the evolution of dynamic capabilities'''
{{header}}
{{article
|author= SA Zahra,G George,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Wheeler's Net-Enabled Business Innovation Cycle (NEBIC) integrates IS and strategy research to offer an interesting and timely perspective on value creation. We extend Wheeler's theoretical propositions, highlighting the interplay between strategy, IS, and entrepreneurship in a quest for competitive advantage. This interplay is crucial to the creation of the dynamic capabilities that enable companies to gain an advantage through NEBIC. The importance of opportunity recognition and absorptive capacity in bringing about the changes that make NEBIC viable is also highlighted.
|keyword = absorptive capacity,capabilities,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing a firm's Web presence: A heuristic evaluation procedure for the measurement of usability'''
{{header}}
{{article
|author= R Agarwal,V Venkatesh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Web site usability is a critical metric for assessing the quality of a firm's Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability of Web sites. The procedure utilizes a comprehensive set of usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered.
|keyword = usability,heuristic evaluation,microsoft usability guidelines,human-computer interaction,Web interface,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring factors that influence the success of Internet commerce'''
{{header}}
{{article
|author= G Torkzadeh,G Dhillon,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Efforts to develop measures of Internet commerce success have been hampered by (1) the rapid development and use of Internet technologies and (2) the lack of conceptual bases necessary to develop success measures. In a recent study, Keeney (1999) proposed two sets of variables labeled as means objectives and fundamental objectives that influence Internet shopping. Means objectives, he argues, help businesses achieve what is important for their customers-fundamental objectives. Based on Keeney's work, this paper describes the development of two instruments that together measure the factors that influence Internet commerce success. One instrument measures the means objectives that influence online purchase (e.g., Internet vendor trust) and the other measures the fundamental objectives that customers perceive to be important for Internet commerce (e.g., Internet product value). In phase one of the instrument development process, we generated 125 items for means and fundamental objectives. Using a sample of 199 responses by individuals with Internet shopping experience, these constructs were examined for reliability and validity. The Phase 1 results suggested a 4-factor, 21-item instrument to measure means objectives and a 4-factor, 17-item instrument to measure fundamental objectives. In Phase 2 of the instrument development process, we gathered a sample of 421 responses to further explore the 2 instruments. With minor modifications, the Phase 2 data support the 2 models. The Phase 2 results suggest a 5-factor, 21-item instrument that measures means objectives in terms of Internet product choice, online payment, Internet vendor trust, shopping travel, and Internet shipping errors. Results also suggest a 4-factor, 16-item instrument that measures fundamental objectives in terms of Internet shopping convenience, Internet ecology, Internet customer relation, and Internet product value. Evidence of reliability and discriminant, construct, and content validity is presented for the hypothesized measurement models. The paper concludes with discussions on the usefulness of these measures and future research ideas.
|keyword = Internet commerce,customer behavior,customer value,success factors,means objectives,fundamental objectives,instrument development,construct validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Applying the technology acceptance model and flow theory to online consumer behavior'''
{{header}}
{{article
|author= M Koufaris,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = In this study, we consider the online consumer as both a shopper and a computer user. We test constructs from information systems (Technology Acceptance Model), marketing (Consumer Behavior), and psychology (Flow and Environmental Psychology) in an integrated theoretical framework of online consumer behavior. Specifically, we examine how emotional and cognitive responses to visiting a Web-based store for the first time can influence online consumers' intention to return and their likelihood to make unplanned purchases. The instrumentation shows reasonably good measurement properties and the constructs are validated as a nomological network. A questionnaire-based empirical study is used to test this nomological network. Results confirm the double identity of the online consumer as a shopper and a computer user because both shopping enjoyment and perceived usefulness of the site strongly predict intention to return. Our results on unplanned purchases are not conclusive. We also test some individual and Web site factors that can affect the consumer's emotional and cognitive responses. Product involvement, Web skills, challenges, and use of value-added search mechanisms all have a significant impact on the Web consumer. The study provides a more rounded, albeit partial, view of the online consumer and is a significant step towards a better understanding of consumer behavior on the Web. The validated metrics should be of use to researchers and practitioners alike.
|keyword = TAM,flow theory,nomological validity,web skills,value-added search mechanisms,online consumer behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Learning to implement enterprise systems: An exploratory study of the dialectics of change'''
{{header}}
{{article
|author= D Robey,JW Ross,MC Boudreau,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = This paper reports on a comparative case study of 13 industrial firms that implemented an enterprise resource planning (ERP) system. It compares firms based on their dialectic learning process. All firms had to overcome knowledge barriers of two types: those associated with the configuration of the ERP package, and those associated with the assimilation of new work processes. We found that both strong core teams and carefully managed consulting relationships addressed configuration knowledge barriers. User training that included both technical and business processes, along with a phased implementation approach, helped firms to overcome assimilation knowledge barriers. However, all firms in this study experienced ongoing concerns with assimilation knowledge barriers, and we observed two different approaches to address them. In a piecemeal approach, firms concentrated on the technology first and deferred consideration of process changes. In a concerted approach, both the technology and process changes were undertaken together. Although most respondents clearly stated a preference for either piecemeal or concerted chance, all firms engaged in practices that reflected a combination of these approaches.
|keyword = dialectics of change,Enterprise Resource Planning,information technology implementation,organizational learning,process theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Implementation team responsiveness and user evaluation of Customer Relationship Management: A quasi-experimental design study of social exchange theory'''
{{header}}
{{article
|author= D Gefen,CM Ridings,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Customer Relationship Management (CRM) systems require extensive configuration during which users come into extensive contact with the technical implementation team. Previous research examining other Enterprise Resource Planning (ERP) modules has shown that user perception of the responsiveness of such teams, as an indicator of a possible social exchange, is significantly associated with an increased favorable assessment of the new system and ultimately its adoption, the reason being that perceived responsiveness creates a constructive social exchange. However, previous research, using survey data alone, did not examine causation. The objective of this study is to examine, using a quasi-experimental design, whether different degrees of actual responsiveness in different sites during CRM implementation result in significant differences in the users' favorable assessment of the correctness and ultimately their approval of a new CRM. The data support these hypotheses, but show that the downstream effects of actual responsiveness are mediated by perceived responsiveness. Implications concerning the social exchange relationship during CRM adoption are discussed.
|keyword = Customer Relationship Management,Enterprise Resource Planning,social exchange theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investment in Enterprise Resource Planning: Business impact and productivity measures'''
{{header}}
{{article
|author= LM Hitt,DJ Wu,XG Zhou,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Enterprise Resource Planning (ERP) software systems integrate key business and management processes within and beyond a firm's boundary. Although the business value of ERP implementations has been extensively debated in trade periodicals in the form of qualitative discussion or detailed case studies, there is little large-sample statistical evidence on whether the benefits of ERP implementation exceed the costs and risks. With multiyear multi-firm ERP implementation and financial data, we find that firms that invest in ERP tend to show higher performance across a wide variety of financial metrics. Even though there is a slowdown in business performance and productivity shortly after the implementation, financial markets consistently reward the adopters with higher market valuation (as measured by Tobin's q). Due to the lack of mid- and long-term post-implementation data, future research on the long-run impact of ERP is proposed.
|keyword = Enterprise Resource Planning systems,information technology,productivity,productivity analysis,ROI,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The influence of multimedia on improving the comprehension of organizational information'''
{{header}}
{{article
|author= KH Lim,I Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Text is the predominant form of organizational information. Comprehending text-based information requires intensive cognitive processing effort on the part of readers. Drawing on multimedia literature, this study identified a characteristic of multimedia presentations, namely complementary cues, which have the potential to improve the comprehensibility of organizational information. A set of hypotheses about the benefits of multimedia over text-based presentations was generated based on the theoretical perspective that we developed. These predictions were tested through a laboratory experiment using a simulated multimedia intranet. Results show that multimedia facilitates the retention and subsequent recall of explanative information but not of descriptive information. Explanative information is organized facts connected by their underlying functional relationships. Descriptive information consists of isolated facts without an explanation of the relationships between these facts. The ability to retain and recall explanative information, in turn, leads to a greater ability to make correct inferences about new organizational situations.
|keyword = human information processing,information comprehension,information presentation,information recall,learning and inference,multimedia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Multiparty negotiation support: The role of visualization's influence on the development of shared mental models'''
{{header}}
{{article
|author= RI Swaab,T Postmes,P Neijens,MH Kiers,ACM Dumay,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = The study examines a method for supporting multiparty negotiations by means of a Negotiation Support System (NSS). More specifically, this study investigated the effect of visualization support on the development of shared mental models among negotiators who resolved a spatial planning dispute. The objective of this study is to determine how to support the development of shared mental models in order to stimulate more productive negotiations. A further goal is to provide guidelines for the design of NSS. Compared with a control condition, visualization improved three aspects of negotiations: visualization support aided negotiators' convergence of perceptions of reality and had positive socio-emotional consequences in terms of increasing cohesiveness and entitativity. As a result, groups with visualization support reached consensus more easily and were more satisfied with the process. In sum, the current study provides support for the idea that presenting negotiators with unambiguous information helps negotiators develop shared mental models.
|keyword = multiparty negotiation,negotiation support systems,prosocial climate,shared mental model,visualization of information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software functionality: A game theoretic analysis'''
{{header}}
{{article
|author= KL Hui,KY Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Digital products are now widely traded over the Internet. Many researchers have started to investigate the optimal competitive strategies and market environments for such products. This paper studies the competitive decisions made about software, a major class of digital products that can be easily sold through computer networks. Instead of focusing on traditional competitive dimensions, such as price or quantity, we study the number of functions that should be incorporated into the software. Using, game theoretic analysis, we show that there is no fixed strategy that is optimal for software developers in a duopoly market with one-stage simultaneous moves. This happens because, given one developer's decision, there is always an incentive for the other developer to deviate and achieve higher payoffs. Nevertheless, a unique reactive equilibrium does emerge if we consider the two-stage variation of the model, where the two developers both enjoy substantial profits by serving different segments of the market. Essentially, the first mover commits himself to a certain functionality level that induces a rational follower to target his software to the (previously) unattended segment. We discuss our results in light of scale economies in the software development process and market segmentation.
|keyword = development cost,digital products,functionality,game theory,software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An integrated performance model of information systems projects'''
{{header}}
{{article
|author= AM Aladwani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = This study makes an initial attempt to validate an integrated, theoretically driven performance model of information systems (IS) projects. IS project performance is defined in terms of task, psychological, and organizational outcomes. We draw upon different theoretical perspectives including IS, organizational teams, and project management to link six categories of variables to IS project performance: technology characteristics, project characteristics, task characteristics, people characteristics, organizational characteristics, and work processes. Data collected via a field survey of IS project leaders in 84 manufacturing organizations were used to test the proposed model. Support is found for three conclusions: (1) IS project performance is a multidimensional construct, (2) certain preconditions falling into the above categories have to exist to achieve a high performing IS project, and (3) there is a possible cross-relationship among the variables studied by IS research, organizational teams research, and project management research. We discuss the implications of this study for future research and managerial practice.
|keyword = information systems development,information systems projects,integrated performance models,project performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Individual trust in online firms: Scale development and initial test'''
{{header}}
{{article
|author= A Bhattacherjee,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = The importance of trust as a key facilitator of electronic commerce is increasingly being recognized in academic and practitioner communities. However, empirical research in this area has been beset by conflicting conceptualizations of the trust construct, inadequate attention to its underlying dimensions, causes, and effects, and lack of a validated trust scale. This paper addresses these limitations in part by theoretically conceptualizing and empirically validating a scale to measure individual trust in online firms. The proposed scale taps into three key dimensions of trust: trustee's ability, benevolence, and integrity, An iterative testing and refinement procedure using two field surveys of online retailing and online banking users, leads to a final seven-item trust scale that exhibits adequate levels of reliability, convergent validity, discriminant validity, and nomological validity. It is expected that the scale presented in this paper will assist future empirical research on trust in online entities.
|keyword = e-commerce metrics,electronic commerce,online trust,scale development,trust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Profiling Web usage in the workplace: A behavior-based artificial intelligence approach'''
{{header}}
{{article
|author= M Anandarajan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Employees' nonwork-related Web surfing behavior results in millions of dollars of expenditure for organizations. This paper proposes the use of a behavior-based artificial intelligence system to profile employee Web usage behavior. Two artificial neural networks (ANN) incorporating genetic algorithm techniques were developed for this purpose. The system was validated with two different data sets. The classification performance of the neural network models was compared to that of a statistical method. The results indicate that one of the ANN models, namely the simple recurrent network, was a superior classifier for this behavior-based problem. In addition, the uncertainty inherent in such classification decisions was examined with a loss matrix, and the holdout samples were reclassified using, a loss matrix. The output of this intelligent system can be highly beneficial to managers in designing effective Web management policies.
|keyword = artificial neural networks,classification models,genetic algorithms,loss matrix,misclassification rate,profiling,Web usage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Shaping up for e-commerce: Institutional enablers of the organizational assimilation of Web technologies'''
{{header}}
{{article
|author= D Chatterjee,R Grewal,V Sambamurthy,
|source= MIS QUARTERLY
|year= 2002
|abstract = The global reach of the Web technological platform, along with the range of services that it supports, makes it a powerful business resource, However, realization of operational and strategic benefits is contingent on effective assimilation of this type III IS innovation. This paper draws upon institutional theory and the conceptual lens of structuring and metastructuring actions to explain the importance of three factors-top management championship, strategic investment rationale, and extent of coordination-in achieving higher levels of Web assimilation within an organization. Survey data are utilized to test a nomological network of relationships among these factors and the extent of organizational assimilation of Web technologies.
|keyword = Web technology,Web implementation,IT management,innovation assimilation,structuring actions,metastructuring actions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A representational scheme for analyzing information technology and organizational dependency'''
{{header}}
{{article
|author= J Tillquist,JL King,C Woo,
|source= MIS QUARTERLY
|year= 2002
|abstract = This paper presents a new representation methodology, dependency network diagrams (DNDs), which enables the essential elements governing organizational relations to be captured, communicated, and evaluated under changing conditions. By depicting important features of organizational relations, information systems can be designed explicitly for control and coordination of organizational activities. The rules and construction algorithm for DNDs are presented and applied to a case study of a Canadian automobile insurance company. Analysis of the case reveals how IT was used to create strategic change within the Canadian vehicle repair market.
|keyword = management information systems,organizational systems,competitive use of IS,IS planning methodologies,IS strategic planning,organizational change,organizational design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Inducing sensitivity to deception in order to improve decision making performance: A field study'''
{{header}}
{{article
|author= DP Biros,JF George,RW Zmud,
|source= MIS QUARTERLY
|year= 2002
|abstract = When an organization's members depend on the data contained in computer-based systems, they become vulnerable to strategic information manipulation. That is, they become susceptible to situations where their decision-making behaviors can be influenced by others able to access and manipulate this data. This paper describes the results of a field experiment that examines the effects of alternative interventions aimed at inducing sensitivity to the possibility of manipulated date on professionals' task-related decision behaviors: deception detection, false alarms, and task accuracy. While traditional training had no effect on detection success or the issuance of false alarms, warnings about data quality resulted in better detection success. Warnings combined with just-in-time training resulted in better detection success but at the cost of an increased number of false alarms, Higher levels of detection success increased task accuracy and the time spent solving each problem. A higher number of false alarms was associated with lower levels of task accuracy.
|keyword = information quality,date security,data integrity,error detection,deception detection,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring information system service quality: Servqual from the other side'''
{{header}}
{{article
|author= JJ Jiang,G Klein,CL Carr,
|source= MIS QUARTERLY
|year= 2002
|abstract = There has been much debate as of late over the use of the SERVQUAL instrument to measure Information Systems service quality. Detractors argue that the difference score leads to unreliable measures and that the dimensionality and validity is erratic. Proponents argue for the diagnostic power of the gap between expectations and perceived delivery while demonstrating some empirical stability and reliability. To extend the discussion requires the examination of the instrument from the viewpoint of the information system professional. Importantly, a large variety of samples must view the instrument and measures in the same light for the instrument to have applicability. Likewise, analysis of differences between users and providers requires that both populations have similar structural views of the instrument. Empirical evidence collected from information system professionals demonstrated a structure similar to previously published studies with adequate reliability, convergent validity, and discriminant validity. The structure is the same as is found for a gap between users and IS professionals.
|keyword = service quality,measurement,SERVQUAL,user expectations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Workflow management issues in e-business'''
{{header}}
{{article
|author= A Basu,A Kumar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Trends towards increased business process automation, e-commerce, and e-business have led to increasing interest in the field of workflow management. In this paper, we provide a perspective on the state of research in workflow management systems, and discuss possible future research directions in this area, with a particular emphasis on workflow systems in integrating interorganizational processes and enabling e-commerce solutions.
|keyword = workflow systems,e-business,work-flow specification,work-flow modeling,B-to-B exchange,e-hubs,e-services,composition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Model composition using filter spaces'''
{{header}}
{{article
|author= K Chari,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Decision support systems (DSS) typically contain data and models to facilitate decision making. DSS users, in response to a particular decision-making situation, often execute a sequence of models, in which inputs to a model in the sequence are obtained from outputs of other models upstream in the sequence and from database retrievals. The problem of generating a sequence of models from the set of available models is known as the model composition problem. In this paper, we propose a new construct called filter spaces to support model composition. We show how filter spaces can significantly facilitate automation of model composition and execution process, and provide effective means to integrate partial solutions from multiple composite models and databases.
|keyword = decision support systems,model management,model composition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical examination of the concern for information privacy instrument'''
{{header}}
{{article
|author= KA Stewart,AH Segars,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = The arrival of the "information age" holds great promise in terms of providing organizations with access to a wealth of information stores. However, the free exchange of electronic information also brings the threat of providing easy, and many times unwanted, access to personal information. Given the potential backlash of consumers, it is imperative that both researchers and practitioners understand the nature of consumers' concern for information privacy and accurately model the construct within evolving research and business contexts. Drawing upon a sample of 355 consumers and working within the framework of confirmatory factor analysis, this study examines the factor structure of the concern for information privacy (CFIP) instrument posited by Smith et al. (1996). Consistent with prior findings, the results suggest that each dimension of this instrument is reliable and distinct. However, the results also suggest that CFIP may be more parsimoniously represented as a higher-order factor structure rather than a correlated set of first-order factors. The implication of these results is that each dimension of CFIP as well as the supra dimension derived from the associations among dimensions are important in capturing CFIP and associating the construct to other important antecedents and consequences.
|keyword = privacy,ethical issues,measures,reliability,validity,confirmatory factor analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing the validity of IS success models: An empirical test and theoretical analysis'''
{{header}}
{{article
|author= A Rai,SS Lang,RB Welker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = he purpose of the present study is to empirically and theoretically assess DeLone and McLean's (1992) and Seddon's (1997) models of information systems (IS) success in a quasi-voluntary IS use context. Structural modeling techniques were applied to data collected by questionnaire from 274 system users of an integrated student information system at a midwestern university. The Seddon structural model and the DeLone and McLean structural model each contained five variables (system quality, information quality, perceived usefulness, user satisfaction, and IS use). Both models exhibit reasonable fit with the collected data. The empirical findings are assessed in the broader theoretical context of the IS success literature, including the Technology Acceptance Model and the Theory of Planned Behavior. Our results support DeLone and McLean's focus on integrated IS success models and their observation that IS success models need to be carefully specified in a given context. The Seddon model conceptually elaborates and clarifies aspects of the DeLone and McLean model, thereby effectively integrating core theoretical relationships espoused in the IS success literature. Our study also supports Seddon's three construct categories (system and information quality, general perceptual measures about net benefits about IS use, and IS behavior), as defining IS success and its impact on nature of IS use.
|keyword = information systems success,information systems usefulness,information systems use,user satisfaction,system quality,information quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Group polarization and computer-mediated communication: Effects of communication cues, social presence, and anonymity'''
{{header}}
{{article
|author= CL Sia,BCY Tan,KK Wei,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Group polarization is the tendency of people to become more extreme in their thinking following group discussion. It may be beneficial to some, but detrimental to other, organizational decisions. This study examines how computer-mediated communication (CMC) may be associated with group polarization. Two laboratory experiments were carried out. The first experiment, conducted in an identified setting, demonstrated that removal of verbal cues might not have reduced social presence sufficiently to impact group polarization, but removal of visual cues might have reduced social presence sufficiently to raise group polarization. Besides confirming the results of the first experiment, the second experiment showed that the provision of anonymity might also have reduced social presence sufficiently to raise group polarization. Analyses of process data from both experiments indicated that the reduction in social presence might have increased group polarization by causing people to generate more novel arguments and engage in more one-upmanship behavior. Collectively, process and outcome data from both experiments reveal how group polarization might be affected by level of social presence. Specifically, group discussion carried out in an unsupported setting or an identified face-to-face CMC setting tends to result in weaker group polarization. Conversely, group discussion conducted in an anonymous face-to-face CMC setting or a dispersed CMC setting (with or without anonymity) tends to lead to stronger group polarization. Implications of these results for further research and practice are provided.
|keyword = group polarization,computer-mediated communication,social presence,communication cues,anonymity,persuasive argumentation,social comparison),
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Better theory through measurement - Developing a scale to capture consensus on appropriation'''
{{header}}
{{article
|author= WD Salisbury,WW Chin,A Gopal,PR Newsted,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = Proper measurement is critical to the advancement of theory (Blalock 1979). Adaptive Structuration Theory (AST) is rapidly becoming an important theoretical paradigm for comprehending the impacts of advanced information technologies (DeSanctis and Poole 1994). Intended as a complement to the faithfulness of appropriation scale developed by Chin et at. (1997), this research note describes the development of an instrument to capture the AST construct of consensus on appropriation. Consensus on appropriation (COA) is the extent to which group participants perceive that they have agreed on how to adopt and use a technology. While consensus on appropriation is an important component of AST, no scale is currently available to capture this construct. This research note develops a COA instrument in the context of electronic meeting systems use. Initial item development, statistical analyses, and validity assessment (convergent, discriminant, and nomological) are described here in detail. The contribution of this effort is twofold: First, a scale is provided for an important construct from AST. Second, this report serves as an example of rigorous scale development using structural equation modeling. Employing rigorous procedures in the development of instruments to capture AST constructs is critical if the sound theoretical base provided by AST is to be fully exploited in understanding phenomena related to the use of advanced information technologies.
|keyword = adaptive structuration theory,scale development,electronic meeting systems,technology appropriation,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Increasing returns to information technology'''
{{header}}
{{article
|author= S Kudyba,R Diwan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2002
|abstract = This work analyzes firm-level investment in information technology and corresponding productivity through the use of a production function over the period from 1995-1997. The results are then compared to previous studies that utilized similar data and methodologies to compare productivity estimates over time. The analysis indicates that investment in IT enhances productivity over the period in question and has illustrated increasing returns over time. These findings are supported by the corresponding empirical analysis which yielded IT capital coefficients in a production function of (0.12, 0.16, 0.18) and IT flow coefficients in a similar function of (0.17, 0.24, 0.22) for the years 1995, 1996, and 1997, respectively. These results reflect the change in firm output given a one-percent change in the natural log of dollars invested in IT capital and flow, and are statistically significant.
|keyword = productivity,information technology,production function,efficiency,innovation,information economy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Performance-centered design of knowledge-intensive processes'''
{{header}}
{{article
|author= AP Massey,MM Montoya-Weiss,TM O'Driscoll,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = We develop and illustrate a performance-centered design (PCD) methodology for structuring knowledge-intensive, ill-defined processes. PCD provides a holistic view of a performance environment by considering the complex interdependencies between the organizational context, business processes, and individual performers. The context for our theoretical exposition is the fuzzy front-end of the new product development (NPD) process. Despite the fact that front-end concept definition and selection is central to a firm's innovation capability, these activities are ill-structured and typically the most poorly managed in the entire innovation process. Through a case study, we illustrate the proposed PCD methodology as applied to the fuzzy front-end and additionally illustrate how electronic performance support technology can be utilized to support the fuzzy front-end process. Although specifically applied within the context of one firm, we contend that the PCD methodology is applicable to other knowledge-intensive and relatively unstructured processes.
|keyword = electronic performance support,fuzzy front-end,new product development,performance-centered design,problem-solving theory,problem structure,systems theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An examination of the impact of stimuli type and GSS structure on creativity: Brainstorming versus non-brainstorming techniques in a GSS environment'''
{{header}}
{{article
|author= JM Hender,DL Dean,TL Rodgers,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Of the techniques available for idea generation with group support systems (GSS), little research attention has been given to techniques that challenge problem assumptions or that use unrelated stimuli to promote creativity. When implementing such techniques with GSS, choices must be made regarding how to configure the GSS to deploy the initial creative stimuli and to present the pool of emerging ideas that act as additional stimuli. This paper reports the results of an experiment that compares Electronic Brainstorming (few unnamed rotating dialogues) with Assumption Reversals (many related stimuli. many named dialogues, free movement among dialogues) and Analogies (many unrelated stimuli, many named dialogues, free movement among dialogues). Analogies produced creative, but fewer, ideas, due to the use of unrelated stimuli. Assumption Reversals produced the most, but less creative. ideas, possibly due to fragmentation of the group memory and cognitive inertia caused by lack of forced movement among dialogues.
|keyword = analogy,assumption reversal,brainstorming,creativity,group support system,idea generation,idea quality,idea quantity,laboratory experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Contents matching defined by prototypes: Methodology verification with books of the bible'''
{{header}}
{{article
|author= A Visa,J Toivonen,H Vanharanta,B Back,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = It is common that text documents are characterized and classified by key words, index terms, or headings. We have developed a new methodology based on prototype matching. The prototype is an interesting document or a part of an extracted, interesting text. This prototype is matched with the existing document database or with the monitored document flow. The claim is that the new methodology is capable of extracting the contents of the document. To verify this hypothesis, a test with the Bible was designed. Different translations in English, Latin, Greek, and Finnish were selected to test materials. Verification tests that included the search of the ten nearest books to every book of the Bible were performed with a designed prototype version of the software application. The test results are reported in this paper.
|keyword = bible,document classification,knowledge discovery,methodology,prototype matching,text mining,verification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Operational knowledge representation for practical decision-making'''
{{header}}
{{article
|author= JC Pomerol,P Brezillon,L Pasquier,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = For the design of an "intelligent" assistant system aimed at supporting operators' decision in subway control, we modeled operators' activity and know-how. As a result, we introduce the notion of a contextual graph, which appears as a simple solution to describe and manage operational decision-making.
|keyword = context representation,contextual graphs,decision tree,knowledge representation,operational knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Asynchronous collaboration around multimedia applied to on-demand education'''
{{header}}
{{article
|author= D Bargeron,J Grudin,A Gupta,E Sanocki,F Li,S Leetiernan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Multimedia content is a central component of on-demand training and education delivered over the World Wide Web. Supporting asynchronous collaboration around educational multimedia is potentially a significant tool for delivering online educational content effectively. A multimedia annotation system tightly integrated with e-mail provides a powerful platform on which to base such functionality. In this paper we describe a series of studies of such a system. First, we built a prototype annotation system and refined it based on results of laboratory tests. We then extended the system to support asynchronous collaboration for on-demand training and studied its effectiveness in two corporate training courses, assessing student experience, instructor experience, and user interface appropriateness. Having identified possibilities for enhancing engagement and collaboration with the tool, we conducted another set of laboratory studies. Through this iterative process we are creating a platform and identifying processes for its use, which enable students and instructors to exploit the advantages of asynchronous education while compensating for the reduction in face-to-face interaction.
|keyword = annotation,asynchronous collaboration,distance education,group interaction,multimedia,workplace training,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Training for crisis decision-making: Psychological issues and computer-based solutions'''
{{header}}
{{article
|author= JA Sniezek,DC Wilkins,PL Wadlington,MR Baumann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Crises demand swift and effective decision-making; yet there are many problems in training personnel on the skills necessary to achieve the goals of crisis management. This paper has three objectives concerning training for crisis management. First we integrate diverse literatures and present a framework for an understanding of the unique challenges in crisis management training, and the role of training systems with capabilities for simulation, immersion, and critiquing. Second, we describe an example of a trainer for ship damage control, called DC-Train, which addresses these challenges. This system consists of a first-principles simulator that generates large numbers of realistic scenarios. an immersive multimedia interface that helps elicit psychological processes involved in actual crisis management, and a critiquing expert system that provides real-time and post-session feedback on human decision-making performance. Finally, we present an empirical method for evaluating the effectiveness of such a system for crisis management training. Results of evaluation experiments with participants in a ship damage control training program indicate that the described computer-based trainer has psychological realism and improves decision-making performance.
|keyword = artificial intelligence,computer-based training,crisis management,human-computer interaction,human resource management,ship damage control,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Becoming a virtual professor: Pedagogical roles and asynchronous learning networks'''
{{header}}
{{article
|author= NW Coppola,SR Hiltz,NG Rotter,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = Asynchronous Learning Networks (ALN) are a form of "e-learning" that emphasizes the use of the Internet to support class discussions and activities. This paper presents a qualitative study of role changes that occur when faculty become online or "virtual" professors. In 20 semi-structured interviews of faculty, coded with pattern analysis software, the authors captured role changes enacted by instructors in ALN settings-cognitive roles, affective roles, and managerial roles. The cognitive role, which relates to mental processes of learning, information storage, and thinking, shifts to one of deeper cognitive complexity. The affective role, which relates to influencing the relationships between students, the instructor, and the classroom atmosphere, required faculty to find new tools to express emotion, yet they found the relationship with students more intimate. The managerial role, which deals with class and course management, requires greater attention to detail, more structure, and additional student monitoring. Overall, faculty reported a change in their teaching persona, toward more precision in their presentation of materials and instructions, combined with a shift to a more Socratic pedagogy, emphasizing multilogues with students. The main sources of frustration and of fulfillment of the virtual professor are explored.
|keyword = affective role,asynchronous learning networks,cognitive role,e-learning,managerial roles,role theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining a model of information technology acceptance by individual professionals: An exploratory study'''
{{header}}
{{article
|author= PYK Chau,PJ Hu,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2002
|abstract = The recent proliferation of information technology designed to support or enhance an individual professional's task performance has made the investigation of technology acceptance increasingly challenging and significant. This study investigates technology acceptance by individual professionals by examining physicians' decisions to accept telemedicine technology. Synthesized from relevant prior research, a generic research framework was built to provide a necessary foundation upon which a research model for telemedicine technology acceptance by physicians could be developed. The research model was then empirically examined, using data collected from more than 400 physicians practicing in public tertiary hospitals in Hong Kong. Results of the study suggest several areas where individual "professionals" might subtly differ in their technology acceptance decision-making, as compared with end users and business managers in ordinary business settings. Specifically, physicians appeared to be fairly pragmatic, largely anchoring their acceptance decisions in the usefulness of the technology rather than in its ease of use. When making decisions to accept a technology, physicians expressed considerable concerns about the compatibility of the technology with their practices, placed less importance on controlling technology operations, and attached limited weight to peers' opinions about using the technology. Based on results obtained from this study, the initially proposed framework for technology acceptance by individual professionals was revised to a "hierarchical, three-layer" structure with the individual context at the inner core, the implementation context on the outermost layer, and the technological context residing in the middle. Implications for information systems research and telemedicine management practice that have emerged from the study's findings are also discussed.
|keyword = acceptance of information technology,adoption of information technology,professional users,telemedicine technology management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information systems as a reference discipline'''
{{header}}
{{article
|author= RL Baskerville,MD Myers,
|source= MIS QUARTERLY
|year= 2002
|abstract = The conventional wisdom amongst information systems (IS) researchers is that information systems is an applied discipline drawing upon other, more fundamental, reference disciplines. These reference disciplines are seen as having foundational value for IS. We believe that it is time to question the conventional wisdom. We agree that many disciplines are relevant for IS researchers, but we suggest a re-think of the idea of "reference disciplines" for IS. In a sense, IS has come of age. Perhaps the time has come for IS to become a reference discipline for others.
|keyword = computing reference disciplines,research tradition,knowledge networks,diffusion of research,IS research agenda,IS research issues,future of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Beyond sabre: An empirical test of expertise exploitation in electronic channels'''
{{header}}
{{article
|author= E Chirstiaanse,N Venkatraman,
|source= MIS QUARTERLY
|year= 2002
|abstract = This paper develops a perspective of interorganizational relationships based on the concept of exploitation of expertise. Insights from marketing channel theory and resource-based views of the firm are integrated to test the effects of expertise exploitation capabilities in electronic channels. The distinctiveness of this study is based on the role of information and computer technology in creating advantage through differential expertise. A model of IT-induced quasi-integration was developed and tested on a sample of 117 travel agencies targeted by American Airlines using the Sabre system and SMARTS. We find that while the degree of quasi-integration is moderately explained by the Sabre link, it is more significantly explained by American Airlines' use of an expertise exploitation capability using SMARTS. These results show the necessity of extending the theoretical perspectives on IT-induced interorganizational relationships from an efficiency perspective to an expertise point of view.
|keyword = electronic channels,expertise-driven advantage,interorganizational systems,resource based approaches,airline industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The repertory grid technique: A method for the study of cognition in information systems'''
{{header}}
{{article
|author= FB Tan,MG Hunter,
|source= MIS QUARTERLY
|year= 2002
|abstract = Recent studies have confirmed the importance of understanding the cognition of users and information systems (IS) professionals. These works agree that organizational cognition is far too critical to be ignored as it can impact on IS outcomes. While cognition has been considered in a variety of IS contexts, no specific methodology has dominated. A theory and method suitable to the study of cognition-de fined as personal constructs that individuals use to understand IT in organizations-is Kelly's (1955) personal construct theory and its cognitive mapping tool known as the repertory grid (RepGrid). This article expounds on the potential of this technique to IS researchers by considering the variety of ways the RepGrid may be employed. The flexibility of the RepGrid is illustrated by examining published studies in IS. The diagnostic qualities of the RepGrid and its mapping outcomes can be used for practical intervention at the individual and organizational levels.
|keyword = cognitive mapping,repertory grid,personal construct theory,qualitative research,quantitative research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Transformational issues in researching IS and net-enabled organizations'''
{{header}}
{{article
|author= DW Straub,RT Watson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = The conduct of net-enabled business, known variously as electronic commerce (EC) or e-Business, has changed the landscape and opportunities for IS research by shifting the focus from internal to customer/partnering systems. The article examines the two primary dyadic net-enabled relationships in the marketplace: B2C and B2B. It also considers issues that extend beyond these two relationships. B2C practice and research are analyzed from: (1) consumer, (2) service, and (3) risk perspectives. Three central issues of B2B or supply chain practice and research are next considered: (1) beyond simple efficiencies, (2) innovations in B2B technology, and (3) information visibility. Finally, four overarching research issues are examined: (1) strategy, (2) organizational design, (3) metrics, and (4) managing IS. Not all research on the net-enabled organization (NEO) is IS research, and it is critical that IS journals maintain their distinctive focus. Within the bounds of the net-enabled revolution, though, the IS field has an opportunity to shape the phenomenon with timely, theory-based work that will disseminate beyond the IS academic and practitioner communities.
|keyword = net-enabled organizations (NEOs),digital business,e-Commerce,information technology,e-Business developments,future IS research issues,trends,forecasts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Membership size, communication activity, and sustainability: A resource-based model of online social structures'''
{{header}}
{{article
|author= BS Butler,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = As telecommunication networks become more common, there is an increasing interest in the factors underlying the development of online social structures. It has been proposed that these structures are new forms of organizing which are not subject to the same constraints as traditional social structures. However, from anecdotal evidence and case studies it is difficult to evaluate whether online social structures are subject to the same problems as traditional social structures. Drawing from prior studies of traditional social structures and empirical analyses of longitudinal data from a sample of Internet-based groups, this exploratory work considers the role of size and communication activity in sustainable online social structures. A resource-based theory of sustainable social structures is presented. Members contribute time, energy, and other resources, enabling a social structure to provide benefits for individuals. These benefits, which include information, influence, and social support, are the basis for a social structure's ability to attract and retain members. This model focuses on the system of opposing forces that link membership size as a component of resource availability and communication activity as an aspect of benefit provision to the sustainability of an online social structure. Analyses of data from a random sample of e-mail-based Internet social structures (listservs) indicate that communication activity and size have both positive and negative effects on a structure's sustainability. These results suggest that while the use of networked communication technologies may alter the form of communication, balancing the opposing impacts of membership size and communication activity in order to maintain resource availability and provide benefits for current members remains a fundamental problem underlying the development of sustainable online social structures.
|keyword = online communities,electronic groups,membership,dynamics,social resources,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Collaborative decision making: A connectionist paradigm for dialectical support'''
{{header}}
{{article
|author= TS Raghu,R Ramesh,AM Chang,AB Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = The facilitation and analytical support of argumentation-based collaborative decision making is the focus of this research. We model collaborative decision making as an argumentation process. We develop a connectionist modeling framework, a network representation formalism for argument structures, connectionist network mechanisms, and their models of computations to extract the behavior of argument structures. We use two examples from the case study literature to illustrate the concepts. Several interesting properties of the connectionist network models are observed from our computational results. We find that although the length of the computation is affected by parametric values, the final activation levels of the units are largely unaffected. We observe that the initial activation levels of the defeasible units seem to have no effect on their final activation levels. The proposed modeling approach generates valuable insights into the characteristics of specific argumentative discussions. While the intention of this work is not to introduce the connectionist paradigm as a means to bring arguments to a closure (resolution), we show that certain resolution mechanisms can be easily implemented under the connectionist framework.
|keyword = collaborative decision making,connectionist modeling,dialectical support,argumentation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Should optional properties be used in conceptual modelling? A theory and three empirical tests'''
{{header}}
{{article
|author= F Bodart,A Patel,M Sim,R Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = An important feature of some conceptual modelling grammars is the features they provide to allow database designers to show real-world things may or may not possess a particular attribute or relationship. In the entity-relationship model, for example, the fact that a thing may not possess an attribute can be represented by using a special symbol to indicate that the attribute is optional. Similarly, the fact that a thing may or may not be involved in a relationship can be represented by showing the minimum cardinality of the relationship as zero. Whether these practices should be followed, however, is a contentious issue. An alternative approach is to eliminate optional attributes and relationships from conceptual schema diagrams by using subtypes that have only mandatory attributes and relationships. In this paper, we first present a theory that led us to predict that optional attributes and relationships should be used in conceptual schema diagrams only when users of the diagrams require a surface-level understanding of the domain being represented by the diagrams. When users require a deep-level understanding, however, optional attributes and relationships should not be used because they undermine users' abilities to grasp important domain semantics. We describe three experiments which we then undertook to test our predictions. The results of the experiments support our predictions.
|keyword = database design,data models,entity-relationship model,semantic data models,systems theory,ontology,optional attributes,optional relationships,subtyping,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Beyond EDI: Impact of continuous replenishment program (CRP) between a manufacturer and its retailers'''
{{header}}
{{article
|author= S Raghunathan,AB Yeh,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Electronic data interchange (EDI), used traditionally to exchange business documents, has recently been extended to facilitate interorganizational collaborative processes such as the continuous replenishment program (CRP). The key characteristics of CRP are the sharing of real-time inventory data by retailers with manufacturers and continuous replenishment of retailer inventory by manufacturers. Prior research on EDI has focused on the transaction efficiency of EDI. We analyze the impact of information sharing and continuous replenishment in the CRP context and study the factors that affect the value of CRP. The study quantifies the value derived from CRP and the optimal number of retailers a manufacturer should partner with.
|keyword = electronic data interchange,continuous replenishment program,supply chain partnerships,interorganizational systems,IT justification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leadership effectiveness in global virtual teams'''
{{header}}
{{article
|author= TR Kayworth,DE Leidner,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The trend toward physically dispersed work groups has necessitated a fresh inquiry into the role and nature of team leadership in virtual settings. To accomplish this, we assembled thirteen culturally diverse global teams from locations in Europe, Mexico, and the United States, assigning each team a project leader and task to complete. The findings suggest that effective team leaders demonstrate the capability to deal with paradox and contradiction by performing multiple leadership roles simultaneously (behavioral complexity). Specifically, we discovered that highly effective virtual team leaders act in a mentoring role and exhibit a high degree of understanding (empathy) toward other team members. At the same time, effective leaders are also able to assert their authority without being perceived as overbearing or inflexible. Finally, effective leaders are found to be extremely effective at providing regular, detailed, and prompt communication with their peers and in articulating role relationships (responsibilities) among the virtual team members. This study provides useful insights for managers interested in developing global virtual teams, as well as for academics interested in pursuing virtual team research.
|keyword = collaboration technology,computer-mediated communication systems,computer-supported cooperative work,global virtual teams,virtual teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A contingency approach to software project coordination'''
{{header}}
{{article
|author= HP Andres,RW Zmud,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Before software project managers can enhance productivity and satisfaction of the software project team member, the effect of task characteristics, goal orientations, and coordination strategies on design and coding-task outcomes must be understood. A research model, which suggests that task interdependence, goal conflict, and coordination strategies significantly affect productivity and satisfaction associated with software design and coding activities, is presented. Issues such as contingency/design misfit, conflicting contingencies, and the extent of deviation to theoretically prescribed coordination mechanisms applied to contingencies are used to make predictions on productivity and process satisfaction. A 2x2x2 factorial experiment was utilized. Overall, projects characterized by low task interdependence exhibited greater productivity than projects with high task interdependence. Also, in general, organic coordination was more productive than mechanistic coordination. There was also a significant interaction between task interdependence and coordination strategy. Low goal conflict and organic coordination each lead to greater process satisfaction. Productivity results for the goal conflict manipulation was opposite to the hypothesized direction. Unconflicted contingencies addressed with consistent coordination and partially conflicted contingencies, regardless of the coordination used, exhibited significant gains in productivity. In comparison, unconflicted contingencies with inconsistent coordination and conflicted contingencies, regardless of the coordination applied, resulted in lower productivity. This suggests that there are instances where multiple contingencies, which warrant the use of different coordination strategies, can be adequately addressed with a specific coordination strategy.
|keyword = coordination,contingency theory,goal interdependence,goal conflict,multiple contingencies,process satisfaction,software project management,task interdependence,team productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relationships between job skills and performance: A study of webmasters'''
{{header}}
{{article
|author= MR Wade,M Parent,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The main purpose of this study is to determine the mix of organizational and technical skills demanded of Webmasters, and the degree to which those skills influence job performance. The study is composed of two parts. First, a job-content analysis of 800 Webmaster positions is conducted in order to determine the mix of skills demanded of Webmasters by employers. Second, a survey of 232 Webmasters is conducted to test the relationships between those skills and job performance. The job-content analysis suggested that employers seek technical skills over organizational skills, and, in contrast, the survey results showed that Webmasters regard organizational skills as more important in performing their jobs. Structured equation modeling on the survey data showed that deficiency in both technical and organizational skills leads to lower job performance. Moreover, the effect of organizational skill deficiencies on job performance was found to be larger than that of technical skill deficiencies. For researchers, the establishment of an empirical link between job skills and job performance opens the field to further research in the skills of information systems personnel. For employers, the results suggest more attention should be paid to attracting organizational skills when recruiting information systems personnel, such as Webmasters.
|keyword = electronic commerce,individual level performance,information systems skills,information systems staffing,Internet,organizational skills,PLS,technical skills,Webmasters,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of user acceptance of digital libraries: An empirical examination of individual differences and system characteristics'''
{{header}}
{{article
|author= W Hong,JYL Thong,WM Wong,KY Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The explosion in Internet usage and huge government funding initiatives in digital libraries have drawn attention to research on digital libraries. Whereas the traditional focus of digital library research has been on the technological development. there is now a call for user-focused research. Although millions of dollars have been spent on building "usable" systems, research on digital libraries has shown that potential users may not use the systems in spite of their availability. There is a need for research to identify the factors that determine users' adoption of digital libraries. Using the technology acceptance model (TAM) as a theoretical framework, this study investigates the effect of a set of individual differences (computer self-efficacy and knowledge of search domain) and system characteristics (relevance, terminology, and screen design) on intention to use digital libraries. Based on a sample of 585 users of a university's award-winning digital library, the results strongly support the utilization of TAM in predicting users' intention to adopt digital libraries, and demonstrate the effects of critical external variables on behavior intention through perceived ease of use and perceived usefulness. All of the individual differences and system characteristics have significant effects on perceived ease of use of digital libraries. In addition, relevance has the strongest effect on perceived usefulness of digital libraries.
|keyword = computer self-efficacy,digital libraries,individual differences,information technology acceptance,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Critical factors for assimilation of object-oriented programming languages'''
{{header}}
{{article
|author= H Cho,YG Kim,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Object-oriented (OO) technology was expected to rapidly replace traditional functional software technology due to its productivity and quality improvement potential in software development. Still, OO technology is not yet fully understood and utilized by information systems (IS) organizations. Despite the growing interest and attention of the IS researchers and practitioners, empirical research on the assimilation process of OO technology has been limited. The present study assesses the current status of OO technology assimilation in IS organizations and identifies the factors influencing such assimilation from a software process innovation perspective. Innovation attributes and organizational characteristics were tested as determinants of the organizational OO technology assimilation based on a survey of 220 organizations. Logistic regression analysis was used to assess the relationships of innovation and organizational variables with the level of OO technology assimilation. The findings indicate that, among the innovation characteristics, perceived complexity and perceived maturity of technology have been found to have positive relationships with organizational assimilation of OO technology. Among the organizational characteristics, intensity of new technology education was positively related to organizational assimilation of OO technology, and satisfaction with existing technology was negatively related to organizational assimilation of OO technology.
|keyword = information technology innovation,object-oriented technology assimilation,software process innovation,technology assimilation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dynamic work distribution in workflow management systems: How to balance quality and performance'''
{{header}}
{{article
|author= A Kumar,WMP van der Aalst,EMW Verbeek,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Today's workflow management systems offer work items to workers using rather primitive mechanisms. Although most workflow systems support a role-based distribution of work, they have problems dealing with unavailability of workers as a result of vacation or illness, overloading, context-dependent suitability, deadlines, and delegation. As a result, the work is offered to too few, too many, or even the wrong set of workers. Current practice is to offer a work item to one person, thus causing problems when the person is not present or too busy, or to offer it to a set of people sharing a given role, thus not incorporating the qualifications and preferences of people. Literature on work distribution is typically driven by considerations related to authorizations and permissions. However, workflow processes are operational processes where there is a highly dynamic trade-off between quality and performance. For example, an approaching deadline and an overloaded specialist may be the trigger to offer work items to less qualified workers. This paper addresses this problem by proposing a systematic approach to dynamically create a balance between quality and performance issues in workflow systems. We illustrate and evaluate the proposed approach with a realistic example and also compare how a workflow system would implement this scenario to highlight the shortcomings of current, state of the art workflow systems. Finally, a detailed simulation model is used to validate our approach.
|keyword = metrics for work distribution,workflow management,workflow systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An approach to distribution of object-oriented applications in loosely coupled networks'''
{{header}}
{{article
|author= S Purao,HK Jain,DL Nazareth,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = With the move to distributed systems and an increasing emphasis on the use of object-orientation for new system design, effective distribution of object-oriented applications is becoming an important concern for designers. Early research in this area has focused on object-clustering schemes for shared memory configurations that have limited value to business applications, which must be distributed over loosely coupled networks. These applications also exhibit the properties of simpler structural relationships and a large number of instances, demanding approaches closer to fragmentation and allocation instead of clustering. This paper develops an approach to distribution of object-oriented applications over geographically dispersed sites in loosely coupled networks-taking account of concerns such as encapsulation, inheritance, messaging, and implicit joins. The approach consists of two phases. First, we develop a scheme for generating class fragments, which ensures that encapsulation is not violated and inheritance is not stretched across sites. Second, considering the message-intensive operation of object-oriented systems, we devise models for allocation of class fragments to sites that minimize inter-site traffic. A nonarbitrary procedure to compile traffic volume estimates exploiting the notion of implicit joins in object-oriented applications provides the natural linkage between the two phases. A research prototype was implemented to establish feasibility of the proposals. We demonstrate usefulness of the approach by its application for distribution of a real-world information system.
|keyword = allocation models,distributed systems,horizontal fragmentation,object distribution,object-oriented development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Investigating the moderators of the group support systems use with meta-analysis'''
{{header}}
{{article
|author= AR Dennis,BH Wixom,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = This paper presents a meta-analysis that investigates five moderators (task, tool, the type of group, the size of the group, and facilitation) and their influences on the overall effects of group support systems (GSS). Results show that process satisfaction is higher for idea-generation tasks than for decision-making tasks. The GSS tool (that is, the use of level 1 or level 2 GSS) influences decision quality. Level 1 tools support the exchange of information, whereas, level 2 tools are designed to aid in decision-making. Decision quality is higher when using level 2 tools, however, there is no difference in the number of ideas generated when using level 1 or level 2 tools. Decision quality is lower for virtual teams, but there is no difference in the number of ideas generated between virtual teams and face-to-face teams using GSS. Group size is an important moderator when measuring decision time and satisfaction with process. The former is shorter for larger groups, and the latter is higher for larger groups. Process facilitation leads to higher decision quality and higher satisfaction with the process. These results illustrate the importance of examining the moderators of GSS use and the viability of conducting a meta-analysis to investigate a large body of research with seemingly conflicting or equivocal results.
|keyword = group support systems,groupware,meta-analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of decision guidance and problem modeling on group decision-making'''
{{header}}
{{article
|author= R Barkhi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Despite the advances in group decision support system (GDSS) research, few GDSS studies concentrate on problem-modeling tools to support decisions that cross boundaries of functional areas within the business. These decisions have a substantial effect on the profitability of the firm and account for much time and effort of senior management. This research investigates the effect of problem structuring and modeling with a GDSS on coordinated decision-making of managers in a group faced with a mixed-motive production-planning task. In a laboratory experiment, a GDSS with features supporting problem modeling is contrasted with a GDSS without such features. The results indicate that the groups using a GDSS with a problem-modeling tool outperformed the groups using a GDSS without a problem-modeling tool, but they were less efficient with respect to the time and number of messages it takes the group to converge to a final solution. User confidence in the solution did not differ between the two groups. The results of this study indicate that the problem modeling feature of a GDSS significantly influences group decision process and outcomes.
|keyword = group decision support systems,human-computer interaction,problem modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Web-based virtual learning environments: A research framework and a preliminary assessment of effectiveness in basic IT skills training'''
{{header}}
{{article
|author= G Piccoli,R Ahmad,B Ives,
|source= MIS QUARTERLY
|year= 2001
|abstract = Internet technologies are having a significant impact on the learning industry. For-profit organizations and traditional institutions of higher education have developed and are using web-based courses, but little is known about their effectiveness compared to traditional classroom education. Our work focuses on the effectiveness of a web-based virtual learning environment (VLE) in the context of basic information technology skills training. This article provides three main contributions. First, it introduces and defines the concept of VLE, discussing how a VLE differs from the traditional classroom and differentiating it from the related, but narrower, concept of computer aided instruction (CAI). Second, it presents 2 framework of VLE effectiveness, grounded in the technology-mediated learning literature, which frames the VLE research domain, and addresses the relationship between the main constructs. Finally, it focuses on one essential VLE design variable, learner control, and compares 2 web-based VLE to a traditional classroom through a longitudinal experimental design. Our results indicate that, in the context of IT basic skills training in undergraduate education, there are no significant differences in performance between students enrolled in the two environments. However, the VLE leads to higher reported computer self-efficacy, while participants report being less satisfied with the learning process.
|keyword = virtual learning environments,Web-based training,experimental research,basic skills training,information technology training,computer self-efficacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of aggregation in the measurement of IT-related organizational innovation'''
{{header}}
{{article
|author= RG Fichman,
|source= MIS QUARTERLY
|year= 2001
|abstract = The extent of organizational innovation with information technology, an important construct in the IT innovation literature, has been measured in many different ways. Some measures have a narrow focus while others aggregate innovative behaviors across a set of innovations or stages in the assimilation lifecycle. There appear to be some significant tradeoffs involving aggregation: more aggregated measures can be more robust and generalizable and can promote stronger predictive validity, while less aggregated measures allow more context-specific investigations and can preserve clearer theoretical interpretations. This article begins with a conceptual analysis that identifies the circumstances when these tradeoffs are most likely to favor aggregated measures. It is found that aggregation should be favorable when: (1) the researcher's interest is in general innovation or a model that generalizes to a class of innovations, (2) antecedents have effects in the same direction in all assimilation stages, (3) characteristics of organizations can be treated as constant across the innovations in the study, (4) characteristics of innovations Can not be treated as constant across organizations in the study, (5) the set of innovations being aggregated includes substitutes or moderate complements, and (6) sources of noise in the measurement of innovation may be present. The article then presents an empirical study using data on the adoption of software process technologies by 608 U.S. based corporations. This study-which had circumstances quite favorable to aggregation-found that aggregating across three innovations within a technology class more than doubled the variance explained compared to single innovation models. Aggregating across assimilation stages also had a slight positive effect on predictive validity. Taken together, these results provide initial confirmation of the conclusions from the conceptual analysis regarding the circumstances favoring aggregation.
|keyword = assimilation,innovation adoption,innovation diffusion,implementation,infusion,routinization,measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Revolution or evolution? A comparison of object-oriented and structured systems development methods'''
{{header}}
{{article
|author= S Sircar,SP Nerur,R Mahapatra,
|source= MIS QUARTERLY
|year= 2001
|abstract = This paper examines the changes engendered when moving from a structured to an object-oriented systems development approach and reconciles the differing views concerning whether this represents an evolutionary or revolutionary change. Author co-citation analysis is used to elucidate the ideational and conceptual relationships between the two approaches. The difference in conceptual distance at the analysis and design level compared to that at the programming level is explained using Henderson's framework for organizational change. The conceptual shift during analysis and design is considered architectural, whereas for programming it is deemed merely incremental. The managerial implications of these findings are discussed and suggestions for improving the likelihood of success in the adoption of object-oriented systems development methods are provided.
|keyword = IS development,structured development approach,object-oriented approach,software development methodologies,author co-citation analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Introducing a third dimension in information systems design the case for incentive alignment'''
{{header}}
{{article
|author= SL Ba,J Stallaert,AB Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Prior research has generated considerable knowledge on information systems design from software engineering and user-acceptance perspectives. As organizational processes are increasingly embedded within information systems, one of the key considerations of many business processes-organizational incentives-should become an important dimension of any information systems design and evaluation, which we categorize as the third dimension: incentive alignment. Incentive issues have become important in many IS areas, including distributed decision support systems (DSS), knowledge management, and e-business supply chain coordination. In thus paper we outline why incentives are important in each of these areas and specify requirements for designing incentive-aligned information systems. We identify and define important unresolved problems along the incentive-alignment dimension of information systems and present a research agenda to address them.
|keyword = information systems design,incentive alignment,distributed decision support systems,knowledge management,supply chain coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business planning for network services: A systems thinking approach'''
{{header}}
{{article
|author= A Dutta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = As demand for online network services continues to grow, service providers are looking to meet this need and avail themselves of business opportunities. However, despite strong growth in demand, providers continue to have difficulty achieving profitability, customer churn remains high, and network performance continues to draw complaints. We suggest that strategic business planning for network services would benefit from a systems thinking approach that analyzes the feedback effects present in the underlying business process. These feedback loops can be complex and have significant impact on business performance. For instance, while the size of a provider's customer base depends on price and network performance, network performance is itself dependent on the size of the customer base. In this paper, we develop a planning model that represents these feedback effects using the finite difference equations methodology of systems dynamics. The model is validated by showing its fit with essential characteristics of the underlying problem domain, and by showing its ability to replicate observed reference mode behaviors. Simulations are then carried out under a variety of scenarios to examine issues important to service providers. Among other findings, the simulations suggest that (a) under flat-rate pricing, lowering price to increase customer base can hurt profitability as well as network performance; (b) under usage-based pricing, lowering price need not necessarily lead to a larger customer base; and (c) in addition to price, the customers' threshold of tolerance for performance degradation plays a significant role in balancing market share with profitability. We briefly present a prototype decision support system based on the systems thinking approach, and suggest ways in which it could be used to help business planning for network services.
|keyword = online services,systems dynamics,business performance,decision support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On heterogeneous database retrieval: A cognitively guided approach'''
{{header}}
{{article
|author= R Krishnan,XP Li,D Steier,JL Zhao,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Retrieving information from heterogeneous database systems involves a complex process and remains a challenging research area. We propose a cognitively guided approach for developing an information-retrieval agent that takes the user's information request, identifies relevant information sources, and generates a multidatabase access plan. Our work is distinctive in that the agent design is based on an empirical study of how human experts retrieve information from multiple, heterogeneous database systems. To improve on empirically observed information-retrieval capabilities, the design incorporates mathematical models and algorithmic components. These components optimize the set of information sources that need to be considered to respond to a user query and are used to develop efficient multidatabase-access plans. This agent design, which integrates cognitive and mathematical models, has been implemented using Soar, a knowledge-based architecture.
|keyword = global query optimization,heterogeneous database systems,information integration,maximal objects,soar-based AI systems,universal relational model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Empirical test of an EDI adoption model'''
{{header}}
{{article
|author= P Chwelos,I Benbasat,AS Dexter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = This paper is the first test of a parsimonious model that posits three factors as determinants of the adoption of electronic data interchange (EDI): readiness, perceived benefits, and external pressure. To construct the model, we identified and organized the factors that were found to be influential in prior EDI research. By testing all these factors together in one model, we are able to investigate their relative contributions to EDI adoption decisions. Senior purchasing managers, chosen for their experience with EDI and proximity to the EDI adoption decision, were surveyed and their responses analyzed using structural equation modeling. All three determinants were found to be significant predictors of intent to adopt EDI, with external pressure and readiness being considerably more important than perceived benefits. We show that the constructs in this model can be categorized into three levels: technological, organizational, and interorganizational. We hypothesize that these categories of influence will also be determinants of the adoption of other emerging forms of interorganizational systems (IOS).
|keyword = electronic data interchange,electronic commerce,interorganizational systems,adoption of IT,empirical research,partial least squares,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Modifying paradigms - Individual differences, creativity techniques, and exposure to ideas in group idea generation'''
{{header}}
{{article
|author= MJ Garfield,NJ Taylor,AR Dennis,JW Satzinger,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = In today's networked economy, ideas that challenge existing business models and paradigms are becoming more important. This study investigated how individual differences, groupware-based creativity techniques, and ideas from others influenced the type of ideas that individuals generated. While individual differences were important (in that some individuals were inherently more likely to generate ideas that followed the existing problem paradigm while others were more likely to generate paradigm-modifying ideas that attempted to change the problem paradigm), the exposure to paradigm-modifying ideas from others and the use of intuitive groupware-based creativity techniques rather than analytical groupware-based creativity techniques were found to increase the number of paradigm-modifying ideas produced.
|keyword = groupware,creativity,idea generation,feedback,creativity techniques,individual differences,cognitive style,group simulator,Myers-Briggs type indicator,MBTI,Kirton adaption-innovation inventory,KAI,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of technology investments on a firm's production efficiency, product quality, and productivity'''
{{header}}
{{article
|author= ME Thatcher,JR Oliver,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = For over a decade, empirical studies in the information technology (IT) value literature have examined the impact of technology investments on various measures of performance. However, the results of these studies, especially those examining the contribution of IT to productivity, have been mixed. One reason for these mixed empirical findings may be that these studies have not effectively accounted for the impact of technology investments that increase production efficiency and improve product quality on firm productivity. In particular, it is commonly assumed that such investments should lead to gains in both profits and productivity. However, using a closed-form analytical model we challenge this underlying assumption and demonstrate that investments in certain efficiency-enhancing technologies may be expected to decrease the productivity of profit-maximizing firms. More specifically, we demonstrate that investments in technologies that reduce the firm's fixed overhead costs do not affect the firm's product quality and pricing decisions but do increase profits and improve productivity. In addition, we demonstrate that investments in technologies that reduce the variable costs of designing, developing, and manufacturing a product encourage the firm to improve product quality and to charge a higher price. Although this adjustment helps the firm to capture higher profits, we show that it will also increase total production costs and will, under a range of conditions, decrease firm productivity. Finally, we show that the direction of firm productivity following such investments depends upon the relationship between the fixed costs of the firm and the size of the market.
|keyword = analytical modeling,information technology value,product quality,production efficiency,productivity paradox,technology investments,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Should we wait? Network externalities, compatibility, and electronic billing adoption'''
{{header}}
{{article
|author= YA Au,RJ Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = This study examines the adoption of electronic bill presentment and payment (EBPP) technology. EBPP continues to grow and will become a multibillion dollar e-commerce industry. The technology adoption configuration in this context is quite interesting because it involves four stakeholders: billers, bill consolidators, banks, and consumers. Banks and bill consolidators compete to act as an intermediary between billers and consumers. Network externalities play a significant role: the more billers that adopt the technology, the more consumers are willing to use the services. Our analysis is based on the welfare economics concept of finding the socially optimum adoption configuration and the resulting adoption pattern in a market with sponsored technologies. The results show that due to network externalities, billers are more likely to adopt the existing technology early, though the next technology might be superior to the current one. When the higher costs of early adoption are taken into account, the model shows that billers are more willing to wait, ceteris paribus. Our results also show that anticipation of a new and better, but compatible, technology might cause billers to wait, depending on what benefits they expect by adopting early, and how much cost they anticipate to incur upgrading their technology later.
|keyword = electronic billing presentment and payment (EBPP),electronic commerce,financial services network externalities,standards,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Achieving the optimal balance between investment in quality and investment in self-promotion for information products'''
{{header}}
{{article
|author= R Aron,EK Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = When producers of goods (or services) are confronted by a situation in which their offerings no longer perfectly match consumer preferences, they must determine the extent to which the advertised features of the product reflect the product's actual attributes. We find that the two important determinants of sellers' advertising strategy are the Repeg Cost Ratio, and the Repeat Sales Coefficient. The interplay of these two factors gives rise to four possible strategic scenarios. We show that sellers' strategy is clearly explainable in three out of these four scenarios. In the ambiguous fourth scenario, we show that sellers' strategy for information production goods will differ considerably from information consumption goods based on product complexity and cost of product return (borne by the buyer). Finally, we demonstrate that markets are often characterized by self-reinforcing limits on the extent of opportunistic advertising by sellers.
|keyword = information products,internet advertising,product positioning,signaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information goods and vertical differentiation'''
{{header}}
{{article
|author= HK Bhargava,V Choudhary,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Second-degree price discrimination, that is, vertical differentiation, is widely practiced by firms selling physical goods to consumers with heterogeneous valuations. This strategy leads to market segmentation and has been shown to be optimal by many researchers. On the other hand, researchers have also demonstrated, under certain restrictive conditions, that vertical differentiation may not be optimal for information goods. We analyze vertical differentiation for a monopolist, continuing the practice of modeling consumer valuation as a linear function of product quality and consumer type but generalizing assumptions about marginal costs and consumer distributions. We show that the firm's optimal product line depends on the benefit-to-cost ratio of qualities in the choice vector. We find that a vertical differentiation strategy is not optimal when the highest quality product has the best benefit-to-cost ratio. Many information goods satisfy this property.
|keyword = information goods,market segmentation,multiproduct monopoly,price discrimination,versioning,vertical differentiation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Forward versus spot buying of information goods'''
{{header}}
{{article
|author= P Gundepudi,N Rudi,A Seidmann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Several information goods, such as movie distribution rights or newspapers, are sold either at spot prices, or through forward subscription buying. Our paper considers a firm that offers an information good through spot buying, forward buying at a reduced price, or a combination of the two. The time lag between forward buying and spot buying brings about an uncertainty in a consumer's reservation price for the good at the time of advance purchase. We propose a consumer decision-making model that captures this fundamental feature and provides interesting insights into the key elements of consumer behavior. We establish that a consumer offered the choice between forward buying and waiting to (possibly) buy the good on spot faces the tradeoff between a lower unit price and the value of updated preferences. We also establish that consumers preferring forward buying have a relatively high expectation and low uncertainty in their reservation prices for the good at the time of advance purchase, while those preferring spot buying have a relatively low expectation and high uncertainty in their reservation prices for the good. We apply the model to formulate and analyze the firm's problem when it is either a price taker or a price setter. When the firm is a price taker, the choice is whether to offer the good for only forward buying, only spot buying, or a combination of the two. With an example, we show that when both the spot price and the discount on forward buying are moderate in values, the seller chooses the mixed strategy of offering both forward and spot buying simultaneously. When the firm is a price setter, the goal is to choose the offering(s) and the price level(s). With the example, we show how firms selling information goods can increase their revenues by using a mixed offering strategy with both spot and forward offerings. This strategy lends itself to second-degree price discrimination by the seller when there are groups of customers potentially heterogeneous in terms of the distribution of their reservation prices. Our work takes significant importance in the context of information goods, which are becoming increasingly prominent and are being delivered on the Web through the mechanisms of forward and spot buying.
|keyword = forward buying,information goods,spot buying,subscription,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Retail bank services strategy: A model of traditional, electronic, and mixed distribution choices'''
{{header}}
{{article
|author= RE Byers,PJ Lederer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Design of a retail banking distribution strategy is an important issue in that industry. This paper shows the effect of new electronic distribution technologies such as PC banking on the choice of a bank's distribution strategy. We present a competitive model of distribution strategy choice, including heterogeneous consumers and banks, that allows a rich variety of customer preference and technology cost parameters. Sensitivity analysis shows how several parameters affect the competitive outcome. This analysis suggests that changing consumer behavior and attitudes, instead of banks' cost structure with new technologies significantly affects the bank's distribution strategy choice. If the segment of consumers that prefers PC banking remains small relative to the segment that prefers branches, then there will still be a market for specialized branch banks. Branch banking without PC banking services will be a viable strategy until the segment that prefers PC banking grows larger (amounting to about 40 percent of all transactions). Banks offering both branch and PC banking services can prevent successful and profitable entry by virtual banks (Internet banks offering only PC banking services) as long as the segment of customers that prefer PC banking remains relatively small (less than two-thirds of all transactions). Beyond this fraction, virtual banks will be profitable. This analysis suggests that it may be a long time (if ever) before virtual banks turn a profit.
|keyword = automated teller machines,ATM,banking industry,branch banking,channel selection,distribution channels,electronic banking,financial services,PC banking,retail banking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''New buyers' arrival under dynamic pricing market microstructure: The case of group-buying discounts on the Internet'''
{{header}}
{{article
|author= RJ Kauffman,B Wang,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Dynamic pricing mechanisms occur on the Internet when buyers and sellers negotiate the final transaction price for the exchange of goods or services. These mechanisms are used in online auctions (e.g., eBay.com, uBid.com) and name-your-own-price (Priceline.com) formats, for example. The current research studies the dynamics of one instance of dynamic pricing-group-buying discounts-used by MobShop.com., whose products' selling prices drop as more buyers place their orders. We collect and analyze changes in the number of orders for MobShop-listed products over various periods of time, using an econometric model that reflects our understanding of bidder behavior in the presence of dynamic pricing and different levels of bidder participation. We find that the number of existing orders has a significant positive effect on new orders placed during each three-hour period, indicating the presence of a positive participation externality effect. We also find evidence for expectations of falling prices, a price drop effect. This occurs when the number of orders approaches the next price drop level and the price level for transacting will fall in the near future. The results also reveal a significant ending effect, as more orders were placed during the last three-hour period of the auction cycles. We also assess the efficacy of group-buying business models to shed light on the recent failures of many group-buying Web sites.
|keyword = bidding,dynamic pricing,electronic markets,group-buying discounts,Internet-based selling,market microstructure,online retailing,pricing mechanisms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Keeping mum as the project goes under: Toward an explanatory model'''
{{header}}
{{article
|author= HJ Smith,M Keil,G Depledge,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The problem of "runaway" information systems (IS) projects can be exacerbated by the reluctance of organizational members to transmit negative information concerning a project and its status. Drawing upon relevant bodies of literature, this paper presents a model of the reluctance to report negative project news and develops hypotheses to be tested. An experiment, which was designed to test these hypotheses for both internal and external reporting alternatives, is then described. Two factors are manipulated: (1) the level of impact associated with project failure should an individual fail to report negative information, and (2) the level of observed behavioral wrongdoing associated with the project. The results explain a significant portion of the variance in the reluctance to report negative information and suggest that there are some differences in internal and external reporting behavior. Implications for research and practice are discussed.
|keyword = information system development,mum effect,project management,whistle-blowing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A method for generation of alternatives by decision support systems'''
{{header}}
{{article
|author= B Fazlollahi,R Vahidov,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = An essential feature of active Decision Support Systems (DSS) is the ability to take the initiative in performing decision-related tasks. One possibility for providing active high cognitive level decision support is through facilitating alternative generation in DSS. The method proposed in this work enables the generation of several diverse alternatives in a single run. The method relies on the principles of effective problem-solving/decision-making and facilitates divergent processes, the separation of alternative generation from evaluation, as well as the diminishing of human cognitive biases. A hybrid DSS based on genetic algorithms (GA) and fuzzy sets is used to operationalize the approach. The paper outlines the design requirements for alternative generation in DSS and discusses the inadequacies of the "what-if" simulation and traditional optimization methods in light of these requirements. The paper further elaborates on the appropriateness of GA as a tool for alternative generation in DSS for solving complex ill-structured problems. The method is illustrated using marketing mix problem in a simulated business environment. The results suggest that the GA-based alternative generation leads to promising diverse alternatives. An active DSS incorporating the proposed method reduces the time-consuming manual search for promising alternatives and provides a higher degree of man-machine collaboration.
|keyword = alternative generation,decision support systems,genetic algorithms,marketing mix problem,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Work outcomes and job design for contract versus permanent information systems professionals on software development teams'''
{{header}}
{{article
|author= S Ang,SA Slaughter,
|source= MIS QUARTERLY
|year= 2001
|abstract = Organizations have significantly increased their use of contracting in information systems (IS), hiring contractors to work with permanent professionals. Based on theories of social exchange and social comparison, we hypothesize differences in work attitudes, behaviors, and performance across the two groups, and evaluate our hypotheses with a sequential mixed-methods design. Our first study surveys contract and permanent professionals on software development teams in a large transportation company. Our second study involves in-depth interviews with contract and permanent IS professionals in three organizations. We find support for many of our hypotheses but also some surprising results. Contrary to our predictions, contractors perceive a more favorable work environment than permanent professionals but exhibit lower in-role and extra-role behaviors than their permanent counterparts. Supervisors perceive their contract subordinates as lower-performing and less loyal, obedient, and trustworthy. In-depth interviews help to explain these findings. Job design emerges as an important factor influencing contractors' work attitudes, behaviors, and performance. Supervisors restrict the scope of contractors' jobs, limiting their job behaviors and performance. To compensate, permanent professionals are assigned considerably enlarged job scopes, leading to their lower perceptions of the work environment. We propose a theoretical model that embraces job design in explaining differences in work outcomes for contract versus permanent professionals on software development teams. The results from our study imply that organizations should carefully design and balance the jobs of their contractors and permanent employees to improve attitudes, behaviors, and workplace performance.
|keyword = IS contracting,IS staffing issues,management of IS,IS project teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding information systems continuance: An expectation-confirmation model'''
{{header}}
{{article
|author= A Bhattacherjee,
|source= MIS QUARTERLY
|year= 2001
|abstract = This paper examines cognitive beliefs and affect influencing one's intention to continue using (continuance) information systems (IS). Expectation-confirmation theory is adapted from the consumer behavior literature and integrated with theoretical and empirical findings from prior IS usage research to theorize a model of IS continuance. Five research hypotheses derived from this model are empirically validated using a field survey of online banking users. The results suggest that users' continuance intention is determined by their satisfaction with IS use and perceived usefulness of continued IS use. User satisfaction, in turn, is influenced by their confirmation of expectation from prior IS use and perceived usefulness, Post-acceptance perceived usefulness is influenced by users' confirmation level. This study draws attention to the substantive differences between acceptance and continuance behaviors, theorizes and validates one of the earliest theoretical models of IS continuance, integrates confirmation and user satisfaction constructs within our current understanding of IS use, conceptualizes and creates an initial scale for measuring IS continuance, and offers an initial explanation for the acceptance-discontinuance anomaly.
|keyword = IS use,continuance,acceptance,user satisfaction,confirmation,expectation-confirmation theory,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Media and group cohesion: Relative influences on social presence, task participation, and group consensus'''
{{header}}
{{article
|author= Y Yoo,M Alavi,
|source= MIS QUARTERLY
|year= 2001
|abstract = Organizations deploy advanced communication media such as audio and videoconferencing to enhance and extend group communication interactions. However, established groups (i.e., groups with a history of working together) can view and use the same technology differently from groups without any past experiences of working together. This study examines the relative influences of media condition and group cohesion on social presence, task participation, and group consensus. Results from a controlled laboratory experiment with 45 triads of college students working on a decision-making task showed that media condition (audio conferencing vs. desktop videoconferencing) has significantly smaller influences on social presence and task participation than group cohesion in established groups. The study found that influence of group cohesion over social presence is additive, rather than substitutive, to that of media condition. The study also established that task participation played a more important role than social presence in determining the degree of consensus among group members in computer-mediated communication environments.
|keyword = desktop videoconferencing,group cohesion,social presence,media richness,group history,group consensus,task participation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Desperately seeking the "IT" in IT research - A call to theorizing the IT artifact'''
{{header}}
{{article
|author= WJ Orlikowski,CS Iacono,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = The field of information systems is premised on the centrality of information technology in everyday socio-economic life. Yet, drawing on a review of the full set of articles published in Information Systems Research (ISR) over the past ten years, we argue that the field has not deeply engaged its core subject matter-the information technology (IT) artifact. Instead, we find that IS researchers tend to give central theoretical significance to the context (within which some usually unspecified technology is seen to operate), the discrete processing capabilities of the artifact las separable from its context or use), or the dependent variable (that which is posited to be affected or changed as technology is developed, implemented, and used). The IT artifact itself tends to disappear from view, be taken for granted, or is presumed to be unproblematic once it is built and installed. After discussing the implications of our findings, we propose a research direction for the IS field that begins to take technology as seriously as its effects, context, and capabilities. In particular, we propose that IS researchers begin to theorize specifically about IT artifacts, and then incorporate these theories explicitly into their studies. We believe that such a research direction is critical if IS research is to make a significant contribution to the understanding of a world increasingly suffused with ubiquitous, interdependent, and emergent information technologies.
|keyword = information systems research,information technology,IT research,IT theory,technological artifacts,technology change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of e-commerce announcements on the market value of firms'''
{{header}}
{{article
|author= M Subramani,E Walden,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Firms are undertaking growing numbers of e-commerce initiatives and increasingly making significant investments required to participate in the growing online market. However, empirical support for the benefits to firms from e-commerce is weaker than glowing accounts in the popular press, based on anecdotal evidence, would lead us to believe. In this paper, we explore the following questions: What are the returns to shareholders in firms engaging in e-commerce? How do the returns to conventional, brick and mortar firms from e-commerce initiatives compare with returns to the new breed of net firms? How do returns from business-to-business e-commerce compare with returns from business-to-consumer e-commerce? How do the returns to e-commerce initiatives involving digital goods compare to initiatives involving tangible goods? We examine these issues using event study methodology and assess the cumulative abnormal returns to shareholders (CARs) for 251 e-commerce initiatives announced by firms between October and December 1998. The results suggest that e-commerce initiatives do indeed lead to significant positive CARs for firms' shareholders. While the CARs for conventional firms are not significantly different from those for net firms, the CARs for business-to-consumer (B2C) announcements are higher than those for business-to-business (B2B) announcements. Also, the CARs with respect to e-commerce initiatives involving tangible goods are higher than for those involving digital goods. Our data were collected in the last quarter of 1998 during a unique bull market period and the magnitudes of CARs (between 4.9 and 23.4% for different subsamples) in response to e-commerce announcements are larger than those reported for a variety of other firm actions in prior event studies. This paper presents the first empirical test of the dot com effect, validating popular anticipations of significant future benefits to firms entering into e-commerce arrangements.
|keyword = event study,electronic commerce,market value,resource-based view,business-to-business,business-to-consumer,digital goods,tangible goods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Technology requirements and work group communication for telecommuters'''
{{header}}
{{article
|author= F Belanger,RW Collins,PH Cheney,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = As organizations implement more and more distributed work arrangements such as telecommuting, there is a need to understand the determinants of success of this new work setting. This research investigated three variables believed to impact outcomes in telecommuting: the availability of information system technology, the availability of communication technologies, and the communication patterns of telecommuters within their work groups. Two perspectives are used in this study. The direct effects of these three variables on perceived productivity, performance, and satisfaction were tested. A second perspective, based on the concept of fit and contingency theory, posits that successful telecommuting outcomes, measured by perceived productivity, performance, and satisfaction, are predicted by interactions between these independent variables. The study was conducted by surveying multiple respondents from different organizations who were members of work groups in which some or all employees were telecommuters. The results indicate that technology variables positively impact productivity, performance, and satisfaction of telecommuters, while the interaction between the technology variables is significant in predicting perceived productivity. Work group communication, as measured by the centrality of individuals, negatively affects perceived productivity and performance. The paper presents a discussion of the theoretical significance of these findings, and offers recommendations for future research.
|keyword = telecommuting,telecommuters,fit,communication technology,information system technology,contingency theory,work group communication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An evaluation of self-organizing map networks as a robust alternative to factor analysis in data mining applications'''
{{header}}
{{article
|author= MY Kiang,A Kumar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Kohonen's self-organizing map (SOM) network is one of the most important network architectures developed during the 1980s. The main function of SOM networks is to map the input data from an n-dimensional space to a lower dimensional (usually one- or two-dimensional) plot while maintaining the original topological relations. Therefore, it can be viewed as an analog of factor analysis. Ln this research, we evaluate the feasibility of using SOM networks as a robust alternative to factor analysis and clustering for data mining applications. Specifically, we compare SOM network solutions to factor analytic and K-Means clustering solutions on simulated data sets with known underlying factor and cluster structures. The comparisons indicate that the SOM networks provide solutions superior to unrotated factor solutions in general and provide more accurate recovery of underlying cluster structures when the input data are skewed. Our findings suggest that SOM networks can provide robust alternatives to traditional factor analysis and clustering techniques in data mining applications.
|keyword = data mining,Kohonen networks,factor analysis,data reductive,clustering analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of time pressure on quality in software development: An agency model'''
{{header}}
{{article
|author= RD Austin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = An agency framework is used to model the behavior of software developers as they weigh concerns about product quality against concerns about missing individual task deadlines. Developers who care about quality but fear the career impact of missed deadlines may take "shortcuts." Managers sometimes attempt to reduce this risk via their deadline-setting policies; a common method involves adding slack to best estimates when setting deadlines to partially alleviate the time pressures believed to encourage shortcut-taking. This paper derives a formal relationship between deadline-setting policies and software product quality. It shows that: (1) adding slack does not always preserve quality, thus, systematically adding slack is an incomplete policy for minimizing costs; (2) costs can be minimized by adopting policies that permit estimates of completion dates and deadlines that are different and; (3) contrary to casual intuition, shortcut-taking can be eliminated by setting deadlines aggressively, thereby maintaining or even increasing the time pressures under which developers work.
|keyword = agency theory,principal-agent,software quality,software measurement,software estimating,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Richness versus parsimony in modeling technology adoption decisions-understanding merchant adoption of a smart card-based payment system'''
{{header}}
{{article
|author= CR Plouffe,JS Hulland,M Vandenbosch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = The Technology Acceptance Model (TAM) has received considerable research attention in the IS field over the past decade, placing an emphasis on the roles played by perceived ease-of-use and perceived usefulness in influencing technology adoption decisions. Meanwhile, alternative sets of antecedents to adoption have received less attention. In this paper, sets of antecedent constructs drawn from both TAM and the Perceived Characteristics of Innovating (PCI) inventory are tested and subsequently compared with one another. The comparison is done in the context of a large-scale market trial of a smart card-based electronic payment system being evaluated by a group of retailers and merchants. The PCI set of antecedents explains substantially more variance than does TAM, while also providing managers with more detailed information regarding the antecedents driving technology innovation adoption.
|keyword = TAM,PCI,adoption,managers,perceptions,attitudes,intentions,field study,high technology,smart cards,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''General perspectives on knowledge management: Fostering a research agenda'''
{{header}}
{{article
|author= V Grover,TH Davenport,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = We trace in pragmatic terms some of what we know about knowledge, information technology, knowledge management practice and research, and provide two complementary frameworks that highlight potential opportunities for building a research agenda in this area. The papers in this special issue are then discussed.
|keyword = information technology,knowledge,knowledge management,knowledge market,knowledge process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational knowledge management: A contingency perspective'''
{{header}}
{{article
|author= I Becerra-Fernandez,R Sabherwal,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Prior research examines several knowledge management processes, considering each as universally appropriate. Instead, we propose that the context influences the suitability of a knowledge management process. We develop a contingency framework, including two attributes of the organizational subunit's tasks: process or content orientation, and focused or broad domain, and links knowledge management processes to them: internalization for focused, process-oriented tasks; externalization for focused, content-oriented tasks; combination for broad, content-oriented tasks; and socialization for broad, process-oriented tasks. The empirical research was done at the Kennedy Space Center (KSC), based on several interviews and survey data from 159 individuals across 8 subunits. The results supported the contingency framework. All the knowledge management processes except externalization had a positive impact in the expected cell. At the overall level, combination and externalization, but not internalization and socialization, affect knowledge satisfaction. Some implications for practice and research are identified.
|keyword = contingency theory,knowledge management,structural equation modeling,task characteristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward a theory of knowledge reuse: Types of knowledge reuse situations and factors in reuse success'''
{{header}}
{{article
|author= ML Markus,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = This paper represents a step toward a theory of knowledge reusability, with emphasis on knowledge management systems and repositories, often called organizational memory systems. Synthesis of evidence from a wide variety of sources suggests four distinct types of knowledge reuse situations according to the knowledge reuser and the purpose of knowledge reuse. The types involve shared work producers, who produce knowledge they later reuse; shared work practitioners, who reuse each other's knowledge contributions; expertise-seeking novices; and secondary knowledge miners. Each type of knowledge reuser has different requirements for knowledge repositories. Owing to how repositories are created, reusers' requirements often remain unmet. Repositories often require considerable rework to be useful for new reusers, but knowledge producers rarely have the resources and incentives to do a good job of repurposing knowledge. Solutions include careful use of incentives and human and technical intermediaries.
|keyword = collaboration,communities of practice,experts,group work,intermediaries,knowledge management,knowledge repositories,knowledge reuse,novices,organization memory,teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using mentoring and storytelling to transfer knowledge in the workplace'''
{{header}}
{{article
|author= W Swap,D Leonard,M Shields,L Abrams,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The core capabilities of an organization include critical skills of employees, management systems, and norms and values. Core capabilities may be transferred formally and explicitly. However, much knowledge, particularly knowledge with rich tacit dimensions, is transferred informally through processes of socialization and internalization. We focus on two transfer mechanisms - mentoring and storytelling - that can leverage the knowledge of an organization, particularly its tacit knowledge, to build core capabilities. We draw on relevant research in learning and cognitive psychology to clarify the conditions under which mentoring and storytelling can be most effective as carriers of knowledge. Finally, we present recommendations for specific managerial practices that follow from our analysis.
|keyword = cognition,core capabilities,informal learning,mentoring,stories,tacit knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Situated learning and the situated knowledge web: Exploring the ground beneath knowledge management'''
{{header}}
{{article
|author= SR Nidumolu,M Subramani,A Aldrich,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Knowledge is now recognized as an important basis for competitive advantage and many firms are beginning to establish initiatives to leverage and manage organizational knowledge. These include efforts to codify knowledge in repositories as well as efforts to link individuals using information technologies to overcome geographic and temporal barriers to accessing knowledge and expertise. We suggest that Knowledge Management (KM) efforts, to be successful, need to be sensitive to features of the context of generation, location, and application of knowledge. To this end, we highlight the situated organizational learning perspective that views knowledge as embedded in individuals, in connections between individuals, and in artifacts as a useful lens to examine phenomena related to the establishment of KM initiatives. In an ethnographic case study of an effort to change knowledge-work processes in a market research firm, we apply the situated knowledge perspective to highlight the factors responsible for the limited success of the initiative in the firm. This study suggests that a consideration of the situated knowledge web and the alignment of the initiatives with the features of the knowledge web are central to success in knowledge management efforts in firms.
|keyword = ethnographic methods,knowledge management,organizational change,qualitative research,situated knowledge web,situated learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring perceptions of organizational ownership of information and expertise'''
{{header}}
{{article
|author= SL Jarvenpaa,DS Staples,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Beliefs of organizational ownership relate to whether information and knowledge created by an individual knowledge worker are believed to be owned by the organization. Beliefs about property rights affect information and knowledge sharing. This study explored factors that help determine an individual's beliefs about the organizational ownership of information and expertise that he or she has created. Four different situations of organizational ownership (information vs. expertise/internal vs, external sharing) were considered. The study found that a belief in self-ownership was positively associated with organizational ownership-suggesting a collaborative type of ownership situation for both information and expertise and for both internal (intraorganizational) and external (interorganizational) sharing situations. Organizational culture and the type of employee also influenced the beliefs of organizational ownership in all four scenarios. We conclude the paper with implications for practice and future research.
|keyword = information sharing,knowledge management,organizational culture,ownership of information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge management strategies: Toward a taxonomy'''
{{header}}
{{article
|author= M Earl,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = This paper draws on primary and secondary data to propose a taxonomy of strategies, or "schools," for knowledge management. The primary purpose of this framework is to guide executives on choices to initiate knowledge management projects according to goals, organizational character, and technological, behavioral, or economic biases. It may also be useful to teachers in demonstrating the scope of knowledge management and to researchers in generating propositions for further study.
|keyword = business strategy,knowledge,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interpersonal conflict and its management in information system development'''
{{header}}
{{article
|author= H Barki,J Hartwick,
|source= MIS QUARTERLY
|year= 2001
|abstract = Researchers from a wide range of management areas agree that conflicts are an important part of organizational life and that their study is important. Yet, interpersonal conflict is a neglected topic in information system development (ISD). Based on definitional properties of interpersonal conflict identified in the management and organizational behavior literatures, this paper tests a model of how individuals participating in ISD projects perceive interpersonal conflict and examines the relationships between interpersonal conflict, management of the conflict, and ISD outcomes. Questionnaire data was obtained from 265 IS staff and 272 users working on 162 ISD projects. Results indicated that the construct of interpersonal conflict was reflected by three key dimensions: disagreement, interference, and negative emotion. While conflict management was found to have positive effects on ISD outcomes, it did not substantially mitigate the negative effects of interpersonal conflict on these outcomes. In other words, the impact of interpersonal conflict was perceived to be negative, regardless of how it was managed or resolved.
|keyword = IS project management,IS project teams,user/analyst interaction,conflict resolution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Radical innovation without collocation: A case study at Boeing-Rocketdyne'''
{{header}}
{{article
|author= A Malhotra,A Majchrzak,R Carman,V Lott,
|source= MIS QUARTERLY
|year= 2001
|abstract = This paper describes how a unique type of virtual team, deploying a computer-mediated collaborative technology, developed a radically new product The uniqueness of the team-what we call VC3 teams, for Virtual Cross-value-chain, Creative Collaborative Teams-stemmed from the fact that it was inter-organizational and virtual, and had to compete for the attention of team members who also belong to collocated teams within their own organizations. Existing research on virtual teams does not fully address the challenges of such VC3 teams. Using the case of Boeing-Rocketdyne, we describe the behavior of members of a VC3 team to derive implications for research on virtual teaming, especially for studying teams within emerging contexts such as the one we observed. The data we collected also allowed us to identify successful managerial practices and develop recommendations for managers responsible for such teams.
|keyword = virtual teams,supply-chain collaboration,innovation,collaboration technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alignment between business and IS strategies: A study of prospectors, analyzers, and defenders'''
{{header}}
{{article
|author= R Sabherwal,YE Chan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Alignment between business strategy and IS strategy is widely believed to improve business performance. This paper examines the impact of alignment on perceived business performance using Miles and Snow's popular classification of Defender, Analyzer, and Prospector business strategies. A priori theoretical profiles for these business strategies are developed using Venkatraman's (1989a) measure of business strategy. Theoretical profiles for IS strategies are developed in terms of four types of systems-operational support systems, market information systems, strategic decision-support systems, and interorganizational systems. Empirical data from two multirespondent surveys of 164 and 62 companies, respectively, are analyzed. Results indicate that alignment affects perceived business performance but only in some organizations. Alignment seems to influence overall business success in Prospectors and Analyzers but not in Defenders. Implications for future research and practice are discussed.
|keyword = alignment,information systems strategy,strategy profiles,defenders, analyzers, prospectors,profile deviation approach,strategic information systems management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A foundation for flexible automated electronic communication'''
{{header}}
{{article
|author= SA Moore,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = In this paper the author describes a formal language for communication based on linguistics-more specifically, a theory of natural language communication and models of natural language conversations. The language has a small number of general message types that are formally defined by their intended effects on the recipient. For each message type he defines a standard automated method of responding that depends only on the message type and is independent of the message's content. For more complex conversations he provides methods for responding that do depend on the content. In this system, a message's sender-automated or human-constructs and sends a message knowing that he cannot know, but can only predict, how it will be interpreted. The agent receiving the message interprets it and then uses it as a basis for inferring how he should respond. The message interpretation mechanism for this language is reusable, modular, and shared by all applications. The benefit of this communication system is that it makes the communication infrastructure more flexible, easier to modify, easier to expand, and more capable.
|keyword = agent communication language,formal communication,EDI,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cognitive support for real-time dynamic decision making'''
{{header}}
{{article
|author= FJ Lerch,DE Harter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = This research examines how decision makers manage their attentional resources when making a series of interdependent decisions in a real-time environment. Decision strategies for real-time dynamic tasks consist of two main overlapping cognitive activities: monitoring and control. Monitoring refers to decision makers' tracking of key system variables as they work toward arriving at a decision. Control refers to the decision maker's generation, evaluation, and selection of alternative actions. In real-time tasks, these two activities compete for the same attentional resources. The questions that motivate the two studies presented here are: (1) can decision making be improved by increasing individuals' attentional resources, thereby enhancing their ability to monitor the system, and (2) can decision making be improved by providing individuals with feedback and/or feedforward control support? Our findings show that some kinds of cognitive support degrade performance, rather than enhance it. These results indicate that providing support for real-time dynamic decision making may be very difficult, and that designing effective decision aids requires a detailed understanding of the underlying cognitive processes.
|keyword = decision support,dynamic decision making,real-time environments,individual differences,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A conceptual model and algebra for on-line analytical processing in decision support databases'''
{{header}}
{{article
|author= H Thomas,A Datta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Data warehousing and On-Line Analytical Processing (OLAP) are two of the most significant new technologies in the business data processing arena. A data warehouse, or decision support database, can be defined as a "very large" repository of historical data pertaining to an organization. OLAP refers to the technique of performing complex analysis over the information stored in a data warehouse. The complexity of queries required to support OLAP applications makes it difficult to implement using standard relational database technology. Moreover, currently there is no standard conceptual model for OLAP. There clearly is a need for such a model and an algebra as evidenced by the numerous SQL extensions offered by many vendors of OLAP products. In this paper we address this issue by proposing a model of a data cube and an algebra to support OLAP operations on this cube. The model we present is simple and intuitive, and the algebra provides a means to concisely express complex OLAP queries.
|keyword = data warehouse,On-Line Analytical Processing (OLAP),data models,algebra,multidimensional databases,decision support databases,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: A reexamination of IT investment and the market value of the firm - An event study methodology'''
{{header}}
{{article
|author= KS Im,KE Dow,V Grover,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2001
|abstract = Evaluating the effectiveness of Information Technology (IT) investments has always been an elusive but important goal of IS researchers. This study builds on a prior study that examined changes in the market value of the firm as reflected by the stock price in response to IT investment announcements. Data on stock prices were analyzed for 238 publicly traded companies. In addition to the stock price analysis, reaction of trading volume to the announcements was also examined to identify whether IT investment announcements affect investors' beliefs about IT value. Potentially confounding factors such as industry, size, and time lag effects were also analyzed. Size and time lag effects were found for all IT investment announcements. Reactions of price and volume were negatively related to firm size and became more positive over time. The positive excess return for smaller firms shows that smaller firms can leverage the lower price/performance ratio of new IT and reap greater rewards from IT investments than larger firms. Also, the result of time lag effect demonstrates that the stock market has recently begun to identify both tangible and intangible benefits of IT investments. For recent IT investment announcements, industry classification and firm size also affected the reactions of stock price to the announcements. This study provides optimism on the stock market reaction to IT investment announcements as well as further insight into the study of IT impacts on organizational performance.
|keyword = IT announcements,market value,event study,stock price reaction,stock volume reaction,IT impacts,firm size effect,organizational performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Identifying software project risks: An international Delphi study'''
{{header}}
{{article
|author= R Schmidt,K Lyytinen,M Keil,P Cule,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Advocates of software risk management claim that by identifying and analyzing threats to success (i.e., risks) action can be taken to reduce the chance of failure of a project. The first step in the risk management process is to identify the risk itself, so that appropriate countermeasures can be taken. One problem in this task, however, is that no validated lists are available to help the project manager understand the nature and types of risks typically faced in a software project. This paper represents a first step toward alleviating this problem by developing an authoritative list of common risk factors. We deploy a rigorous data collection method called a "ranking-type" Delphi survey to produce a rank-order list of risk factors. This data collection method is designed to elicit and organize opinions of a panel of experts through iterative, controlled feedback. Three simultaneous surveys were conducted in three different settings: Hong Kong, Finland, and the United States. This was done to broaden our view of the types of risks, rather than relying on the view of a single culture-an aspect that has been ignored in past risk management research. In forming the three panels, we recruited experienced project managers in each country. The paper presents the obtained risk factor list, compares it with other published risk factor lists for completeness and variation, and analyzes common features and differences in risk factor rankings in the three countries. We conclude by discussing implications of our findings for both research and improving risk management practice.
|keyword = Delphi technique,IS project risk management,IS risk management,risk assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An integrative contingency model of software project risk management'''
{{header}}
{{article
|author= H Barki,S Rivard,J Talbot,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Drawing both from the IS literature on software project risk management and the contingency research in Organization Theory literature, the present study develops an integrative contingency model of software project risk management. Adopting a profile deviation perspective of fit, the outcome of a software development project (Performance) is hypothesized to be influenced by the fit between the project's risk (Risk Exposure) and how project risk is managed (Risk Management Profile). The research model was tested with longitudinal data obtained from project leaders and key users of 75 software projects. The results support the contingency model proposed and suggest that in order to increase project performance a project's risk management profile needs to vary according to the project's risk exposure. Specifically, high-risk projects were found to call for high information processing capacity approaches in their management. However, the most appropriate management approach was found td depend on the performance criterion used. When meeting project budgets was the performance criterion, successful high-risk projects had high levels of internal integration, as well as high levels of formal planning. When system quality was the performance criterion, successful high-risk projects had high levels of user participation.
|keyword = contingency models,software project management,software project risk,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Telecommunications and economic activity: An analysis of Granger causality'''
{{header}}
{{article
|author= A Dutta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = The pervasive role of telecommunications in contemporary commerce is well documented, and has dramatically increased the demand for services. Across the world, countries are seeking to improve telecommunications infrastructure and benefit from anticipated increases in economic activity, and a causal relation between the two is often tacitly assumed. This paper analyzes aggregate data at the national level to see if there is any empirical evidence that supports this assumption. We apply the well established Granger test For causality using time series data for levels of telecommunications infrastructure and economic activity from thirty countries. We find that the evidence for causality from levels of telecommunications infrastructure to economic activity is stronger than that for causality in the opposite direction. Moreover, this pattern appears to hold for both industrialized and developing economies, even though the former has strong service sectors that are heavily dependent on telecommunications. These findings provide additional insights into the complex relationship between telecommunications and economic activity. Some potential policy implications are also discussed. Granger causality tests have not seen much application in the IS literature, and we mention some IS research issues that may benefit from such analysis.
|keyword = economic activity,Granger causality,telecommunications infrastructure,telecommunications policy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A choice model for the selection of computer vendors and its empirical estimation'''
{{header}}
{{article
|author= KY Tam,KL Hui,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Despite the important role of vendors in the IT procurement process, very few studies have considered vendor characteristics and their effects on the decision outcome of IT managers. In this paper we present a discrete choice model to examine the effects of vendor characteristics on the purchase decisions of IT managers. Our intent is to empirically assess the effects of product variety, brand name, average price, and network externalities in the selection of computer vendors. To ensure that the effects are not technology-dependent, we deliberately use long time series data to calibrate the model. Annual data at the vendor level from 1965 to 1993 is used to infer the choice criteria of IT managers in three computer categories: mainframe, minis, and small systems. Our empirical findings indicate that a broader product line and a strong brand can effectively enhance the choice probability of a vendor. Implications of these findings and possible extensions are also discussed.
|keyword = brand,choice model,IT procurement,network externality,price,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Impact of information technology management practices on customer service'''
{{header}}
{{article
|author= J Karimi,TM Somers,YP Gupta,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Recently, despite huge incentives and subsequent increases in investment in customer relationship management technology, many firms have not been able to increase their customer satisfaction index ratings. The purpose of this paper is to gauge whether IT management practices differ among firms where IT has a major role in transforming marketing, operations, or both, which give the firms advantage by affecting their customer service. Several research hypotheses are tested using data obtained from a survey of 213 IT-leaders in the financial services industry. The results clearly indicate that the IT-leader firms have a higher level of IT management sophistication and a higher role for their IT-leaders compared to IT-enabled customer focus, IT-enabled operations focus, and IT-laggard firms. This paper concludes with the implications for both researchers and practitioners.
|keyword = customer relationship management,customer service,information technology impact on marketing and operation functions,information technology leader's role,information technology management practices,information technology management sophistication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology competence of business managers: A definition and research model'''
{{header}}
{{article
|author= G Bassellier,BH Reich,I Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = This research explores the concept of the information technology (IT) competence of business managers, defined as the set of IT-related explicit and tacit knowledge that a business manager possesses that enables him or her to exhibit IT leadership in his or her area of business. A manager's knowledge of technologies, applications, systems development, and management of IT form his or her explicit IT knowledge. This domain further extends to include knowing who knows what, which enables the manager to leverage the knowledge of others. Tacit IT knowledge is conceptualized as a combination of experience and cognition. Experience relates to personal computing, IT projects, and overall management of IT. Cognition refers to two mental models: the manager's process view and his or her vision for the role of IT. The outcomes expected from IT-competent business managers are chiefly two behaviors: an increased willingness to form partnerships with IT people and an increased propensity to lead and participate in IT projects.
|keyword = explicit and tacit knowledge,information technology competence,information-technology management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Rapid information technology change, coping mechanisms, and the emerging technologies group'''
{{header}}
{{article
|author= J Benamati,AL Lederer,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Information technology (IT) changes rapidly, seriously challenging IT management. In response, many organizations create a formal group of IT professionals to evaluate emerging IT so they can better cope with its change. A survey based on structured interviews was mailed to a nationwide sample of 1,000 IT organizations. Two hundred forty-six respondents provided data to identify categories of coping mechanisms to handle changing IT. Five categories emerged: Education and Training, Internal Procedures, Vendor Support, Consultant Support, and Endurance. Organizations apply Education and Training more extensively than the others. Thus the research contributes to understanding the means by which organizations cope with rapid IT change. The research also found that organizations with a group dedicated to investigating emerging IT cope more extensively, but not more successfully, than do those without one. Thus the research contributes not only by providing an understanding of how organizations cope with rapid IT change, but also by suggesting the need to achieve more from the group charged with emerging IT.
|keyword = emerging technology group,environmental change,information technology management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical analysis of data requirements for financial forecasting with neural networks'''
{{header}}
{{article
|author= S Walczak,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Neural networks have been shown to be a promising tool for forecasting financial time series, Several design factors significantly impact the accuracy of neural network forecasts. These factors include selection of input variables, architecture of the network, and quantity of training data. The questions of input variable selection and system architecture design have been widely researched, but the corresponding question of how much information to use in producing high-quality neural network models has not been adequately addressed. In this paper, the effects of different sizes of training sample sets on forecasting currency exchange rates are examined. It is shown that those neural networks-given an appropriate amount of historical knowledge-can forecast future currency exchange rates with 60 percent accuracy, while those neural networks trained on a larger training set have a worse forecasting performance. In addition to higher-quality forecasts, the reduced training set sizes reduce development cost and time.
|keyword = forecasting,foreign exchange,neural networks,prediction accuracy,time series,training set size,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical investigation of user requirements elicitation: Comparing the effectiveness of prompting techniques'''
{{header}}
{{article
|author= GJ Browne,MB Rogich,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2001
|abstract = Eliciting requirements from users and other stakeholders is of central importance to information systems development. Despite this importance, surprisingly little research has measured the effectiveness of various requirements elicitation techniques, The present research first discusses theory relevant to information requirements determination in general and elicitation in particular. We then develop a model of the requirements elicitation process. This model and its underlying theory were then used to construct a new requirements elicitation prompting technique. To provide a context for testing the relative effectiveness of the new technique, two other questioning methodologies were also operationalized as prompting techniques: (1) the interrogatories technique, which involves asking "who," "what," "when" "where," "how," and "why" questions; and (2) a semantic questioning scheme, which involves asking questions based on a theoretical model of knowledge structures, To measure the usefulness of the prompting techniques in eliciting requirements, a set of generic requirements categories was adapted from previous research to capture requirements evoked by users. The effectiveness of the three methods in eliciting requirements for a software application was then tested in an experiment with users. Results showed that the new prompting technique elicited a greater quantity of requirements from users than did the other two techniques. Implications of the findings for research and systems analysis practice are discussed.
|keyword = information systems development,prompting techniques,requirements elicitation,systems analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Validation in information systems research: A state-of-the-art assessment'''
{{header}}
{{article
|author= MC Boudreau,D Gefen,DW Straub,
|source= MIS QUARTERLY
|year= 2001
|abstract = Over 10 years ago, the issue of whether IS researchers were rigorously validating their quantitative, positivist instruments was raised (Straub 1989). In the years that have passed since that time, the profession has undergone many changes. Novel technologies and management trends have come and gone. New professional societies have been formed and grown in prominence and new demands have been placed on the field's research and teaching obligations. But the issue of rigor in IS research has persisted throughout all such changes. Without solid validation of the instruments that are used to gather data upon which findings and interpretations are based, the very scientific basis of positivist, quantitative research is threatened. As a retrospective on the Straub article, this research seeks to determine if and how the field has advanced in instrument validation. As evidence of the change, we coded positivist, quantitative research articles in five major journals over a recent three year period for use of validation techniques. Findings suggest that the field has advanced in many areas, but, overall, it appears that a majority of published studies are still not sufficiently validating their instruments. Based on these findings, approaches are suggested for reinvigorating the quest for validation in IS research via content/construct validity, reliability, and manipulation validity.
|keyword = IS research methods,measurement,psychometrics,validation,reliability,content validity,construct validity,manipulation validity,quantitative research,positivist research,guidelines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical investigation of the factors affecting data warehousing success'''
{{header}}
{{article
|author= BH Wixom,HJ Watson,
|source= MIS QUARTERLY
|year= 2001
|abstract = The IT implementation literature suggests that various implementation factors play critical roles in the success of an information system; however, there is little empirical research about the implementation of data warehousing projects. Data warehousing has unique characteristics that may impact the importance of factors that apply to if. In this study, a cross-sectional survey investigated a model of data warehousing success. Data warehousing managers and data suppliers from 111 organizations completed paired mail questionnaires on implementation factors and the success of the warehouse. The results from a Partial Least Squares analysis of the data identified significant relationships between the system quality and data quality factors and perceived net benefits. It was found that management support and resources help to address organizational issues that arise during warehouse implementations; resources, user participation, and highly-skilled project team members increase the likelihood that warehousing projects will finish on-time, on-budget, with the right functionality; and diverse, unstandardized source systems and poor development technology will increase the technical issues that project teams must overcome. The implementation's success with organizational and project issues, in turn, influence the system quality of the data warehouse; however, data quality is best explained by factors not included in the research model.
|keyword = data warehousing,success,IS implementation,partial least squares,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the shareholder wealth effects of announcements of newly created CIO positions'''
{{header}}
{{article
|author= D Chatterjee,VJ Richardson,RW Zmud,
|source= MIS QUARTERLY
|year= 2001
|abstract = While information technology (IT) has been transforming the business landscape for a long time now, it is only recently that empirical evidence demonstrating the positive impact of IT on firm performance has begun to accumulate. The strategic importance of a firm's IT capabilities is prompting an increasing number of companies to appoint chief information officers (CIOs) to effectively manage these assets. Such moves are reflective of changes in top management thinking and policy regarding the role of IT and firms' approaches to IT governance. This paper uses the event study methodology to examine market reactions to announcements of new CIO positions. Findings strongly support the notion that, for firms competing in industries undergoing IT driven transformation, announcements of newly created CIO positions do indeed provoke positive reactions from the marketplace.
|keyword = CIO position announcements,IT leadership,IT management,IT assets,IT-driven transformation,organization innovation,external hires,event study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: An agenda for information technology research in heterogeneous and distributed environments'''
{{header}}
{{article
|author= S March,A Hevner,S Ram,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Application-driven, technology-intensive research is critically needed to meet the challenges of globalization, interactivity, high productivity, and rapid adaptation faced by business organizations. Information systems researchers are uniquely positioned to conduct such research, combining computer science, mathematical modeling, systems thinking, management science, cognitive science, and knowledge of organizations and their functions. We present an agenda for addressing these challenges as they affect organizations in heterogeneous and distributed environments. We focus on three major capabilities enabled by such environments: Mobile Computing, Intelligent Agents, and Net-Centric Computing. We identify and define important unresolved problems in each of these areas and propose research strategies to address them.
|keyword = heterogeneous and distributed systems,interoperability,Mobile Computing,Intelligent Agents,Net-Centric Computing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Memory-based feedback controls to support groupware coordination'''
{{header}}
{{article
|author= A Bordetsky,G Mark,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = In this paper we first present an empirical study of groupware use illustrating problems that users faced with restricted feedback about others' activities. Awareness can aid users in learning interdependencies, and in forming conventions to regulate system use and information-sharing. As a solution to providing awareness, we integrate the framework of organizational memory with intelligent agent technology to provide a coordination mechanism that enables the structuring of awareness events and gives information about the users' feedback control. In the proposed model, feedback control relationships are captured into a multilayered model of organizational memory and transferred to users by agents-facilitators. The approach is based on a system dynamics approach to organizational learning.
|keyword = groupware,awareness,conventions,feedback control,coordination,intelligent agents,case-based reasoning,organizational memory,collaborative technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Providing decisional guidance for multicriteria decision making in groups'''
{{header}}
{{article
|author= M Limayem,G DeSanctis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Intelligent user interfaces, particularly in interactive group settings, can be based on system explanations that guide model building, application, and interpretation. Here we extend Silver's (1990, 1991) conceptualization of decisional guidance and the theory of breakpoints in group interaction to operationalize feedback and feedforward for a complex multicriteria modeling system operating within a group decision support system context. We outline a design approach for providing decisional guidance in GDSS and then test the feasibility of the design in a preliminary laboratory experiment. Findings show how decisional guidance that provides system explanations at breakpoints in group interaction can improve MCDM GDSS usability. Our findings support Dhaliwal and Benbasat's (1996) conjecture that system explanations can improve decisional outcomes due to improvement in user understanding of decision models. Further research on intelligent agents, particularly in interactive group settings, can build on the concepts of decisional guidance outlined in this paper.
|keyword = decision support,group decision support,multicriteria decision making,user interface,intelligent systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: The role of behavioral modeling in computer skills acquisition - Toward refinement of the model'''
{{header}}
{{article
|author= RD Johnson,GM Marakas,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Recent empirical work by Compeau and Higgins (1995) investigated the role of behavioral modeling training in the development of computer skills. Their efforts have provided insight into our understanding of the role of computer self-efficacy (CSE) and behavioral modeling (BM) techniques with regard to training effectiveness. Contrary to their expectations, however, several of the hypothesized relationships were not supported, especially those relating to outcome expectancy. In this paper, an empirically derived model of the CSE construct proposed by Marakas, Yi, and Johnson (1998) is offered to highlight potential theoretical, methodological, and measurement issues which may have contributed to or exacerbated the unexpected results obtained in the Compeau and Higgins study. The empirical work contained herein is intended to both replicate and extend the work of Compeau and Higgins and to assist in resolving several key issues left unsettled by their seminal work in this area.
|keyword = computer self-efficacy,training,behavioral modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: The evolving relationship between general and specific computer self-efficacy - An empirical assessment'''
{{header}}
{{article
|author= R Agarwal,V Sambamurthy,RM Stair,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = he concept of computer self-efficacy (CSE) recently has been proposed as important to the study of individual behavior toward information technology. This paper extends current understanding about the concept of self-efficacy in the context of computer software. We describe how two broad types of computer self-efficacy beliefs, general self-efficacy and task-specific self-efficacy, are constructed across different computing tasks by suggesting that initial general CSE beliefs will strongly predict subsequent specific CSE beliefs. The theorized causal relationships illustrate the malleability and development of CSE beliefs over time, within a training environment where individuals are progressively provided with greater opportunity for hands-on experience and practice with different software. Consistent with the findings of prior research, judgments of self-efficacy then serve as key antecedents of the perceived cognitive effort (ease of use) associated with technology usage. Further, we theorize that self-efficacy judgments in the task domain of computing are strongly influenced by the extent to which individuals believe that they are personally innovative with respect to information technology. Panel data were collected using a longitudinal research design within a training context where 186 subjects were taught two software packages in a sequential manner over a 14-week period. The emergent patterns of the hypothesized relationships are examined using structural equation modeling techniques. Results largely support the relationships posited.
|keyword = computer self-efficacy,technology acceptance,software training,longitudinal study,causal model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging tacit organizational knowledge'''
{{header}}
{{article
|author= D Stenmark,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Although tacit knowledge constitutes the major part of what we know, it is difficult for organizations to fully benefit from this valuable asset. This is because tacit knowledge is inherently elusive, and in order to capture, store, and disseminate it, it is argued that it first has to be made explicit. However, such a process is difficult, and often fails due to three reasons: (1) we are not necessarily aware of our tacit knowledge, (2) on a personal level, we do not need to make it explicit in order to use it, and (3) we may not want to give up a valuable competitive advantage. During an empirical study of recommender system usage, it was noticed how such technology could be used to circumvent these problems, and make tacit knowledge, in the form of our professional interests, available to the organization as a whole. Using Polanyi's theories, it will be shown how intranet documents can be used to make tacit knowledge tangible without becoming explicit, suggesting that tacitly expressed entities are not necessarily beyond the reach of information technology.
|keyword = knowledge management,organizational knowledge,tacit knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An experiment to assess the performance of a redesign knowledge system'''
{{header}}
{{article
|author= ME Nissen,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Today, a second generation of computer-based reengineering tools employs knowledge systems technology to automate and support key intellectual activities required for effective process redesign. But a central question remains as to the effectiveness of redesign through such knowledge systems. The research described in this paper is focused on testing the effectiveness of knowledge-based, process-redesign systems. We employ one such system, called "KOPeR-lite," as a platform for experimentation to assess the relative efficacy of redesigns generated by computer versus those developed by people. In this sense, we conduct a modified Turing Test to compare redesign performance of reengineering analysts with that of the knowledge system. KOPeR-lite performs comparatively well in certain respects, but human subjects outperform the machine in others. The results provide evidence to support claims of redesign efficacy through knowledge systems, and they offer insight into the relative strengths and weaknesses of people and software applications in the reengineering domain. This study further opens up new lines of research and highlights implications for process redesign and practice, including issues associated with leading adoption of knowledge system technology and extension of redesign automation systems such as KOPeR-lite.
|keyword = business process reengineering,expert systems,knowledge systems,process redesign,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Workflow-centric information distribution through e-mail'''
{{header}}
{{article
|author= JL Zhao,A Kumar,EA Stohr,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Organizations require ways to efficiently distribute information such as news releases, seminar announcements, and memos. While the machinery for information storage, manipulation, and retrieval exists, research dealing directly with its distribution in an organizational context is scarce. In this paper, we address this need by first examining the pros and cons of the conventional "mailing lists" approach and then proposing new workflow mechanisms that improve the efficiency and effectiveness of information distribution through e-mail. The proposed approach is relevant to other information distribution approaches beyond e-mail. The main contributions of this study include: (1) offering a workflow perspective on organizational information distribution; (2) analysis of workflows in two new information distribution methods based on dynamic mailing lists and profile matching, respectively; and (3) proposing a new way of matching supply and demand of information that extends existing information filtering algorithms.
|keyword = electronic mail,information distribution,knowledge management,workflow management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Conversation map: An interface for very large-scale conversations'''
{{header}}
{{article
|author= W Sack,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Very large-scale conversation (VLSC) involves the exchange of thousands of electronic mail (e-mail) messages among hundreds or thousands of people. Usenet newsgroups are good examples (but not the only examples) of online sites where VLSCs take place. To facilitate understanding of the social and semantic structure of VLSCs, two tools from the social sciences-social networks and semantic networks-have been extended for the purposes of interface design. As interface devices, social and semantic networks need to be flexible, layered representations that are useful as a means for summarizing, exploring, and cross-indexing the large volumes of messages that constitute the archives of VLSCs. This paper discusses the design criteria necessary for transforming these social scientific representations into interface devices. The discussion is illustrated with the description of the Conversation Map system, an implemented system for browsing and navigating VLSCs.
|keyword = browsing,conversation analysis,discourse analysis,electronic mail,interface design,navigation,news groups,semantic networks,social networks,summarization,visualization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Stimulating thinking: Cultivating better decisions with groupware through categorization'''
{{header}}
{{article
|author= KM Hilmer,AR Dennis,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Previous research shows that groupware improves the exchange of information within groups. However, the additional information does not often lead to better group decisions, probably because individuals fail to process the new information they receive. This study explored the use of groupware processes that required individuals in groups to categorize information, in order to induce group members to better attend to the new information received from others and to integrate it into their own individual decision-making processes. Different groupware processes had different effects on attention to and integration of information, and ultimately on decision quality. Groupware processes that provided categories to organize information and groupware processes that required the receiver of information to categorize information increased attention to information and integration of information, which led to improved individual decision quality.
|keyword = decision-making,group support systems (GSS),groupware,individual cognition,information exchange,information processing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Group support systems: A descriptive evaluation of case and field studies'''
{{header}}
{{article
|author= J Fjermestad,SR Hiltz,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = This paper presents a descriptive evaluation of 54 case and field studies from 79 published papers spanning two decades of group support systems (GSS) research. It organizes the methodology and results of these Studies into a four-factor framework consisting of contextual factors, intervening factors, adaptation factors, and outcome factors. The tables will provide the GSS researcher with a summary of what has been studied. The appendices provide a detailed description of the methodology and the results.
|keyword = case studies,descriptive evaluation,field studies,group support system,research integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The propagation of technology management taxonomies for evaluating investments in information systems'''
{{header}}
{{article
|author= Z Irani,PED Love,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = The management of Information Technology (IT) and Information Systems (IS) is considered a complex exercise by academics and practitioners alike. The reason for this is that there are ubiquitous portfolios of tangible and intangible benefits that are offered to an organization following the adoption of IT/IS that, in turn, all need managing to ensure realization. Organizations also have to take into account the direct and often larger indirect costs that are typically associated with IT/IS deployments. To provide managers with a critical insight into the management of new technology, this paper uses a case study research strategy to examine the technology management experiences of a leading U.K. manufacturing organization during its adoption of a vendor-supplied Manufacturing Resource Planning (MRPII) information system. Following the lack of attention given to human and organizational technology management factors while implementing MRPII, the vendor-based information system was later abandoned and deemed a failure. In addressing those technology management factors that were later identified as important, it was found that key employees were able to overcome a number of organizational barriers and develop and implement a bespoke MRPII system that significantly improved the organization's competitive position. Technology management taxonomies that contributed to the failure and later successful implementation of MRPII are identified and discussed. The organization's experiences in solving the problems associated with the implementation of their IS offers a learning opportunity for those companies that are seeking a competitive advantage through technology management.
|keyword = benefits,costs,evaluation,investment,MRPII,taxonomies,technology management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Data warehousing supports corporate strategy at First American Corporation'''
{{header}}
{{article
|author= BL Cooper,HJ Watson,BH Wixom,DL Goodhue,
|source= MIS QUARTERLY
|year= 2000
|abstract = From 1990 through 1998, First American Corporation (FA C) changed its corporate strategy from a traditional banking approach to a customer relationship-oriented strategy that placed FAC's customers at the center of all aspects of the company's operations. The transformation made FAC an innovative leader in the financial services industry. This case study describes FAC's transformation and the way in which a data warehouse called VISION helped make it happen. FAC's experiences suggest lessons for managers who plan to use technology to support changes that are designed to significantly improve organizational performance. In addition, they raise interesting questions about the means by which information technology can be used to gain competitive advantage.
|keyword = data warehousing,corporate strategy,organizational transformation,customer relationship management,IS management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Technology adaptation: The case of a computer-supported inter-organizational virtual team'''
{{header}}
{{article
|author= A Majchrzak,RE Rice,A Malhotra,N King,SL Ba,
|source= MIS QUARTERLY
|year= 2000
|abstract = The adaptation process for new technology is not yet well understood. This study analyzes how an inter-organizational virtual team, tasked with creating a highly innovative product over a 10 month period, adapted the use of a collaborative technology and successfully achieved its challenging objectives. The study of such a virtual team is especially useful for extending our understanding of the adaptation process as virtual teams have more malleable structures than typical organizational units and controlled group experiments. Data were obtained from observations of weekly virtual meetings, electronic log files, interviews, and weekly questionnaires administered to team members. We found that the team initially experienced significant misalignments among the pre-existing organizational environment, group, and technology structures. To resolve these misalignments, the team modified the organizational environment and group structures, leaving the technology structure intact. However, as the team proceeded, a series of events unfolded that caused the team to reevaluate and further modify its structures. This final set of modifications involved reverting back to the pre-existing organizational environment, while new technology and group structures emerged as different from both the pre-existing and the initial ones. A new model of the adaptation process-one that integrates these findings and those of several previous models-is proposed.
|keyword = CBCS,collaborative work systems,groups,problem solving,information attributes,longitudinal study,remote work,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Is a map more than a picture? The role of SDSS technology, subject characteristics, and problem complexity on map reading and problem solving'''
{{header}}
{{article
|author= BE Mennecke,MD Crossland,BL Killingsworth,
|source= MIS QUARTERLY
|year= 2000
|abstract = This research investigated how the use of a spatial decision support system (SDSS)-a type of geographic information system (GIS)-influenced the accuracy and efficiency of different types of problem solvers (i.e., professionals versus students) completing problems of varied complexity. This research-the first to simultaneously study these variables-examined subjects who completed a problem involving spatially-referenced information. The experiment was guided by a research model synthesized from various perspectives, including the theory of cognitive fit, prior research on map reading and interpretation, and research examining subject expertise and experience. The results are largely supportive of the research model and demonstrate that SDSS, an increasingly important class of management decision-making technology, increased the efficiency of users working on more complex problems. Professionals were found to be more accurate but less efficient then students; however, professionals who used the SDSS were no more accurate than professionals using paper maps. Need for cognition, a construct that focuses on an individual's willingness to engage in problem solving tasks, was found to be marginally related to accuracy. The implications of these findings for researchers and practitioners are presented and discussed.
|keyword = cognitive fit theory,geographic information systems,map reading,problem solving,spatial decision support systems,subject characteristics,task complexity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why software projects escalate: An empirical analysis and test of four theoretical models'''
{{header}}
{{article
|author= M Keil,J Mann,A Rai,
|source= MIS QUARTERLY
|year= 2000
|abstract = Software projects can often spiral out of control to become "runaway systems" that far exceed original budget and schedule projections. The behavior that underlies many runaway systems can best be characterized as "escalation of commitment to a failing course of action." The objectives of this study were to: (1) understand the extent to which IS projects are prone to escalate, (2) compare the outcomes of projects that escalate with those that do not, and (3) test whether constructs associated with different theories of escalation can be used to discriminate between projects that escalate and those that do not. A survey was administered to IS audit and control professionals and, to establish a baseline for comparison, the survey was designed to gather data on projects that did not escalate as well as those that did escalate. The results of our research suggest that between 30% and 40% of all IS projects exhibit some degree of escalation. Projects that escalated had project outcomes that were significantly worse in terms of perceived implementation performance and perceived budget/schedule performance, as compared to projects that did not escalate. Using constructs from theories that have been used to explain the escalation phenomenon, we were able to test various logistic regression models for their ability to discriminate between projects that escalate and those that do not. To construct our models, we explored constructs derived from self-justification theory, prospect theory, agency theory, and approach avoidance theory. While constructs derived from all four theories were significant in logistic regression models, the completion effect construct derived from approach avoidance theory provided the best classification of projects, correctly classifying over 70% of both escalated and non-escalated projects.
|keyword = software project management,escalation,IS project failure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Time flies when you're having fun: Cognitive absorption and beliefs about information technology usage'''
{{header}}
{{article
|author= R Agarwal,E Karahanna,
|source= MIS QUARTERLY
|year= 2000
|abstract = Extant explanations of why users behave in particular ways toward information technologies have tended to focus predominantly on instrumental beliefs as drivers of individual usage intentions. Prior work in individual psychology, however, suggests that holistic experiences with technology as captured in constructs such as enjoyment and flow are potentially important explanatory variables in technology acceptance theories. In this paper, we describe a multi-dimensional construct labeled cognitive absorption and defined as a state of deep involvement with software. Cognitive absorption, theorized as being exhibited through the five dimensions of temporal dissociation, focused immersion, heightened enjoyment, control, and curiosity, is posited to be a proximal antecedent of two important beliefs about technology use: perceived usefulness and perceived ease of use. In addition, we propose that the individual traits of playfulness and personal innovativeness are important determinants of cognitive absorption. Based on the conceptual definition of this construct, operational measures for each dimension are developed. Using the World Wide Web as the target technology, scale validation indicates that the operational measures have acceptable psychometric properties and confirmatory factor analysis supports the proposed multi-dimensional structure. Structural equation analysis provides evidence for the theorized nomological net of cognitive absorption. Theoretical and practical implications are offered.
|keyword = user beliefs,cognitive absorption,world-wide web,user behavior toward information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The moderating effects of structure on volatility and complexity in software enhancement'''
{{header}}
{{article
|author= RD Banker,SA Slaughter,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = The cost of enhancing software applications to accommodate new and evolving user requirements is significant. Many enhancement cost-reduction initiatives have focused on increasing software structure in applications. However, while software structure can decrease enhancement effort by localizing data processing, increased effort is also required to comprehend structure. Thus, it is not clear whether high levels of software structure are economically efficient in all situations. In this study, we develop a model of the relationship between software structure and software enhancement costs and errors. We introduce the notion of software structure as a moderator of the relationship between software volatility, total data complexity, and software enhancement outcomes. We posit that it is efficient to more highly structure the more volatile applications, because increased familiarity with the application structure through frequent enhancement enables localization of maintenance effort. For more complex applications, software structure is more beneficial than for less complex applications because it facilitates the comprehension process where it is most needed. Given the downstream enhancement benefits of structure for more volatile and complex applications, we expect that the optimal level of structure is higher for these applications. We empirically evaluate our model using data collected on the business applications of a major mass merchandiser and a large commercial bank. We find that structure moderates the relationship between complexity, volatility, and enhancement outcomes, such that higher levels of structure are more advantageous for the more complex and more volatile applications in terms of reduced enhancement costs and errors. We also find that more structure is designed in for volatile applications and for applications with higher levels of complexity. Finally, we identify application type as a significant factor in predicting which applications are more volatile and more complex at our research sites. That is, applications with induction-based algorithms such as those that support planning, forecasting, and management decision-making activities are more complex and more volatile than applications with rule-based algorithms that support operational and transaction-processing activities. Our results indicate that high investment in software quality practices such as structured design is not economically efficient in all situations. Our findings also suggest the importance of organizational mechanisms in promoting efficient design choices that lead to reduced enhancement costs and errors.
|keyword = software enhancement,software structure,software volatility,software complexity,structured design,management of information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The performance impacts of quick response and strategic alignment in specialty retailing'''
{{header}}
{{article
|author= JW Palmer,ML Markus,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = The Quick Response (QR) program is a hierarchical suite of information technologies (IT) and applications designed to improve the performance of retailers. Consultants advise retailers to adopt the program wholesale, implying that more and higher levels of technology are better than less technology and lower levels. Academicians, on the other hand, argue that good technology is "appropriate" technology. That is, firms should adopt only those technologies that suit the specific strategic directions pursued by the firm. Who is right? Which approach to investing in IT yields better performance results? Surprisingly, this cross-sectional survey of 80 specialty retailers found more support for the practitioners' claims than for the academicians'. Adoption of the QR program at a minimal level was associated with higher performance, although there was no performance impact due to higher levels of QR use. Firms did appear to match their IT usage to their business strategies, but there was no Linkage between strategic alignment and firm performance, and there was surprisingly little variation in business or IT strategy. In short, the findings of our study suggest that both practitioners and academicians need to refine their theories and advice about what makes IT investments pay off.
|keyword = performance impacts of IT,quick response,retailing,strategic alignment,strategic use of IT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge base decomposition to facilitate verification'''
{{header}}
{{article
|author= S Sarkar,M Ramaswamy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = We examine the verification of large knowledge-based systems. When knowledge bases are large, the verification process poses several problems that are usually not significant for small systems. We focus on decompositions that allow verification of such systems to be performed in a modular fashion. We identify a graphical framework, that we call an ordered polytree, for decomposing systems in a manner that enables modular verification. We also determine the nature of information that needs to be available for performing local checks to ensure accurate detection of anomalies. We illustrate the modular verification process using examples, and provide a formal proof of its accuracy. Next, we discuss a meta-verification procedure that enables us to check if decompositions under consideration do indeed satisfy the requirements for an ordered polytree structure. Finally, we show how the modular verification algorithm leads to considerable improvements in the computational effort required for verification as compared to the traditional approach.
|keyword = directed hypergraphs,knowledge base partitioning,knowledge base verification,polytree decomposition,rule-based systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How do we understand a system with (so) many diagrams? Cognitive integration processes in diagrammatic reasoning'''
{{header}}
{{article
|author= J Kim,J Hahn,H Hahn,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = In order to understand diagrammatic reasoning with multiple diagrams, this study proposes a theoretical framework that focuses on the cognitive processes of perceptual and conceptual integration. The perceptual integration process involves establishing interdependence between relevant system elements that have been dispersed across multiple diagrams, while the conceptual integration process involves generating and refining hypotheses about a system by combining higher-level information inferred from the diagrams. This study applies a diagrammatic reasoning framework of a single diagram to assess the usability of multiple diagrams as an integral part of a system development methodology. Our experiment evaluated the effectiveness and usability of design guidelines to aid problem solving with multiple diagrams. The results of our experiment revealed that understanding a system represented by multiple diagrams involves a process of searching for related information and of developing hypotheses about the target system. The results also showed that these perceptual and conceptual integration processes were facilitated by incorporating visual cues and contextual information in the multiple diagrams as representation aids. Visual cues indicate which elements in a diagram are related to elements in other diagrams; the contextual information indicates how the individual datum in one diagram is related to the overall hypothesis about the entire system.
|keyword = diagrammatic reasoning,system analysis and design,business engineering,cognitive integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Disruptive technologies - Explaining entry in next generation information technology markets'''
{{header}}
{{article
|author= BR Nault,MB Vandenbosch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = The most difficult challenge facing a market leader is maintaining its leading position. This is especially true in information technology and telecommunications industries, where multiple product generations and rapid technological evolution continually test the ability of the incumbent to stay ahead of potential entrants. In these industries, an incumbent often protects its position by launching prematurely to retain its leadership. Entry, however, happens relatively frequently. We identify conditions under which an entrant will launch a next generation product thereby preventing the incumbent from employing a protection strategy. We define a capabilities advantage as the ability to develop and launch a next generation product at a lower cost than a competitor, and a product with a greater market response is one with greater profit flows. Using these definitions, we find that an incumbent with a capabilities advantage in one next generation product can be overtaken by an entrant with a capabilities advantage in another next generation product only if the entrant's capabilities advantage is in a disruptive technology that yields a product with a greater market response. This can occur even though both next generation products are available to both firms. We also show that the competition may require the launching firm to lose money at the margin on the next generation product.
|keyword = competitive strategy,defensive strategy,disruptive technology,game theory,product research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Modeling the incidence of postrelease errors in software'''
{{header}}
{{article
|author= JC Westland,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Error search and correction are major contributors to software development cost, yet typically uncover only a small fraction of software errors. Postrelease errors, i.e., those that are only observed after a system is released, threaten a variety of potential failures and consequences, each with low individual probability of occurrence. The combined effect of postrelease errors can and often does result in a significant rate of occurrence of these potential failures, with unpredictable consequences and severity. One particular source of postrelease errors that has received extensive publicity is the year 2000, or Y2K, error. The modeling in this research report suggests that testing probably needs to be conducted over more than half of the useful Life of a system in order to discover even one-third of the total errors in the system. It suggests that short product Lifecycles, Lifetime testing, and effective feedback loops for error reporting are necessary to assure reliable software.
|keyword = software errors and reliability,economics of information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Adoption of Internet-based product customization and pricing strategies'''
{{header}}
{{article
|author= R Dewan,B Jing,A Seidmann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = The Internet commerce technologies have significantly reduced sellers' costs of collecting buyer preference information and managing multiple prices. Advanced manufacturing technologies have also improved sellers' manufacturing flexibility. These changes allow an online seller to offer custom products at discriminatory prices. We show that these technologies offer significant advantages to an early adopter who gains market share and profits at the expense of the conventional seller. Not only does the customizing seller charge more for customized products, it also provides standard products but charges more for them than in a conventional market. The benefits of customization disappear when both the competing sellers adopt customization. They now compete not just on prices but also on degree of customization. Consequently, we see that the sellers "over-customize" to the detriment of their profits. Both the sellers know this when choosing their customization strategies and yet they both end up choosing to customize. A seller that does not customize sees a sharp decrease in profits if its competitor customizes. This is an instance of the "Prisoner's Dilemma" type of situation in technology adoption. This confirms some key findings in IT productivity and strategic IT investments literature.
|keyword = adoption of information technology,customization,electronic commerce,flexible manufacturing systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managing the costs of informational privacy: Pure bundling as a strategy in the individual health insurance market'''
{{header}}
{{article
|author= ME Thatcher,EK Clemons,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Advances in genetic testing and data mining technologies have increased the availability of genetic information to insurance companies and insureds (applicants and policy holders) in the individual health insurance market (IHIM). Regulators, concerned that insurance companies will use this information to discriminate against applicants who have a genetic risk factor but who are still healthy, have implemented genetic privacy legislation in at least Is states. However, in previous work we have demonstrated that such legislation will have unintended consequences it will reduce consumer participation in the market without making those remaining better off. This paper identifies a mechanism, a pure bundling strategy, that insurance companies may implement in this regulatory environment to restore (or maximize) consumer participation in the market and to discourage such discrimination among insureds. This problem is examined through System Dynamics, a simulation-based modeling technique. The results will have significant implications for policy designs implemented by insurance companies, and for legislation implemented by industry regulators, and therefore, for the insurability of the individuals that rely on this market for health insurance coverage.
|keyword = bundling,information privacy,insurance markets,insurance policy,privacy,privacy cost,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Limits to value in electronic commerce-related IT investments'''
{{header}}
{{article
|author= AM Chircu,RJ Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = This paper extends the limits-to-value model of Davern and Kauffman to explore market and process-level factors that impact value flows to firms for their information technology (IT) investments. We characterize IT value in terms of potential value and realized value, and show how each is subject to different effects limits to value-that diminish the benefits of the investment. Our typology identifies barriers specific to the valuation process (industry and organizational barriers), and to the conversion process (resource, knowledge and usage barriers). Following the development of our analytical framework from existing economic and organizational theories of IT valuation and technology adoption and diffusion, we analyze a series of case studies of Internet-based travel reservation systems in electronic commerce (EC). These cases provide evidence in support of the usefulness of the framework, and illustrate the extent of the difficulties faced by organizations in making their investments in EC systems pay off.
|keyword = business value,case study,corporate travel industry,conversion barriers,electronic commerce,information technology investments,valuation barriers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitating interorganizational learning with information technology'''
{{header}}
{{article
|author= JE Scott,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Increasingly, organizations collaborate to complement their core competencies. New product development, for example, is often a collaborative process, with customers and suppliers contributing complementary knowledge and skills. This study uses grounded theory to determine how and why information technology facilitates interorganizational learning. Semi-structured interviews in the disk drive industry were coded to develop a conceptual model. An important finding is that organizations collaborate closely through virtual integration. They need interorganizational learning to help them cope with the complexity of new products and the capital intensity in the disk drive industry. However, effective interorganizational collaboration needs trust. The main contribution of the model is in explaining the role of information technology in lower and higher levels of interorganizational learning, cognitive and affective trust, and virtual and humanistic interorganizational collaboration.
|keyword = collaboration,interorganizational learning,new product development,tacit knowledge,trust,virtual integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Institutional bridging: How conceptions of IT-enabled change shape the planning process'''
{{header}}
{{article
|author= J Tillquist,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Organizations are continually influenced by notions of management promoted through broadly held visions of managerial practice. These notions often incorporate models that generally prescribe information technologies as enabling agents for directed organizational change. Such concepts reflect highly cohesive, self-referential systems of beliefs, gears, and rules that structure perspectives about computerization and work in organizations. To achieve "breakthrough" changes in efficiency, performance, or competitive advantage, organizations must translate these high concepts into a specific model of change appropriate for their organizational context. This study shows how abstract and institutional-level conceptions about change are translated into actionable and individual-level realities, and how within this translation the organization's ability to reform can be locked into a constraining process. As consultants bridge institutionalized conceptions of management to discrete organizational activities, participants of change adopt not only the vision of change but also new ways to talk, act, and plan. This adoption may inhibit change by blocking effective discussion and forcing compliance to ill-fitting prescriptions.
|keyword = business process reengineering,change methodologies,interpretivist field study,IT-enabled organizational changes,IT planning,organizational transformation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The use of explanations in knowledge-based systems: Cognitive perspectives and a process-tracing analysis'''
{{header}}
{{article
|author= JY Mao,I Benbasat,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = This exploratory research investigates the nature of explanation use and factors that influence it during users' interaction with a knowledge-based system (KBS) for decision-making. It draws upon several cognitive perspectives to help understand when, why, and how explanations are used. A verbal protocol analysis was conducted based on a laboratory experiment involving a KBS for financial analysis. Major categories of explanation use were identified, and accounted for with relevant cognitive perspectives. Results show that explanations were requested to deal with comprehension difficulties caused by various types of perceived anomalies in KBS output. There were qualitative and quantitative differences in the nature and extent of explanation use between novices and experienced professionals. These results offer new insights to why explanations are useful and important, what factors influence explanation use, and what information should be included in explanations.
|keyword = explanations,explanations in expert systems,knowledge-based systems,verbal protocol analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of MIS steering committees on information technology management sophistication'''
{{header}}
{{article
|author= J Karimi,A Bhattacherjee,YP Gupta,TM Somers,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Despite the ever increasing importance of information technology (IT) in firms, the extent to which IT management practices are applied creatively to critical tasks varies widely across firms. For over a decade, firms have employed IT steering committees to manage their IT resources. However, the impacts of such committees on the IT management function have not been examined in depth. This paper hypothesized relationships between the level of sophistication of IT steering committees and level of IT sophistication of management within firms, and tested those relationships empirically via a field survey of 213 IT managers in the financial services industry. Results of the study suggest that presence and roles of IT steering committees are significantly related to the level and nature of IT management sophistication within firms. Firms interested in achieving the most benefit from their steering committees should carefully select their preferred roles depending on the type and the level of IT management sophistication desired. The article concludes with discussion and implications for IT researchers and firms' executives.
|keyword = information technology management,information technology management sophistication,steering committees,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The supply and demand of information systems doctorates: Past, present, and future'''
{{header}}
{{article
|author= LA Freeman,SL Jarvenpaa,BC Wheeler,
|source= MIS QUARTERLY
|year= 2000
|abstract = This paper reports on a survey of North American IS programs and secondary data assessing the supply and demand of Information Systems (IS) doctorates. The data document a large and growing lack of supply to meet current and future demand. Demographic factors-including the number of university students, their selection of majors, and retirements among IS faculty-favor a probable scenario for continuing strong demand for IS faculty in the longer term. We argue that the severe imbalance will continue if the current state of the economy and businesses' need for technically-savvy managers continues. Implications and recommendations are presented for ensuring the long-term health of the IS discipline in addressing this imbalance.
|keyword = Al0104,BA0101,BA0202,ID05,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''De-escalating information technology projects: Lessons from the denver international airport'''
{{header}}
{{article
|author= R Montealegre,M Keil,
|source= MIS QUARTERLY
|year= 2000
|abstract = Project failure in the information technology area is a costly problem, and troubled projects are not uncommon. In many cases, these projects seem to take on a life of their own, continuing to absorb valuable resources, while failing to deliver any real business value. While prior research has shown that managers can easily become locked into a cycle of escalating commitment to a failing course of action, there has been comparatively little research on de-escalation, or the process of breaking such a cycle. Through de-escalation, troubled projects may be successfully turned around or sensibly abandoned. This study seeks to understand the process of de-escalation and to establish a model for turning around troubled projects that has both theoretical and practical significance. Through a longitudinal case study of the IT-based baggage handling system at Denver International Airport (DIA), we gathered qualitative data on the de-escalation of commitment to a failing course of action, allowing us to inductively develop a model of the de-escalation process as if unfolded at DIA. The model reveals de-escalation as a four-phase process: (1) problem recognition, (2) re-examination of prior course of action, (3) search for alternative course of action, and (4) implementing an exit strategy. For each phase of the model, we identified key activities that may enable de-escalation to move forward. Implications of this model for both research and practice are discussed.
|keyword = Information systems (IS) project management,escalation,de-escalation,IS project failure,systems implementation,field study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effect of multimedia on perceived equivocality and perceived usefulness of information systems'''
{{header}}
{{article
|author= KH Lim,I Benbasat,
|source= MIS QUARTERLY
|year= 2000
|abstract = With the advent of multimedia and intranet technologies, managers and information systems designers face a new challenge: how to capture and present information using a variety of representation formats (text, graphics, audio, video, and animations) so that members of an organization can make better sense out of the information available. In this study, we develop a task-representation fit model to generate several predictions about the potential of multimedia to alleviate the limitations of text-based information in the context of individual decision makers utilizing organizational data and test them in a laboratory experiment. Results support the task-representation fit relationships predicted. For analyzable tasks, text-based representation and multimedia representation are equally effective in reducing perceived equivocality levels. For less-analyzable tasks, only multimedia representation was instrumental in reducing perceived equivocality levels.
|keyword = multimedia,perceived equivocality,task analyzability,information presentation,task-media fit,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding software operations support expertise: A revealed causal mapping approach'''
{{header}}
{{article
|author= KM Nelson,S Nadkarni,VK Narayanan,M Ghods,
|source= MIS QUARTERLY
|year= 2000
|abstract = This paper utilizes a qualitative methodology, revealed causal mapping (RCM), to investigate the phenomenon of software operations support expertise. Software operations support is a large portion of the IS work performed in organizations. While we as researchers have access to generalized theories and frameworks of expertise, very little is known about expertise in this critical area. To understand software operations support expertise, a mid-range theory is evoked from interviews with experts and the construction of RCMs from those interviews. The results of this study indicate that software operation support expertise is comprised of five major constructs. personal competencies, environmental factors, support personnel motivation, IS policies, and support personnel outcomes. Additionally, this study revealed that these constructs interact differently in contexts where software support is the main activity versus contexts where the focus is development This study demonstrates that the use of the RCM methodology yields constructs of software operations support expertise that are not suggested by generalized theory In addition, the use of RCM as an evocative, qualitative methodology reveals the interaction and linkages between these constructs. This paper also provides a history of and tutorial to the RCM methodology for use by the research community.
|keyword = cognitive mapping,IS personnel,IS maintenance,IS policy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding GDSS in symbolic context: Shifting the focus from technology to interaction'''
{{header}}
{{article
|author= A Gopal,P Prasad,
|source= MIS QUARTERLY
|year= 2000
|abstract = GDSS has enjoyed about a decade and a half of vigorous research activity. Throughout this time, a problem that has occupied the research community is the inconsistent research results that have been obtained. The purpose of this paper is to assess whether the reason for these inconsistencies is rooted in the epistemological mode that has prevailed and to offer an alternative epistemological lens that might help advance our understanding of GDSS use and research. Using qualitative research methods and a symbolic interactionist theoretical basis, this paper examines how a particular group used a GDSS over two meetings. The findings indicate that GDSS use may result in reactions from its users that are difficult to capture using conventional methodological assumptions, thereby helping explain why past results have not been consistent. Based on these findings, a shift in focus is advocated from an emphasis on the technology to an emphasis on human interaction, one that embraces the reasons underlying past inconsistencies rather than attempting to overcome them.
|keyword = group decision support systems,symbolic interaction,qualitative research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: The organizing logic for an enterprise's IT activities in the digital era - A prognosis of practice and a call for research'''
{{header}}
{{article
|author= V Sambamurthy,RW Zmud,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Prior research has generated considerable knowledge about the design of effective IT organizational Today, however, increasing signs have wisdom might be inadequate in shaping appropriate insights for contemporary practice. This essay seeks to direct research attention toward the following question: How should firms organize their IT activities in order to manage the imperatives of the business and technological environments in the digital economy? We articulate the platform logic as a conceptual framework for both viewing the organizing of IT management activities as well as for framing important questions for future research. In articulating this logic, we aim to shift thinking away from the traditional focus on governance structures (i.e., choice of centralized, decentralized, or federal forms) and sourcing structures (i.e., insourcing, outsourcing) and toward more complex structures that are reflective of contemporary practice. These structures are designed around important IT capabilities and network architectures.
|keyword = organizational design,IT management,IT capabilities,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of multimedia in changing first impression bias'''
{{header}}
{{article
|author= KH Lim,I Benbasat,LM Ward,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = First impression bias refers to a limitation of human information processing in which people are strongly influenced by the first piece of information that they are exposed to, and that they are biased in evaluating subsequent information in the direction of the initial influence. The psychology literature has portrayed first impression bias as a virtually "inherent" human bias. Drawing from multimedia literature, this study identifies several characteristics of multimedia presentations that have the potential to alleviate first impression bias. Based on this literature, a set of predictions was generated and tested through a laboratory experiment using a simulated multimedia intranet. Half of the 80 subjects were provided with a biased cue. Subjects were randomly assigned to four groups: (1) text with first impression bias cue, (2) multimedia with first impression bias cue, (3) text without biased cue, and (4) multimedia without biased cue. The experimental task involved conducting a five-year performance appraisal of a department head. The first impression bias cue was designed to provide incomplete and unfavorable information about the department head, but the information provided subsequently was intended to be favorable of his performance. Results show that the appraisal score of the text with biased cue group was significantly lower than the text only (without biased cue) group. On the other hand, the appraisal score of the multimedia with biased cue group was not significantly different from the multimedia only (without biased cue) group. As a whole, the results suggest that multimedia presentations, but not tart-based presentations, reduce the influence of first impression bias.
|keyword = multimedia,first impression bias,information presentation,human information processing,primacy effect,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Mean-risk trade-offs in inductive expert systems'''
{{header}}
{{article
|author= VS Mookerjee,MV Mannino,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Notably absent in previous research on inductive expert systems is the study of mean-risk trade-offs. Such trade-offs may be significant when there are asymmetries such as unequal classification costs, and uncertainties in classification and information acquisition costs. The objective of this research is to develop models to evaluate mean-risk trade-offs in value-based inductive approaches. We develop a combined mean-risk measure and incorporate it into the Risk-Based induction algorithm. The mean-risk measure has desirable theoretical properties (consistency and separability) and is supported by empirical results on decision making under risk. Simulation results using the Risk-Based algorithm demonstrate: (i) an order of magnitude performance difference between mean-based and risk-based algorithms and (ii) an increase in the performance difference between these algorithms as either risk aversion, uncertainty, or asymmetry increases given modest thresholds of the other two factors.
|keyword = risk aversion in expert systems,value-based system design,inductive expert systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The production of information services: A firm-level analysis of information systems budgets'''
{{header}}
{{article
|author= V Gurbaxani,N Melville,K Kraemer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Previous research has demonstrated that the production of information services can be characterized at the aggregate economy-wide level by the Cobb-Douglas production function. However, the underlying production process at the firm level has not yet been ascertained. The objective of this paper is to determine the form of the production process for information systems services at the firm level by conducting an empirical analysis of IS budget data. The production of information services is modeled using a production function with two inputs, hardware and personnel. We estimate various econometric specifications to determine several characteristics of the provision of information services, including the allocation of the information systems budget to its two largest components-hardware and personnel-and its implications for the form of the production function. After controlling for industry sector, we find that the ratio of personnel to hardware is independent of scale, which indicates a homothetic production function. We also find that the ratio of factor shares is constant with time, consistent with the Cobb-Douglas production function. We conclude that the underlying form of the production function is the same at the level of both the firm and the economy. Our analysis demonstrates how the application of production theory to the production of information services can yield useful insights from both a theoretical and managerial perspective.
|keyword = information systems management,information systems budgets,production function,economics of information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Integrating user preferences and real-time workload in information services'''
{{header}}
{{article
|author= P Konana,A Gupta,AB Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = We propose priority pricing as an on-line adaptive resource scheduling mechanism to manage real-time databases within organizations. These databases provide timely information for delay sensitive users. The proposed approach allows diverse users to optimize their own objectives while collectively maximizing organizational benefits. We rely on economic principles to derive priority prices by modeling the fixed-capacity real-time database environment as an economic system. Each priority is associated with a price and a delay, and the price is the premium (congestion toll resulting from negative externalities) for accessing the database. At optimality, the prices are equal to the aggregate delay cost imposed on all other users of the database. These priority prices are used to control admission and to schedule user jobs in the database system. The database monitors the arrival processes and the state of the system, and incrementally adjusts the prices to regulate the flow. Because our model ignores the operational intricacies of the real-time databases (e.g., intermediate queues at the CPU and disks, memory size, etc.) to maintain analytical tractability, we evaluate the performance of our pricing approach through simulation. We evaluate the database performance using both the traditional real-time database performance metrics (e.g., the number of jobs serviced on time, average tardiness) and the economic benefits (e.g., benefits to the organization). The simulation results, under various database workload parameters, show that our priority pricing mechanism not only maximizes organizational benefits but also outperforms in all aspects of traditional performance measures compared to frequently used database scheduling techniques, such as first-come-first-served, earliest deadline first and least slack first.
|keyword = user preference,information services,electronic commerce,response time,real-time databases,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Innovation and control in standards architectures: The rise and fall of Japan's PC-98'''
{{header}}
{{article
|author= J West,J Dedrick,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = For more than a decade NEC dominated the Japanese PC market with its PC-98 architecture, which was incompatible both with its major Japanese rivals and the global PC standard. However, NEC was powerless to prevent the introduction of Japanese versions of Windows 3.1 and 95 that ran on its competitors' architectures as well as on the PC-98, unifying the Japanese PC market and creating a common set of application programming interfaces for all Intel-based Japanese PCs. The introduction of Windows rendered obsolete the large DOS-based software library that had provided strong positive externalities for the NEC architecture. Absent those advantages, the market share of the PC-98 standard fell from 60% to 33% in five years, and NEC finally abandoned the PC-98 in favor of the global standard. An examination of the unusual rise and fall of the PC-98 shows how victory in a standards competition can be negated by the introduction of a new architectural layer that spans two or more previously incompatible architectures.
|keyword = standards competition,computer architecture,application programming interface,network externalities,personal computers,Japan,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Extracting consumers' private information for implementing incentive-compatible Internet traffic pricing'''
{{header}}
{{article
|author= A Gupta,B Jukic,DO Stahl,AB Whinston,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Internet traffic pricing is necessary for the vitality of electronic commerce because uncontrolled congestion creates a detrimental effect on quality of the Internet services. Pricing approaches based on negative externality have potential to address the issue of congestion. However, most externality-based pricing approaches require the knowledge of consumers' private demand characteristics, and this requirement is often pointed out as the single most important shortcoming of these mechanisms. The fact that the Internet is a "public good" presents challenging information extraction problems for network managers in implementing any pricing mechanism. Ideally, we seek an incentive-compatible mechanism-a means of extracting the required information that provides no incentives for users to alter their behavior in an attempt to manipulate the information extraction and price setting processes. We present a solution based on a new nonparametric statistical technique that was developed for this purpose. While the results in this paper are presented in the context of our prior research on pricing, the approach presented here applies to information extraction and implementation in other resource pricing approaches.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Trading volumes with and without private information: A study using computerized market experiments'''
{{header}}
{{article
|author= YA Tung,JR Marsden,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Insider trading and asymmetric information have been the subject of a significant body of research since the 1960s. Yet little work has been directed at analyzing the impact of different market regulations. Along with difficulties in correctly identifying trades made on inside information, empirical field study methods have not been capable of analyzing the impact of different market regulations. We develop a controllable networked market trading environment that incorporates accurate identification of information possessed by each trader studied and that provides the flexibility necessary to analyze market impacts of different regulatory schemes to limit trading on inside information. We illustrate our methods through a series of controlled induced-value laboratory experiments using human subjects. Subject rewards are performance-based, with cash incentives tied to the outcomes of each market transaction. Experimental results indicate that markets with inside, privately informed traders led to greater trading volumes than markets with traders having access to private information only. In addition to reporting the results of initial sets of the experiments, we use these outcomes to frame future research issues involving the use of IT systems in surveillance and links between trading patterns and insider activity.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Private markets for public goods: Pricing strategies of online database vendors'''
{{header}}
{{article
|author= LA West,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = The online database industry has annual sales of US$6.5 billion for a product that can be easily appropriated, duplicated, reused, and redistributed. This paper examines how the industry developed dynamic pricing and delivery strategies as a response to technological and market changes, and shows how each strategy specifically compensated for the public good properties of information. Readers will see that specific pricing strategies reduce the incentive to improperly reuse downloaded information. Thus, these strategies can lead to the sustainability and growth of the online database industry. These findings are then extended to the broader context of information delivery via the Internet.
|keyword = information economics,information pricing,Internet information markets,online databases,public goods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software editions: An application of segmentation theory to the packaged software market'''
{{header}}
{{article
|author= S Raghunathan,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Introducing multiple editions of the same software is a relatively recent innovation in the software market. The editions serve to differentiate among different user segments. Introduction of similar low- and high-end products in other markets has been analyzed using segmentation theory. However, the software market is fundamentally different from other product markets in two respects: (1) Software is characterized by negligible marginal production cost, and (2) the option of offering upgrades also exists. We analyze the problem of software introduction using segmentation theory. Our analysis shows that if cannibalization is low, the vendor should introduce the full software as one edition. This result differs from that obtained in prior research, which showed that the seller should introduce two distinct products in such cases. When cannibalization is high, introducing multiple editions simultaneously is optimal under a variety of conditions. The strategy of introducing a highend edition in the first period followed by the low-end edition in the second period is optimal only when the consumers are extremely impatient and the software is large. A significant result of our analysis is that offering upgrades is clearly superior to other strategies only in a very restricted range of parameters. Our analysis also suggests that the vendor's profit is higher when it announces the future strategy. Our theoretical results are supported by evidence from the software market.
|keyword = economics of software,information goods,market segmentation,software editions,software introduction,software marketing,software pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software preannouncements and their impact on customers' perceptions and vendor reputation'''
{{header}}
{{article
|author= JA Hoxmeier,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Software preannouncements are often called "vaporware" (systems or features announced long before a ship date). The challenge confronting software vendors and consumers is understanding the balance between the need to inform the market and the negative consequences of unfulfilled promises. Based on signaling theory from marketing science and research, this study looks at the perceived importance of software preannouncement factors on customers, of unfulfilled promises and unreliable software on a company's reputation, and whether vendor dependence changes these perceptions. Database administrators were surveyed on the perceptions of their database software vendor. Fulfilling commitments to software functionality was more strongly correlated with vendor reputation than on-time delivery of the software. Customer dependence on the vendor was not correlated with perceptions of vendor reputation and credibility. Thus, unlike other industries, it seems that vendors can use software delivery time preannouncements for competitive purposes with minimal concern for the impact on customers, provided the software ultimately delivers the features and functionality promised and is largely free of errors.
|keyword = corporate reputation,signaling,software preannouncements,vaporware,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Distributing multimedia content to balance quality of service and cost'''
{{header}}
{{article
|author= S Purao,TD Han,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = An increasing number of computer applications today use multimedia content such as images, sound, and video over distributed networks of computers. Often, a dispersed set of users, with varying demands, requires ongoing access to this content. Effective placement of the multimedia content at different locations/processors thus becomes essential to ensure acceptable quality of service at a reasonable cost. Achieving this requires the consideration of a set of issues quite different from that required for traditional data distribution. These include (a) scale, both in terms of individual objects and in aggregate, (b) importance of form or appearance, making resolution levels an important, controllable variable, and (c) the temporal dimension, placing stringent demands on response time. These concerns make distribution of multimedia content more than a straightforward extension of traditional distribution approaches. We develop a model and a supporting approach to facilitate effective distribution of multimedia content, focusing on multimedia applications in corporate intranets. The model consists of multiple criteria to reflect different aspects of quality of service and cost which we formulate by leveraging variance in resolution levels to capture trade-offs among these criteria. Since the multiple-criteria allocation model is NP-complete, we propose a decision support approach that generates locally efficient solutions using designer-specified targets and evaluates them using fuzzy-set-based heuristics. The complete model and the approach have been implemented in a prototype to ensure feasibility. We demonstrate use of the prototype for a medical imaging application that illustrates applicability and usefulness of our proposals.
|keyword = fuzzy heuristics,multimedia content,multiple-criteria decision making,object distribution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring the flexibility of information technology infrastructure: Exploratory analysis of a construct'''
{{header}}
{{article
|author= TA Byrd,DE Turner,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Researchers and practitioners alike have taken note of the potential value of an organization's IT infrastructure. TT infrastructure expenditures account for over 58 percent of an organization's IT budget and the percentage is growing at If percent a year. Some even have called IT infrastructure the new competitive weapon and see it as being crucial in developing a sustained competitive advantage. Unique characteristics of an IT infrastructure determine the value of that infrastructure to an organization. One characteristic, IT infrastructure flexibility, has captured the attention of researchers and practitioners. In fact, in most recent surveys featuring issues of most importance to IT executives, the development of a flexible and responsive IT infrastructure and related topics are always at or near the top of the responses. Although the importance of IT infrastructure flexibility has been established, the development of a valid, reliable instrument to measure this construct has not been reported in the literature. The purpose of this paper is to better define the IT infrastructure flexibility construct and to develop a valid, reliable measurement instrument for this construct. In addition to the definition and operationalization of the IT infrastructure flexibility construct, this study explores the instrument's predictive validity with possible antecedent and consequent variables.
|keyword = flexibility of information systems,information architecture,information technology infrastructure,information technology personnel,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business process reengineering in the public sector: The case of the housing development board in Singapore'''
{{header}}
{{article
|author= JYL Thong,CS Yap,KL Seah,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Our existing knowledge of business process reengineering (BPR) is mainly derived from the experiences of private sector organizations, which have fundamentally different characteristics from public organizations. This paper represents a first step in understanding how BPR may be different in public organizations. Drawing on the public administration literature, it examines the differences between public and private organizations and their implications for BPR. Following that, it examines the BPR experience of a large public organization through an intensive case study. The case analysis shows that while there are similarities in the BPR experiences of public and private organizations, there are also notable differences. In this specific case, there were social and political pressures to reengineer, press publicity to promote BPR, a reengineering team comprised mainly of neutral staff, performance benchmarks adapted from the private sector, high-level approval for redesigned processes, and a pilot site implementation to secure further funding. It concludes with lessons learned for implementing BPR in public organizations.
|keyword = business process reengineering,case study,information technology,public sector,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Justifying electronic banking network expansion using real options analysis'''
{{header}}
{{article
|author= M Benaroch,RJ Kauffman,
|source= MIS QUARTERLY
|year= 2000
|abstract = The application of real options analysis to information technology investment evaluation problems recently has been proposed in the IS literature (Chalasani et al. 1997; Dos Santos 1991, Kambil et al. 1993; Kumar 1996; Taudes 1998). The research reported on in this paper illustrates the value of applying real options analysis in the context of a case study involving the deployment of point-of-sale (POS) debit services by the Yankee 24 shared electronic banking network of New England. in the course of so doing, the paper also attempts to operationalize real options analysis concepts by examining claimed strengths of this analysis approach and balancing them against methodological difficulties that this approach is believed to involve. The research employs a version of the Black-Scholes option pricing model that is adjusted for risk-averse investors, showing how it is possible to obtain reliable values for Yankee 24's "investment timing option, "even in the absence of a market to price it. To gather evidence for the existence of the timing option, basic scenario assumptions, and the parameters of the adjusted Black-Scholes model a structured interview format was developed. The results obtained using real options analysis enabled the network's senior management to identify conditions for which entry into the POS debit market would be profitable. These results also indicated that, in the absence of formal evaluation of the timing option, traditional approaches for evaluating information technology investments would have produced the wrong recommendations.
|keyword = Black-Scholes model,investment decision making under uncertainty,electronic banking networks,POS debit systems,project investments,IT investment evaluation,option pricing models,real options,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Options analysis of software platform decisions: A case study'''
{{header}}
{{article
|author= A Taudes,M Feurstein,A Mild,
|source= MIS QUARTERLY
|year= 2000
|abstract = In recent years, the use of option pricing models to support IT investment decisions has been proposed in the MIS literature. In this paper, we discuss the practical advantages of such techniques for the selection of a software platform. First, we argue that traditional quantitative approaches to a cost-benefit analysis give only a partial picture of such decision situations: due to the long planning horizon required because of the time-consuming and resource-intensive implementation process, it is not possible to exactly predict which applications will, in fact, run on the system over time. Thus, the investor is faced with the problem of valuing "implementation opportunities." We then compare different valuation techniques for this task and discuss their respective advantages and drawbacks. The practical advantages of employing such models are demonstrated by describing a real-life case study where option pricing models were used for deciding whether to continue employing SAP R/2 or to switch to SAP R/3.
|keyword = software platform,strategic IS management,real options,cost-benefit analysis,SAP R/3,IS investment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''System life expectancy and the maintenance effort: Exploring their equilibration'''
{{header}}
{{article
|author= EB Swanson,E Dans,
|source= MIS QUARTERLY
|year= 2000
|abstract = Aging information systems are expensive to maintain and most are eventually retired and replaced. But what determines (in the choices made by managers) whether and when a system reaches end-of-life? What shapes managers' judgements about a system's remaining life expectancy and do these judgments influence the maintenance effort itself? System maintenance and prospective replacement are examined here in new terms, positing that managers "equilibrate" (balance) their allocation of maintenance effort with their expectations of a system's remaining life. Drawing from data on 758 systems among 54 organizations, support is found for an exploratory structural equation model in which the relationship between maintenance effort and remaining life expectancy is newly explained. A "portfolio effect, " reflecting a system's familial complexity, is also found to be directly and positively related to the maintenance effort. A further finding is that a system's size is directly and positively associated with ifs remaining life expectancy. Notwithstanding normative research suggesting the contrary, larger systems may tend to be longer-lived than smaller systems. Practically, the suggestion is made that better documented and monitored portfolios together with regular, periodic performance assessments, can lead to better management of systems' life cycles.
|keyword = maintenance effort,life expectancy,systems replacement,familial complexity,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A cross-cultural study on escalation of commitment behavior in software projects'''
{{header}}
{{article
|author= M Keil,BCY Tan,KK Wei,T Saarinen,V Tuunainen,A Wassenaar,
|source= MIS QUARTERLY
|year= 2000
|abstract = One of the most challenging decisions that a manager must confront is whether to continue or abandon a troubled project. Published studies suggest that failing software projects are often allowed to continue for too long before appropriate management action is taken to discontinue or redirect the efforts. The level of sunk cost associated with such projects has been offered as one explanation for this escalation of commitment behavior. What prior studies fail to consider is how concepts from risk-taking theory (such as risk propensity and risk perception) affect decision makers' willingness to continue a project under conditions of sunk cost. To better understand factors that may cause decision makers to continue such projects, this study examines the level of sunk cost together with the risk propensity and risk perception of decision makers. These factors are assessed for cross-cultural robustness using matching laboratory experiments carried out in three cultures (Finland, the Netherlands, and Singapore). With a wider set of explanatory factors than prior studies, we could account fora higher amount of variance in decision makers' willingness to continue a project. The level of sunk cost and the risk perception of decision makers contributed significantly to their willingness to continue a project. Moreover, the risk propensity of decision makers was inversely related to risk perception. This inverse relationship was significantly stronger in Singapore (a low uncertainty avoidance culture) than in Finland and the Netherlands (high uncertainty avoidance cultures). These results reveal that some factors behind decision makers' willingness to continue a project am consistent across cultures while others may be culture-sensitive. Implications of these results for further research and practice are discussed.
|keyword = software project management,escalation of commitment behavior,sunk cost,risk propensity,risk perception,uncertainty avoidance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Aligning the IT human resource with business vision: The leadership initiative at 3M'''
{{header}}
{{article
|author= R Roepke,R Agarwal,TW Ferratt,
|source= MIS QUARTERLY
|year= 2000
|abstract = Increasingly, business leaders are demanding that IT play the role of a business partner and a strategic enabler. In such an environment, IT human capital has assumed considerable significance. Insightful IT leaders recognize that the greatest impediments to success often to people rather than to information, technology, and systems. What is not quite clear to IT leaders, however, is exactly how to develop and leverage this human capital in support of business needs. The transformation of IT from a back-office support role to a strategic business partner requires new roles acid competencies for IT leaders and professionals. Key challenges for IT leaders are to envision these roles and competencies and to develop and implement programs to translate this vision to reality. This paper describes the IT human resource vision that is guiding such a transformation at 3M-a large multi-product, diversified manufacturing firm (1998 sales: $15 billion)-and focuses on the implementation of its leadership initiative. This initiative was instrumental in not only allowing 3M to develop needed skills and behaviors among its IT professionals, it also helped 3M evade an industry-wide recruitment and retention trend. The major conceptual models guiding the leadership initiative as well as implementation details are presented. Challenges encountered on the way and the lessons learned from the journey are discussed. 3M's experiences provide opportunities for managers in other organizations to develop valuable insights regarding the management of human capital in IT.
|keyword = information systems professionals,leadership development,human resource strategy,managing IT professionals,recruiting and retaining IT professionals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modifications of uncertain data: A Bayesian framework for belief revision'''
{{header}}
{{article
|author= D Dey,S Sarkar,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = The inherent uncertainty pervasive over the real world often forces lousiness decisions to be made using uncertain data. The conventional relational model does not have the ability to handle uncertain data. In recent years, several approaches have been proposed in the literature for representing uncertain data by extending the relational model, primarily using probability theory. The aspect of database modification, however, has not been addressed in prior research. It is clear that any modification of existing probabilistic data, based on new information, amounts to the revision of one's belief about real-world objects. In this paper, we examine the aspect of belief revision and develop a generalized algorithm that can be used for the modification of existing data in a probabilistic relational database. The belief revision scheme is shown to be closed, consistent, and complete.
|keyword = data uncertainty,probabilistic relational model,data updating,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A formal approach to workflow analysis'''
{{header}}
{{article
|author= A Basu,RW Blanning,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Agile manufacturing, fast-response micromarketing, and the rise of the virtual organization have led managers to focus on cross-functional business processes that link various divisions and organizations. These processes may be realized as one or more workflows, each of which is an instantiation of a process under certain conditions. Because an ability to adapt processes to workflow conditions is essential for organizational responsiveness, identifying and analyzing significant workflows is an important activity for managers, organization designers, and information systems specialists. A variety of software systems have been developed to aid in the structuring and implementation of workflow systems, but they are mostly visualization tools with few analytical capabilities. For example, they do not allow their users to easily determine which information elements are needed to compute other information elements, whether certain tasks depend on other tasks, and how resource availability affects information and tasks. Analyses of this type can be performed by inspection, but this gives rise to the possibility of error, especially in large systems. In this paper, we show how a mathematical construct called a metagraph can be used to represent workflows, so that such questions can be addressed through formal operations, leading to more effective design of organizational processes.
|keyword = workflows,metagraphs,information interactions,task interactions,resource interactions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing user competence: Conceptualization and measurement'''
{{header}}
{{article
|author= BL Marcolin,DR Compeau,MC Munro,SL Huff,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Organizations today face great pressure to maximize the benefits from their investments in information technology (IT). They are challenged not just to use IT, but to use it as effectively as possible. Understanding how to assess the competence of users is critical in maximizing the effectiveness of IT use. Yet the user competence construct is largely absent from prominent technology acceptance and fit models, poorly conceptualized, and inconsistently measured. We begin by presenting a conceptual model of the assessment of user competence to organize and clarify the diverse literature regarding what user competence means and the problems of assessment. As an illustrative study, we then report the findings from an experiment involving 66 participants. The experiment was conducted to compare empirically two methods (paper and pencil tests versus self-report questionnaire), across two different types of software, or domains of knowledge (word processing versus spreadsheet packages), and two different conceptualizations of competence (software knowledge versus self-efficacy). The analysis shows statistical significance in all three main effects. How user competence is measured, what is measured, what measurement context is employed: all influence the measurement outcome. Furthermore, significant interaction effects indicate that different combinations of measurement methods, conceptualization, and knowledge domains produce different results. The concept of frame of reference, and its anchoring effect on subjects' responses, explains a number of these findings. The study demonstrates the need for clarity in both defining what type of competence is being assessed and in drawing conclusions regarding competence, based upon the types of measures used. Since the results suggest that definition and measurement of the user competence construct can change the ability score being captured, the existing information system (IS) models of usage must contain the concept of an ability rating. We conclude by discussing how user competence can be incorporated into the Task-Technology Fit model, as well as additional theoretical and practical implications of our research.
|keyword = competence,self-efficacy,software Skills,end-user computing,theoretical framework,empirical,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Opening the "black box" of network externalities in network adoption'''
{{header}}
{{article
|author= RJ Kauffman,J McAndrews,YM Wang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = Recent theoretical work suggests that network externalities are a determinant of network adoption. However, few empirical studies have reported the impact of network externalities on the adoption of networks. As a result, little is known about the extent to which network externalities may influence network adoption and diffusion. Using electronic banking as a context: and an econometric technique called hazard modeling, this research examines empirically the impact of network externalities and other influences that combine to determine network membership. The results support the network externalities hypothesis. We find that banks in markets that can generate a larger effective network size and a higher level of externalities tend to adopt early, while the size of a bank's own branch network (a proxy for the opportunity cost of adoption) decreases the probability of early adoption.
|keyword = duration models,economic analysis,electronic banking,financial services,hazard function,IT investment,network externalities,network goods,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Productivity of information systems in the healthcare industry'''
{{header}}
{{article
|author= NM Menon,B Lee,L Eldenburg,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = his research paper analyzes the impact of information technology (IT) in a healthcare setting using a longitudinal sample of hospital data from 1976 to 1994. We classify production inputs into labor and capital categories. Capital is classified into three components-medical IT capital, medical capital, and IT capital-and labor is classified into two components, medical labor and IT labor. Results provide evidence that IT contributes positively to the production of services in the healthcare industry.
|keyword = organizational productivity,health IS,IS investment,IS performance evaluation,IS research methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using repertory grids to conduct cross-cultural information systems research'''
{{header}}
{{article
|author= MG Hunter,JE Beck,
|source= INFORMATION SYSTEMS RESEARCH
|year= 2000
|abstract = As more business is being conducted internationally and corporations establish themselves globally, the impact of cross-cultural aspects becomes an important research issue. The need to conduct: cross-cultural research is perhaps even more important in the relatively newly emerging and quickly changing information systems (IS) field. This article presents issues relating to qualitative research, emic versus etic approaches, and describes a structured, yet flexible, qualitative research interviewing technique, which decreases the potential for bias on the part of the researcher. The grounded theory technique presented in this article is based on Kelly's Repertory Grid (RepGrid), which concentrates on "laddering," or the further elaboration of elicited constructs, to obtain detailed research participant comments about an aspect within the domain of discourse. The technique provides structure to a "one-to-one" interview. But, at the same time, RepGrids allow sufficient flexibility for the research participants to be able to express their own interpretation about a particular topic. This article includes a brief outline of a series of research projects that employed the RepGrid technique to examine similarities and differences in the way in which "excellent" systems analysts are viewed in two different cultures. Also included is a discussion of the technique's applicability for qualitative research in general and cross-cultural studies specifically. The article concludes by suggesting ways in which the RepGrid technique addresses some of the major methodological issues in cross-cultural research.
|keyword = qualitative,repertory grids,cross cultural,information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Considering social subsystem costs and benefits in information technology investment decisions: A view from the field on anticipated payoffs'''
{{header}}
{{article
|author= SD Ryan,DA Harrison,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Information technology (IT) investment decisions have traditionally focused on financial or technological issues. Responding to what appears to be a lack of payoff in IT investments, researchers as well as practitioners recently have suggested that traditional valuation analyses are incomplete and have called for additional work to identify "hidden" or seldom-considered costs and benefits. The present paper attempts to improve understanding of a chief source of these hidden costs and benefits: those changes in the social subsystem brought about by a new IT. Fifty IT decision-makers in a broad variety of industries were interviewed to gain insight into what, when, and how often social subsystem considerations are included in IT investment-decision processes. Data from the interviews show that in practice some of those issues are often minimized, excluded, or put off until the IT is implemented-thus affecting optimality of investment choices and IT payoff. The paper extends existing theory by describing systematic patterns of inclusion and exclusion of these costs and benefits. In addition, a decision aid is provided to help IT executives begin thinking about which social subsystem costs and benefits they should incorporate in various decisions. Suggestions are also made on how data regarding social subsystem costs and benefits might be gathered. By incorporating social subsystem costs and benefits in the IT investment process, decision-makers gain a greater appreciation for hidden costs and benefits, and thus clarify anticipated IT payoff.
|keyword = information technology investment,decision-making processes,sociotechnical systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology payoff in the health-care industry: A longitudinal study'''
{{header}}
{{article
|author= S Devaraj,R Kohli,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = With the enormous investments in Information Technology (IT), the question of payoffs from IT has become increasingly important. Organizations continue to question the benefits from IT investments especially in conjunction with corporate initiatives such as business process reengineering (BPR). Furthermore, the impact of technology on nonfinancial outcomes such as customer satisfaction and quality is gaining interest. However, studies examining the IT-performance relationship have been far from conclusive. The difficulty in identifying impacts from technology has been the isolation of benefits of IT from other factors that may also contribute to organizational performance. Furthermore, benefits from technology investments may be realized over an extended period of time. Finally, IT benefits may accrue when they are done in concert with other organizational initiatives such as business process reengineering. This calls for studies that take into account control variables as well as data that span time periods. In this study, we examine monthly data collected from eight hospitals over a recent three-year time period. We specify propositions that relate investments in IT to performance, and the combined effect of technology and BPR on performance. We draw upon the literature in health-care management to incorporate appropriate control variables in the analyses. Our results provide support for the IT-performance relationship that is observed after certain time lags. Such a relationship may not be evident in cross-sectional or snapshot data analyses. Also, results indicate support for the impact of technology contingent on BPR practiced by hospitals.
|keyword = business process reengineering,health-care information systems,information technology payoff,information technology productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A framework for assessing the relationship between information technology investments and firm performance'''
{{header}}
{{article
|author= S Sircar,JL Turnbow,B Bordoloi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = There have been several attempts in the past to assess the impact of information technology on firm performance that have yielded conflicting results. Researchers have been unable to conclude that IT spending by an organization results in increases in key performance indicators. Two major recent studies have attempted to address the issue by putting greater emphasis on the theoretical underpinnings of the solution to the problem, although they chose different theoretical frameworks. The present study extends that work to yield a framework that shows the relationship between firm performance and both IT and corporate investments. The data used to validate the framework exceeds that used in previous analyses in both quality and quantity, thereby permitting appropriate statistical analyses. A large database consisting of over 2,000 observations of 624 firms was constructed, using data provided by the International Data Corporation, Standard & Poor's Compustat, and Moody's. This allowed us to pose the following research questions: (a) Can the relationship between sets of investment measures and firm performance be demonstrated (as opposed to individual measures)? (b) How are IT investments related to a firm's market value, market share, sales, and assets? and (c) Is there a difference in the effect of computer capital and noncomputer capital? Seven measures of firm performance were initially incorporated as outputs in the framework, related to sales, assets, and market value. Similarly, seven input measures of IT and corporate investments were initially included. Two output measures and one input were eventually eliminated to formulate a refined framework with strong explanatory power. After careful editing, canonical analyses were performed, resulting in several important findings. Both IT and corporate investments have a strong positive relationship with sales, assets, and equity, but not with net income. Spending on IS staff and staff training is positively correlated with firm performance, even more so than computer capital.
|keyword = information technology payoff,information technology investment,organizational performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology value through different normative lenses'''
{{header}}
{{article
|author= B Lee,NM Menon,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = The influence of IT investments on organizational performance is revisited. Bounded rationality, organizational controls, and political forces may constrain optimal selection of inputs and appropriate substitution between inputs. For example, firms may not be able to attain an optimal level of IT by substituting IT for labor (for reasons such as pressure from the labor union). Besides estimating a link between IT investments and firm output, this paper presents a study of the link between IT investment levels and the efficiency of processes. Nonparametric and parametric techniques were applied to financial data on hospitals collected over a period of eighteen years. We found that cost and technical and allocative efficiencies are statistically significant in the production framework. We also found that hospitals that were characterized by high technical efficiency also used a greater amount of IT capital than firms that exhibited low technical efficiency. A group of hospitals exhibiting high technical efficiency also exhibited low allocative efficiency, indicating that, while processes may have been efficient, resource allocation and budgeting between various categories of capital and labor have not been efficient. Our results also differ from previously published results because we find that IT labor had a negative contribution to productivity and that non-IT capital had a greater contribution to productivity than IT capital.
|keyword = allocative efficiency,information technology performance,process efficiency,technical efficiency,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Discovering potential and realizing value from information technology investments'''
{{header}}
{{article
|author= MJ Davern,RJ Kauffman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Information technology (IT) value has been measured at various levels of analysis, yet few authors would contend that the search for value has reached a point where practitioners and theoreticians are satisfied with its outcomes. We present a new perspective that emphasizes the importance of understanding where potential value lies and how best to relate it contextually to the measurement of the firm's realized value across multiple levels of analysis. We develop the idea that complementary assets (especially business process design and human capital) influence the firm's realization of value, using concepts such as locus of value and value conversion contingencies. Expanding beyond earlier process models of IT value, which begin with IT expenditure, our analysis of IT value emphasizes the consideration of potential value for an IT investment both in ex ante project selection, and ex post investment evaluation. We illustrate and validate the application of our framework using IT investments in a variety of business domains.
|keyword = business-process design,business value of information technology,conversion contingencies,decision-support systems,IT value,potential value,realized value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Executives' perceptions of the business value of information technology: A process-oriented approach'''
{{header}}
{{article
|author= PP Tallon,KL Kraemer,V Gurbaxani,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.
|keyword = business value,impacts of information technology,information technology strategy,value chain,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relating benefits from using IS to an organization's operating characteristics: Interpreting results from two countries'''
{{header}}
{{article
|author= A Ragowsky,M Stern,DA Adams,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = To obtain the greatest benefit from its information system, an organization must determine which applications will provide the most benefit to organizational performance. This study reviews data collected from 310 manufacturing firms in Israel and 197 such firms in the U.S. For each firm, data were obtained about the benefits derived from using information systems, as perceived by a senior manager, and the organization's operating characteristics. Data were pooled across both countries. No meaningful relationship was found between the benefit a firm derives from its overall information systems application portfolio and its organizational operating characteristics. However, for two individual applications, the benefit derived is linked significantly to the organization's operating characteristics. Thus the model relating benefits from information systems to the organization's operating environment, first demonstrated by data collected in Israel, is confirmed by the data collected in the U.S. The model applies across both countries, even though there may be differences between the two countries, for example, in culture, size of businesses, and relationship with customers and suppliers.
|keyword = information economics,information technology value,international information systems,manufacturing systems,organizational operating characteristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Strategic payoff from EDI as a function of EDI embeddedness'''
{{header}}
{{article
|author= AT Chatfield,P Yetton,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 2000
|abstract = A key issue facing IT researchers and practitioners has been the difficulty in realizing strategic payoff from IT investment. This study, drawn on sociological theories of embeddedness, addresses this key issue, with particular attention to the perspective of EDI network initiator. Cross-case analysis is conducted comparing three initiators of sophisticated EDI networks, who realized different levels of strategic payoffs. Results reveal that the achievement of strategic payoffs is a function of EDI embeddedness, which is defined as how central or peripheral a specific EDI network is to managing interfirm interdependence. In a model of EDI initiator strategic payoff, we argue that EDI embeddedness, which is influenced by existing interfirm relationship, moderates the impact of adopter EDI use on initiator strategic payoff derived from the EDI investment. Specifically, while high embeddedness motivates adopter strategic use, low embeddedness deters such use. The model is validated against three reported cases in the literature.
|keyword = electronic data interchange,embeddedness,interfirm cooperation,interfirm interdependence,IT investment,strategic payoff,time-based competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A confessional account of an ethnography about knowledge work'''
{{header}}
{{article
|author= U Schultze,
|source= MIS QUARTERLY
|year= 2000
|abstract = Information systems research has traditionally focused on information as an object that serves as input to decision making. Such a perspective attends mainly to the use of information. Increasingly, however, organizations are concerned about the production of information. This paper focuses on the work of producing informational objects, an activity central to knowledge work. Based on data collected during an eight-month ethnographic study of three groups of knowledge workers-computer system administrators, competitive intelligence analysts, and librarians-I explore the informing practices they relied upon. These are identified as ex-pressing, monitoring and translating. Common to these informing practices is the knowledge workers' endeavor to balance subjectivity and objectivity, where subjectivity is a necessary part of doing value adding work and objectivity promises workers authority and a sense of security. Recognizing that researchers are knowledge workers too, I draw on my own experiences as an ethnographic researcher to identify parallels between my informing practices and those of the knowledge workers I studied. These parallels are intended to challenge the taken-for-granted assumptions underlying scientific practice. I adopt a confessional genre of representation for this purpose.
|keyword = social science,ethnography,evaluation criteria,practice,confessional genre of representation,objectivity,subjectivity,reflexivity,information,knowledge creation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding computer-mediated discussions: Positivist and interpretive analyses of group support system use'''
{{header}}
{{article
|author= EM Trauth,LM Jessup,
|source= MIS QUARTERLY
|year= 2000
|abstract = This research considers whether interpretive techniques can be used to enhance our understanding of computer-mediated discussions. The case study considered in this research is the use of a group support system (GSS) to support employee discussions about gender equity in a university. Transcripts of the four discussions were analyzed using two analysis techniques: a positivist approach, which was focused on the GSS sessions themselves, and an interpretive approach which broadened the scope to include contextual considerations as well. What emerged from the positivist analysis was the conclusion of effective group behavior directed toward consensus around alternative solution scenarios. What emerged from the interpretive analysis was evidence of multiple, rich types of information at three levels: cognitive, affective, and behavioral. The interpretive analysis also uncovered the absence of shared consciousness about the issue and imbalanced participation in the sessions. Comparison of the results of both approaches showed that, while the positivist analysis provided useful information, the interpretive analysis provided a different understanding of the same evidence and new information not found in the positivist analysis of the group discussions. This research adds to the body of knowledge concerning the effects of virtual group meetings on the type of information that is shared and the value of a combination of positivist and interpretive analyses of GSS data.
|keyword = anonymity,computer-mediated communication,ethnography,gender,group decision making,group decision support system,hermeneutics,information richness,interpretive methods,IS research methodologies,positivist methods,virtual group,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Factors that influence the social dimension of alignment between business and information technology objectives'''
{{header}}
{{article
|author= BH Reich,I Benbasat,
|source= MIS QUARTERLY
|year= 2000
|abstract = The establishment of strong alignment between information technology (IT) and organizational objectives has consistently been reported as one of the key concerns of information systems managers. This paper presents findings from a study which investigated the influence of several factors on the social dimension of alignment within 10 business units in the Canadian life insurance industry. The social dimension of alignment refers to the state in which business and IT executives understand and are committed to the business and IT mission, objectives, and plans. The research model included four factors that would potentially influence alignment.. (1) shared domain knowledge between business and IT executives, (2) IT implementation success, (3) communication between business and IT executives and (4) connections between business and IT planning processes. The outcome, alignment, was operationalized in two ways. the degree of mutual understanding of current objectives (shortterm alignment) and the congruence of IT vision (long-term alignment) between business and IT executives. A total of 57 semi-structured interviews were held with 45 informants. Written business and IT strategic plans, minutes from IT steering committee meetings, and other strategy documents were collected and analyzed from each of the 10 business units. All four factors in the model (shared domain knowledge, IT implementation success, communication between business and IT executives, and connections between business and IT planning) were found to influence short-term alignment. Only shared domain knowledge was found to influence long-term alignment. A new factor, strategic business plans, was found to influence both short and long-term alignment. The findings suggest that both practitioners and researchers should direct significant effort toward understanding shared domain knowledge, the factor which had the strongest influence on the alignment between IT and business executives. There is also a call for further research into the creation of an IT vision.
|keyword = alignment,communication,shared knowledge,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''One road to turnover: An examination of work exhaustion in technology professionals'''
{{header}}
{{article
|author= JE Moore,
|source= MIS QUARTERLY
|year= 2000
|abstract = The concept of work exhaustion (or job burnout) from the management and psychology research literature is examined in the context of technology professionals. Data were collected from 270 IT professionals and managers in various industries across the United States. Through structural equation modeling, work exhaustion was shown to partially mediate the effects of workplace factors on turnover intention, in addition, the results of the study revealed that: (1) technology professionals experiencing higher levels of exhaustion reported higher intentions to leave the job and, (2) of the variables expected to influence exhaustion (work overload, role ambiguity and conflict, lack of autonomy and lack of rewards), work overload was the strongest contributor to exhaustion in the technology workers. Moreover, exhausted IT professionals identified insufficient staff and resources as a primary cause of work overload and exhaustion. Implications for practice and future research are discussed.
|keyword = burnout,exhaustion,IS professionals,IT professionals,staffing,technology professionals,turnover,work overload,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information and communication: Alternative uses of the Internet in households'''
{{header}}
{{article
|author= R Kraut,T Mukhopadhyay,J Szczypula,S Kiesler,B Scherlis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Is the Internet a superhighway to information or a high-tech extension of the home telephone? We address this question by operationalizing information acquisition and entertainment as the use of the World Wide Web and interpersonal communication as the use of electronic mail (e-mail), and examine how 229 members of 110 households used these services during their first year on the Internet. The results show that e-mail drives people's use of the Internet. Participants used e-mail in more Internet sessions and more consistently than they used the World Wide Web, and they used e-mail first in sessions where they used both. Participants used the Internet more after they had used e-mail heavily, but they used the Internet less after they had used the Web heavily. While participants' use of both e-mail and the Web declined with time, the decline in Web use was steeper. Those who used e-mail more than they used the Web were also more likely to continue using the Internet over the course of a year. Our findings have implications for engineering and policies for the Internet and, more generally, for studies of the social impact of new technology.
|keyword = interpersonal communication,family communication,social impact,computer-mediated communication,Internet,World Wide Web,online services,user studies,technology adoption,e-mail,electronic mail,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology assimilation in firms: The influence of senior leadership and IT infrastructures'''
{{header}}
{{article
|author= CP Armstrong,V Sambamurthy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = It assimilation is regarded as an important outcome in the efforts of firms to leverage the potential of information technologies in their business activities and strategies. Despite significant investments in information technology, considerable diversity exists in how well firms have been able to assimilate IT and leverage the business value of IT. This research draws upon the emerging knowledge-based and resource-based views of the firm to examine the influence of three factors on IT assimilation: (i) quality of senior leadership, (ii) sophistication of IT infrastructures, and (iii) organizational size. Drawing upon a large-scale sample survey where responses were obtained from CIOs and senior business executives who were members of the firms' top management teams, the study examines a variety of mostly normative prescriptions. The findings provide robust evidence about the impacts of CIOs' business and IT knowledge on IT assimilation. Further, we find that CIOs' membership in top management teams and their informal interactions with TMT members enhance their knowledge, particularly their business knowledge. We find that the intensity of the relationship between CIO's interactions with the top management team and their level of IT and business knowledge is much stronger in firms that articulate a transformational IT vision. The sophistication of IT infrastructures was also found to significantly impact IT assimilation. Surprisingly, the IT knowledge of senior business executives was not found to be a significant influence on IT assimilation. The implications of these findings for evolving a deeper understanding of the dynamics underlying IT assimilation are presented.
|keyword = TT assimilation,IT infrastructure,senior leadership,Chief Information Officer,top management team,TT vision,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Designing workflow coordination: Centralized versus market-based mechanisms'''
{{header}}
{{article
|author= JC Tan,PT Harker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = As a result of the increasing diffusion of decision-making within and between organizations, distributed scheduling methods have been proposed as alternatives to centralized, hierarchical, top-down production control schemes. While distributed scheduling methodologies are appealing, one must first address the fundamental questions of when and where such methods are appropriate. This paper seeks to provide answers to these questions. Using a generalized workflow framework, this paper models and compares the total expected costs of using decentralized and centralized organizational designs to coordinate the flows of information and work. This comparison allows one to define the characteristics of work environments where distributed scheduling methods are more suitable than hierarchical, top-down production approaches. Finally, from this analysis, one can conclude that distributed scheduling methods work well for systems where information technology is inexpensive relative to production cost, processing times are relatively long, and where the number of agents in the system is not too large.
|keyword = distributed work,organizational design,organizational structures,workflow coordination,distributed scheduling,intelligent agent,market mechanisms auction,bidding,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dimensionality of the strategic grid framework: The construct and its measurement'''
{{header}}
{{article
|author= B Raghunathan,TS Raghunathan,Q Tu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = The recent dramatic impact of information technology (IT) on organizational performance has necessitated appropriate strategies for managing this organizational resource. IT impacts firms in different ways, and management needs a clear and systematic understanding of both the current and future relevance and impact of its IT before selecting management tools and approaches. The strategic grid framework, developed for the purpose of helping management gain this understanding, is one of the most highly recognized and quoted conceptual frameworks in information systems literature. Despite such recognition, valid operational measures of this construct are not available for use in empirical research studies. The research reported in this paper is an attempt to develop and validate operational measures for the dimensions of the strategic grid, with the understanding that the availability of such measures will promote future empirical studies. Data for this study consist of the responses of 231 IS executives to a survey instrument. Operational models of the grid dimensions are specified and the measurement properties of the models are assessed using confirmatory factor analysis (CFA) within the LISREL framework. Measures resulting from the analysis are shown to meet the requirements of rigorous tests of measurement properties. The CFA results also show that the current portfolio dimension of the strategic grid is unidimensional, while the future portfolio dimension has three factors: management support, differentiation, and enhancement.
|keyword = management of information technology,IT planning,strategic grid framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating the impact of DSS, cognitive effort, and incentives on strategy selection'''
{{header}}
{{article
|author= P Todd,I Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Decision support system (DSS) researchers have long debated whether or not the provision of a DSS would lead to greater decision-making effectiveness, efficiency, or both. The work described in this paper examines how DSS designers can guide users towards employing more normative decision strategies. Working from notions of restrictiveness and decisional guidance (Silver 1990) supplemented by the cost-benefit framework of cognition, we explain how DSS capabilities influence decision behavior and performance through the manipulation of effort. The results of this work should assist DSS developers to devise directed or nondirected approaches to effect desired behaviors.
|keyword = Decision Support Systems,cognitive effort,financial incentives,decision strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research note. The illusion of electronic brainstorming productivity: Theoretical and empirical issues'''
{{header}}
{{article
|author= A Pinsonneault,H Barki,RB Gallupe,N Hoppen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = After discussing how group size might affect the effectiveness of electronic brainstorming (EBS) as an idea generating tool, Dennis and Valacich (1999) conclude that EBS is not likely to surpass nominal brainstorming for small groups, but that for large groups (i.e., nine or more members), "EBS offers clear performance benefits over nominal group brainstorming, as well as verbal brainstorming." However, in our view, the existing theoretical and empirical evidence does not provide sufficient justification to clearly establish EBS' superiority over nominal brainstorming for large groups.
|keyword = group process,brainstorming,electronic meeting systems,laboratory study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evidential reasoning for WebTrust assurance services'''
{{header}}
{{article
|author= RP Srivastava,TJ Mock,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = We study two aspects of assurance services in electronic commerce. The first deals with the type(s) of evidential networks that will allow a professional accountant to provide assurance. Here, we develop an evidential network model for "WebTrust Assurance," a service being provided by the American Institute of Certified Public Accountants (AICPA) and the Canadian Institute of Chartered Accountants (CICA). Our model augments the AICPA/CICA approach and provides goals, subgoals and evidence relevant to the overall assurance to be provided. The aggregation of evidence and the resolution of uncertainties follow the belief-function approach. Next we develop a decision-theoretic model for the assurance-planning problem. Our approach is based on estimating the expected value of providing various levels of assurance and is illustrated with several different scenarios that may be faced in practice. We also consider the role of ambiguity in decision situations such as planning WebTrust engagements and calculate bounds in expected value based on whether auditors are conservative or not in their approach to risk.
|keyword = assurance services,decision theory,electronic commerce,risk management,WebTrust,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Testing the interactivity model: Communication processes, partner assessments, and the quality of collaborative work'''
{{header}}
{{article
|author= JK Burgoon,JA Bonito,B Bengtsson,A Ramirez,NE Dunbar,N Miczo,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = A major consideration in designing and adopting new communication technologies is their impact on communication processes and outcomes. One way to understand this impact is according to the principle of interpersonal interactivity. Findings from two investigations are reported here that address how properties of task-related communication conducted with differing interfaces relate to perceptions of interaction partners and the outcomes of their collaborative work. Study I manipulated the interface affordances of mediation, contingency, and modality richness. Study 2 examined the affordance of mediation. Results show that interfaces that promote higher mutuality and involvement lead to more favorable perceptions of partners' credibility and attraction, and these perceptions are systematically related to higher-quality decisions and more influence. Discussion focuses on the relation between user perceptions, design features, and task outcomes in human-computer interaction and computer-mediated communication.
|keyword = collaborative work,communication interfaces,computer-mediated communication,decision making,human-computer interaction,interactivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Verifying the proximity and size hypothesis for self-organizing maps'''
{{header}}
{{article
|author= CT Lin,HC Chen,JF Nunamaker,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The Kohonen Self-Organizing Mag (SOM) is an unsupervised learning technique for summarizing high-dimensional data so that similar inputs are, in general, mapped close to one another. When applied to textual data, SOM has been shown to be able to group together related concepts in a data collection and to present major topics within the collection with larger regions. This article presents research in which we sought to validate these properties of SOM, called the Proximity and Size Hypotheses, through a user evaluation study. Building upon our previous research in automatic concept generation and classification, we demonstrated that the Kohonen SOM was able to perform concept clustering effectively, based on its concept precision and recall7 scores as judged by human experts. We also demonstrated a positive relationship between the size of an SOM region and the number of documents contained in the region. We believe this research has established the Kohonen SOM algorithm as an intuitively appearing and promising neural-network-based textual classification technique for addressing part of the longstanding "information overload" problem.
|keyword = document clustering techniques,experimental research,group support systems,self-organizing maps,unsupervised learning algorithms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Linguini: Language identification for multilingual documents'''
{{header}}
{{article
|author= JM Prager,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Given the vast and still growing availability of electronic documents from around the world, it is becoming increasingly important for managers of the information systems on which these documents are stored to sort or tag these documents so that their end users can most readily access those documents that are of most interest and use to them, which in our context means in a language they can understand. Linguini is a vector-space-based categorizer tailored for high-precision language identification. This paper determines the functional dependencies of Linguini's performance and demonstrates that it can identify the language of documents as short as 5 to 10 percent of the size of average Web documents with 100 percent accuracy. It also describes how to determine if a document is in two or more languages, without incurring any appreciable extra computational overhead. This approach can be applied equally to subject-categorization systems to distinguish between cases where, when the system recommends two or more categories, the document belongs strongly to all or really to none.
|keyword = categorization,information retrieval,language identification,vector-space models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Data is more than knowledge: Implications of the reversed knowledge hierarchy for knowledge management and organizational memory'''
{{header}}
{{article
|author= I Tuomi,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The knowledge management literature often points out the importance of distinguishing among data, information, and knowledge. The generally accepted view sees data as simple facts that become information as data are combined into meaningful structures, which subsequently become knowledge as meaningful information is put into a context and when it can be used to make predictions. According to this view, data are a prerequisite for information, and information is a prerequisite for knowledge. This paper explores the conceptual hierarchy of data, information, and knowledge, showing that data emerge only after we have information, and that information emerges only after we already have knowledge. The reversed hierarchy of knowledge is shown to lead to a different approach in developing information systems that support knowledge management and organizational memory. It is also argued that this difference may have major implications for organizational flexibility and renewal.
|keyword = information,knowledge,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information overload: Addressing the productivity paradox in face-to-face electronic meetings'''
{{header}}
{{article
|author= ML Grise,RB Gallupe,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The Electronic Brainstorming System (EBS) is a group support system (GSS) tool considered particularly productive in supporting idea generation. Unfortunately, as computer-supported groups are confronted with larger numbers of ideas and supporting comments to organize and evaluate, they may experience information overload. This study explores the problem of information overload within the context of an idea-organization task in a face-to-face electronic meeting. Integrative Complexity Theory provides the primary theoretical foundation, and an Information Overload Model for GSS is introduced. Results from a laboratory experiment provide support for the idea that effective GSS tools can be designed based on a theoretical understanding of information processing, in particular, how information is processed under conditions of high information load. Use of a GSS tool designed to regulate the flow of information, called an Idea Regulator, led subjects to organize ideas with higher levels of complexity, but they also reported higher levels of mental workload. This research suggests that particular attention to the problem of information overload, and research focused on finding theory-based solutions, can lead to more effective meetings.
|keyword = electronic brainstorming,group support systems,information overload,Integrative Complexity Theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Exploring mediation between environmental and structural attributes: The penetration of communication technologies in manufacturing organizations'''
{{header}}
{{article
|author= CC Lee,V Grover,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The relationship between organizational structures and information technology (IT) has been the subject of much discussion in IS research. While studies have not yielded conclusive results, the importance of examining the relationship between structure and technology is increasing in an environment where organizations are using contemporary IT to redesign themselves in order to compete more effectively. This paper presents a study that examines the relationship between the use of an important class of IT communications technologies, and organizational structural attributes within a brand contingency context. Hypotheses are proposed, based on theory from the information-processing paradigm examining the mediating role of communications technologies (CT) in the relationship between environmental characteristics and organizational structural characteristics. Data from 153 manufacturing firms are collected and analyzed. The results show that CT seems to play a direct role in reinforcing structures that emerge from environmental dictates. The expanded set of variables considered in this study and the results provide potentially strong implications for future work in this important area.
|keyword = communication technologies,environmental characteristics,mediation,organizational structure,telecommunications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Supervisor support and career anchor impact on the career satisfaction of the entry-level information systems professional'''
{{header}}
{{article
|author= JJ Jiang,G Klein,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Keeping satisfied information systems (IS) professionals at the entry level has become a difficult task for IS managers. Financial incentives have led to escalating costs without widespread improvement of the situation. To capitalize on nonfinancial rewards, organizations must understand the relationships between employees' internal desires, the environment fostered by the organization, and career satisfaction. This paper is a report on a study of 101 entry-level IS professionals. The data analysis indicates that IS professionals find more satisfaction with their career when supervisor support is prominent and an adequate range of opportunities that satisfy career desires exist within the organization.
|keyword = career anchors,career satisfaction,MIS entry-level personnel,supervisor support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Six myths of information and markets: Information technology networks, electronic commerce, and the battle for consumer surplus'''
{{header}}
{{article
|author= V Grover,P Ramanlal,
|source= MIS QUARTERLY
|year= 1999
|abstract = The infusion of powerful information networks into business environments is beginning to have a profound impact on the nature of governance between buyers and sellers in the marketplace. Most articles in this area emphasize the benefits to the consumer side of the equation due to reduced coordination, search, and transactional costs. This article presents a broader view of information and markets by elucidating innovative ways that sellers can survive in intensely competitive markets. The article is framed in terms of six myths and counter-myths of information technology and effective markers. The myths provide a conventional view of how increased customization and outsourcing, open architectures, a larger customer base, and low price guarantees will benefit the buyer. The counter-myths illustrate that it is altogether feasible for IT to enable supplier strategies that extract consumer surplus. For instance, suppliers could use IT to price discriminate by tailoring product offerings and charging buyers as much as they are willing to pay. They could also segment markets making comparative shopping difficult, thus avoiding the competitive equilibrium. Also, suppliers could focus on the creation of networks that lock in customers or follow aggressive pricing strategies that deter price competition. Both the myths and counter-myths are presented and examined in a polemical format using simple, fundamental economic arguments. We hope to provide provocative new avenues for discourse in this area by recognizing the complexity of interactions between buyers and suppliers in a highly networked environment.
|keyword = electronic markets,networked markets,myths of markets,economic theory,seller strategies,buyer strategies,electronic commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of goals on software project management: An experimental investigation'''
{{header}}
{{article
|author= TK Abdel-Hamid,K Sengupta,C Swett,
|source= MIS QUARTERLY
|year= 1999
|abstract = Over the last three decades, a significant stream of research in organizational behavior has established the importance of goals in regulating human behavior. The precise degree of association between goals and action, however, remains an empirical question since people may, for example, make errors and/or lack the ability to attain their goals. This may be particularly true in dynamically complex task environments, such as the management of software development. To date, goal setting research in the software engineering field has emphasized the development of tools to identify, structure, and measure software development goals. In contrast, there has been little microempirical analysis of how goals affect managerial decision behavior. The current study attempts to address this research problem. It investigated the impact of different project goals on software project planning and resource allocation decisions and, in turn, on project performance. The research question was explored through a role-playing project simulation game in which subjects played the role of software project managers. Two multigoal structures were tested, one for cost/schedule and the other quality/schedule. The cost/schedule group opted for smaller cost adjustments and was more willing to extend the project completion time. The quality/schedule group, on the other hand, acquired a larger staff level in the later stages of the project and allocated a higher percentage of the larger staff level to quality assurance. A cost/schedule goal led to lower cost, while a quality/schedule goal led to higher quality. These findings suggest that given specific software project goals, managers do make planning and resource allocation choices in such a way that will meet those goals. The implications of the results for project management practice and research are discussed.
|keyword = goals,software project management,software cost,software quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''How much bandwidth is enough? A longitudinal examination of media characteristics and group outcomes'''
{{header}}
{{article
|author= K Burke,L Chidambaram,
|source= MIS QUARTERLY
|year= 1999
|abstract = This study addresses three key issues related to media differences among computer-mediated groups meeting face-to-face, synchronously, and asynchronously: First, do groups using different media perceive characteristics of these media differently! Second, do media-related perceptions remain static or change as they are used over time? And finally, do media differences result in performance differences, i.e., do richer media result in better performance for equivocal tasks as predicted by bandwidth theories? Some results of this study tend to support media-characteristics theories while others offer limited support for more evolutionary perspectives. For instance, initially, face-to-face groups found their medium to be warmer, have a better interface, and be more effective compared to their distributed counterparts. While many of these initial perceptions lingered over time, there was one notable exception. At the start of the study, face-to-face groups rated their medium as being more effective than synchronous groups; however, by the end of the study, no significant differences were apparent. Moreover, despite the persistent ly lower social presence of leaner media, distributed-synchronous groups performed better than their face-to-face counterparts. Finally, the two types of distributed groups-synchronous and asynchronous-did not differ significantly in their perceptions or performance. This study improves our understanding of distributed interaction while simultaneously highlighting the need to further investigate the relationships among tasks, technologies and teams over time.
|keyword = communication media,computer-mediated communication,group performance,group perceptions,longitudinal research strategies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Learning dysfunctions in information systems development: Overcoming the social defenses with transitional objects'''
{{header}}
{{article
|author= DG Wastell,
|source= MIS QUARTERLY
|year= 1999
|abstract = Given the continuing prevalence of IS failure, this paper contends that a fresh theoretical perspective and new methodological principles are required. It is argued that learning is crucial to the success of ISD, and that many IS projects miscarry due to the inherently high levels of stress and anxiety that imbue ISD and that elicit defense-avoidance behavior patterns in project teams. Such social defenses reflect modes of group behavior that operate primarily to reduce anxiety, rather than reflecting genuine engagement with the task at hand. It is argued that the operation of these defenses can come to paralyze the learning processes that are critical to effective IS development Following a clinical research strategy, case studies are presented illustrating the working of defensive processes which undermined three IS projects. Three social defenses are illustrated: the organizational ritual, the sibling horde, and paranoid isolationism. Drawing on psychodynamic theory, the concept of transitional space is introduced. Such spaces have two important aspects: a supportive psychological climate and a supply of appropriate transitional objects (i.e., entities that provide temporary emotional support). It is argued that IS development should be reframed as a transitional space, with particular attention given to the selection of appropriate transitional objects to assist in breaking down defensive processes. The cases are revisited to illustrate this approach in action; useful insights and positive practical outcomes are shown. It is concluded that the present psychodynamic perspective has considerable value in relation to the IS discipline: theoretically, in terms of our understanding of the social dynamics of ISD and at a practical level too, through the provision of diagnostic concepts and remedial measures that have significant potential to enhance IS praxis and to redress the high rate of IS failure.
|keyword = IS development,organizational learning,stress and anxiety,social defenses,transitional space,transitional objects,methodologies and models,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing the health of an information systems applications portfolio: An example from process manufacturing'''
{{header}}
{{article
|author= P Weill,M Vitale,
|source= MIS QUARTERLY
|year= 1999
|abstract = This paper presents a model of the IS applications portfolio and illustrates how its "health" can be assessed through an example from process manufacturing. The health assessment is based on an evaluation by senior managers of a business unit's portfolio of information systems. This assessment is made on five separate, but related, attributes of each system in the portfolio: importance, investment, technical quality, use, and management value. The "Health Grid" is introduced as a way of representing the IS applications portfolio in order to facilitate the assessment and interpretation of its health. One of the advantages of using the grid is to make such an assessment explicit, transparent, and discussible. In an example, the Health Grid is used to assess and interpret the IS applications portfolio of the most profitable business unit in a large process manufacturing firm. The applications portfolio, consisting of 18 systems, was generally assessed as requiring attention. For example, there was no evidence of any relationship between the investment in a given system and its management value. The paper includes a suggested approach for using the Health Grid and an analysis of the strengths and weaknesses of the approach. A description of the changes made to the IS portfolio in the example firm after the initial data collection completes the paper.
|keyword = applications portfolio,value of IS,IS assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Morality and computers: Attitudes and differences in moral judgments'''
{{header}}
{{article
|author= UE Gattiker,H Kelley,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Business ethics is an emerging area of research in many subfields of management, including information systems (IS). Empirical IS research has studied differences in users' attitudes and in moral judgments regarding ethical computer-related behavior. This study applied the "domains of morality" approach to determine how users felt about certain computer-related behaviors. Vignettes describing ethical dilemmas involving computer technology (e.g., uploading a computer virus on an electronic network/bulletin board system) were presented to a sample of Internet users. The research findings offered several interesting and, in some cases, unexpected results. The empirical results indicated that older computer users have a less permissive sense of what is right and wrong for an illegal game. When computers were used to test a banned game, men and women differed in their assessment of its appropriateness. A surprising finding was that participants were not likely to endorse civil liberties, and were more concerned about the harm to, and violations of, social norms when the scenario described a situation involving a computer virus. How users perceive, prejudge, and discriminate computer ethics and abusive computer actions raises numerous questions and implications for IS researchers, IS practitioners, and policy makers. The results of this study foster a better understanding of Internet users' moral categorization of specific computer behaviors and, hopefully, help to further reduce risks and vulnerabilities of systems by identifying computer actions deemed ethically acceptable by users. Opportunities for IS researchers to further explore this timely issue are also discussed.
|keyword = computer security,domain theory of moral development,data encryption,computer viruses,gender,ethics,socioeconomic status,age,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The illusory diffusion of innovation: An examination of assimilation gaps'''
{{header}}
{{article
|author= RG Fichman,CF Kemerer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Innovation researchers have known for some time that a new information technology may I be widely acquired, but then only sparsely deployed among acquiring firms. When this happens, the observed pattern of cumulative adoptions will vary depending on which event in the assimilation process (i.e., acquisition or deployment) is treated as the adoption event. Instead of mirroring one another, a widening gap-termed here an assimilation gap-will exist between the cumulative adoption curves associated with the alternatively conceived adoption events. When a pronounced assimilation gap exists, the common practice of using cumulative purchases or acquisitions as the basis for diffusion modeling can present an illusory picture of the diffusion process-leading to potentially erroneous judgments about the robustness of the diffusion process already observed, and of the technology's future prospects. Researchers may draw inappropriate theoretical inferences about the forces driving diffusion. Practitioners may commit to a technology based on a belief that pervasive adoption is inevitable, when it is not. This study introduces the assimilation gap concept, and develops a general operational measure derived from the difference between the cumulative acquisition and deployment patterns. It describes how two characteristics-increasing returns to adoption and knowledge barriers impeding adoption-separately and in combination may serve to predispose a technology to exhibit a pronounced gap. It develops techniques for measuring assimilation gaps, for establishing whether two gaps are significantly different from each other, and for establishing whether a particular gap is absolutely large enough to be of substantive interest. Finally, it demonstrates these techniques in an analysis of adoption data for three prominent innovations in software process technology-relational database management systems (RDBs), general purpose fourth generation languages (4GLs), and computer aided software engineering tools (CASE). The analysis confirmed that assimilation gaps can be sensibly measured, and that their measured size is largely consistent with a priori expectations and recent research results. A very pronounced gap was found for CASE, while more moderate-though still significant-gaps were found for RDBs and 4GLs. These results have the immediate implication that, where the possibility of a substantial assimilation gap exists, the time of deployment should be captured instead of, or in addition to, time of acquisition as the basis for diffusion modeling. More generally, the results suggest that observers be guarded about concluding, based on sales data, that an innovation is destined to become widely used. In addition, by providing the ability to analyze and compare assimilation gaps, this study provides an analytic foundation for future research on why assimilation gaps occur, and what might be done to reduce them.
|keyword = assimilation gap,software process innovation,adoption,deployment,diffusion modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report. Learning from goal-directed error recovery strategy'''
{{header}}
{{article
|author= MK Sein,R Santhanam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Research on training has traditionally viewed errors made by trainees as detrimental to learning. A great deal of effort has been devoted to finding effective ways of preventing errors from occurring during training. Recently, some researchers have adopted a different perspective: that errors may provide a learning opportunity for trainees. What has been investigated less is the specific mechanism through which errors can foster learning. The objective of our research was to investigate and possibly reconcile these differing viewpoints by examining the error recovery process. We found that, in some situations, errors enhance learning when the trainee adopts an error recovery process that emphasizes the goal structure of the task. We suggest several ways of coaching trainees in training sessions to adopt such error recovery strategies.
|keyword = end-user learning,training,errors,error management,human-computer interaction,GOMS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Rosenbluth International: Strategic transformation of a successful enterprise'''
{{header}}
{{article
|author= EK Clemons,IH Hann,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Successful companies find it exceedingly difficult to change their business strategy radically in response to impending changes in their competitive environment. Here, we analyze how Rosenbluth International, one of the largest travel agencies, was able to accomplish that. Threatened with disintemediation during the period of drastic restructuring of travel brokerage, the company strategically revised its value proposition. It has divested itself of its leisure travel segment and offered a range of its services on a fee basis to its customers, rather than rely on commissions by the suppliers of travel services, as it had done previously. This strategic change was enabled by information systems, several of them highly innovative, which Rosenbluth had used strategically in its prior business initiatives. The paper analyzes the management of strategic transition at Rosenbluth International in the light of general theory of organizational resistance to change.
|keyword = competitive information systems,organizational change,strategic transformation,travel industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Next-generation trading in futures markets: A comparison of open outcry and order matching systems'''
{{header}}
{{article
|author= BW Weber,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The introduction of new screen-based systems for trading securities and futures contracts has led to the emergence of a "market for markets," and exchanges, broker-dealer firms, and market data vendors are competing to offer trade execution services that will attract customers and trading volumes. This competition is favored in the United States by regulatory bodies such as the SEC and the CFTC, which have taken steps such as encouraging the listing of equity options on multiple exchanges and approving the applications of screen-based systems for designation as contract markets. This paper examines the design of one screen-based futures market, the Canter Financial Futures Exchange (CX), and describes its capabilities relative to the rival, floor-based market in Chicago. In comparison to traditional open-outcry mechanisms, the CX order-matching system maintains strict first ill-first out time priority among submitted orders. Using a simple simulation model, we see that order matching leads to faster completion of desired trades and about a one-third reduction in transactions costs.
|keyword = electronic futures trading,screen-based trading,trading automation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information systems for optimal transaction implementation'''
{{header}}
{{article
|author= JT Rickard,NG Torre,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = In a securities market, the initiator of a large transaction can expect the realized price of his or her trade to be inferior to the current market price immediately prior to his appearance in the market. This "transaction implementation cost" phenomenon is a major concern of institutional money managers, both in portfolio selection and in trade implementation strategy. A considerable amount of current research in finance theory deals with modeling and prediction of these costs for equities trading, and commercial products and services recently have become available for probabilistically estimating real-time transaction implementation costs versus transaction size on a stock-specific basis. Earlier papers described the concept of satisfaction- or preference-based trading, with optimization of trade matching on the basis of mutual preference. A market structure based on this design began trading listed equities on the Pacific Exchange on January 29, 1999 under the trade name OptiMark(TM). The Nasdaq market plans to begin trading using the OptiMark system later in 1999, followed by the Osaka Securities Exchange and the Toronto Stock Exchange in 2000. A prima-facie benefit of this approach is the ability to specify trading strategies that explicitly account for transaction implementation cost estimates as a function of the trade size. In this paper, we present the underlying theoretical framework that unites the concepts of preference-based trading and probabilistic transaction cost estimation. In particular, we develop an analytical generalization of the current market structure constructs of market orders and limit orders. We describe a feasible optimization problem whose solution yields optimal preference profiles, given current market conditions (as reflected by the probability distribution of transaction implementation cost) and a trader-specified coefficient of urgency. This enables the seamless integration of the functions of portfolio selection (the purview of modern portfolio theory) and transaction implementation. We illustrate the application of this theory to a prototype trading workstation.
|keyword = electronic trading,fuzzy sets,preference-based trading,securities markets,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Adoption and utilization of commercial software packages: Exploring utilization equilibria, transitions, triggers, and tracks'''
{{header}}
{{article
|author= KS Lassila,JC Brancheau,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Researchers and managers are beginning to realize that the full advantages of information technologies are not likely to be realized unless both the information technology and the organizational context are adapted during implementation. This highlights the importance of understanding and managing the relationship between information technology and organizational change. This study takes a new look at an old concept: information system utilization. A theoretical framework grounded in the punctuated equilibrium model contends that the mutual adaptation of commercial software packages and organizational processes follows a discontinuous change pattern where stable periods of utilization are occasionally disturbed by internal and external change triggers. Qualitative data gathered from interviews with software vendor personnel and a representative sample of users provide preliminary support for the existence of utilization equilibrium states. The evidence suggests that, when significant changes occur in the appropriation of technology, the users, or the organization context, the existing equilibrium state is disturbed. Following a temporary transition state characterized by redefinition of the technology and/or its context, the changes are incorporated into altered work precesses, and a, new equilibrium state develops. Various factors associated with the creation, maintenance, and disruption of equilibrium states are identified. Managers and users can enhance and prolong the useful life of software packages by paying careful attention to implementation efforts that heavily influence initial utilization equilibrium, identifying periods of equilibrium and transition,and managing the internal and external change triggers that influence transitions between equilibrium states.
|keyword = information system implementation,information system utilization,innovation reinvention,punctuated equilibrium model,structuration model,technology assimilation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the technology acceptance model using physician acceptance of telemedicine technology'''
{{header}}
{{article
|author= PJ Hu,PYK Chau,ORL Sheng,KY Tam,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The rapid growth of investment in information technology (IT) by organizations worldwide has made user acceptance an increasingly critical technology implementation and management issue. While such acceptance has received fairly extensive attention from previous research, additional efforts are needed to examine or validate existing research results, particularly those involving different technologies, user populations, and/or organizational contexts. In response, this paper reports a research work that examined the applicability of the Technology Acceptance Model (TAM) in explaining physicians' decisions to accept telemedicine technology in the health-care context. The technology, the user group, and the organizational context are all new to IT acceptance/adoption research. The study also addressed a pragmatic technology management need resulting from millions of dollars invested by healthcare organizations in developing and implementing telemedicine programs in recent years. The model's overall fit, explanatory power, and the individual causal links that it postulates were evaluated by examining the acceptance of telemedicine technology among physicians practicing at public tertiary hospitals in Hong Kong. Our results suggested that TAM was able to provide a reasonable depiction of physicians' intention to use telemedicine technology. Perceived usefulness was found to be a significant determinant of attitude and intention but perceived ease of use was not. The relatively low R-square of the model suggests both the limitations of the parsimonious model and the need for incorporating additional factors or integrating with other IT acceptance models in order to improve its specificity and explanatory utility in a health-cafe context. Based on the study findings, implications for user technology acceptance research and telemedicine management are discussed.
|keyword = information technology acceptance,information technology management in health care,Technology Acceptance Model,telemedicine,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Toward a contingency model for selecting an information system prototyping strategy'''
{{header}}
{{article
|author= BC Hardgrave,RL Wilson,K Eastman,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Many proposed contingencies regarding the conditions when the use of prototyping will lead to successful system development appear in the literature. Using an industry survey, this exploratory study empirically investigates the effect of certain contingencies on system success. Overall, results indicate that five variables, when combined with prototyping, affect system success (as indicated by user satisfaction): innovativeness of the project, impact of the system on the organization, user participation, number of users, and developer experience with prototyping. These results provide some insight into the proper uses of prototyping to improve system success. The results also indicate that several of the current contingencies, if followed, do not ensure high levels of system success.
|keyword = contingency theory,information system prototyping,systems development,system success,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alignment is not enough: Integrating business and information technology management with the balanced business scorecard'''
{{header}}
{{article
|author= JTM Van der Zee,B De Jong,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = The continuously growing importance of information technology (IT) requires organizations to integrate IT decisions with their common planning and decision-making processes at all organizational levels. Trying to align distinct and separate business with IT management cycles is just not enough. Multilevel, integrated business and IT management are aimed at fully integrating the capabilities of IT with business strategies and management's expectations, and vice versa. Although the need to integrate business and IT strategy has long been advocated, until now a practical framework did not exist. We seek to contribute to the discipline of IT value management. Our framework includes the planning of and setting goals for IT, and the evaluation of results, integrated with the business context. First approaches to IT planning and evaluation, and their limitations, are discussed. We then elaborate on the concepts of the Balanced Business Scorecard in two case studies. By investigating the benefits and limitations of the framework, and comparing them with other common frameworks, we conclude that the Balanced Business Scorecard can be a valuable contributor to implementation of an integrated business and IT planning and evaluation process.
|keyword = Balanced Business Scorecard,information technology planning and control,information technology strategy,strategic alignment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Applying code inspection to spreadsheet testing'''
{{header}}
{{article
|author= RR Panko,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = In programming, reliability requires an extensive testing phase. Spreadsheet development, which has about the error rate as program development, also needs to be followed by an extensive testing phase if spreadsheets are to be reliable. In this study, sixty undergraduate MIS students code-inspected a spreadsheet seeded with eight errors. They first inspected the spreadsheet working alone. They then met in twenty groups of three to reinspect the spreadsheet together. Effort was made to prevent hasty inspection. Individual code inspection, consistent with past studies of both spreadsheet and program code inspection, caught only 63 percent of the errors. Group inspection raised this to 83 percent. However, the group phase never found new errors; it merely pooled the errors found during the individual phase by the three members. One group even "lost" an error found during the individual phase. This raises the question of whether a group code inspection phase is really necessary. Other findings were that subjects were overconfident when inspecting alone, that certain types of errors are especially difficult to detect, and that the benefits of the group phase is greatest for these difficult-to-detect types of errors.
|keyword = code inspection,error detection,spreadsheet errors,spreadsheet testing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Gaining competitive advantage for trading in emerging capital markets with neural networks'''
{{header}}
{{article
|author= S Walczak,
|source= JOURNAL OF MANAGEMENT INFORMATION SYSTEMS
|year= 1999
|abstract = Emerging capital markets may not be as efficient as the more established equity markets. Because of the possible inefficiency in these markets, various indicators that are external to the emerging capital market may provide a significant trading advantage. A preliminary analysis suggests that the Singapore market appears to be efficient. Neural network models are used to evaluate the claim that emerging equity markets, specifically the Singapore exchange, are affected by external signals and attempt to exploit any trading advantage imparted by these signals. The neural network technique as it is applied to trading on market indices in the "emerging" Singapore market is compared with the more established Dow Jones market index. Results indicate that external market signals can significantly improve forecasting on the Singapore DBS5O index but have little or no effect on forecasts for the more established Dow Jones Industrial Average index. The research demonstrates the efficacy of using neural network methods to capitalize on discovered market inefficiencies. Utilizing external market signals, a neural network forecasting model achieved a 63 percent trading prediction accuracy.
|keyword = emerging markets,trading,neural networks,Pacific Rim,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IT-intensive value innovation in the electronic economy: Insights from Marshall Industries'''
{{header}}
{{article
|author= OA El Sawy,A Malhotra,S Gosain,KM Young,
|source= MIS QUARTERLY
|year= 1999
|abstract = The emerging electronic economy is bringing with it new forms of IT-enabled intermediation, virtual supply chains, rapidly changing electronic commerce technologies, increasing knowledge intensity, and unprecedented sensitivity for time-to-market by customers. Customers are demanding more value, customized to their exact needs, at less cost, and as quickly as possible. The enterprises that will survive in such a demanding environment will need to innovate and invent new ways of creating value, and will require different enterprise architectures and different IT infrastructures. This article focuses on providing a framework for guiding an enterprise as it transforms itself to function more effectively in the electronic economy. Using the distribution industry in general and Marshall Industries in particular as a context, the article draws insights for transforming an extended enterprise's architecture and its IT infrastructure to enable new ways of creating value in the electronic economy. The article provides a staged junction box model for guiding the transformation and also articulates the elements of the new value logic for enterprises in the electronic economy.
|keyword = electronic value chains,electronic economy,intermediation,distribution industry,supply chain management,intranet,extranet,Internet,electronic commerce,value innovation,time-based competition,fast response,CIO,IT architecture,strategic information systems,total quality management,systems approach,e-business,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Seeding the line: Understanding the transition from IT to non-IT careers'''
{{header}}
{{article
|author= BH Reich,ML Kaarst-Brown,
|source= MIS QUARTERLY
|year= 1999
|abstract = As organizations face increased competitive pressures and technological changes, their attention is focusing on how to attain strategic benefits from information technology investments, including investments in people. From a human resources perspective, one debate centers on how to attract and retain information technology (IT) professionals. Somewhat paradoxically, if is suggested that to retain IT professionals, organizations must provide both technical and business oriented career opportunities. This paper presents a case study of one organization, The Mutual Group, in which more than 70 IT professionals permanently moved into non-IT; business unit jobs during the 1980s and early 1990s. Using interviews and surveys of 51 former IT professionals, this research investigated the characteristics of the individual, the organization, the first non-IT job, and the transition period. The conclusion from the findings is that IT professionals who moved to non-IT jobs in the line made the transition without the benefit of deliberate preparation, formal transition programs, or safety nets to reduce the risk. Some conditions existed at The Mutual Group that may have assisted them, including good relations between IT and the line, friends and mentors in line units, and a willingness to take risks in pursuit of new challenges. One contribution of this paper is that it begins to fill a gap in the career mobility literature, based on individuals and their stories of change. It also attempts to understand the role of context in one organization that is a recognized leader in the use of IT for competitive advantage.
|keyword = job transfers,IT careers,career mobility,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational mechanisms for enhancing user innovation in information technology'''
{{header}}
{{article
|author= S Nambisan,R Agarwal,M Tanniru,
|source= MIS QUARTERLY
|year= 1999
|abstract = Fostering information technology innovation has assumed primacy in discussions of information systems management. Changes in the nature of available information technologies and their potential applications underscore the importance of creating new knowledge for deploying a technology within an organization rather than transferring such knowledge from external sources. Technology users remain a largely untapped source for such knowledge creation. This paper argues that deliberate organizational design actions in the form of mechanisms can enhance technology users' propensity to innovate in information technology. Specifically, a taxonomy of organizational mechanisms is developed based on the ability of various mechanisms to facilitate knowledge acquisition and knowledge conversion. The conceptual taxonomy is populated with specific design actions described in the literature utilizing a Delphi study. The effects of various classes of mechanisms on three key antecedents of user propensity to innovate in IT-technology cognizance, ability to explore a technology, and intention to explore a technology-are tested using a field study. Results provide support for the conceptual taxonomy. implications for theory and practice are offered.
|keyword = IS innovation,organizational mechanisms,technology cognizance,technology exploration,propensity to innovate,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Chartjunk or goldgraph? Effects of presentation objectives and content desirability on information presentation'''
{{header}}
{{article
|author= N Tractinsky,J Meyer,
|source= MIS QUARTERLY
|year= 1999
|abstract = Most research on information presentation is based on the rational approach to display design. This approach assumes that the quality of displays is determined by their relative efficacy to provide the relevant information for the viewer, as assessed through variables such as response latency, accuracy, or decision quality. However, presentations often are intended to convince viewers and create desired impressions. These considerations may lead to the choice of displays that differ from chose prescribed by the rational approach. Three experiments addressed the degree to which the presenter's objectives and the desirability of the presented information affect presenters' preferences for display formats. Presenters exhibited different preferences when they tried to create a favorable impression compared to when they tried to reach optimal decisions or provide information for optimal decision making by others. There was an increased use of depth in graphic displays when impressions were crucial, and this tendency was particularly strong when the presented information was undesirable for the presenter. The results demonstrate the importance of understanding the social circumstances of information presentation when evaluating the adequacy of display formats.
|keyword = information presentation,communication visuals,self-presentation,impression management,presentation preferences,business graphics,information desirability,graphic displays,display design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Horizontal mechanisms under differing is organization contexts'''
{{header}}
{{article
|author= CV Brown,
|source= MIS QUARTERLY
|year= 1999
|abstract = Horizontal mechanisms are structural overlays (such as roles and groups) and non-structural devices (such as physical colocation) that are designed to facilitate cross-unit collaboration. The objective of this study is to increase our cumulative knowledge about what top-down mechanisms are being used to promote the coordination of IS activities across corporate/division boundaries. Propositions about how mechanism usage differs under centralized versus federal IS organization contexts are developed based on a synthesis of prior organization science and empirical IS literature. Multiple methods are used to collect data from IS and non-IS senior managers from two case sites with theoretically different IS coordination needs. As predicted, multiple types of structural and non-structural mechanisms were implemented for business-IS coordination in the company with a centralized IS context, and for corporate IS-decentralized IS coordination in the company with a federal IS context. An unexpected finding was that mechanisms for both of these kinds of IS coordination were valued at each case site. The prediction that a formal group mechanism would be perceived as more effective for achieving cross-unit coordination than an integrator role mechanism was not supported. The article concludes with a discussion of implications for research and practice.
|keyword = issues in organizing IS,IS management,organization design,steering committees,IS staffing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary. Academic rewards for teaching, research, and service: Data and discourse'''
{{header}}
{{article
|author= ME Whitman,AR Hendrickson,AM Townsend,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = In most institutions faculty members are expected to teach, research and perform community service. The emphasis placed on each activity is expected to vary considerably between institutions and departments. To examine this expectation, a nationwide survey was made of both American Assembly of Collegiate Schools of Business (AACSB) institutions and nonAACSB institutions. participants rated SO publications for their value in reviews of research performance, and responded to a series of questions pertaining to the importance of publication types on the merit compensation, promotion, and tenure processes. These results were made available to the IS community, and approximately 150 comments were obtained. The survey results and the comments suggest that there might be some convergence in expectations of academic performance across institutions, as research-oriented institutions require better performance on teaching, teaching-oriented institutions require better performance in research, and all institutions impose greater service demands on IS faculty.
|keyword = IS research issues,IS journals,computer science education,promotion and tenure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic brainstorming: The illusion of productivity'''
{{header}}
{{article
|author= A Pinsonneault,H Barki,RB Gallupe,N Hoppen,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Electronic brainstorming (EBS) has been proposed as a superior approach to both nominal brainstorming (working alone) and face-to-face brainstorming (verbal). However, existing empirical evidence regarding EBS's superiority over nominal brainstorming is weak. Through a comprehensive examination of the process gains and process losses inherent to different brainstorming approaches, this paper explains past results. The paper also suggests that the process gain versus process loss advantages of EBS technologies may not be large enough to enable EBS groups to outperform nominal groups. In an effort to find alternate ways of using EBS more productively, three conditions thought to increase EBS's process gains and decrease its process losses (thus improving its productivity) are identified. A laboratory experiment designed to compare the productivity of ad hoc and established groups using four brainstorming technologies (nominal, EBS-anonymous, EBS-nonanonymous, verbal), generating ideas on socially sensitive and less sensitive topics, in the presence and absence of contextual cues, is then described. The results of the experiment showed that overall, groups using nominal brainstorming significantly outperformed groups using the other three brainstorming approaches. Further, even under conditions thought to be favorable to EBS, nominal brainstorming groups were at least as productive as EBS groups. The paper explains these results by suggesting that the process gains of EBS may not be as large as expected and that the presence of four additional process losses inherent to EBS technologies impair its productivity. It is also argued that the prevailing popularity of group brainstorming (verbal or electronic) in organizations may be explained by the perceived productivity of those approaches. These perceptions, which are at odds with reality, create the illusion of productivity. A similar misperception may also cause an illusion of EBS productivity in the research community, especially when perceptual measures of group performance are used.
|keyword = group processes,brainstorming,electronic meeting systems,group decision making,laboratory study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and firm boundaries: Evidence from panel data'''
{{header}}
{{article
|author= LM Hitt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Previous Literature has suggested that information technology (IT) can affect firm boundaries by changing the costs of coordinating economic activity within and between firms (internal and external coordination). This paper examines the empirical relationship between IT and firm structure and evaluates whether this structure is consistent with prior arguments about IT and coordination. We formulate an empirical model to relate the use of information technology capital to vertical integration and diversification. This model is tested using an 8-year panel data set of information technology capital stock, firm structure, and relevant control variables for 549 large firms. Overall, increased use of IT is found to be associated with substantial decreases in vertical integration and weak increases in diversification. In addition, firms that are less vertically integrated and more diversified have a higher demand for IT capital. While we cannot rule out all alternative explanations for these results, they are consistent with previous theoretical arguments that both internal and external coordination costs are reduced by IT.
|keyword = firm boundaries,transaction costs,markets,hierarchies,information technology,computers,diversification,vertical integration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Straight talk: Delivering bad news through electronic communication'''
{{header}}
{{article
|author= SW Sussman,L Sproull,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Delivering bad news can be an unpleasant task, therefore people often either postpone it or mitigate its effect through positive distortion. However, delivering (and receiving) timely and accurate negative information can be critical for performance improvement and organizational learning. This paper investigates the possibility that computer-mediated communication can increase honesty and accuracy in delivering negative information that has personal consequences for the recipient. In a laboratory experiment, 117 participants delivered positive or negative personally-consequential information to a "student" (confederate) using one of three types of media: computer-mediated communication, telephone, or face-to-face conversation. Participants distorted negative information less, i.e., were more accurate and honest, when they used computer-mediated communication than face-to-face or telephone communication. There was no difference in distortion of positive information across media conditions. Participants also reported higher levels of satisfaction and comfort in the computer-mediated communication situation. The perceived quality of the relationship mediated the impact of medium on satisfaction, but not on distortion.
|keyword = computer-mediated communication,information distortion,dyadic,laboratory experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report. Can EDI benefit adopters?'''
{{header}}
{{article
|author= HG Lee,T Clark,KY Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = Although the potential for EDI (Electronic Data Interchange) to improve performance of firms involved in industry value chain is widely known, little evidence regarding improved performance for the entire supply channel has been reported. Some researchers have found that EDI networks can benefit EDI champion, but it remains largely unclear whether EDI adopters-who are often coerced to implement the electronic networks by the champion-gain similar payoffs from their EDI investments. To measure impacts of EDI investments for the EDI adopters, we have investigated the performance of 31 grocery retail chains (EDI adopters) that implemented EDI networks with Campbell (EDI champion) for a supply channel reengineering innovation known as "continuous replenishment process." Analysis of daily data on inventory and stockouts levels for the 31 retail chains demonstrates that these EDI adopters have achieved a significant increase in their inventory turns while simultaneously reducing stockouts as a result of this EDI-enabled supply channel reengineering. This paper thus provides empirical evidence that EDI adopters can achieve dramatic performance improvements if EDI networks are used for interfirm process reengineering.
|keyword = Electronic Data Interchange,interorganizational systems,channel process reengineering,continuous replenishment process,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Social cognitive theory and individual reactions to computing technology: A longitudinal study'''
{{header}}
{{article
|author= D Compeau,CA Higgins,S Huff,
|source= MIS QUARTERLY
|year= 1999
|abstract = A model, based on Bandura's Social Cognitive Theory, was developed to test the influence of computer self-efficacy, outcome expectations, affect, and anxiety on computer usage. The model was tested using longitudinal data gathered from 394 end users over a one-year interval. Significant relationships were found between computer self-efficacy, and outcome expectations, and between self-efficacy and affect and anxiety and use. Performance outcomes were found to influence affect and use, while affect was significantly related to use. Overall, the findings provide strong confirmation that both self-efficacy and outcome expectations impact on an individual's affective and behavioral reactions to information technology.
|keyword = IS usage,self-efficacy,causal models,longitudinal,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The implications of information technology infrastructure for business process redesign'''
{{header}}
{{article
|author= M Broadbent,P Weill,D St Clair,
|source= MIS QUARTERLY
|year= 1999
|abstract = Business process redesign (BPR) is a pervasive but challenging tool for transforming organizations. Information technology plays an important role by either enabling or constraining successful BPR This paper explores the links between firm-wide IT infra-structure and business process change. IT infrastructure is the base foundation of the IT portfolio, which is shared throughout the firm in the form of reliable services, and is usually coordinated by the IS group. IT infrastructure capability includes both the technical and managerial expertise required to provide reliable physical services and extensive electronic connectivity within and outside the firm. Exploratory case analysis of four firms (two in retail and two in petroleum) was used to understand the ways IT infrastructure contributes to success in implementing BPR. The finding was that all firms needed a basic level of IT infrastructure capability to implement BPR. The firms that had developed a higher level of IT infrastructure capabilities, before or concurrent with undertaking business process redesign, were able to implement extensive changes to their business processes over relatively short time frames. The higher level of infrastructure capability was provided in the form of (1) a set of infrastructure services that spanned organizational boundaries such as those between functions, business units, or firms, and (2) the ability of the infrastructure to reach particular constituencies inside and outside the firm to transfer information and process complex transactions. The more extensive business process changes were more innovative and radical, crossing business and functional unit boundaries, and resulted in more significant business impact. The practical implication of the study is that before embarking on any form of BPR, managers should complete a business audit of their IT infrastructure capabilities, as these capabilities have an important impact on the speed and nature of business process change.
|keyword = IT infrastructure,IT services,business process redesign and reengineering,business strategy,IT alignment,exploratory case analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology adoption across time: A cross-sectional comparison of pre-adoption and post-adoption beliefs'''
{{header}}
{{article
|author= E Karahanna,DW Straub,NL Chervany,
|source= MIS QUARTERLY
|year= 1999
|abstract = The process of information technology adoption and use is critical to deriving the benefits of information technology. Yet from a conceptual standpoint, few empirical studies have made a distinction between individuals' pre-adoption and post-adoption (continued use) beliefs and attitudes. This distinction is crucial in understanding and managing this process over time. The current study combines innovation diffusion and attitude theories in a theoretical framework to examine differences in pre-adoption and post-adoption beliefs and attitudes, The examination of Windows technology in a single organization indicates that users and potential adopters of information technology differ on their determinants of behavioral intention, attitude, and subjective norm. Potential adopter intention to adopt is solely determined by normative pressures, whereas user intention is solely determined by attitude. In addition, potential adopters base their attitude on a richer set of innovation characteristics than users. Whereas pre-adoption attitude is based on perceptions of usefulness, ease-of-use, result demonstrability, visibility, and trialability, post-adoption attitude is only based on instrumentality beliefs of usefulness and perceptions of image enhancements.
|keyword = MIS implementation,innovation diffusion,innovativeness,adoption,Theory of Reasoned Action,IS use,user attitudes,user behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The untapped potential of IT chargeback'''
{{header}}
{{article
|author= JW Ross,MR Vitale,CM Beath,
|source= MIS QUARTERLY
|year= 1999
|abstract = The received wisdom on IT chargeback is that a chargeback system with certain key characteristics, such as usage-based charges, stable rates, understandable bills, and so forth, will help firms make effective decisions on IT investment and use. Eccles' model of transfer pricing provides a theoretical Framework fbr this claim, and it also explains why chargeback systems can raise issues of fairness or create conflict between IT and its clients, as the IT literature has pointed out. Applying Eccles' model, this paper reports on a study of 10 organizations' IT chargeback systems and their impacts on business managers' economic decisions and on evaluations of IT and business unit performance. Respondents in just four of the 10 firms reported that chargeback had significantly influenced IT investment decisions. In addition, the business unit respondents at those same four firms offered more positive assessments of IT than their counterparts at other sites. These differences in chargeback-related outcomes could not be accounted for by looking at differences in the chargeback characteristics that are most commonly described in the IT literature. What was different in these four firms was that chargeback was being used to foster communication between IT and the business units. This communication was generating a rich shared understanding for both parties of the costs and benefits of alternative IT investments and service offerings. The literature on partnership argues that complex IT investment decisions demand a strong IT-business partnership. The analysis suggests that IT units in just four of the 10 firms were tapping into the potential of chargeback to facilitate the development of a partnership with their business unit counterparts.
|keyword = IT chargeback,transfer pricing,partnership,IS management,IT value,IS performance assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Creation of favorable user perceptions: Exploring the role of intrinsic motivation'''
{{header}}
{{article
|author= V Venkatesh,
|source= MIS QUARTERLY
|year= 1999
|abstract = A key issue facing information systems researchers and practitioners has been the difficulty in creating favorable user reactions to new technologies. Insufficient or ineffective training has been identified as one of the key factors underlying this disappointing reality. Among the various enhancements to training being examined in research, the role of intrinsic motivation as a lever to create favorable user perceptions has not been sufficiently exploited. In this research, two studies were conducted to compare a traditional training method with a training method that included a component aimed at enhancing intrinsic motivation. The results strongly favored the use of an intrinsic motivator during training. Key implications for theory and practice are discussed.
|keyword = user acceptance,adoption,training,motivation,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Arrangements for information technology governance: A theory of multiple contingencies'''
{{header}}
{{article
|author= V Sambamurthy,RW Zmud,
|source= MIS QUARTERLY
|year= 1999
|abstract = IT governance arrangements refers to the patterns of authority for key IT activities in business firms, including IT infrastructure, IT use, and project management During the last 20 years, three primary modes of IT governance have become prevalent: centralized, decentralized, and the federal mode. These modes vary in the extent to which corporate IS, divisional IS, and line management are vested with authority for the key IT activities. While a significant volume of research has examined the influence of contingency factors on the choice of a specific mode of IT governance, most of this research has examined the singular effects of the contingency factors. The,assumption underlying these studies is as though the organizational contingencies act in isolation in influencing the mode of IT governance. However, in reality, business firms are subject to the pulls and pressures of multiple, rather than singular, contingency forces. Therefore, to acknowledge this reality, this study applies the theory of multiple contingencies to examine how contingency forces influence the mode of IT governance. The theory argues that contingency forces interact with each other by either amplifying, dampening, or overriding their mutual influences on the IT governance mode. Three scenarios of multiple, interacting contingencies are identified: reinforcing, conflicting, and dominating. Each of these scenarios of multiple contingencies is hypothesized to influence a particular mode of IT governance. Utilizing rich data from case studies of eight firms, empirical evidence is presented to support these hypotheses. Implications of the multiple contingencies theory for research and for practice are presented.
|keyword = IS organization design,multiple contingencies,federal governance,qualitative research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic trading and work transformation in the London Insurance Market'''
{{header}}
{{article
|author= M Barrett,G Walsham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = The integration of information and communications technologies (IT) is playing a key role in transforming the nature of work. The link between IT and transformation is poorly understood, and further theoretical developments are needed to advance our current knowledge of this relationship. In this paper, we develop a conceptual scheme by drawing on and extending Giddens' social theory of transformation that relates changes in modern institutions to shifts in self-identity. We illustrate the value of these ideas in making sense of the introduction of an electronic trading system, LIMNET EPS, across the London Insurance Market. Furthermore, our case analyses suggest some practical implications on electronic trading and work transformation.
|keyword = electronic trading,Giddens' social theory,modernity and self-identity,work transformation,London insurance market,globalization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Depicting the use and purpose of documents to improve information retrieval'''
{{header}}
{{article
|author= MD Gordon,SA Moore,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = In this paper we discuss a new kind of information system that helps people be ready for information work and locate documents. This system differs from a traditional information retrieval system by relying extensively on descriptions of both how a document is used and the purposes it is used for. These descriptions are gathered as the document is electronically used and manipulated (e.g., by a word processor or e-mail system). A formal language represents this information.
|keyword = information retrieval,work flow,information acts,speech act theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A case for using real options pricing analysis to evaluate information technology project investments'''
{{header}}
{{article
|author= M Benaroch,RJ Kauffman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = The application of fundamental option pricing models (OPMs), such as the binomial and the Black-Scholes models, to problems in information technology (IT) investment decision he application of fundamental option pricing models (OPMs), such as the binomial and making have been the subject nf some debate in the last few years Prior research for example has made the case that pricing "real options" in real world operational and strategic settings offers the potential for useful insights in the evaluation of irreversible investments under uncertainty. However, most authors in the IS literature have made their cases using illustrative, rather than actual real world examples, and have always concluded with caveats and questions for future research about the applicability of such methods in practice. This paper makes three important contributions in this context: (1) it provides a formal theoretical grounding for the validity of the Black-Scholes option pricing model in the context of the spectrum of capital budgeting methods that might be employed to assess IT investments; (2) it shows why the assumptions of both the Black-Scholes and the binomial option pricing models place constraints on the range of IT investment situations that one can evaluate that are similar to those implied by traditional capital budgeting methods such as discounted cash flow analysis; and (3) it presents the first application of the Black-Scholes model that uses a real world business situation involving IT as its test bed. Our application focuses on an analysis of the timing of the deployment of point-of-sale (POS) debit services by the Yankee 24 shared electronic banking network of New England. This application enables us to make the case for the generalizability of the approach we discuss to four IT investment settings.
|keyword = IT investment evaluation,real options,option pricing methods,Black-Scholes model,electronic banking networks,POS debit systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Nonlinear and noncompensatory models in user information satisfaction measurement'''
{{header}}
{{article
|author= V Sethi,RC King,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1999
|abstract = This study applies nonlinear and noncompensatory models to examine how users evaluate their satisfaction with their information systems (IS) environment. Several instruments have been developed in the literature to measure user information satisfaction (UIS). These instruments measure user satisfaction by asking respondents to rate their satisfaction/dissatisfaction with a variety of IS attributes; e.g., EDP services, EDP staff, information product, and involvement in IS development. These responses are then combined linearly to develop a surrogate measure for UIS satisfaction. This linear model is derived from Anderson's information theory (Anderson 1981) and based on the assumption that each attribute judgment has a conditionally monotone relationship with the UIS evaluation. However, the literature on attitude formation and decision making suggests that other nonlinear and noncompensatory models are available to decision makers for combining information and are used frequently in attitude formation. In this study, we use two sets of data to examine the linear model and five nonlinear models of decision making to evaluate whether nonlinear models are more effective in predicting a user's overall satisfaction with information systems. First, responses from faculty members at an academic institution were used to test each model. All the nonlinear models were more efficient predictors than the linear models. In addition, two nonlinear models-the multiplicative and the scatter models-best represented the data with square. multiple correlations of 0.69 and 0.68, as compared to the linear model which had an R-2 of 0.61. Second, data from a previous study (Galletta and Lederer 1989) were analyzed to examine whether nonlinear models were more efficient. Data for this study were collected using the short version of the Bailey and Pearson (1983) UIS instrument. Results of the analysis from the full and cross-validation samples show that nonlinear, noncompensatory models per formed at par or better than the linear model.
|keyword = user satisfaction,linear models,nonlinear models,measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Empirical research in information systems: The practice of relevance'''
{{header}}
{{article
|author= I Benbasat,RW Zmud,
|source= MIS QUARTERLY
|year= 1999
|abstract = This commentary discusses why most IS academic research today lacks relevance to practice and suggests tactics, procedures, and guidelines that the IS academic community might follow in their research efforts and articles to introduce relevance to practitioners. The commentary begins by defining what is meant by relevancy in the context of academic research. It then explains why there is a lack of attention to relevance within the IS scholarly literature. Next, actions that can be taken to make relevance a more central aspect of IS research and to communicate implications of IS research more effectively to IS professionals are suggested.
|keyword = relevance,rigor,academic research,applied research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GIS for district-level administration in India: Problems and opportunities'''
{{header}}
{{article
|author= G Walsham,S Sahay,
|source= MIS QUARTERLY
|year= 1999
|abstract = This paper describes a research study carried out over the period 1993 to 1995, of the efforts made in India to develop and use geographical information systems (GIS)(2) to aid district-level administration. We give a detailed description of our research approach, drawing from contextualism as a broad research methodology and using actor-network theory for analytical purposes. The main section of the paper provides an in-depth analysis of a major GIS initiative from a particular Indian government ministry. We conclude that the creation and maintenance of a relatively stable set of key actors with aligned interests related to the CIS technology had not been achieved in any of the districts studied by the end of the research period. Our analysis leads to implications for future action that go beyond traditional prescriptions, such as improved participation or better training, toward the need for higher-level interventions in such areas as educational processes and administrative structures. We then turn to criteria for judging the merits of an intensive research study and illustrate to what extent this study satisfies the criteria. Finally, we draw conclusions on the contribution of this paper to the promotion of intensive research and to the opening up of new fields of IS research.
|keyword = GIS,implementation,India,developing countries,actor-network theory,contextualism,intensive research,IS research agenda,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A set of principles for conducting and evaluating interpretive field studies in information systems'''
{{header}}
{{article
|author= HK Klein,MD Myers,
|source= MIS QUARTERLY
|year= 1999
|abstract = This article discusses the conduct and evaluation of interpretive research in information systems. While the conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted, this is not the case for interpretive field studies. A set of principles for the conduct and evaluation of interpretive field research in information systems is proposed, along with their philosophical rationale. The usefulness of the principles is illustrated by evaluating three published interpretive field studies drawn from the IS research literature. The intention of the paper is to further reflection and debate on the important subject of grounding interpretive research methodology.
|keyword = IS research methodologies,interpretivist perspective,critical perspective,case study,field study,ethnography,hermeneutics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Structuring time and task in electronic brainstorming'''
{{header}}
{{article
|author= AR Dennis,JE Aronson,WG Heninger,ED Walker,
|source= MIS QUARTERLY
|year= 1999
|abstract = There are many ways in which a GSS can be used to support group brainstorming. This paper reports the results of an experiment that manipulated task structure and time structure. Groups electronically brainstormed on intact tasks (where all parts of the task were presented simultaneously) or on partitioned tasks (where three subcategories of the task were presented to the groups). The time periods in which groups worked were either one 30-minute lime period or three 10-minute periods separated by two-minute breaks. Groups in the partitioned task treatment generated 40% more ideas, but there were no time effects. These differences are attributed to the ability of the partitioned task to refocus members' attention more evenly across the entire solution space.
|keyword = groupware,group support systems,electronic brainstorming,entrainment,problem structure,time,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ethics and information systems: The corporate domain'''
{{header}}
{{article
|author= HJ Smith,J Hasnas,
|source= MIS QUARTERLY
|year= 1999
|abstract = IS-related ethical quandaries are receiving an increasing amount of attention. However, linkages to the normative theories of business ethics, which can be used in resolving these quandaries in the corporate domain, have been lacking. This paper enumerates and explains the three major normative theories. The stockholder theory holds that managers should resolve ethical quandaries by taking actions which increase the long-term profits to the stockholders without violating the law or engaging in fraud or deception. The stakeholder theory claims that managers should resolve ethical quandaries by balancing stakeholder interests without violating the rights of any stakeholder. The social contract theory states that managers should increase social welfare above what it would De in the absence of the existence of corporations without violating the basic canons of justice. The application of these theories to IS-related ethical quandaries is discussed and a specific quandary dealing with a real-world example-Blockbuster Video's reported plans to market customer lists-is explored in depth. The managerial challenges associated with the theories are then explored.
|keyword = ethics,corporate social responsibility,ethical quandaries,theoretical frameworks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Globalization and increasing returns: Implications for the US computer industry'''
{{header}}
{{article
|author= KL Kraemer,J Dedrick,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = Over the last twenty years, the computer industry has become global with respect to computer production as well as computer use, a trend which has raised concerns among U.S. policymakers of hollowing out the industry and exporting employment. This paper uses the framework of increasing returns to analyze the issue. It classifies market segments within the computer industry, shows how the advent of the personal computer created these segments, examines how this change in the structure of the industry led to the evolution of an Asia-Pacific production network, identifies company and country leadership in this network, and evaluates the implications for the United States. It shows that some manufacturing employment, mainly in the decreasing returns segments of the industry, has shifted to the Asia-Pacific region. However, it also shows that employment in some manufacturing segments and in software and services, which are increasing returns or hybrid markets, has increased dramatically in the United States. It concludes that the global division of labor between the United States and both companies and countries in the Asia-Pacific region has been largely positive in that it has supported the continuing U.S, leadership position in the global computer industry.
|keyword = increasing and decreasing returns,globalization,computer industry,industrial policy,industry structure,competition,Asia-Pacific region,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Leveraging the global information revolution for economic development: Singapore's evolving information industry strategy'''
{{header}}
{{article
|author= PK Wong,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = Despite her small size and late industrialization, Singapore has managed to capture a significant share of the global information industry over the last three decades. This paper analyzed the structure and growth dynamics of Singapore's information industries using an analytical framework that integrates the four key components of an information economy: ICT goods industry, content industry, network infrastructure, and informatization. The paper identifies four generic stages in the development path of Singapore's information economy and highlights policy implications for other small, open economies.
|keyword = Singapore,information industries,informatization,network infrastructure,IT policy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An information company in Mexico: Extending the resource-based view of the firm to a developing country context'''
{{header}}
{{article
|author= SL Jarvenpaa,DE Leidner,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = The information industry assumes that information is seen as a valued resource that enables organizations and individuals to improve their effectiveness, efficiency, and overall competitiveness. For the information industry to bridge rather than divide further the global economy to information rich and information poor, we need to understand how firms, particularly local firms, can pioneer or participate in the information industry in emerging economies that do not inherently embrace information as a valued business resource. This research examines how one local firm shaped the external environment to pioneer a local information industry in Mexico and how it competes in a newly opened local market against foreign competitors. In doing so, the paper extends the resource-based view of the firm to a developing country context. The dynamic capabilities of strategic foresight and flexibility, coupled with a core competency of trustworthiness, are found to be critical in effecting internal and external change in an unstable environment.
|keyword = information industry,international IS,case study,computing in developing countries,Mexico,resource-based view,network analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Understanding post-adoption behavior in the context of online services'''
{{header}}
{{article
|author= M Parthasarathy,A Bhattacherjee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = This paper examines post-adoption behavior (continued adoption versus discontinuance) within the context of online service use. Innovation diffusion theory is used as a theoretical framework to extend information technology adoption research to the case of post-adoption behavior. This theory is used to formulate 11 research hypotheses distinguishing discontinuers from continuing adopters and exploring reasons behind their discontinuance (replacement versus disenchantment). These hypotheses were then empirically tested using data collected from a field survey of online service users. Our results indicate that potential discontinuers can be discriminated from continued adopters based on their sources of influence (external and interpersonal), perceived service attributes (usefulness and compatibility), service utilization, and network externality (complementary product usage), during their time of initial adoption. We also found that later adopters are more likely to discontinue due to disenchantment than replacement, and are more influenced by interpersonal sources and utilize the service less during their adoption period than replacement discontinuers. Implications for research and practice are drawn.
|keyword = discontinuance,post-adoption,information technology adoption,innovation diffusion theory,multiple discriminant analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''International software piracy: Analysis of key issues and impacts'''
{{header}}
{{article
|author= RD Gopal,GL Sanders,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = The pervasiveness of software piracy throughout the world is having a profound effect on the software publishing industry and the development of digital intellectual properties and technologies-especially in developing countries, where the piracy rates are extremely high. An economic model is first presented that incorporates the incentive structures for governments, software publishers, and individual consumers. The analytical model provides the economic rationale for the reluctance of a number of governments to aggressively enact and enforce intellectual property rights. An important proposition derived from the analysis states that the government's incentive to enact and enforce copyright laws are closely related to the size of the domestic software industry. The ensuing empirical study provides support for the proposition and further suggests that this relationship holds regardless of the income levels of the countries. Our analysis reveals that alliances between foreign and domestic software publishers through product relationships can be mutually beneficial and will provide an environment of increased copyright enforcement. These results provide a viable strategy to combat global software piracy. With strong policies on copyright enforcement, and a vigorous promotion of alliances between foreign and domestic publishers, a government can increase the net welfare of the country and help establish a strong domestic software industry. Through product relationships with domestic publishers, a foreign publisher can improve profits and operate in an environment of increased intellectual property protection. We then present a general model of ethical behavior related to the impact of behavioral and cultural factors on software piracy. The purpose of this model is to examine whether these determinants of piracy behavior are supranational and transcend cultural and ethical barriers. An empirical study involving U.S. and Indian graduate students suggests that the general model of ethics as related to software piracy is valid in the United States. However, the model results from the Indian sample suggest that additional cross-cultural research with revised models and improved scales is necessary.
|keyword = economics,ethics,software piracy,intellectual property,culture,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer industry executives: An analysis of the new barons' compensation'''
{{header}}
{{article
|author= E Talmor,JS Wallace,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = In this paper we study the compensation determinants for CEOs in the computer industry and compare these findings with a large sample of firms from other manufacturing and service industries. We examine whether superior performance is rewarded by higher levels of compensation and find cash-based compensation, such as salary and bonus, is influenced by performance. Depending on the growth prospects of the company, pay is tied either to accounting measures of performance or to stock return. In contrast, stock-based compensation, such as options and restricted stock awards, is not reflective of performance but depends upon the firm's growth. Two other interesting findings are that the prevalent use of stock-based compensation in the computer industry does not appear to be the result of computer firms being "cash starved." In addition, stock-based compensation does not appear to lead to larger executive stock ownership, as is widely believed.
|keyword = executive compensation,pay-for-performance,cash-based compensation,stock-based compensation,stock options,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Clockspeed and informational response: Evidence from the information technology industry'''
{{header}}
{{article
|author= H Mendelson,RR Pillai,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = This paper presents evidence on ways in which firms in the IT industry respond to increased business dynamics. We show that the use Of internal and external communication technologies and the adoption of informational "focus" strategies increase with the "clockspeed," or dynamics, of the business environment. Our results support the information processing view of the firm.
|keyword = information technology industry,clockspeed,dynamics,information processing,communications,focus,information overload,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer-aided systems and communities: Mechanisms for organizational learning in distributed environments'''
{{header}}
{{article
|author= PS Goodman,ED Darr,
|source= MIS QUARTERLY
|year= 1998
|abstract = This paper examines the role of computer-aided systems (CAS) for enhancing organizational learning in distributed environments. The basic research questions are: how do features of CAS enhance organizational learning and how does organizational context influence the role of CAS in organizational learning? The theoretical framework focuses on the decision to contribute and adopt knowledge in distributed environments. Specifically, the paper investigates the intersections between the features of CAS and inhibitors to contributing or adopting knowledge, in the light of different organizational context variables. Two cases of information environments for knowledge sharing are examined: a formal electronic library system and an informal community that uses a variety of communication technologies. The cases are used to illustrate how the intersection between CAS features and the decisions to adopt and contribute enhance or inhibit knowledge sharing.
|keyword = CBCS,knowledge sharing,organizational learning,field study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Coping with systems risk: Security planning models for management decision making'''
{{header}}
{{article
|author= DW Straub,RJ Welke,
|source= MIS QUARTERLY
|year= 1998
|abstract = The likelihood that the firm's information systems are insufficiently protected against certain kinds of damage or loss is known as "systems risk." Risk can be managed or reduced when managers are aware of the full range of controls available and implement the most effective controls. Unfortunately, they often lack this knowledge, and their subsequent actions to cope with systems risk are less effective than they might otherwise be. This is one viable explanation for why losses from computer abuse and computer disasters today are uncomfortably large and still so potentially devastating after many years of attempting to deal with the problem. Results of comparative qualitative studies in two information services Fortune 500 firms identify an approach that can effectively deal with the problem. This theory-based security program includes (I) use of a security risk planning model, (2) education/training in security awareness, and (3) Countermeasure Matrix analysis.
|keyword = information security planning,systems security risk,security awareness training,action research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Uses and consequences of electronic markets: An empirical investigation in the aircraft parts industry'''
{{header}}
{{article
|author= V Choudhury,KS Hartzel,BR Konsynski,
|source= MIS QUARTERLY
|year= 1998
|abstract = An electronic market is an interorganizational information system through which multiple buyers and sellers interact to accomplish one or more of the following market-making activities: (I) identifying potential trading partners, (2) selecting a specific partner, and (3) executing the transaction. It has been suggested that electronic markets, by lowering search costs, may lead to significantly increased price competition among sellers and hence, lower prices for buyers. Or, by allowing efficient, direct interaction between buyers and sellers, they may eliminate the role of intermediaries. Little evidence exists to support the claims. This paper empirically examines the validity of these arguments in the context of one electronic market: inventory Locator Service (ILS) in the aircraft parts industry. Specifically, the paper addresses two questions: When do buyers use an electronic market? How do electronic markets affect each of the following: prices, inventory levels, and the role of brokers? The data show that current models do not adequately capture the complexity of electronic markets. For instance, while ILS sometimes helps buyers find a better price, in other cases it can help suppliers extract an extra premium by providing more accurate information on parts availability. ILS has also had little impact on the extent to which brokers are used, although the specific nature of the value added by brokers appears to be changing. Finally, inventory levels in the industry have been unaffected by the use of ILS. The scope of ILS is limited to the identification process only, so caution must be exercised in generalizing the findings to systems that also support selection and execution. However, the data do suggest additional variables that must be considered in understanding the uses and impacts of electronic markets, including the scope of the electronic market.
|keyword = interorganizational information systems,electronic markets,aircraft parts industry,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Redesigning reengineering through measurement-driven inference'''
{{header}}
{{article
|author= ME Nissen,
|source= MIS QUARTERLY
|year= 1998
|abstract = This first decade of business process reengineering (BPR) is blemished by sporadic success, pathological performance, and inefficiency. Reengineering inefficiency is driven in part by cost and cycle time for process redesign, a process itself that requires deep reengineering knowledge and specialized expertise. However, such knowledge and expertise are not addressed by extant, first-generation redesign tools, so these intellectual activities must be performed manually at present, or provided through expensive BPR consulting services. Knowledge-based systems (KBS) address the requirements for knowledge and expertise directly, and they can augment first-generation fools to reduce redesign cost and cycle time, and hence increase reengineering efficiency. This study employs the methods and tools of reengineering recursively, to redesign the process of process redesign itself. Using measurement-driven inference, a second-generation KBS redesign tool called KOPeR is developed to automate three key intellectual activities required for process redesign-process measurement, pathology diagnosis, and transformation matching. This KBS tool is used in the laboratory to redesign a commercial process from the reengineering literature and then employed in the field to redesign operational procurement processes in the context of an "industrial strength" reengineering project. The study finds that KOPeR-supported redesign enables new reengineering efficiencies in terms of direct automation effects and indirect knowledge effects. Results of this investigation highlight new opportunities available to the IS manager-such as improving the return on investment from BPR, enhancing the capability for knowledge management and organizational memory, and achieving competitive advantage through knowledge integration-opportunities that do not necessarily require KBS automation to seize. This investigation also lays a research cornerstone and foundation for development of process redesign theory and investigation of reengineering effectiveness.
|keyword = business process reengineering,expert systems,knowledge-based systems,metrics,process measurement,process redesign,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Production and transaction economies and IS outsourcing: A study of the US banking industry'''
{{header}}
{{article
|author= S Ang,DW Straub,
|source= MIS QUARTERLY
|year= 1998
|abstract = This paper studies economic determinants of IS outsourcing. It argues that a focus on comparative economic theories and models can improve our ability to explain outsourcing within the larger context of organizational strategy and environment. Specifically, the research constructs of production cost, transaction cost, and financial slack are examined simultaneously to understand what influences the outsourcing decision. To empirically test these relationships, information was gathered from senior IT managers in 243 U.S. banks. Financial indices from the archives of the Federal Reserve Bank were a second important source of data. Results of the study show that IS outsourcing in banks was strongly influenced by production cost advantages offered by vendors. Transaction costs played a role in the outsourcing decision, but they were much smaller than production costs. Finally, financial slack was not found to be a significant explanator, although firm size was a significant control factor. The paper has important implications for research and practice. For researchers, the findings provide evidence that financial criteria can be key factors in outsourcing decisions and compare the relative effects of production and transaction costs. For practitioners, the findings suggest that managerial sourcing strategies need to weigh both costs when hiring systems integrators.
|keyword = IS outsourcing,systems integration,transaction cost theory,production cost economics,financial slack,outsourcing measures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Survey instruments in information systems'''
{{header}}
{{article
|author= PR Newsted,SL Huff,MC Munro,
|source= MIS QUARTERLY
|year= 1998
|abstract = Due to the popularity of survey research in information systems we have launched a compilation of survey instruments and related information. This work started in 1988, as the disk-based Calgary Surveys Query System, and has now been extended to the world wide web via a contribution of "living scholarship" to MISQ Discovery. This work includes actual IS survey instruments-either in full text or via links to the appropriate citations-as well as introductory information to help get researchers started with the survey methodology.
|keyword = survey research,research methodology,questionnaire development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Firm characteristics and investments in information technology: Scale and scope effects'''
{{header}}
{{article
|author= S Dewan,SC Michael,CK Min,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = This paper conducts an empirical analysis of the link between the scale and scope of the firm and information technology (IT) investments, emphasizing the role of IT in coordination and control. We extend the economic production function framework to include variables related to the boundaries of the firm, including related and unrelated diversification, vertical integration and growth options, and we estimate the resulting model on a data set based on annual surveys of IT spending by large U.S. firms, conducted by Computerworld during the period 1988-1992. Our results suggest that the level of IT investment is positively related to the degree of firm diversification, perhaps reflecting the greater need for coordination of assets within diversified firms. We further find that related diversification demands greater IT investment than unrelated diversification. Firms that are less vertically integrated have a higher level of IT investment. Finally, firms with fewer growth options in their investment opportunity set tend to have a higher IT investment, consistent with an agency perspective which predicts excessive IT investment by managers with "free" cash flow. Put together, these empirical relations between IT investments and firm characteristics help us better understand the role of IT in coordination and control and the choices firms make in information systems and strategy.
|keyword = information technology investment,information systems strategy,scale and scope,coordination,diversification,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Testing media richness theory in the new media: The effects of cues, feedback, and task equivocality'''
{{header}}
{{article
|author= AR Dennis,ST Kinney,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = Media richness theory argues that performance improves when team members use "richer" media for equivocal tasks. This experiment studied the effects of media richness on decision making in two-person teams using "new media" (i.e., computer-mediated and video communication). Media richness was varied based on multiplicity of cues and immediacy of feedback. Subjects perceived differences in richness due to both rues and feedback, but matching richness to task equivocality did not improve decision quality, decision time, consensus change, or communication satisfaction. Use of media providing fewer cues (i.e., computer mediated communication) led to slower decisions and more so for the less equivocal task. In short the results found no support for the central proposition of media richness theory; matching media richness to task equivocality did not improve performance.
|keyword = media richness theory,information cues,feedback,equivocality,videoconferencing,group support system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Automating the discovery of AS-IS business process models: Probabilistic and algorithmic approaches'''
{{header}}
{{article
|author= A Datta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = In the current corporate environment, business organizations have to reengineer their processes to ensure that process performance efficiencies are increased. This goal has lead to a recent surge of work on Business Process Reengineering (BPR) and Workflow Management. While a number of excellent papers have appeared on these topics, all of this work assumes that existing (AS-IS) processes are known. However, as is also widely acknowledged, coming up with AS-IS process models is a nontrivial task, that is currently practiced in a very ad-hoc fashion. With this motivation, in this paper, we postulate a number of algorithms to discover, i.e., come up with models of, AS-IS business processes. Such methods have been implemented as tools which can automatically extract AS-IS process models. To the best of our knowledge, no such work exists in the BPR and workflow domain. We back up our theoretical work with a case study that illustrates the applicability of these methods to large real-world problems. We draw on previous work on process modeling and grammar discovery. This work is a requisite first step in any reengineering endeavor. Our methods, if adopted, have the potential to severely reduce organizational costs of process redesign.
|keyword = workflow management,business process reengineering,AS-IS business process models,process discovery,algorithms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Desktop videoconferencing: Experiences of complete users, wary users, and non-users'''
{{header}}
{{article
|author= J Webster,
|source= MIS QUARTERLY
|year= 1998
|abstract = This longitudinal case study examines the use of desktop videoconferencing in one organization. Three theoretical perspectives, communication media choice, systems analysis and design, and privacy, help to inform the findings. The paper concludes by drawing implications for future research and practice.
|keyword = desktop videoconferencing,media choice,implementation,privacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and the nature of managerial work: From the productivity paradox to the icarus paradox?'''
{{header}}
{{article
|author= A Pinsonneault,S Rivard,
|source= MIS QUARTERLY
|year= 1998
|abstract = Modern organizations are investing heavily in information technology (IT) with the objective of increasing overall profitability and the productivity of their knowledge workers. Yet it is often claimed that the actual benefits of IT are disappointing at best and that IT spending has failed to yield significant productivity gains-hence the productivity paradox. Evidence is fragmented and somewhat mitigated. This paper argues that the current state of empirical research results from a failure to understand the interplay between IT and managerial work. It addresses this issue by analyzing patterns of association between IT usage and the nature of managerial work in different organizational contexts. Fifty-nine semi-structured interviews were conducted with middle line managers in three large companies: a Bank, a Telecommunications company, and a Utility. In addition, daily activities and IT usage were logged. The data indicate that the relationship between the level of IT usage and the nature of managerial work was stronger in the two organizations that were reorienting their strategies (Bank, Telecommunications) than in the one pursuing ifs existing strategy (Utility). It was also found that the pattern of the relationship between IT usage and the nature of managerial work depended on the kind of strategic reorientation implemented by the firm. For instance, in the Bank, the level of IT usage was associated with the amount of time spent by managers on information-related activities (e.g., reading reports, gathering information) and on disturbance handling activities (e.g., resolving conflicts, managing crises). In the Telecommunications company, IT usage was associated with more time spent on information-related activities and less on negotiation-related activities (e.g., discussions with colleagues on resource sharing, discussions with subordinates on performance standards). This finding suggests that heavy IT users paid greater attention to and spent more time on the relies they performed best with the technology (information-related activities) and may in fact have been embarking on an over-specialization trajectory.
|keyword = IS evaluation,IS impacts,management roles,organizational strategies,IS usage,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A theory of task/technology fit and group support systems effectiveness'''
{{header}}
{{article
|author= I Zigurs,BK Buckland,
|source= MIS QUARTERLY
|year= 1998
|abstract = The characteristics of a group's task have been shown to account for more than half the variation in group interaction. In the context of group support systems (GSS), the importance of task has been underscored by the recommendation that achieving a fit between task and technology should be a principle for effective GSS use. Although the body of group support systems research has grown in recent years, and experience with different tasks and technologies now exists, no generally accepted theory of task/technology fit has emerged. This paper develops a theory of task/technology fit in GSS environments based on attributes of task complexity and their relationship to relevant dimensions of GSS technology. Propositions to guide further research are developed from the theory.
|keyword = group support systems,electronic meeting systems,group tasks,task complexity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An investigation of media selection among directors and managers: From "self" to "other" orientation'''
{{header}}
{{article
|author= PJ Carlson,GB Davis,
|source= MIS QUARTERLY
|year= 1998
|abstract = The study investigates the media selection behavior of directors (executives) and managers through the use of multiple methods. The findings indicate the directors were more "self" oriented in their media choices, more often choosing media based on access/ease of use criteria, while the managers were more "other" oriented, more often making choices based on media richness/social presence criteria. These differences have implications in the interpretation of communication from directors and managers to the rest of the organization and suggest a model for understanding the use of "rich" and "lean" communication media. The literature review of the study makes a major contribution by fitting together the multiple theories applied to the area and showing how conflicting results from all the established media selection theories make sense in different circumstances.
|keyword = media selection,organizational roles,organizational communication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical investigation of information technology sourcing practices: Lessons from experience'''
{{header}}
{{article
|author= MC Lacity,LP Willcocks,
|source= MIS QUARTERLY
|year= 1998
|abstract = Following Kodak's landmark information technology (IT) outsourcing decisions in 1989 the IT outsourcing market grew to 76 billion dollars in 1995. As the outsourcing market continues to grow and as new contracting options emerge, the accumulated experiences of firms offer significant opportunities for learning. This paper builds on a previous collection of data on 61 IT sourcing decisions made in 40 US. and U.K. organizations during the period 1991 to 1995. This paper reanalyzed transcribed interviews from 145 participants. Using "expected cost savings achieved" as an indicator of success, five best practices were identified in the case companies. First, selective outsourcing decisions had higher success rates than total outsourcing or total insourcing decisions. Second, senior executives and IT managers who made decisions together had higher success rates than either stakeholder group acting alone. Third, organizations that invited both internal and external bids had higher success rates than organizations that merely compared external bids with current IT costs. Fourth, short-term contracts achieved higher success rates than long-term contracts. Fifth, detailed fee-for-service contracts had higher success rates than other types of fee-for-service contracts. The critical elements of three contracting models the described: fee-for-service contracts, strategic alliances/partnerships, and buying-in of vendor resources. When the practices generated from the case studies are compared with current practices, we begin to understand which practices are proving robust and why new practices emerge. For example, in the participating companies, the rhetoric of a "partnership" was misused to describe contracts that are actually fee-for-service contracts. Today practitioners who understand the inherent conflicts in fixed fee-for-service contracts are demanding what they perceive to be more favorable contracting options, such as flexibly-priced contracts, performance-based contracts, and strategic alliances based on shared risks and rewards. This analysis reconciles some of the apparent discrepancies in past findings about the best ways to source IT.
|keyword = empirical research,management of computing and IS,measuring IS success,contract,strategic alliances,outsourcing of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using geographical information systems for decision making: Extending cognitive fit theory to map-based presentations'''
{{header}}
{{article
|author= AR Dennis,TA Carte,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = As the use of Geographical information Systems (GIS) by business becomes more common, we need to better understand when these systems are and are not useful. This research uses a laboratory experiment to extend cognitive fit theory (Vessey 1991) to geographic tasks performed using either map-based presentations or tabular presentations. The experiment found that decision makers using a map-based presentation made faster and more accurate decisions when working on a geographic task in which there were adjacency relationships among the geographic areas. Decision makers using a map-based presentation made faster but less accurate decisions when working on a geographic task in which there were no relationships among the geographic areas.
|keyword = geographical information systems,cognitive fit,maps,graphics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A conceptual and operational definition of personal innovativeness in the domain of information technology'''
{{header}}
{{article
|author= R Agarwal,J Prasad,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = The acceptance of new information technologies by their intended users persists as an important issue for researchers and practitioners of information systems. Several models have been developed in the Literature to facilitate understanding of the process by which new information technologies are adopted. This paper proposes a new construct that further illuminates the relationships explicit in the technology acceptance models and describes an operational measure for this construct that possesses desirable psychometric properties. The construct, personal innovativeness in the domain of information technology, is hypothesized to exhibit moderating effects on the antecedents as well as the consequences of individual perceptions about a new information technology. The construct was developed and validated in the context of the innovation represented by the World-Wide Web. Implications for theory and practice are discussed.
|keyword = instrument development,innovation,IT adaption,World-Wide Web,It implementation,personal innovativeness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effects of customizability and reusability on perceived process and competitive performance of software firms'''
{{header}}
{{article
|author= SR Nidumolu,GW Knotts,
|source= MIS QUARTERLY
|year= 1998
|abstract = This study addresses the broad research issue of how software firms can manage their software development efforts in order to compete effectively under intensified competition. Based on recent research in manufacturing strategy and software process engineering, a research model and six hypotheses were derived. Reusability and customizability were expected to positively affect process flexibility and predictability. In turn, these perceived process performance dimensions were expected to positively influence perceived competitive performance, assessed in terms of market responsiveness and product cost efficiency. Using a survey design, responses were obtained from a random sample of 100 software firms. Two kinds of respondents were used: the senior manager in charge of software development (58% response rate) and the marketing manager (36% response rate). The model and hypotheses were assessed using EQS, a structural equations modeling package that can be used for path analysis. The results from both the marketing manager and development manager responses suggested that process flexibility was an important determinant of perceived competitive performance. However, process predictability was an important determinant of perceived competitive performance in the development manager, but not the marketing manager, responses. Finally, while customizability had a significant positive effect on the perceived process performance dimensions, reusability did not. The research model is a potentially useful contribution to an important new area of MIS research concerning the performance of software firms, which draws from manufacturing strategy and software process engineering.
|keyword = software process management,customizability,reusability,process predictability,process flexibility,perceived competitive performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Strategic information systems planning success: An investigation of the construct and its measurement'''
{{header}}
{{article
|author= AH Segars,V Grover,
|source= MIS QUARTERLY
|year= 1998
|abstract = Strategic information systems planning (SISP) requires significant outlays of increasingly scarce human and financial resources. Yet, there exists very little understanding of how the success of this planning activity is measured. Using classical frameworks for measurement development as well as contemporary statistical techniques for assessing dimensionality, this study theoretically develops and empirically tests a measurement model of SISP success. The results suggest that SISP success can be operationalized as a second-order factor model. The first order constructs of the model are termed alignment, analysis, cooperation, and improvement in capabilities. These factors are governed by a second-order construct of SISP success. The results of the study are framed as a tool, for benchmarking planning efforts as well as a foundation for operationalizing a key dependent variable in SISP research.
|keyword = IS strategic planning,planning effectiveness,second-order factor modeling,structural equation modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Polarization and persuasive argumentation: A study of decision making in group settings'''
{{header}}
{{article
|author= M El-Shinnawy,AS Vinze,
|source= MIS QUARTERLY
|year= 1998
|abstract = This research focuses on group decision making from both an outcome and a process-based perspective. This study draws from the well-established literature of group polarization, as well as the growing body of GSS literature, to develop a model to study group polarization in a contemporary communication context. The proposed model focuses on communication medium, task characteristics, group composition, and their interaction as explanations for the outcome of group polarization and the process that precedes it. An experimental research method is used to test the relationships suggested by the model. In this study, group polarization is recorded by comparing decisions at the individual level, face-to-face group settings, and GSS mediated settings. The initial agreement index indicates the diversity of individual stances within the group. This index is used as a covariate to enhance understanding of the extent of group polarization. The group process is documented by protocol analyzing transcriptions of the TTF and GSS group sessions for persuasive content. A 2x2x2 factorial design was used to analyze the results. The analysis indicates that, for both process and outcomes, the medium of communication and task characteristics interact with one another to provide the dominant explanation. Surprisingly, group composition had no impact on either polarization or persuasive arguments. The findings reported in this study are of importance to organizations that increasingly rely on groups as units of decision making. The results also provide insight to researchers of group decision making and to future developers and users of group support systems.
|keyword = group polarization,choice shifts,group support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The merchant of Prato - Revisited: Toward a third rationality of information systems'''
{{header}}
{{article
|author= K Kumar,HG van Dissel,P Bielli,
|source= MIS QUARTERLY
|year= 1998
|abstract = The failure of SPRINTEL, an interorganizational information system in Prato (Italy), raises a number of interesting questions with regard to the technical-economic and socio-political perspectives that currently dominate the information systems/information technology literature. These questions underscore the importance of developing additional theoretical perspectives in order to better understand the role of information systems in organizations. This article reflects upon these questions and their theoretical foundations in the context of a case study. The case study describes the implementation, usage, and outcome of an interorganizational information system. An analysis is made of the extent to which the technical-economic and socio-political perspectives are sufficient to explain the failure of this system. The outcome of the analysis shows that these two perspectives are insufficient to provide an explanation. Based on the literature from a variety of sources, a third, complementary perspective is developed. Like the socio-political perspective (Kling 1980), this perspective is also an interactionist perspective. However, instead of focusing on politics and conflict as the primary interaction mode, it focuses on collaboration and cooperation as the key to understanding interaction processes. This perspective introduces a third rationality of information systems in which trust, social capital, and collaborative relationships become the key concepts for interpretation.
|keyword = interorganizational systems,network organizations,transaction costs economics,trust,IS implementation,IS failure,cross-cultural issues,interpretivist perspective,case study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and worker composition: Determinants of productivity in the life insurance industry'''
{{header}}
{{article
|author= C Francalanci,H Galal,
|source= MIS QUARTERLY
|year= 1998
|abstract = This paper investigates the impact of IT investments and worker composition on the productivity of life insurance companies. The majority of previous IT productivity studies follow a technological imperative, hypothesizing a direct relationship between higher IT investments and increased productivity. This paper shifts the focus toward the organizational imperative, which views returns on IT investments as a result of the alignment between technology and other critical management choices. Specifically, the study focuses on the alignment between IT investments and worker composition, measured in terms of relative numbers of clerical, managerial, and professional positions to the total number of employees. Hypotheses are tested using a data set compiled over a 10-year period for 52 life insurance companies. With respect to prior research, the study is novel in its adoption of a model of productivity that accounts for both separate and combined effects of IT investments and worker composition. Premium income per employee and total operating expense to premium income are used as indicators of productivity. Study findings show that increases in IT expenses are associated with productivity benefits when accompanied by changes in worker composition. Life insurance companies that have decreased their proportion of clericals and professionals while at the same time investing in IT have experienced productivity improvements. On the other hand, companies decreasing their proportion of managers while investing in IT are found to have reduced productivity.
|keyword = organizational productivity,information economics,IT organizational alignment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reengineering the Dutch flower auctions: A framework for analyzing exchange organizations'''
{{header}}
{{article
|author= A Kambil,E van Heck,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = This paper specifies a generalizable model of exchange processes and develops a process-stakeholder analysis framework to evaluate alternative market designs. This framework is applied to analyze a number of information technology initiatives in the Dutch flower markets. The Dutch flower auctions are the world's leading centers for trading cut flowers and potted plants. We undertake a cross-case analysis and apply our framework to analyse successes and failures in the introduction of new IT-based trading mechanisms in these markets. Based on our study, we develop a number of testable propositions on: the separation of physical and informational processes in trading, the responses of stakeholders to changes in available information due to IT initiatives, and economic and incentive conditions required for adoption of new trading processes. Finally, our detailed cases illustrate the institutional and incentive constraints, and complexities encountered in the introduction of new electronic markets.
|keyword = electronic markets,transaction costs,reengineering,technology adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitator influence in group support systems: Intended and unintended effects'''
{{header}}
{{article
|author= TL Griffith,MA Fuller,GB Northcraft,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = This paper addresses facilitation, a developing area of Group Support Systems (GSS) research. The facilitator role is one of improving a group's communication and information flow; facilitators are meant to enhance the manner in which a group makes decisions without making those decisions for the group. However, there is a paradox in facilitation: The influence required to facilitate a group changes the group's outcomes. Additionally, strict impartiality for facilitation may be too much to expect because facilitators may unintentionally bias group outcomes, or because facilitators may have their own agendas. Acknowledgment, training, and standards for facilitators may prove useful ways for groups to retain the benefits of facilitation without incurring the costs of inappropriate facilitator influence. Implications are drawn for new research acknowledging the complexity of the GSS sociotechnical system, and the importance of sociotechnical facilitation in organizations.
|keyword = facilitation,group support systems,electronic meeting systems,groups,power in organizations,sociotechnical systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Semantic structuring in analyst acquisition and representation of facts in requirements analysis'''
{{header}}
{{article
|author= GM Marakas,JJ Elam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = The determination of information requirements is one of the most crucial stages in the software design and development process (Montezemi 1988). It is during this stage that the greatest degree of interaction occurs between the analyst and the user (Lauer et al. 1992). Despite the system development method employed, the functional success of many aspects of requirements determination ultimately rests on how well the user(s) and analyst(s) communicate (Holtzblatt and Beyer 1995). The purpose of this paper is to report the results obtain from a laboratory experiment that investigated the effects of a semantic structuring process of inquiry on the process of interview-derived information acquisition and the subsequent overall correctness of the logical representation of the facts obtained. The study focused on the specific question types used by systems analysts and the role their semantic construction played in representing the information flows in a business system. Three underlying semantic patterns of questions emerged from the analysis. The results showed that certain question types were associated with increased accuracy of logical representations regardless of analyst experience level. Further, the semantic and process patterns that emerged were also directly related to accurate representation of facts and demonstrated an experience-revel independence. The results indicate that disciplined questioning strategies are not necessarily learned from practice and they can be improved via structured training. Each of the patterns provide insight into the questioning process employed and the effectiveness of different strategies of inquiry. Implications for both the practitioner and the academic research communities with regard to analyst interview behavior are discussed.
|keyword = requirements analysis,information gathering,systems analysis,semantic structuring,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Integrated modeling environments in organizations: An empirical study'''
{{header}}
{{article
|author= GP Wright,AR Chaturvedi,RV Mookerjee,S Garrod,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = Considerable attention in the information systems and management science Literature has focused on computer-based modeling environments, sometimes called integrated modeling environments or model management systems. This research has been primarily concerned with suggesting features/components of modeling environments such as improved executable modeling languages for model creation, integration, and data representation; specialized database systems for managing model data; and customized model-solver software. However, there has been little (if any) empirical guidance offered in the Literature about the specific needs of business and industry for computer-based integrated modeling environments. Using a data set compiled from a national survey of modelers (analysts) and model users (decision makers), we empirically investigate the validity of several of the key assumptions of modeling environment research reported in the Literature, and examine the relationships between the modeling factors: data complexity, model complexity, modeling intensity, modelerfuser requirements, and need for computer-based integrated modeling environments in organizations. Our empirical analysis of the data set shows that practitioners rank automated access to model data and automated error checking (e.g., model syntax and semantics checking) high as desirable components in modeling environments. We find that users prefer to have modeling environments Linked to their current modeling and modeling-support software systems. Our findings further suggest that a high percentage of modelers and users are dissatisfied with the software systems they are currently using to support their modeling activities. Finally, a covariance structure analysis of the modeling environment factors clearly shows that: (a) model complexity has a direct positive effect on modeling intensity; (b) data complexity has an insignificant direct effect on modeling intensity, but has a negative effect on modeler/user requirements; and (c) modeler/user requirements have a direct positive effect on need for computer-based integrated modeling environments in organizations.
|keyword = model management systems,integrated modeling environments,structured modeling,decision support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of information technology investments on firm performance and evaluation: Evidence from newly industrialized economies'''
{{header}}
{{article
|author= KY Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1998
|abstract = The impact of information technology (IT) investments on firm performance has been the subject of active research in recent years. However, findings of almost all studies are based on data collected in the United States. Little work has been done elsewhere to validate these results and to see if they are applicable across national boundaries. In this study, we fill this gap by comparing four newly-industrialized economies (NIEs) with regard to the impact of IT capital on business performance. Secondary data collected from various sources are used to assess the impact over the period from 1983 to 1991. Findings based on four business measures and a market valuation model based on Tobin's q are reported. While the current results are consistent with work done in the United States in general, discrepancies among the four NIEs are observed. Combined with findings from previous work, three pieces of evidence seem to emerge that are generally observed across country boundaries. First, IT investment is not correlated with shareholder's return. Second, there is little evidence that the level of computerization is valued by the market in developed and newly-developed countries. Third, there is no consistent measurement of IT investment as indicated by the mixed results across different performance ratios. Modeling and measurement concerns expressed in previous U.S.-based studies are also observed in our comparative study. Our findings provide a starting point to accumulate a body of comparative studies for the development of a theory that links IT investment, firm performance, and macro factors such as national technology policy in an integrated framework.
|keyword = information technology,computers,investment,performance valuation,economics,business value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Success of data resource management in distributed environments: An empirical investigation'''
{{header}}
{{article
|author= H Jain,K Ramamurthy,HS Ryu,M Yasai-Ardekani,
|source= MIS QUARTERLY
|year= 1998
|abstract = The trend toward distributed processing has significantly increased the awareness of data as a key corporate resource and underscored the importance of its management. In spite of this, there is a lack of empirical investigation of issues related to data resource management (DRM) in distributed processing environments. Being perhaps the first empirical attempt, this exploratory study identifies four information systems (IS) variables related to DRM in a distributed environment. It also seeks to examine the notion of gestalt fit to describe the nature of the relationships among these variables. in addition, the study evaluates whether internally congruent groups outperform their opposites in realizing DRM success. The results of cluster analysis support the view of gestalt fit by identifying five clusters. The results also suggest that organizations represented by a well-blended configuration of high intersite data dependence, high centralization of IS decisions, high concentration IS resources at the central site, and low DRM-related autonomy granted to local sites appear to realize a greater degree of DRM success than the other groups. The implications of the study are discussed, and further research directions are proposed.
|keyword = data resource management,distributed processing,distributed databases,gestalt fit,cluster analysis,autonomy,centralization,intersite data dependence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Modeling IT ethics: A study in situational ethics'''
{{header}}
{{article
|author= D Banerjee,TP Cronan,TW Jones,
|source= MIS QUARTERLY
|year= 1998
|abstract = Misuse of computer information systems has caused significant losses to business and society, even though computing has benefited both businesses and professionals. To this end, several measures have been suggested that both prevent and deter losses. One deterrent measure is to identify individual and situational characteristics of people who act ethically/unethically. This study identifies specific characteristics that are associated with and may influence the ethical behavior intention of information systems employees when faced with ethical dilemmas. The results of the study show that individual and situational characteristics do influence ethical behavior intention.
|keyword = ethics,ethical behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring information systems service quality: Lessons from two longitudinal case studies'''
{{header}}
{{article
|author= RT Watson,LF Pitt,CB Kavan,
|source= MIS QUARTERLY
|year= 1998
|abstract = IS service quality was measured three times in an information management consulting firm and an information service business. After the first measurement, IS management initiated several actions to improve service quality. The second measurement indicated that service quality improved in the intervening period. When service quality was measured a third time, it had returned to the levels of the first measurement. The evidence suggests that management's attention to service quality waned after about a year, and IS management needs to recognize that service quality is not a fad but an ongoing commitment. The paper concludes by recognizing that delivering IS service quality requires action at three levels (strategic, tactical, and operational) and that the CIO must pay continuing attention to IS service quality. A model for building service quality into IS is described.
|keyword = IS management,service quality,longitudinal study,SERVQUAL,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The dependent variable in research into the effects of creativity support systems: Quality and quantity of ideas'''
{{header}}
{{article
|author= B Wierenga,GH van Bruggen,
|source= MIS QUARTERLY
|year= 1998
|abstract = Creativity support systems (CSS) aim at enhancing the creativity of users. There is an emerging stream of research in which the effects of CSS on the creative output of respondents are measured. In this research, if is important to make a clear distinction between the dependent variable, creative output and the independent variable use of CSS. Furthermore, the research design should take the potential effect of other factors on creative output into account, most notably, creative ability as a trait of the respondents. An experimental study on the value of creativity support systems was recently reported in MIS Quarterly (Massetti 1996). That study yielded interesting insights with respect to the value of CSS. However, because of the methodology applied in analyzing the data, the study underestimated the effects of CSS on the creative output of decision makers. In this note, Massetti's experiment is positioned in the broader perspective of current research in the area of CSS, and an alternative framework for analyzing the data is proposed.
|keyword = cognitive science,decision aids,research methodology,experimental research,statistical methods,research models,user characteristics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An ounce of preventative research design is worth a ton of statistical analysis cure'''
{{header}}
{{article
|author= B Massetti,
|source= MIS QUARTERLY
|year= 1998
|abstract = This response addresses the data analysis issues raised by Wierenga and van Bruggen (1998) concerning Massetti (1996). Their analysis suggestions were performed and resulted in no significant differences between the treatment conditions. However, these analyses are misleading because of normality and variance problems present in Massetti's dataset. Specifically, not controlling for individual performance differences in ideational fluency ability during experimentation created the need for the complex, but appropriate, analysis approach used in Massetti. This response further suggests that ideational fluency be included as an independent factor in future research on individual creativity support systems.
|keyword = ideational fluency,creativity support systems,idea generation,measures of creative performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Ownership and investment in electronic networks'''
{{header}}
{{article
|author= JY Bakos,BR Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = We employ the theory of incomplete contracts to examine the relationship between ownership and investment in electronic networks such as the Internet and interorganizational information systems. Electronic networks represent an institutional structure that has resulted from the introduction of information technology in industrial and consumer markets. Ownership of electronic networks is important because it affects the level of network-specific investments, which in turn determine the profitability, and in some cases the viability, of these networks. In our analysis we define an electronic network as a set of participants and a portfolio of assets. The salient concept in this perspective is the degree to which network participants are indispensable in making network assets productive. We derive three main results. First, if one or more assets are essential to all network participants, then all the assets should be owned together. Second, participants that are indispensable to an asset essential to all participants should own all network assets. Third and most important, in the absence of an indispensable participant, and as long as the cooperation of at least two participants is necessary to create value, sole ownership is never the best form of ownership for an electronic network. This latter result implies that as the leading network participants become more dispensable, we should see an evolution toward forms of joint ownership.
|keyword = incomplete contracts,investment externalities,Internet ownership,network externalities,network investment,network ownership,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Advancing the theory of adaptive structuration: The development of a scale to measure faithfulness of appropriation'''
{{header}}
{{article
|author= WW Chin,A Gopal,WD Salisbury,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Adaptive Structuration Theory (AST) is rapidly becoming an influential theoretical perspective in research on advanced information technologies. However, there still exists a paucity of methods to capture critical AST constructs, This paper describes the development of an instrument to capture the extent to which users of an advanced information technology believe they have appropriated its structures faithfully, The development of such instruments is considered critical if the theoretical base provided by AST is to be fully exploited in understanding the use of advanced information technologies. The development procedure, which occurred in the context of the use of an electronic meeting system, was carried out in three phases that began with initial item development and proceeded through an exploratory to a confirmatory phase. Three experiments, two in the exploratory phase and one in the confirmatory phase, were performed. In the final phase, structural equation modeling techniques were used to confirm the convergent, discriminant, and nomological validity of the resulting five-item scale.
|keyword = adaptive structuration theory,technology appropriation,electronic meeting systems,structural equations modeling,scale development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An economic analysis of the introduction of an electronic data interchange system'''
{{header}}
{{article
|author= A Barua,B Lee,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Although electronic data interchange (EDI) holds the promise of significantly increasing the efficiency of business transactions, an installed base of proprietary implementations has been detrimental to the widespread acceptance of the technology. Thus, an important research issue involves strategies for facilitating EDI adoption. We analyze the introduction of an EDI system in a vertical mark involving one manufacturer and two suppliers. The manufacturer initiates an EDI network, and penalizes a supplier for not joining the system by reducing its volume of business with the supplier. Along with a "stick," the manufacturer can also use a "carrot" in the form of a subsidy to partially offset a supplier's setup cost. The competition between the suppliers is characterized by incentive types for joining the EDI system ("motivating" or "threatening") and the Information Technology (IT) efficiency ("efficient" or "inefficient"). We show that regardless of its cost structure, a supplier may have to join the EDI network out of "strategic necessity," due to the presence of an IT-efficient supplier, Our analysis further shows that depending on the supplier competition structure, the EDI system may prove to be a "beneficial" strategic necessity for a large supplier and an "unfortunate" strategic necessity for a small supplier. Another key result is that by increasing the severity of the penalty, both the manufacturer and the follower supplier can be worse off under certain conditions, The analysis of subsidy strategies reveals that unless leadership and followership positions are reversed due to a subsidy, subsidizing a supplier has no impact on the joining time of its competitor. Thus the EDI initiator cannot induce both, suppliers to join earlier by subsidizing one supplier, Also, the larger the slack capacity of the leader, the higher (lower) the manufacturer's incentive to subsidize the leader (follower). These results offer insights for initiators and adopters regarding penalty and subsidy strategies, impact on competition structure, joining decisions and network growth.
|keyword = EDI,beneficial and unfortunate strategic necessity,incentives,efficiency,subsidy,penalty mechanisms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of user participation in systems development: A longitudinal field experiment'''
{{header}}
{{article
|author= JE Hunton,JD Beeler,
|source= MIS QUARTERLY
|year= 1997
|abstract = This study examines the efficacy of user participation in developing an accounting application. The research takes place over a 19-month time frame, involves 516 clerical-level accounting subjects, and includes experimental manipulations in a field setting. The model of user participation and involvement proposed by Hartwick and Barki (1994) provides the foundation for the research framework. Their model is augmented by the inclusion of concepts from procedural justice and self-efficacy research. Participation is manipulated at three increasing levels: (1) no voice, (2) non-instrumental voice, and (3) instrumental voice. Research findings suggest that users' pre-experiment level of involvement with and attitude toward the present system are positively associated with their desire to participate in the development of the new system. Study results also indicate that users' a priori self-efficacy beliefs regarding their perceived ability to effectively contribute to the development process are positively related to desired participation. Pre-to post-experiment gains in psychological and behavioral variables are next assessed. In the instrumental voice condition, user involvement, user attitude, and performance gains are significantly highest; User attitude and involvement gains are significantly higher in the non-instrumental voice condition than in the no voice condition; however, gains in user performance are not significantly different between these treatment conditions. Research findings indicate that user participation can be effective, particularly when users perceive a noticeable degree of instrumental control over the decision outcome.
|keyword = participation,involvement,accounting,procedural justice,self-efficacy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Gender differences in the perception and use of E-mail: An extension to the technology acceptance model'''
{{header}}
{{article
|author= D Gefen,DW Straub,
|source= MIS QUARTERLY
|year= 1997
|abstract = This study extends the TAM model (Davis 1989) and the SPIR addendum (Straub 1994) by adding gender to an IT diffusion model. The technology acceptance model (TAM) has been widely studied in IS research as an explanation of the use of information systems across IS types and nationalities. While this line of research has found significant cross-cultural differences, it has ignored the effects of gender, even though in socio-linguistic research, gender is a fundamental aspect of culture. Indeed, socio-linguistic research has shown that men tend to focus discourse on hierarchy and independence, while women focus an intimacy and solidarity. This literature provides a solid grounding far conceptual extensions to the IT diffusion research and the technology acceptance model. Testing gender differences that might relate to beliefs and use of computer-based media, this study sampled 392 female and male responses via a cross-sectional survey instrument. The sample drew from comparable groups of knowledge workers using e-mail systems in the airline industry in North America, Asia, and Europe. Study findings indicate that women and men differ in their perceptions but not use of e-mail. These findings suggest that researchers should include gender in IT diffusion models along with other cultural effects. Managers and cc-workers, moreover, need to realize that the same mode of communication may be perceived differently by the sexes, suggesting that more favorable communications environments might be created, environments that take into account not only organizational contextual factors, but also the gender of users. The creation of these environments involves not only the actual deployment of communication media, but also organizational training on communications media.
|keyword = technology acceptance model,gender differences,cross-cultural IT research,IT adoption and diffusion,e-mail,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Transformation of the IT function at British petroleum'''
{{header}}
{{article
|author= J Cross,MJ Earl,JL Sampler,
|source= MIS QUARTERLY
|year= 1997
|abstract = In 1989, the IT function of the exploration and production division of British Petroleum Company set out to transform it self in response to a severe economic environment and poor internal perceptions of IT performance. This case study traces and analyzes the changes made over six years. The authors derive a model of the transformed IT organization comprising seven components that they suggest can guide IT departments in general as they seek to reform themselves in the late 1990s. This model is seen to fit well with recent thinking on general management in that the seven components of change can be reclassified into the Bartlett and Ghoshal (1994) framework of purpose, process, and people. Some suggestions are made on how to apply the model in other organizations.
|keyword = information technology,IS management,transformation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Building change-readiness capabilities in the IS organization: Insights from the Bell Atlantic experience'''
{{header}}
{{article
|author= CE Clark,NC Cavanaugh,CV Brown,V Sambamurthy,
|source= MIS QUARTERLY
|year= 1997
|abstract = Change-readiness is the ability of an information systems (IS) organization to deliver strategic IT applications within short development cycle times by utilizing a highly skilled internal IS workforce. This paper examines two important questions: What are the design elements of a change-ready IS organization? How can transformations to such designs be effectively managed? Insights to these questions are generated through a case study of the conceptualization and implementation of an innovative organization design within a large IS unit at Bell Atlantic, a Regional Bell Operating Company. First, a rich description is provided of the components (strategy, structure, processes, people skills, reward systems) of a centers of excellence (CoE) design that has yielded measurable gains in IS performance. Then, an analysis of the two-year implementation of this CoE design is provided in terms of anticipated and unanticipated change actions, as well as a summary of "lessons learned." The conclusion describes the design as a model worthy of consideration by other IS managers for developing change-readiness IT capabilities. Comparisons with other models for the IS organization are then drawn.
|keyword = structure of the IS function,issues in organizing IS,IS staffing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Redesigning the customer support process for the electronic economy: Insights from storage dimensions'''
{{header}}
{{article
|author= O El Sawy,G Bowles,
|source= MIS QUARTERLY
|year= 1997
|abstract = This paper provides insights for redesigning IT-enabled customer support processes to meet the demanding requirements of the emerging electronic economy in which fast response, shared knowledge creation, and internetworked technologies are the dynamic enablers of success. The paper describes the implementation of the TechConnect support system at Storage Dimensions, a manufacturer of high-availability computer storage system products. TechConnect is a unique IT infrastructure for problem resolution that includes a customer support knowledge base whose structure is dynamically updated based an adaptive learning through customer interactions. The paper assesses the impacts of TechConnect and ifs value in creating a learning organization. It then draws insights for redesigning knowledge-creating customer support processes for the business conditions of the electronic economy.
|keyword = customer support process,business process redesign,information technologies for customer integration,learning organization,knowledge management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Portfolios of control modes and IS project management'''
{{header}}
{{article
|author= LJ Kirsch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = In spite of the attention it has received, there is still much that is not understood about the management of systems development efforts. This research examines one aspect of the management process: the function of control. In this paper, control is viewed broadly, encompassing all attempts to ensure individuals in organizations act in a manner that is consistent with meeting organizational goals and objectives. Control is categorized into formal modes (behavioral, outcome) and informal modes (dan, self). Formal and informal control modes are implemented via a variety of mechanisms, such as linking pay with performance, socialization, and team-building. Relatively little is known about the modes of control used to manage information systems development efforts. The objective of this research is to address this lack of understanding in the literature by exploring how control modes are implemented during systems development projects and by investigating why IS and user stakeholders implement particular combinations of control modes. To meet this objective, a series of four case studies of systems development projects was conducted. The results reveal that users, as well as IS managers, play a critical role in controlling systems development projects. Moreover, the results suggest that all stakeholders implement a portfolio of control modes that typically includes both formal and informal modes. This portfolio contains a mix of overlapping and redundant mechanisms used to exercise these modes of control. The results also suggest that constructing a portfolio of control modes is a process that includes selecting appropriate preexisting mechanisms of formal control; designing new mechanisms with which to implement formal control, if necessary; and supplementing the mechanisms of formal control with mechanisms of informal control. Throughout this process of construction the choice of particular control mechanisms depends on task characteristics, role expectations, and project-related knowledge and skills.
|keyword = IS project management,IS project control,organization of work,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A respecification and extension of the DeLone and McLean model of IS success'''
{{header}}
{{article
|author= PB Seddon,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Delone and McLean's (1992) comprehensive review of different information system success measures concludes with a model of ''temporal and causal'' interdependencies between their six categories of IS Success. After working with this model for some years, it has become apparent that the inclusion of both variance and process interpretations in their model leads to so many potentially confusing meanings that the value of the model is diminished. Because of the confusion that this overloading of meanings can cause, this paper presents and justifies a respecified and slightly extended version of DeLone and McLean's model.
|keyword = IS success,IS use,IS evaluation,IS effectiveness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical study of computer system learning: Comparison of co-discovery and self-discovery methods'''
{{header}}
{{article
|author= KH Lim,LM Ward,I Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = This paper reports a study that examined two types of exploratory computer learning methods: self-discovery vs. co-discovery, the latter of which involves two users working together to learn a system. An experiment was conducted to compare these two methods and the results were interpreted within a mental model framework. Co-discovery subjects were better than self-discovery subjects at making inferences about the capability and extended functions of the system. Furthermore, while working by themselves after an initial period of learning, they performed better in a similar, though more complex task than the one they encountered at the learning phase. Process tracing analysis showed that self-discovery subjects focused more on surface structures, such as detailed physical actions, for implementing the task. On the other hand, co-discovery groups focused more on relating lower level actions to higher level goals. Therefore, co-discovery subjects had a better understanding of the relationships between the physical actions and goals, and hence formed mental models with higher inference potential than self-discovery subjects.
|keyword = mental models,verbal protocols,computer system learning,co-discovery learning,process tracing,inference,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Empirical evidence on Swanson's tri-core model of information systems innovation'''
{{header}}
{{article
|author= V Grover,K Fiedler,J Teng,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Research in IS innovation has been isolated and fragmented. These studies typically examine single innovations and do not effectively integrate notions of IS innovation with organizational innovation. Swanson (1994) extends the prior dual-core model of innovation into a tri-core model specifically for the unique nature of IS innovation. This model provides a useful typology of IS innovation that can form the foundation for innovation theory in this important area. In this paper we present Swanson's tri-core model of IS innovation along with preliminary data to test aspects of the model proposed by Swanson. Adoption of ten IS innovations is studied using two analyses, one based only on adopter sub-samples and the other using a more rigorous treatment of nonadopters based on survival analysis. The objective of this study is simple-to test theory and encourage continued focused inquiry in IS innovation. The results of this study provide partial support for the proposed hypotheses, leading us to conclude on an optimistic note regarding the viability of this model as an integrating framework for IS innovation.
|keyword = IS innovation,organizational innovation,tri-core model,technical innovation,survival analysis,innovation theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Diffusion of information systems outsourcing: A reevaluation of influence sources'''
{{header}}
{{article
|author= Q Hu,C Saunders,M Gebelt,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Information systems outsourcing is an increasingly popular IS management practice in companies of all sizes. Examining the adoption of IS outsourcing from the well-developed theoretical foundation of innovation diffusion may shed some light on significant factors that affect the adoption decision, and clarify some misperceptions. This study explores the sources of influence in the adoption of IS outsourcing. Using a sample of 175 firms that outsourced their IS functions during the period from January 1985 to January 1995, we tested three hypotheses of sources of influences using four diffusion models: internal influence, external influence, and two mixed influence models. Our findings suggest that the mixed influence is the dominant influence factor in the diffusion of IS outsourcing, and that there is no evidence of the ''Kodak effect'' in the IS diffusion process. This directly contradicts the conclusions of the Loh and Venkatraman (1992) study. Further discussions are provided about the potential problems in studies of influence sources of IT innovation diffusion.
|keyword = information system outsourcing,innovation diffusion,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''When processes learn: Steps toward crafting an intelligent organization'''
{{header}}
{{article
|author= D Zhu,MJ Prietula,WL Hsu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Two trends in information systems research provide an opportunity to add an additional link between information technology and organizational learning. First, there is an increasing penetration of information technology into the firm's processes and structures. Second, research in artificial intelligence has given rise to the first generation of fully computational architectures of general intelligence. In this research note we explore a melding of these two trends. In particular, we present the crafting of an organizational process which can learn, and develop and apply a new set of organizational learning metrics to that process. The process is a simplification of a complex, parallel-machine production scheduling task performed in a local manufacturing firm. The system Dispatcher-Soar, generally supports a symbolic, constraint propagation approach based, in part, on the reasoning methods of the human scheduler at the firm. The implementation of this process is based on a dispatching rule used by the expert. The behavior of Dispatcher-Soar centered around a small case study examining the effects of scheduling volume and learning on performance. Results indicated that the knowledge gained can reduce within-trial scheduling effort. An analysis of the generated knowledge structures (chunks) provided insight into how that learning was accomplished and contributed to process improvements. As the knowledge generated was in a form standardized to a common architecture, metrics were used to evaluate the production efficiency (eta(prod)), utility (eta(util)), and effectiveness (eta(eff)) Of the accumulated organizational knowledge across trials.
|keyword = information systems,organizational learning,artificial intelligence,machine learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Developing an historical tradition in MIS research'''
{{header}}
{{article
|author= RO Mason,JL McKenney,DG Copeland,
|source= MIS QUARTERLY
|year= 1997
|abstract = MIS as a discipline has not yet developed a tradition of historical research. Historical analyses broaden our understanding of the processes by which information technology is introduced into organizations and of the forces that shape its use. Paramount among these processes are those Schumpeter called ''creative destruction.'' These are events that change entire organizations and industries. The end product of a Schumpeterian process is called a ''dominant design,'' a new configuration of an organization's technology, strategy, and structure. A dominant design is manifested in several ways: a new organizational infrastructure, new functionality, new products, new services, new production functions, or new cost structures. By changing the basis of competition in the industry, a firm that institutes a dominant design secures an initial competitive edge. Although the understanding of these processes is central to the concerns of many researchers and practitioners in the field, the information systems research literature contains very few examples of historical analyses of this type. A contingency framework is developed for conducting a class of information technology-based historical studies that focuses on innovation and competition within an industry.
|keyword = history,strategy,management information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Personal computing acceptance factors in small firms: A structural equation model'''
{{header}}
{{article
|author= M Igbaria,N Zinatelli,P Cragg,ALM Cavaye,
|source= MIS QUARTERLY
|year= 1997
|abstract = This study draws upon the technology acceptance model as the theoretical basis and empirical findings for a pragmatic explanation of key factors affecting personal computing acceptance in small firms. The study uses results from a survey of 358 users in small firms in New Zealand to test a structural model examining the hypothesized relationships among the following constructs: intraorganizational factors, extraorganizational factors, perceived ease of use, perceived usefulness, and personal computing acceptance (i.e., system usage). The findings indicate that perceived ease of use is a dominant factor in explaining perceived usefulness and system usage, and that perceived usefulness has a strong effect on system usage. The results also indicate that exogenous variables influence both perceived ease of use and perceived usefulness, particularly management support and external support. Inconsistent with prior research in large firms, relatively little support was found for the influence of both internal support and internal training. Implications for the acceptance of personal computing and future research on personal computing acceptance in small firms are discussed.
|keyword = personal computing acceptance,small firms,intraorganizational factors,extraorganizational factors,technology acceptance model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An historical method for MIS research: Steps and assumptions'''
{{header}}
{{article
|author= RO Mason,JL McKenney,DG Copeland,
|source= MIS QUARTERLY
|year= 1997
|abstract = Historical research offers perspectives on phenomena that are unavailable by any other methodological means. They reflect the cultural circumstances and ideological assumptions that underlie phenomena and the role played by key decision makers together with long-term economic, social, and political forces in creating them. Each of these benefits is accompanied by limitations such as, in most cases, a lack of mathematical tractability. The careful application of historical methods can overcome some of these limitations. A seven-step methodology is proposed: begin with focusing questions, specify the domain, gather evidence, critique the evidence, determine patterns, tell the story, and write the transcript.
|keyword = history,strategy,management information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Bank of America: The crest and trough of technological leadership'''
{{header}}
{{article
|author= JL McKenney,RO Mason,DG Copeland,
|source= MIS QUARTERLY
|year= 1997
|abstract = The Bank of America literally changed the banking industry during the 1950s by means of its ERMA and IBM 702 computer systems. These innovations in information technology resulted in a dominate design that helped keep the Bank of America in the lead for over a decade and a half. They were the collective work of a leader, Clark Beise, a maestro, Al Zipf, and a group of supertechs, ail of whom became the prototypes for these crucial roles. Bank of America was the first organization, among a selected few, to successfully negotiate the innovation cascade leading from crisis to a dominant IT design. Due in large part to IBM's failure to deliver a fully operational operating system for its 360/65, however, coinciding with the leadership's attention toward international markets, in the late 1960s the Bank of America lost its lead. After several decades ''in the trough,'' as a result of aggressive investment and leadership, the bank re-emerged as a strong competitor. This story of achieving alignment in strategy and structure by means of technological innovation, of the almost tragic breaking of that alignment, and of fervent efforts made to gain realignment illuminates some of the most important lessons of IT management that can be learned from the field's relatively recent, but dramatic, history.
|keyword = management theory,economic environment,computer systems,task characteristics,IS project management,IS implementation,organizational use of IS,IS characteristics,history of IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interorganizational cooperation: The role of information technology - an empirical comparison of US and Japanese supplier relations'''
{{header}}
{{article
|author= M Bensaou,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = This paper explores the comparative and cumulative influence of a number of factors on the perceived level of cooperation in a dyadic relationship. Drawing upon the transaction costs economics, organization theory and information systems literatures, we hypothese three sets of key influences: (1) factors exogenous to the relationship, i.e., the characteristics of the environment within which the relationship operates, and factors endogeneous to the relationship including (2) economic and behavioral characteristics of the relationship, and (3) interorganizational information technology applications. These factors have been independently examined in separate research streams. A key contribution of this study is therefore to conceptually and empirically capture their collective influence on cooperation. We empirically test the five hypotheses we develop within the context of buyer-supplier relationships in the U.S. and Japanese automobile industries. Multiple regressions conducted on a data set of 447 distinct relationships indicate that the relational characteristics (i.e., the behavioral climate of the relationship) are the most robust predictor of cooperation in both countries when compared with other structural (e.g., asset specificity) or technological factors (use of EDI-electronic data interchange). Environmental uncertainty (i.e., technological unpredictability) is positively associated with cooperation in Japanese supplier relations, which suggests that cooperation can act as an uncertainty absorption mechanism. Governance structure is a strong and significant predictor of cooperation in both samples, but with the opposite sign. Similarly, information technology (IT) does not play the same predictive role in the two country samples. Significant only in Japan it reflects an 'electronic partnership' approach to the use of IT in supplier relations.
|keyword = interorganizational relations,buyer-supplier relations,interorganizational systems (IOS),Japan,cross-country comparative studies,interorganizational cooperation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business strategic orientation, information systems strategic orientation, and strategic alignment'''
{{header}}
{{article
|author= YE Chan,SL Huff,DW Barclay,DG Copeland,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Information systems strategic alignment-the fit between business strategic orientation and information systems (IS) strategic orientation-is an important concept. This study measured business strategic orientation, IS strategic orientation, and IS strategic alignment, and investigated their implications for perceived IS effectiveness and business performance. Analyses of data gathered in a mail survey of North American financial services and manufacturing firms indicated that 1) business strategic orientation, IS strategic orientation, and IS strategic alignment are modeled best by utilizing holistic, 'systems' approaches instead of dimension-specific, 'bivariate' approaches, 2) three generic IS strategic orientations can be detected, 3) user information satisfaction does not capture important strategic aspects of IS effectiveness, 4) IS strategic alignment is a better predictor of IS effectiveness than is strategic orientation, and 5) business strategic orientation, IS strategic alignment, and IS effectiveness have positive impacts on business performance.
|keyword = information systems strategy,strategic orientation,strategic alignment,fit,systems/technology effectiveness,business performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information acquisition policies for resource allocation among multiple agents'''
{{header}}
{{article
|author= JC Moore,HR Rao,A Whinston,K Nam,TS Raghu,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = This paper investigates a problem of resource allocation where a manager allocates discrete resources among multiple agents in a team in a socially optimal manner. In making this allocation, the manager needs to understand the preference orders of the agents for the discrete resources. The manager does this by adopting an information acquisition policy. Three different information acquisition policies are investigated here. The trade off between the amount of information elicited and the costs involved are studied for each of the policies.
|keyword = information acquisition policies,discrete resource allocation,team problem solving,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Executive decisions about adoption of information technology in small business: Theory and empirical tests'''
{{header}}
{{article
|author= DA Harrison,PP Mykytyn,CK Riemenschneider,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = The Theory of Planned Behavior (TPB) was used to explain and predict small business executives' decisions to adopt information technology (IT). These theories were tested in a multiphase field study involving 162 small businesses (25 less than or equal to n less than or equal to 200 employees) from a broad set of industries considering a variety of ITs. Results indicate strong support for a decision process based on attitude (perceived positive and negative consequences for the firm), subjective norm (social expectations), and perceived control (resources to overcome obstacles) regarding IT adoption. Additional variables such as firm and individual executive characteristics had no unique effect on adoption decisions. However, as business size increased, so did the importance of expectations from the (social) environment, while the importance of intra-firm consequences and control over potential adoption barriers declined.
|keyword = information technology usage in small business,theory of planned behavior,information technology adoption decisions,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: Information technology and investment incentives in distributed operations'''
{{header}}
{{article
|author= BR Nault,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = In distributed operations with positive externalities between branches, local underinvestment occurs because one branch does not account for the impact of its actions on other branches. Previous work found that an IT-enabled incentive mechanism called ''ownership of customers'' (OoC) reduced the problem of local underinvestment by accounting for inter-branch transactions. This report examines the impact of including investment by a central office on the set of previously developed results for local investment by branches. It shows that ownership of customers can reduce the problem of both central and local underinvestment. It also demonstrates how central investment can yield second-best levels of profitability-optimal profits given contracting problems in local investment with branches. It highlights how charging branches a unit fee to fund the needed level of central investment is consistent with that second-best solution.
|keyword = positive network externalities,centralization,decentralization,channels,ownership of customers,underinvestment,branch operations,franchising,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research report: The effectiveness of multiple dialogues in electronic brainstorming'''
{{header}}
{{article
|author= AR Dennis,JS Valacich,TA Carte,MJ Garfield,BJ Haley,JE Aronson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Members of brainstorming groups often pursue the same set of ideas rather than considering a wide and diverse range of ideas, which may reduce the number of ideas they produce. One way to reduce this cognitive inertia may be to encourage groups to engage in several simultaneous discussions or dialogues. This experiment, which studied groups brainstorming electronically, found that groups generated more ideas, more high-quality ideas, and more novel ideas when using multiple dialogues than when using single dialogues.
|keyword = brainstorming,idea generation,group support systems,groupware,cognitive inertia,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Communication richness in electronic mail: Critical social theory and the contextuality of meaning'''
{{header}}
{{article
|author= OK Ngwenyama,AS Lee,
|source= MIS QUARTERLY
|year= 1997
|abstract = Information Richness Theory (IRT) has enjoyed acceptance by information systems researchers throughout the last decade, but recent unfavorable empirical evidence has precipitated a shift away from it and a search for a new theory Because of this shift, a new definition of communication richness is needed to succeed the IRT definition. Since its inception, IS research on communication richness has been limited to the perspective of positivism and, more recently, interpretivism. In this study, a new perspective to the study of communication richness in computer mediated communication, critical social theory (CST), introduced. The paper outlines (1) a CST-based definition of communication richness and compares it with positivist and interpretivist definitions of communication richness and (2) a CST-based social action framework for empirical study of organizational communication in any media use situation. The CST definition and framework are used in an intensive investigation of an episode of the managerial use of electronic mail in a company illustrate how research on communication richness can be conducted from the CST-perspective. This illustration also points out the usefulness of the CST perspective in recognizing instances of communication richness in electronic mail communications that would escape detection in not just the IRT perspective in particular, but also positivist and interpretive perspectives in general. Finally, the paper concludes by outlining the potential for future research on organizational communication and information technology from the CST perspective. In addition to the specific contribution the development of a new theory of communication richness in electronic media, this study also contributes an example of CST research on IS and extends the domain of the CST-IS research program.
|keyword = computer mediated communication,critical social theory,media richness,qualitative research,organizational communication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring information systems service quality: Concerns on the use of the SERVQUAL questionnaire'''
{{header}}
{{article
|author= TP VanDyke,LA Kappelman,VR Prybutok,
|source= MIS QUARTERLY
|year= 1997
|abstract = A recent MIS Quarterly article rightfully points out that service is an important part of the role of the information systems (IS) department and that most IS assessment measures have a product orientation (Pitt et al. 1995). The article went on to suggest the use of an IS-context-modified version of the SERVQUAL instrument to assess the quality of the services supplied by an information services provider (Parasuraman et al. 1985, 1988, 1991).(2) However, a number of problems with the SERVQUAL instrument have been discussed in the literature (e.g., Babakus and Boiler 1992; Carman 1990; Cronin and Taylor 1992, 1994; Teas 1993). This article reviews that literature and discusses some of the implications for measuring service quality in the information systems context. Findings indicate that SERVQUAL suffers from a number of conceptual and empirical difficulties. Conceptual difficulties include the operationalization of perceived service quality as a difference or gap score, the ambiguity of the expectations construct, and the unsuitability of using a single measure of service quality across different industries. Empirical problems, which may be linked to the use of difference scores, include reduced reliability, poor convergent validity, and poor predictive validity. This suggests that (1) some alternative to difference scores is preferable and should be utilized; (2) ii used, caution should be exercised in the interpretation of IS-SERVQUAL difference scores; and (3) further work is needed in the development of measures for assessing the quality of IS services.
|keyword = IS management,evaluation,measurement,service quality,user attitudes,user expectations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring information systems service quality: Concerns for a complete canvas'''
{{header}}
{{article
|author= LF Pitt,RT Watson,CB Kavan,
|source= MIS QUARTERLY
|year= 1997
|abstract = This paper responds to the research note in this issue by Van Dyke et al. concerning the use of SERVQUAL, an instrument to measure service quality, and its use in the IS domain. This paper attempts to balance some of the arguments they raise from the marketing literature on the topic with the well-documented counterarguments of SERVQUAL's developers, as well as our own research evidence and observations in an IS-specific environment. Specifically, evidence is provided to show that the service qualify perceptions-expectations subtraction in SERVQUAL is far more rigorously grounded than Van Dyke et al. suggest; that the expectations construct, while potentially ambiguous, is generally a vector in the case of an IS department; and that the dimensions of service quality seem to be as applicable to the IS department as to any other organizational setting. Then, the paper demonstrates that the problems of reliability of difference score calculations in SERVQUAL are not nearly as serious as Van Dyke et al. suggest; that while perceptions-only measurement of service quality might have marginally better predictive and convergent validity, this comes at considerable expense to managerial diagnostics; and reiterate some of the problems of dimensional instability found in our previous research, highlighted by Van Dyke et al. and discussed in many other studies of SERVQUAL across a range of settings. Finally, four areas for further research in this area are identified.
|keyword = measurement,reliability,validity,service quality,marketing of IS,IS research agenda,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pragmatic perspectives on the measurement of information systems service quality'''
{{header}}
{{article
|author= WJ Kettinger,CC Lee,
|source= MIS QUARTERLY
|year= 1997
|abstract = In this research note, we join the debate between Van Dyke, Kappelman, and Prybutok and Pitt, Watson, and Kavan pertaining to the conceptual and empirical relevance of SERVQUAL as a measure of IS service quality. Adopting arguments from marketing, Van Dyke et al. (1997) question the SERVQUAL gap measurement approach, the interpretation and operationalization of the SERVQUAL expectation construct, and the reliability and validity of SERVQUAL dimensionality. In a response to those arguments, Pitt et al. (1997) defend their previous work (1995) in a point-by-point counterargument that suggests that the marginal empirical benefit of a perceptual-based (SERVPERF) service quality measure does not justify the loss of managerial diagnostic capabilities found in a gap measure. While siding with many of the positions taken by Pitt et al. (1997), we attempt to add value to the debate by presenting discrepancies we have with the two other research teams and by suggesting alternative approaches to resolve, or at least alleviate, problems associated with SERVQUAL. We believe that the theoretical superiority of an alternative IS service quality measure should be backed by empirical evidence in the IS context, hence answering some of the criticism by Van Dyke et al, and offering a construct valid version of the IS-adapted SERVQUAL. From a pragmatic viewpoint, we believe that the justification of using SERVQUAL's gap measure should be driven by more effective ways to utilize expectations in IS service management. To this end, we introduce the newer Parasuraman et al. (1994b) measures, the concept of a ''zone of tolerance'' for expectation management and an illustration of its practical use in an IS setting. Overall, we attempt to set the direction of where we think this debate should lead the IS field, namely, toward practical and timely IS service quality measures.
|keyword = IS service quality,measurement,SERVQUAL,SERVPERF,IS management,evaluation,user expectations,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Qualitative research in information systems'''
{{header}}
{{article
|author= MD Myers,
|source= MIS QUARTERLY
|year= 1997
|abstract = Qualitative research involves the use of qualitative data, such as interviews, documents, and participant observation, to understand and explain social phenomena As the focus of information systems research shifts from technological to managerial and organizational issues, qualitative research methods become increasingly useful. This example of ''living scholarship'' within MISQ Discovery's worldwide web archive prov,des an overview of qualitative research for the newcomer and a set of resources for those more experienced. The work discusses philosophical perspectives that can inform qualitative research, qualitative research methods, techniques, and modes of analysis. Links to citation lists, Internet resources, software tools, and calls for papers are also included.
|keyword = research methodology,action research,case study,ethnography,discourse analysis,hermeneutics,positivist perspective,interpretivist perspective,critical perspective,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Strategic choices in the development of interorganizational information systems'''
{{header}}
{{article
|author= V Choudhury,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = This paper discusses two critical strategic choices that a firm must make in developing interorganizational information systems (IOISs)-what kind(s) of IOIS(s) to develop, and how to develop the IOIS(s). With respect to the kinds of IOISs, it proposes a typology of three different types of IOISs: electronic monopolies, electronic dyads, and multilateral IOISs. With respect to development approach, a firm can develop an IOIS competitively or cooperatively. Based on a case study of the development of IOISs in the aircraft parts industry, two sets of transaction characteristics-demand uncertainty and market variability-were found to influence the choice of IOISs. Two factors-strategic significance of the IOIS, and size and bargaining power of the firm-were found to influence the choice of development approach. The observations from the case study are used to develop a set of propositions about when a firm will choose a particular IOIS and whether a cooperative or a competitive approach will be used.
|keyword = interorganizational information systems,aircraft parts industry,transaction characteristics,cooperation and competition,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Why is programming (sometimes) so difficult? Programming as scientific discovery in multiple problem spaces'''
{{header}}
{{article
|author= JW Kim,FJ Lerch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Our theoretical framework views programming as search in three problem spaces: rule, instance, and representation. The main objectives of this study are to find out how programmers change representation while working in multiple problem spaces, and how representation change increases the difficulty of programming tasks. Our theory of programming indicates that programming is similar to the way scientists discover and test theories. That is, programmers generate hypotheses in the rule space and test these hypotheses in the instance space. Moreover, programmers change their representations in the representation space when rule development becomes too difficult or alternative representations are available. We conducted three empirical studies with different programming tasks: writing a new program, understanding an existing program, and reusing an old program. Our results indicate that considerable cognitive difficulties stem from the need to change representations in these tasks. We conclude by discussing the implications of viewing programming as a scientific discovery for the design of programming environments and training methods.
|keyword = empirical studies of programmers,object-oriented programming,scientific discovery,multiple problem spaces,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Redesigning case retrieval to reduce information acquisition costs'''
{{header}}
{{article
|author= VS Mookerjee,MV Mannino,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = Retrieval of a set of cases similar to a new case is a problem common to a number of machine learning approaches such as nearest neighbor algorithms, conceptual clustering, and case based reasoning. A limitation of most case retrieval algorithms is their lack of attention to information acquisition costs. When information acquisition costs are considered, cost reduction is hampered by the practice of separating concept formation and retrieval strategy formation. To demonstrate the above claim, we examine two approaches. The first approach separates concept formation and retrieval strategy formation. To form a retrieval strategy in this approach, we develop the CRlc (case retrieval loss criterion) algorithm that selects attributes in ascending order of expected loss. The second approach jointly optimizes concept formation and retrieval strategy formation using a cost based variant of the ID3 algorithm (ID3(c)). ID3(c) builds a decision tree wherein attributes are selected using entropy reduction per unit information acquisition cost. Experiments with four data sets are described in which algorithm, attribute cost coefficient of variation, and matching threshold are factors. The experimental results demonstrate that (i) jointly optimizing concept formation and retrieval strategy formation has substantial benefits, and (ii) using cost considerations can significantly reduce information acquisition costs, even if concept formation and retrieval strategy formation are separated.
|keyword = case based systems,case retrieval algorithms,information costs,cost reduction,joint versus separate optimization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Examining the emergence of hybrid IS governance solutions: Evidence from a single case site'''
{{header}}
{{article
|author= CV Brown,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = The prior IS literature points to the importance of organizational context for predicting a firm's IS governance adopt a uniform IS governance solution for all business units and that this solution can be predicted by context variables at the overall organization level. The purpose of this study is to increase our knowledge about why firms implement a hybrid IS governance solution in which a subset of IS functions that includes systems development is decentralized to some business units, but not to other business units, in the same enterprise. A theoretical framework of context variables at the business unit level is first developed. An embedded, single case study provides an initial test of eight propositions derived from the framework, as well as an opportunity for theory building. Data are collected utilizing both deductive and inductive methods from IS and non-IS executives of a divisionalized Fortune 500 firm in which a uniform decentralized solution for systems development in place for almost a decade has recently been replaced by a hybrid solution. The case study findings suggest that a configuration of four variables characterizes a business unit context conducive to decentralized systems development governance (organic decision-making, high business unit autonomy, a differentiation competitive strategy, and an unstable industry environment). As predicted, however, the influence of these variables is likely to be overridden and a ''deviant'' solution adopted when deficiencies in IT capabilities are perceived and there is a culture that supports structural change at the business unit level. Additional interview and survey data collected from the key stakeholders are then analyzed in order to develop a richer understanding of the dimensions of the IT capabilities construct at the business unit level. The notion of absorptive capacity provides a theoretical argument for the emergent findings. Implications for researching today's increasingly complex IS governance forms are then drawn.
|keyword = organization design,structure of the IS function,IS centralization/decentralization,IS alignment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring software engineering evolution: A rasch calibration'''
{{header}}
{{article
|author= S Dekleva,D Drehmer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1997
|abstract = This investigation provides an empirical scaling of software engineering practices derived from the software process of Carnegie Mellon University (Humphrey et al. 1989). An analysis of data collected in an extended software maintenance study has shown that the responses to Humphrey's key software practice items fit the Rasch psychometric model providing an alternative framework in which to understand the software development practices. The Rasch model analysis describes the likelihood of a practice deployment for any level of evolution and provides precise and meaningful measures.
|keyword = measurement and scaling,software process maturity,software process evolution,Rasch model,software productivity,software quality,risks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Factors affecting the adoption of open systems: An exploratory study'''
{{header}}
{{article
|author= PYK Chau,KY Tam,
|source= MIS QUARTERLY
|year= 1997
|abstract = Advocates of open systems believe that problems related to compatibility, interoperability, scalability, and efficient use of IT resources can be resolved by setting software and hardware standards and strictly adhering to these standards in systems development and management. Representing a major departure from the traditional way of running an IS operation, the adoption of open systems has major ramifications on the IT infrastructure with longlasting effects. Unfortunately little research has been done to study this ubiquitous phenomenon despite its impacts on organizational computing worldwide. To fill this research gap, a model that incorporates seven factors perceived to affect the adoption is developed tested. In-depth interviews with senior executives responsible for managing corporate functions from 89 organizations were conducted to collect data for empirical analysis. findings suggest that organizations tend to focus more on their ''ability to adopt'' than the ''benefits from adoption,'' and (2) take ''reactive'' rather than ''proactive'' attitude a dop ting open systems technology Managerial implications are also discussed.
|keyword = information systems infrastructure,technology diffusion,open systems,IT adoption,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information specificity and environmental scanning: An economic perspective'''
{{header}}
{{article
|author= V Choudhury,JL Sampler,
|source= MIS QUARTERLY
|year= 1997
|abstract = This paper addresses two questions. First, how does an organization allocate its environmental scanning resources among all the potential sources of information in the environment? Second, how does an organization allocate responsibility for acquiring environmental information? Specifically, when does an organization choose to monitor an environmental source within its hierarchy, and when does it outsource the task? In the former case, when does the responsibility for acquiring information rest with the ultimate user, and when is it delegated, either to a subordinate or to a central environmental scanning unit? The paper proposes a set of economic arguments to answer these questions. Borrowing from transaction cost theory the paper develops the concept of information specificity to parallel the idea of asset specificity. Information specificity has two dimensions-knowledge specificity and time specificity. The paper uses transaction cost theory and agency theory to propose that the information acquisition choices made by managers and organizations are based on the specificity of the desired information. In making its arguments, the paper introduces the notion of cognitive transaction and agency costs to complement the behavioral costs that are the focus of traditional transaction cost and agency theory logic.
|keyword = information acquisition,information specificity,transaction cost theory,agency theory,specific and general knowledge,cognitive transaction costs,cognitive agency costs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Business process change: A study of methodologies, techniques, and tools'''
{{header}}
{{article
|author= WJ Kettinger,JTC Teng,S Guha,
|source= MIS QUARTERLY
|year= 1997
|abstract = Growth in Business Process Reengineering (BPR) consulting services has led to a proliferation of methods for conducting BPR. Sifting through vendor promotional hype and identifying a set of techniques and tools that best meets a particular projectAEs needs can be a daunting task. This article investigates BPR Methods, Techniques, and Tools (MTTs) and places them within an empirically derived reference framework. A comprehensive picture of BPR emerges that includes MTTs that help in reengineering strategy, people, management, structure, and technology dimensions of business processes. A BPR planning approach for customizing this framework based on unique project characteristics is then offered to assist in selecting those BPR project activities and techniques to be emphasized. This flexible framework and comprehensive survey of commonly used BPR techniques and tools forms a knowledge base to improve business process change practice and provides a basis for future BPR research.
|keyword = business process redesign,reengineering,methodology,techniques,organizational process change,impact and socio-technical systems design,IS career development,software tools,qualitative and quantitative methods,strategy,quality,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Searching and scanning: How executives obtain information from executive information systems'''
{{header}}
{{article
|author= B Vandenbosch,SL Huff,
|source= MIS QUARTERLY
|year= 1997
|abstract = Executive information systems may be used in different ways by managers in retrieving information. Two common modes of use are scanning, or general browsing of data; and focused search, or seeking answers to specific questions or well-defined problems. The purpose of this study was to investigate the antecedents of these two different modes of EIS use and also to examine their implications for perceived performance changes. The results show that, when executives focus their use of NS to answer specific questions or solve well-defined problems, they help to fine-tune operations and verify assumptions-in other words, they help to make the organization more efficient. However, an NS may also lead an executive to challenge fundamental managerial assumptions and preconceptions when using it to scan through information without having specific questions in mind. In this mode, an NS may be used to help formulate problems and foster creativity-thereby improving organizational effectiveness. EISs were found to contribute to gains in efficiency much more frequently than to gains in effectiveness. Companies that want to achieve greater effectiveness should pay attention to the role of the EIS in the scanning behavior of their managers. Factors that influenced the extent to which managers would engage their EIS in scanning included the extent to which NS scanning was undertaken by others in the organization and the characteristics of the NS itself.
|keyword = executive information systems,information search behavior,information scanning,focused information search,IS performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Discovery and representation of causal relationships in MIS research: A methodological framework'''
{{header}}
{{article
|author= B Lee,A Barua,AB Whinston,
|source= MIS QUARTERLY
|year= 1997
|abstract = The lack of theories and methodological weakness have been pointed out as two distinct but related problems in empirical management information systems (MIS) research. Reinforcing the existing belief that too much attention has been devoted to ''what'' as opposed to ''why'' or ''when'' relationships exist, this paper focuses on a subset of model building and methodology issues involving the systematic discovery and representation of causal relationships. Our analysis of the existing empirical MIS literature reveals the need to build richer causal models, to increase the flexibility of model representation, to integrate the isolated worlds of pure latent and pure manifested variables, and to provide a fighter linkage between the exploratory and confirmatory research phases. Based on philosophy of science and advances in the fields of experimental economics and sociology, we propose a foundation for developing richer models by explicitly considering the exogeneity and endogeneity of constructs and a manipulative account of causality, and by recognizing the role of incentives, agent, and organizational characteristics in MIS models. Since richer models require more flexible tools and techniques, the paper describes the representational shortcomings and statistical pitfalls of factor-analytic methods commonly deployed in empirical research. We suggest that weak exploratory phase tools and approaches may allow violations of causal assumptions to pass undetected to the confirmatory phase. Since confirmatory tools like LISREL also make factor-analytic assumptions, these violations are not likely to be detected at the confirmatory phase either. We propose using TETRAD, a non-parametric tool, at the exploratory phase for its ability to accommodate a wide variety of causal models. The findings are summarized within an integrated framework, which enhances the likelihood of discovering relationships through richer theoretical support and powerful exploratory analysis.
|keyword = MIS research methodology,causality,exogeneity,endogeneity,manipulative account,LISREL,TETRAD,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Rethinking ''diversity'' in information systems research'''
{{header}}
{{article
|author= I Benbasat,R Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Three types of diversity have been prominent in the Information Systems discipline for over a decade: (a) diversity in the problems addressed. (b) diversity in the theoretical foundations and reference disciplines used to account for IS phenomena; and (c) diversity in the methods used to collect, analyze, and interpret data. History has played a major part in encouraging IS researchers to use diversity as a means of countering criticisms of their discipline and increasing their research rigor and productivity. In particular, frequent recourse to reference disciplines has underpinned much of the research that has been undertaken since the early 1980s. There are now signs, however, that the level of diversity that currently exists in IS research may be problematic. In this paper, we consider some of the benefits and costs of allowing diversity to reign in the IS discipline. We also propose a structure that we hope will facilitate discourse on the benefits and costs of diversity and on the role that diversity should now play in the IS discipline.
|keyword = diversity,IS research,history,reference disciplines,paradigms,ethics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Research commentary: Diversity in information systems research: Threat, promise, and responsibility'''
{{header}}
{{article
|author= D Robey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = This paper confirms the evidence of diversity in information systems (IS) research and identifies the ways in which diversity both threatens and advances the field of IS. While advocating diversity within the field of IS, the paper also discusses the responsibilities that must be assumed by IS researchers. Responsibilities include a ''disciplined methodological pluralism'' (Landry and Banville 1992) in which researchers clearly justify their research aims, theories, and methods. Responsibilities also include researchers' commitment to collaborative ideals.
|keyword = research methods,information systems research,collaboration,scientific paradigms,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The calculus of reengineering'''
{{header}}
{{article
|author= A Barua,CHS Lee,AB Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Advances in new Information Technologies (IT) and changes in the business environment such as globalization and competitive pressure have prompted organizations to embark on reengineering projects involving significant investments in IT and business process redesign. However, the evidence of payoff from such investments can be classified as mixed as' best, a problem we partly attribute to the absence of a strong theoretical foundation to assess and analyze reengineering projects. We seek to apply complementarity theory and a business value modeling approach to address some questions involving what, when, and how much to reengineer. Complementarity theory is based on the notion that the value of having more of one factor increases by having more of another complementary factor. Further, related developments in the optimization of ''supermodular'' functions provide a useful way to maximize net benefits by exploiting complementary relationships between variables of interest. Combining this theory with a multi-level business value model showing relationships between key performance measures and their drivers, we argue that organizational payoff is maximized when several factors relating to IT, decision authority, business processes and incentives are changed in a coordinated manner in the right directions by the right magnitude to move toward an ideal design configuration. Our analysis further shows that when a complementary reengineering variable is left unchanged either due to myopic vision or self-interest, the organization will not be able to obtain the full benefits of reengineering due to smaller optimal changes in the other variables. We also show that by increasing the cost of changing the levels of design variables, unfavorable preexisting conditions (e.g., too much heterogeneity in the computing environment) can lead to reengineering changes of smaller magnitude than in a setting with favorable conditions.
|keyword = business value,complementarity,reengineering,organizational design,radical change,supermodularity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Facilitation, GSS, and training as sources of process restrictiveness and guidance for structured group decision making: An empirical assessment'''
{{header}}
{{article
|author= BC Wheeler,JS Valacich,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Structured decision techniques have been a mainstay of prescriptive decision theory for decades. Group Support Systems (GSSs) automate many of the features found in decision techniques, yet groups often choose to ignore both the technique and the technology in favor of more familiar decision processes. This research empirically tests propositions and hypotheses for a specific instantiation of Adaptive Structuration Theory. A controlled laboratory experiment tests the ability of three appropriation mediators (e.g., facilitation, GSS configuration, and training) to directively affect group decision making through guidance and restrictiveness. The experiment used a hidden-profile task and structured decision technique which directed group members to reach a decision by identifying the problem, choosing criteria, and selecting a solution. The results supported the proposition that appropriation mediators can increase the faithful use of structured decision techniques and that faithful use can improve decision quality.
|keyword = group decision support systems,group decision making,facilitation,structuration theory,restrictiveness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of communication mode and prediscussion information distribution characteristics on information exchange in groups'''
{{header}}
{{article
|author= R Hightower,L Sayeed,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = One advantage of groups is that they have access to a larger pool of expertise and knowledge than individual group members. However, groups are sometimes ineffective at exchanging information. This tendency has been called biased discussion. The present study examines the effects of communication mode (face-to-face vs. computer mediated), and Prediscussion information distribution characteristics on biased discussion. Biased discussion was found to occur to a greater degree when communication mode was computer-mediated, and the group members were not in conflict prior to the discussion.
|keyword = computer mediated communication systems (CMCS),group support systems (GSS),information sharing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The relationship of information system training methods and cognitive ability to end-user satisfaction, comprehension, and skill transfer: A longitudinal field study'''
{{header}}
{{article
|author= SJ Simon,V Grover,JTC Teng,K Whitcomb,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = This study compares traditional and nontraditional training techniques with regard to computer related training. Its purpose was to determine which training methods could best be utilized in computer related training to maximize a trainee's retention of material and transfer of learning. A field experiment was conducted using two hundred members of active duty U.S. Naval Construction Battalion as subjects. Evaluation of trainees included a pre-training screening, post-training evaluation (immediately after training), and a follow-up session (four weeks after the post-training session) utilizing previously validated instruments. Training treatments included instruction (lecture), exploration (independent study), and a nontraditional technique-behavior modeling (an enhanced combination of the other two methods). Performance outcomes were operationalized using hands-on task performance and comprehension of the computer system as dependent variables. End-user satisfaction with the computer system was also measured. Two covariates, cognitive ability and system use, were also introduced into the study. The use of hands-on training methods, especially behavior modeling, resulted in superior retention of knowledge, transfer of learning, and end-user satisfaction. Cognitive ability failed to be a good predictor of trainee success but a connection was established between training methodology, system use, and end-user satisfaction.
|keyword = computer training,end-user satisfaction,behavior modeling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Change agentry - the next IS frontier'''
{{header}}
{{article
|author= ML Markus,RI Benjamin,
|source= MIS QUARTERLY
|year= 1996
|abstract = We wrote this essay to stimulate IS specialists' efforts to become more effective - and more credible - agents of organizational change. The essay describes what we believe to be a view of the IS specialists' change-agent role that is very commonly held by IS specialists. We believe that this role, while well-intentioned and supported by structural conditions in IS work, often has negative consequences for organizations and for the credibility of IS specialists. Further, it does not fit the emerging structural condition, of IS. We describe two alternative models of what it means to be a change agent, their potential consequences, and the structural conditions that support or inhibit behavior in that role. We conclude that increased behavioral flexibility of IS specialists - the ability to switch roles in different circumstances - would improve organizational effectiveness and IS specialist credibility. Finally, we discuss the implications of our analysis for research, teaching, and practice.
|keyword = change management,change agent,IS function,IS personnel,IS management,IS implementation,IS education,future IS professionals,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The contribution of shared knowledge to IS group performance'''
{{header}}
{{article
|author= KM Nelson,JG Cooprider,
|source= MIS QUARTERLY
|year= 1996
|abstract = A major issue facing managers of information systems organizations is the increasing pressure to demonstrate the business value of the firm's investment in information technology. The working relationship between the IS department and other diverse organizational groups can have a major contribution to increasing IS performance. This paper explores the concept of shared knowledge between IS groups and their line customers as a contributor to IS performance. Shared knowledge is achieved through the mechanisms of mutual trust and influence between these groups. The relationship of mutual trust, influence, and shared knowledge with IS performance is tested empirically using path analysis in a study of 86 IS departments. The results of this study show that shared knowledge mediates the relationship between IS performance and trust and influence and that increasing levels of shared knowledge between IS and line groups leads to increased IS performance. Recommendations are given for ways managers can develop mutual trust and influence between these diverse groups and, therefore, achieve higher levels of shared knowledge and IS performance.
|keyword = shared knowledge,IS group performance,trust,influence,IS-user interaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information exchange and use in group decision making: You can lead a group to information, but you can't make it think'''
{{header}}
{{article
|author= AR Dennis,
|source= MIS QUARTERLY
|year= 1996
|abstract = Organizations often build groups with members from different areas so that a wider range of information and opinions can be considered. When members of such groups share the information they have, the group as a whole can access a larger Fool of information than any one member acting alone, potentially enabling them to make better decisions. This experiment studied groups working an a hidden profile task in which each participant received different (but not conflicting) information about the task, which they needed to combine to identify the optimal decision. Verbally interacting groups exchanged only a small portion of the available information and made poor decisions as a result. Groups interacting using a GSS exchanged about 50% more information, providing sufficient information to enable all groups to identify the optimal decision. However, GSS groups did not accurately process this information - only one GSS group chose the optimal decision. Possible explanations for this lack of information processing are that participants were unable to integrate into their existing base of information the information received during discussions, that the way in which the GSS was used impeded information processing, that the anonymity and delayed feedback in the GSS reduced the credibility of new information so that participants chose not to process it, or that information in the GSS was less salient than verbally contributed information.
|keyword = GSS,group support systems,information exchange,hidden profile,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer-based monitoring: Common perceptions and empirical results'''
{{header}}
{{article
|author= JF George,
|source= MIS QUARTERLY
|year= 1996
|abstract = Computer-based monitoring, the practice of collecting performance information on employees through the computers they use at work, continues to be a popular topic. How mush is known about computer-based monitoring as it is practiced in the workplace? Unfortunately, very little, even though much has been written on the subject. This article reports on five case studies of organizations that employ computer-based monitoring to collect performance data on clerical workers. Although all five organizations utilize similar data collection methods and procedures, no two organizations use the data collected in the same ways to evaluate employee performance. Each site reports different levels of employee satisfaction with monitoring, different abilities of employees to balance demands for work quantity and quality, different levels of work-related illnesses, and different perceptions of supervision. Although these results do not appear surprising on the surface, much of the popular literature on computer-based monitoring stresses the negative effects of monitoring on workers, no matter how or where it is implemented. In this study, the simple presence of computer-based monitoring was not enough to explain differences between sites. Rather, other factors, such as which data were used for evaluation and outside economic pressures, helped to explain variations in monitoring and its effects across sites. Computer-based monitoring, like other information technologies, is a malleable technology.
|keyword = computer-based monitoring,surveillance,work,stress,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Controlling prototype development through risk analysis'''
{{header}}
{{article
|author= RL Baskerville,J Stage,
|source= MIS QUARTERLY
|year= 1996
|abstract = This article presents a new approach to the management of evolutionary prototyping projects. The prototyping approach to systems development emphasizes learning and facilitates meaningful communication between systems developers and users. These benefits are important for rapid creation of flexible, usable information resources that are well-tuned to present and future business needs. The main unsolved problem in prototyping is the difficulty in controlling such projects. This problem severely limits the range of practical projects in which prototyping can be used. The new approach suggested in this article uses an explicit risk mitigation model and management process that energizes and enhances the value of prototyping in technology delivery. An action research effort validates this risk analysis approach as one that focuses management attention on consequences and priorities inherent in a prototyping situation. This approach enables appropriate risk resolution strategies to be placed in effect before the prototyping process breaks down. It facilitates consensus building through collaborative decision making and is consistent with a high degree of user involvement.
|keyword = action research,IS project risk management,IS control,IS management issues,IS development strategies,prototyping,iterative design,evolutionary design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The contribution of information technology to consumer welfare'''
{{header}}
{{article
|author= E Brynjolfsson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Over the past two decades, American businesses have invested heavily in information technology (IT) hardware. Managers often buy IT to enhance customer value in ways that are poorly measured by conventional output statistics. Furthermore, because of competition, firms may be unable to capture the full benefits of the value they create. This undermines researchers' attempts to determine IT value by estimating its contribution to industry productivity or to company profits and revenues. An alternative approach estimates the consumers' surplus from IT investments by integrating the area under the demand curve for IT. This methodology does not directly address the question of whether managers and consumers are purchasing the optimal quantity of II, but rather assumes their revealed willingness-to-pay for IT accurately reflects their valuations. Using data from the U.S. Bureau of Economic Analysis, we estimate four measures of consumers' surplus, including Marshallian surplus, Exact surplus based on compensated (Hicksian) demand curves, a ''nonparametric'' estimate, and a value based on the theory of index numbers. Interestingly, all four estimates indicate that in our base year of 1987, IT spending generated approximately $50 billion to $70 billion in net value in the United States and increased economic growth by about 0.3% per year. According to our estimates, which are likely to be conservative, IT investments generate approximately three times their cost in value for consumers.
|keyword = consumers' surplus,economic contributions,productivity,performance,econometrics,computers,growth,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pricing computer services under alternative control structures: Tradeoffs and trends'''
{{header}}
{{article
|author= S Dewan,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = This paper extends the analysis of the long-run pricing and capacity decision problem for shared computer services by Dewan and Mendelson (1990) and makes two further contributions. First, we show that simple marginal capacity cost pricing is often optimal in the absence of private user information, and it outperforms cost recovery and profit center pricing methods. Second, we provide insights into the implications of declining computing costs on the tradeoff between capacity costs and user time. In equilibrium, expected user delay costs are bounded by capacity costs due to the substitution of cheaper information processing capacity for valuable user time.
|keyword = pricing,computer services,marginal cost,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''On data representation and use in a temporal relational DBMS'''
{{header}}
{{article
|author= J Clifford,A Croker,A Tuzhilin,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Numerous proposals for extending the relational data model to incorporate the temporal dimension of data have appeared over the past-decade. It has long been known that these proposals have adopted one of two basic approaches to the incorporation of time into the extended relational model. Recent work formally contrasted the expressive power of these two approaches, termed temporally ungrouped and temporally grouped, and demonstrated that the temporally grouped models are more expressive. In the temporally ungrouped models, the temporal dimension is added through the addition of some number of distinguished attributes to the schema of each relation, and each tuple is ''stamped'' with temporal values for these attributes. By contrast, in temporally grouped models the temporal dimension is added to the types of values that serve as the domain of each ordinary attribute, and the application's schema is left intact. The recent appearance of TSQL2, a temporal extension to the SQL-92 standard based upon the temporally ungrouped paradigm, means that it is Likely that commercial DBMS's will be extended to support time in this weaker way. Thus the distinction between these two approaches-and its impact on the day-to-day user of a DBMS-is of increasing relevance to the database practitioner and the database user community. In this paper we address this issue from the practical perspective of such a user. Through a series of example queries and updates, we illustrate the differences between these two approaches and demonstrate that the temporally grouped approach more adequately captures the semantics of historical data.
|keyword = temporal relational databases,temporal query languages,temporal grouping,temporal relational completeness,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Inductive model analysis systems: Enhancing model analysis in decision support systems'''
{{header}}
{{article
|author= R Sharda,DM Steiger,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = After building and validating a decision support model, the decision maker frequently solves (often many times) different instances of the model. That is, by changing various input parameters and rerunning different model instances, the decision maker develops insight(s) into the workings and tradeoffs of the complex system represented by the model. The purpose of this paper is to explore inductive model analysis as a means of enhancing the decision maker's capabilities to develop insight(s) into the business environment represented by the model. The justification and foundation for inductive model analysis is based on three distinct literatures: 1) the cognitive science (theory of learning) literature, 2) the decision support system literature, and 3) the model management system literature. We also propose the integration of several technologies that might help the modeler gain insight(s) from the analysis of multiple model instances. Then we report on preliminary tests of a prototype built using the architecture proposed in this paper The paper concludes with a discussion of several research questions. Much of the previous MIS/DSS and management science research has focused on model formulation and solution. This paper posits that it is time to give more attention to enhancing model analysis.
|keyword = decision support systems,model management,cognitive science,inductive model analysis,insight,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The use and effects of knowledge-based system explanations: Theoretical foundations and a framework for empirical evaluation'''
{{header}}
{{article
|author= JS Dhaliwal,I Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Ever since MYCIN introduced the idea of computer-based explanations to the artificial intelligence community, it has come to be taken for granted that all knowledge-based systems (KBS) need to provide explanations. While this widely-held belief has led to much research on the generation and implementation of various kinds of explanations, there has been no theoretical basis to justify the use of explanations by KBS users. This paper discusses the role of KBS explanations to provide an understanding of both the specific factors that influence explanation use and the consequences of such use. The first part of the paper proposes a model based on cognitive learning theories to identify the reasons for the provision of KBS explanations from the perspective of facilitating user learning. Using the feedforward and feedback operators of cognitive learning the paper develops strategies for providing KBS explanations and classifies the various types of explanations found in current KBS applications. This second part of the paper presents a two-part framework to investigate empirically the use of KBS explanations. The first part of the framework focuses on the potential factors that influence the explanation seeking behavior of KBS users, including user expertise, the types of explanations provided and the level of user agreement with the KBS. The second part of the framework explores the potential effects of the use of KBS explanations and specifically considers four distinct categories of potential effects: explanation use behavior, learning, perceptions, and judgmental decision making.
|keyword = knowledge-based system explanations,expert systems,cognitive learning,feedforward and feedback information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Providing design assistance: A case-based approach'''
{{header}}
{{article
|author= AP Sinha,JH May,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = This paper presents an integrated and comprehensive framework for decision support. A model integrating case-based reasoning with constraint posting and multicriteria decision making is proposed for providing effective and efficient assistance in solving routine design problems. The model is developed based on an analysis of the knowledge acquired from experts in engineering design, and is subsequently operationalized as a computer-based design assistant called IDEA. IDEA employs constraint posting to initially bound the design space and to maintain consistency of the design solutions. Case-based reasoning allows IDEA to generate new designs by retrieving, adapting, and composing from similar cases in memory. Finally, IDEA optimizes multiple objectives to identify a set of pareto-optimal designs. By organizing computer memory as a collection of cases and case snippets, and by adapting and synthesizing those cases and snippets-using techniques similar to those employed by design experts-IDEA provides valuable design assistance. In addition to providing a framework for decision support, the research makes specific contributions to case-based design. It shows how case snippets can be retrieved, adapted, and synthesized to generate multiple design solutions, whose consistency is enforced through a dynamic constraint management mechanism. The concepts and techniques developed for performing dynamic adaptation (adaptation during composition from case snippets) and for maintaining an evolving solution space (a solution space that shrinks and expands over time) contribute to the state-of-the-art in case-based design.
|keyword = case-based reasoning,constraint posting,decision support,design assistance,multicriteria decision making,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The effect of codes of ethics and personal denial of responsibility on computer abuse judgments and intentions'''
{{header}}
{{article
|author= SJ Harrington,
|source= MIS QUARTERLY
|year= 1996
|abstract = This research asks whether codes of ethics affect computer abuse judgments and intentions of information systems (IS) employees. Codes of ethics examined include both company codes of ethics and those written specifically to deal with IS issues. In addition, since the intent of codes of ethics is to clarify responsibility and deter unethical behavior, both the psychological trait of responsibility denial and its moderating effect on codes was studied. While company codes did not affect the computer abuse judgments and intentions of all IS personnel, they did affect those IS personnel who tend to deny responsibility, thus suggesting that company codes may clarify responsibility and reduce rationalizations for some people. Unlike company codes, IS-specific codes of ethics had a direct effect on computer sabotage judgments and intentions, but had no differential effect on those high in responsibility denial. Finally, responsibility denial was directly related to all computer abuse judgments and intentions studied. Overall, codes had little effect on computer abuse judgments and intentions relative to the psychological trait of responsibility denial.
|keyword = computer crime,IS security,abuse and crime,deterrence,management of security,responsibility,codes of ethics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sustainable collaboration: Managing conflict and cooperation in interorganizational systems'''
{{header}}
{{article
|author= K Kumar,HG vanDissel,
|source= MIS QUARTERLY
|year= 1996
|abstract = Current interorganizational systems literature focuses on describing the role of information technology in enabling the transition from interfirm competition to cooperation. This article points out that the promise of IT enabled cooperation, if not nurtured, can degenerate into conflict. The objective of this article is to identify possible risks of conflict ill the IOS arena and to suggest strategies for minimizing the likelihood of such conflict. It does so by developing a typology for characterizing IOS along the dimension of interorganization interdependency in interfirm relationships. This typology classifies interorganizational systems into three types: pooled information resource IOS, value/supply chain IOS, and networked IOS. By examining the characteristics of these three types of IOSs the article identifies the economic, technical, and socio-political arguments for potential conflict in these systems. The identification of the risks, in turn, leads to a discussion of possible strategies for containing these risks. The article finally suggests that if the intended benefits of the collaboration are to be realized and sustained, corporate ''statesmen'' need to nurture the cooperation by anticipating these risks and managing them proactively.
|keyword = interorganizational systems,sustainability,organizational strategies,collaborative use of IT/IS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Expert systems usage: Task change and intrinsic motivation'''
{{header}}
{{article
|author= TG Gill,
|source= MIS QUARTERLY
|year= 1996
|abstract = What motivates use of an expert system? Recent studies have found that the anticipated performance benefits of using an expert system-such as increases in decision quality, consistency, and speed of decision making-can lead to increases in expected usage. But is motivation limited to performance benefits? Findings in job design theory suggest that other factors-such as increasing a user's sense of control over a task or making a task less routine-might also have an impact. If so, understanding these factors could be extremely valuable to managers seeking to build expert systems that will be readily accepted by users. This paper synthesizes findings from expert systems, information systems, and job design research to model how the task change experienced by an expert systems user during adoption can affect that user's motivation to continue using the system. Using existing task constructs from the job design literature, a simplified version of the model is operationalized and tested on a data set of expert systems (all constructed in the early and mid-1980s) for which extensive quantitative and qualitative task-change data was available, as well as data on systems usage. The findings suggest significant relationships between the nature of the task changes associated with adoption and long-term usage of the systems, ail consistent with the predictions of the job design literature. The study, therefore, concludes that a job design perspective of expert systems adoption can be a valuable tool in predicting user acceptance and, ultimately, systems usage.
|keyword = expert systems,implementation,information technology adoption,job design,job diagnostic survey,motivation,user resistance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring the extent of EDI usage in complex organizations: Strategies and illustrative examples'''
{{header}}
{{article
|author= B Massetti,RW Zmud,
|source= MIS QUARTERLY
|year= 1996
|abstract = As interest into the nature and value of Electronic Data Interchange (EDI) within organizations continues to grow, it becomes increasingly desirable to establish a tactical linkage between the strategic value of EDI and observed operational benefits. This article provides such a tactical linkage by presenting an approach to EDI measurement consisting of four facets: volume, diversity, breadth, and depth of a firm's EDI initiatives. Each of these facets is defined and then described through its application within the contexts of seven case sites, where each case site represents a strategic business unit having a long, successful history of EDI use. The article concludes with suggestions for both practice and research.
|keyword = EDI,interorganizational information systems,information systems usage,information systems measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Teledemocracy: Using information technology to enhance political work'''
{{header}}
{{article
|author= P Ytterstad,S Akselsen,G Svendsen,RT Watson,
|source= MIS QUARTERLY
|year= 1996
|abstract = Norway's Telenor Research and Development developed and implemented B communication system to support local politicians. The system's graphical interface makes it easy for politicians to make phone calls, set up telephone conferences, and send and receive email and faxes. An integrated document handler supports the exchange of documents between politicians and local government officers. A two-year field trial demonstrated that the system provides useful and effective information technology support for local politicians. The field trial also provides insights into furthering the development of teledemocracy.
|keyword = teledemocracy,interface design,graphical interface,local politics,telecommunications,field trial,participatory action research,telephony,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Sustaining process improvement and innovation in the information services function: Lessons learned at the Bose Corporation'''
{{header}}
{{article
|author= WL Harkness,WJ Kettinger,AH Segars,
|source= MIS QUARTERLY
|year= 1996
|abstract = Orchestrating programs of organizational transformation that result in sustained process improvement represents a difficult managerial challenge. Yet, ever-changing customer requirements, electronic partnerships, and increasingly complex intraorganizational arrangements are forcing many well-established firms to transform themselves from function-based forms of organization into process-based systems of managerial, task, and evaluative arrangements. Through a program of managed transformation, the Information Services (IS) function at Bose Corporation has realized dramatic improvements in the delivery of information products/services and is now ''charting the course'' for a sustained process management view that will define and measure business relationships well into the next century. In contrast to many well-publicized programs of change, the drive toward sustained process improvement and innovation by Bose IS resembles an evolutionary model of organizational learning and information sharing rather than a revolutionary model of immediate and drastic transformation. This study describes the defining stages, key events, and obstacles of the road traveled by Bose IS in transforming itself from a corporate utility into an enterprise-wide source of process innovation and improvement.
|keyword = IS service quality,process redesign,process innovation,organizational learning,change management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Are attributes entities? A study of database designers' memory structures'''
{{header}}
{{article
|author= R Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = A longstanding debate in the data modeling literature pertains to whether the grammars used to generate conceptual schemas should sustain a distinction between entities and attributes. The grammars used to generate entity-relationship diagrams and object-oriented conceptual models, for example, provide separate constructs for representing entities and attributes. The grammars used to generate binary data models, however, provide only a single construct for representing both entities and attributes. To sharpen the focus of the debate, a multi-trial free-recall experiment was conducted with database designers who had been trained primarily in a binary conceptual schema design methodology. In the experiment, the designers were first shown conceptual schema diagrams based on a binary model. The designers were then asked to recall the diagrams. Throughout their training as designers, they had been admonished to eschew any distinction between entities and attributes. Moreover, the diagrams they were shown in the experiment did not make a distinction between entities and attributes. Their recall protocols seemed to show, however, that they were considering some elements of the diagrams to be entities and others to be attributes. Their memory structures appear to reflect, therefore, that they perceive entities and attributes to be two distinct constructs.
|keyword = conceptual modelling,entities,attributes,objects,ontology,human memory,semantic networks,spreading activation,chunking,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alternative securities trading systems: Tests and regulatory implications of the adoption of technology'''
{{header}}
{{article
|author= EK Clemons,BW Weber,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Reasons for the mixed reactions to todays electronic off-exchange trading systems are examined, and regulatory implications are explored. Information technology (IT) could provide more automated markets, which have lower costs. Yet for an electronic trading system to form a liquid and widely used market, a sufficient number of traders would need to make a transition away from established trading venues and to this alternative way of trading. This transition may not actually occur for a variety of reasons. Two tests are performed of the feasibility and the desirability of transitions to new markets. In the first test, traders in a series of economic experiments demonstrate an ability to make a transition and develop a critical mass of trading activity in a newly opened market. In the second test, simulation is used to compare the floor-based specialist auction in place in most, U.S. stock exchanges today to a disintermediated alternative employing screen-based order matching. The results indicate that reducing the role of dealer-intermediaries can actually diminish important measures of market quality. Our findings suggest that the low trading volumes on many off-exchange systems do not result from traders' inability to break away from established trading floors. Rather, today's off-exchange trading systems are not uniformly superior to the trading mechanisms of traditional exchanges. Thus, regulatory actions favoring off-exchange trading systems are not warranted; but, improved designs for IT-based trading mechanisms are needed, and when these are available, they are likely to win significant trading volume from established exchanges.
|keyword = electronic markets,trading systems,experimental economics,technology adoption,financial market simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A computational study of distributed rule learning'''
{{header}}
{{article
|author= R Sikora,MJ Shaw,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = This report is concerned with a rule learning system called the Distributed Learning System (DLS). Its objective is two-fold: First, as the main contribution, the DLS as a rule-learning technique is described and the resulting computational performance is presented, with definitive computational benefits clearly demonstrated to show the efficacy of using the DLS. Second, the important parameters of the DLS are identified to show the characteristics of the Group Problem Solving (GPS) strategy as implemented in the DLS. On one hand this helps us pinpoint the critical designs of the DLS for effective rule learning; on the other hand this analysis can provide insight into the use of GPS as a more general rule-learning strategy.
|keyword = Group Problem Solving,genetic algorithms,group learning,hybrid learning system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information acquisition and mental models: An investigation into the relationship between behaviour and learning'''
{{header}}
{{article
|author= B Vandenbosch,C Higgins,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = A cognitive learning perspective is used to develop and test a model of the relationship between information acquisition and learning in the executive support systems (ESS) context. The model proposes two types of learning: mental model maintenance in which new information fits into existing mental models and confirms them; and mental model building in which mental models are changed to accommodate new information. It also proposes that information acquisition objectives determine the type of learning that is possible. When ESS are used to answer specific questions or solve well-defined problems, they help to fine-tune operations and verify assumptions-in other words, they help to maintain current mental models. However, ESS may be able to challenge fundamental assumptions and help to build new mental models if executives scan through them to help formulate problems and foster creativity. Thirty-six interviews with executive ESS users at seven organizations and a survey of 361 users at 18 additional organizations are used to develop scales to measure the model's constructs and provide support for its relationships. These results support the models prediction that mental model building is more likely with scanning than with focused search. ESS also appear to contribute to mental model maintenance much more often than they do to mental model building. Without a clear focus on mental model building, it seems that business as usual is the more likely outcome.
|keyword = mental models,learning,focused search,scanning,executive support systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Overcoming ineffective mental representations in base-rate problems'''
{{header}}
{{article
|author= MC Roy,FJ Lerch,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = Many biases have been observed in probabilistic reasoning, hindering the ability to follow normative rules in decision-making contexts involving uncertainty. One systematic error people make is to neglect base rates in situations where prior beliefs in a hypothesis should be taken into account when new evidence is obtained. Incomplete explanations for the phenomenon have impeded the development of effective debiasing procedures or tools to support decision making in this area. In this research, we show that the main reason behind these judgment errors is the causal representation induced by the problem context. In two experiments we demonstrate that people often possess the appropriate decision rules but are unable to apply them correctly because they have an ineffective causal mental representation. We also show how this mental representation may be modified when a graph is used instead of a problem narrative. This new understanding should contribute to the design of better decision aids to overcome this bias.
|keyword = base-rate fallacy,representational aid,mental representation,decision support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Process structuring in electronic brainstorming'''
{{header}}
{{article
|author= AR Dennis,JS Valacich,T Connolly,BE Wynne,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = One aspect of brainstorming that has received little research attention is how the brainstorming problem should be presented to the group, whether as one all-encompassing question or as a series of separate questions each focsusing on one aspect of the problem. This paper reports the results of two experiments in which subjects (MBAs in the first, senior executives in the second) electronically brainstormed on intact problems (where all parts of the problem were presented simultaneously) or on decomposed problems (where three subcategories of the problem were sequentially posed to the groups). In both experiments, groups using the decomposed process generated 60% more ideas. We attribute these differences to the ability of time constraints to increase the rate of idea generation, and the ability of problem decomposition to refocus members' attention more evenly across the entire problem.
|keyword = group support systems (GSS),electronic brainstorming,decomposition,structure,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Productivity, business profitability, and consumer surplus: Three different measures of information technology value'''
{{header}}
{{article
|author= LM Hitt,E Brynjolfsson,
|source= MIS QUARTERLY
|year= 1996
|abstract = The business value of information technology (IT) has been debated for a number of years. While some authors have attributed large productivity improvements and substantial consumer benefits to IT, others report that IT has not had any bottom line impact on business profitability. This paper focuses on the fact that while productivity, consumer value, and business profitability are related they are ultimately separate questions. Accordingly, the empirical results on IT value depend heavily on which question is being addressed and what data are being used. Applying methods based on economic theory, we are able to define and examine the relevant hypotheses for each of these three questions, using recent firm-level data on IT spending by 370 large firms. Our findings indicate that IT has increased productivity and created substantial value for consumers. However, we do not find evidence that these benefits have resulted in supranormal business profitability. We conclude that while modeling techniques need to be improved, these results are collectively consistent with economic theory. Thus, there is no inherent contradiction between increased productivity, increased consumer value, and unchanged business profitability.
|keyword = IT productivity,business profitability,IS investment,economic theory,consumer surplus,computers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Relational development in computer-supported groups'''
{{header}}
{{article
|author= L Chidambaram,
|source= MIS QUARTERLY
|year= 1996
|abstract = This study examines how group attitudes and outcomes evolve over time with repeated use of a group support system. Social Information Processing (SIP) theory, which suggests that relational intimacy may take longer to develop in computer-supported groups, was used as the basis for testing a temporally bounded model of group behavior. The basic argument underlying this model is that computer-supported groups, given adequate time, will exchange enough social information to develop strong relational links. Thus, while computer support was expected to limit group interactions initially, the model predicted that, over a period of time, such constraints would dissipate. The results show evidence of such shifts among computer-supported groups. Attitudes of GSS users changed over time from highly negative to somewhat positive; outcomes improved more slowly. The turnaround in attitudes of users-toward each other and the interaction process-support the SIP perspective that repeated use of computer support despite some inherent initial restrictions-can help groups affiliate.
|keyword = group support systems (GSS),social information processing,media richness,affiliation motive in groups,relational development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information privacy: Measuring individuals' concerns about organizational practices'''
{{header}}
{{article
|author= HJ Smith,SJ Milburg,SJ Burke,
|source= MIS QUARTERLY
|year= 1996
|abstract = Information privacy has been called one of the most important ethical issues of the information age. Public opinion polls show rising levels of concern about privacy among Americans. Against this backdrop, research into issues associated with information privacy is increasing. Based on a number of preliminary studies, it has become apparent that organizational practices, individuals' perceptions of these practices, and societal responses are inextricably linked in many ways. Theories regarding these relationships are slowly emerging. Unfortunately, researchers attempting to examine such relationships through confirmatory empirical approaches may be impeded by the lack of validated instruments for measuring individuals' concerns about organizational information privacy practices. To enable future studies in the information privacy research stream, we developed and validated an instrument that identifies and measures the primary dimensions of individuals' concerns about organizational information privacy practices. The development process included examinations of privacy literature; experience surveys and focus groups; and the use of expert judges. The result was a parsimonious 15-item instrument with four subscales tapping into dimensions of individuals' concerns about organizational information privacy practices. The instrument was rigorously tested and validated across several heterogenous populations, providing a high degree of confidence in the scales' validity, reliability, and generalizability.
|keyword = privacy,LISREL,ethical issues,measures,reliability,validity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology for local administration support: The governorates project in Egypt'''
{{header}}
{{article
|author= SR Nidumolu,SE Goodman,DR Vogel,AK Danowitz,
|source= MIS QUARTERLY
|year= 1996
|abstract = Experience with information technology (IT) implementation in the local administrations of less developed countries (LDCs) has been largely disappointing. Conventional wisdom suggests that such implementation efforts are usually inappropriate to the information-poor environments of many LDCs. This study describes the Governorates Project in Egypt, which seems to have been an encouraging exception to such ''wisdom.'' The project, which was initiated in 1987 by the Egyptian Cabinet's Information and Decision Support Center (IDSC), represented a significant administrative and technological innovation because it sought to implement an IDSC in each of the 27 governorates of Egypt. The purpose of each governorate IDSC was to provide computer-based information and decision support to the governor and other local administrators. Based on our findings, three stages of the project are identified-implementation, evaluation, and transformation of the innovation. Three theoretical perspectives derived from past research, i.e., functional, political/symbolic, and social information processing, were used to explain the project outcomes, such as the governors' perceptions and behaviors concerning their IDSCs. Results suggest that the symbolic/political and social information processing perspectives had considerable power in explaining the outcomes during implementation, whereas the functional perspective was particularly effective in explaining the outcomes during evaluation and transformation. The theoretical framework and findings suggest considerable potential for understanding IT implementations in both business and administrative settings.
|keyword = IS implementation,IS technology transfer,local government,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Key issues in information systems management: 1994-95 SIM delphi results'''
{{header}}
{{article
|author= JC Brancheau,BD Janz,JC Wetherbe,
|source= MIS QUARTERLY
|year= 1996
|abstract = Over the past 15 years, the Society for Information Management (SIM) has periodically surveyed its members to determine the most critical issues in IS management. Again in 1994-95, SIM institutional and board members were asked to consider what they felt were the most critical issues facing IS executives over the next three to five years. Signaling an evolutionary shift in IS management, this study shows that business relationship issues have declined in importance compared to technology infrastructure issues. For IS executives and general managers, the key issue framework suggests some general directions for emphasis and provides a coarse measure for benchmarking their own concerns against those of their peers. The results df this study also impact educational missions in teaching and research to the extent that they need to be sensitive to the views of practicing IS executives.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The role of IT in the transformation of work: A comparison of post-industrial, industrial, and proto-industrial organization'''
{{header}}
{{article
|author= SJ Winter,SL Taylor,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = The mid-twentieth century was marked by the dominance of large, stable, centralized business; the late twentieth century is marked by a downsizing and disaggregation of the firm. This manuscript investigates this change and considers some possible causes for it using historical analysis. In order to explore the causes of the post-industrial organization of work, we compare it to the history of the industrial organization of work and to the early or proto-industrial system of artisanal and ''putting out'' manufacturing. We identify strong similarities between post-industrialization, which has been attributed to the use of Information Technology (IT) and an information-based economy; proto-industrialization, which was a goods-based manufacturing economy with little IT; and flexible specialization, a form of workplace organization common to the early industrial period and surviving in some areas today. These similarities cast doubt on the argument that the causal link between technology and the organization of work is a simple or direct one. We also review the Literature on the role of technology in the organization of work in all three eras and determine that there is little support for technological determinism in any of them. Alternative determinants of organizational structures and avenues for future research are suggested.
|keyword = new organizational forms,historical analysis,information systems,organizational structure,industrialization,putting-out systems,post-industrial,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and organizational change in the British census, 1801-1911'''
{{header}}
{{article
|author= M CampbellKelly,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = The first British census was taken in 1801 and was processed by a handful of clerks in a tiny office. By the mid-1800s, the census had evolved into an elaborate Victorian data-processing operation involving over a hundred clerks, each of whom had a specialized information-processing role. In 1911 the census was mechanized and the routine data processing was taken over by punched-card machines. This paper explores the changes in information technology within the census over a period of more than a century, and the resulting organizational changes. A contrast is drawn with the U.S. census-which mechanized in 1890-on the adoption of new technology.
|keyword = information technology,census,tabulation,punched-card machinery,organizational change,organizational memory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology in the police context: The ''sailor'' phone'''
{{header}}
{{article
|author= PK Manning,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = his paper examines the use of the cellular telephone in police agencies as an example of 'low tech' innovation in information technology. It draws on qualitative data, including interviews, focus group discussions, and first-hand observations in American police agencies to illustrate the impact of cellular phones on the social organization of police work in the early 1990s. Dramaturgical analysis-the study of the selective use of messages to communicate to an audience-frames the study (Goffman 1959, Burke 1962). Dramaturgy reveals how the emergent meanings of information technology arising from changes in communication and symbolization shape work processes and authority. Significant differences in response to and use of the technology are discovered, and are best understood as consistent with the impressions members of the organization wish to convey to particular audiences. Technology both shapes and is shaped by organizational routines and structures. A natural history approach, which traces the changing impacts of technology, is needed to further specify studies of organizational adaptation to changes in information technology.
|keyword = cellular phones,dramaturgy,police work,routines,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Improvising organizational transformation over time: A situated change perspective'''
{{header}}
{{article
|author= WJ Orlikowski,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = In this paper, I outline a perspective on organizational transformation which proposes change as endemic to the practice of organizing and hence as enacted through the situated practices of organizational actors as they improvise, innovate, and adjust their work routines over time. I ground this perspective in an empirical study which examined the use of a new information technology within one organization over a two-year period. In this organization, a series of Subtle but nonetheless significant changes were enacted over time as organizational actors appropriated the new technology into their work practices, and then experimented with local innovations, responded to unanticipated breakdowns and contingencies, initiated opportunistic shifts in structure and coordination mechanisms, and improvised various procedural, cognitive, and normative variations to accommodate their evolving use of the technology. These findings provide the empirical basis for a practice-based perspective on organizational transformation. Because it is grounded in the micro-level changes that actors enact over time as they make sense of and act in the world, a practice lens can avoid the strong assumptions of rationality, determinism, or discontinuity characterizing existing change perspectives. A situated change perspective may offer a particularly useful strategy for analysing change in organizations turning increasingly away from patterns of stability, bureaucracy, and control to those of flexibility, self-organizing, and leaning.
|keyword = groupware,improvisation,situated practice,technology-based organizational change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Transforming work through information technology: A comparative case study of geographic information systems in county government'''
{{header}}
{{article
|author= D Robey,S Sahay,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = A comparative case study was designed to assess the consequences of implementing a particular geographic information system (GIS) organizations. Respondents reported radically different experiences with, and consequences of, the GIS technology. In North County, participants considered GIS to be responsible for transforming the way that work was accomplished and for changing patterns of communication among departments. In South County, the same GIS technology was implemented with little social consequence. These divergent outcomes are associated with differences in four specific processes related to the implementation of the GIS in the two organizations: initiation, transition, deployment and spread of knowledge. In North County, implementation was initiated by an influential group of users (geographers) who positioned the technology as a shared resource that built upon existing competencies. A distributed configuration was deployed in North County, and conceptual knowledge about GIS was disseminated widely. By contrast, in South County GIS was initiated by a centralized data processing department as one of many revenue producing services. Transition to GIS in South County required a departure from existing competencies, and it was deployed as a centralized system with limited procedural knowledge spread among the potential user community. Taken together, these findings suggest that implementation processes that advance users' learning about potentially transformational technologies are likely to result in perceived transformation. The theoretical perspective of organizational learning is, therefore, suggested as a guide for future research on the role of information technology in organizational transformation.
|keyword = organizational impacts of information technology,IS implementation,organizational learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Steps toward an ecology of infrastructure: Design and access for large information spaces'''
{{header}}
{{article
|author= SL Star,K Ruhleder,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1996
|abstract = We analyze a large-scale custom software effort, the Worm Community System (WCS), a collaborative system designed for a geographically dispersed community of geneticists. There were complex challenges in creating this infrastructural tool, ranging from simple lack of resources to complex organizational and intellectual communication failures and tradeoffs. Despite high user satisfaction with the system and interface, and extensive user needs assessment, feedback, and analysis, many users experienced difficulties in signing on and use. The study was conducted during a time of unprecedented growth in the Internet and its utilities (1991-1994), and many respondents turned to the World Wide Web for their information exchange. Using Bateson's model of levels of learning, we analyze the levels of infrastructural complexity involved in system access and designer-user communication. We analyze the connection between systems development aimed at supporting specific forms of collaborative knowledge work, local organizational transformation, and large-scale infrastructural change.
|keyword = infrastructure,collaboratory,organizational computing,participatory design,ethnography,Internet,scientific computing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Issues and concerns about computer-supported meetings: The facilitator's perspective'''
{{header}}
{{article
|author= F Niederman,CM Beise,PM Beranek,
|source= MIS QUARTERLY
|year= 1996
|abstract = In an effort to boost meeting productivity and success, managers may employ trained group facilitators. They may also implement group support systems (GSS) for the same reason. The two approaches can be taken separately or together. In this study, in-depth interviews with 37 practicing facilitators provided their perspectives on critical factors that influence meeting success and potential benefits and concerns with the use of GSS. Respondents focused on a core of communication and group process skills as critical for facilitator success. Overall, the respondents observed or anticipated more efficient and effective task performance as benefits of GSS technology. Their concerns focused on technology-related issues: participant anxiety, systems inflexibility, and systems reliability Views of facilitators with high and low levels of GSS experience are largely consistent. High-experience GSS facilitators viewed technical issues as more central to meeting success, while low-experience GSS facilitators focused more heavily on attributes of the group. The paper concludes by offering suggestions for identifying and training GSS facilitators and comments on key issues of importance to GSS designers, based on the facilitator's perspective.
|keyword = group support systems,electronic meetings,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of commitment to information systems development: A longitudinal investigation'''
{{header}}
{{article
|author= M Newman,R Sabherwal,
|source= MIS QUARTERLY
|year= 1996
|abstract = Commitment to an information systems (IS) development project is widely believed to affect the eventual success of the system. Problems arising from low commitment have also been described. However, there has been little research on the factors that influence the level of commitment to an IS project. This paper provides some initial insights into the determinants of commitment based on a longitudinal study of an IS project that was stopped and then restarted on several occasions over a 17-year period (1975-1992). The paper draws four types of determinants - project, psychological, social, and structural - from the organization behavior literature and uses them to explain six decisions that were made during the 17-year period. A comparison of these six decisions suggests that project determinants play a central role during the initial commitment decision, but the other determinants assume greater importance in later stages. Moreover, it seems that in this case study, project and psychological determinants affected the decision to increase commitment, whereas social and structural determinants influenced the decision to withdraw commitment to the project. Some implications for practice and future research are examined.
|keyword = systems development,project management,commitment,conflict,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Measuring the linkage between business and information technology objectives'''
{{header}}
{{article
|author= BH Reich,I Benbasat,
|source= MIS QUARTERLY
|year= 1996
|abstract = The establishment of linkage between business and information technology objectives has consistently been reported as one of the key concerns of information systems (IS) managers. The two objectives of this paper are: (1) to clarify the nature of the linkage construct, and (2) to report on a project that developed and tested measures of the social dimension of linkage. According to our research, the linkage construct has two dimensions: 1. Intellectual: the content of information technology and business plans are internally consistent and externally valid. 2. Social: the IS and business executives understand each others' objectives and plans. We conducted a study of measurement issues associated with the social dimension of linkage. The following candidate measures of linkage were examined: 1. Cross references between written business and information technology plans; 2. IS and business executives' mutual understanding of each other's current objectives; 3. Congruence between IS and business executives' long-term visions for information technology deployment; 4. Executives' self-reported rating of linkage. Data were collected from 10 business units in three large Canadian life insurance companies. In addition to examining written documents such as strategic plans and minutes of steering committee meetings, extensive interviews were conducted with information systems and business unit executives. Based on this data, understanding of current objectives and shared vision for the utilization of information technology are proposed as the most promising potential measures for short- and long-term aspects of the social dimension of linkage, respectively. With some precautions, self-reports may also be used as a surrogate measure for short-term linkage.
|keyword = IS strategic planning,alignment of IS plans with business plans,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An empirical examination of the value of creativity support systems on idea generation'''
{{header}}
{{article
|author= B Massetti,
|source= MIS QUARTERLY
|year= 1996
|abstract = Because organizations seek more innovative ways to compete, the ability of their employees to generate new and valuable ideas becomes a fundamental survival skill. To the extent that computer software might enhance the creative performance of individual users, organizations might ultimately apply such tools to enhance the creative performance of their employees. A controlled laboratory experiment was performed to determine whether two popular creativity support applications significantly enhanced the creative performance of individual users. The results suggest that responses generated with software support are significantly more novel and valuable than responses generated by pen and paper. The results also question the previous creativity research practice of not directly controlling for idea fluency prior to experimental manipulation. It is hoped the findings from this investigation can be used to improve individual creative performance, further research concerning factors relevant to creativity, and guide future ICSS development efforts.
|keyword = DSS,software quality,brainstorming,software packages,interface characteristics,user satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reconciling variance and process strategies for studying information system development'''
{{header}}
{{article
|author= R Sabherwal,D Robey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = Information systems researchers commonly describe variance and process strategies for studying information system development (ISD) as alternatives that may be difficult to reconcile. In this paper, we argue that it is possible to reconcile these two strategies, despite the clear differences that exist between them. Some possible methods of combining variance and process strategies are examined, the most powerful of which jointly applies these strategies while maintaining their distinct forms. This method is used in this paper, with variance strategy being implemented using levels of participation of key actors and process strategy being implemented using sequences of actions. Based on empirical analysis of 50 ISD projects, five clusters of ISD processes are examined. Results show that projects that are similar based on levels of participation are also similar based on event sequences, thus indicating that variance and process strategies can be reconciled. The insights that variance strategy, process strategy, and joint application provide into each cluster are examined.
|keyword = research strategies,information system development,event sequences,variance strategy,process strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Improving the performance stability of inductive expert systems under input noise'''
{{header}}
{{article
|author= VS Mookerjee,MV Mannino,R Gilson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = Inductive expert systems typically operate with imperfect or noisy input attributes. We study design differences in inductive expert systems arising from implicit versus explicit handling of input noise. Most previous approaches use an implicit approach wherein inductive expert systems are constructed using input data of quality comparable to problems the system will be called upon to solve. We develop an explicit algorithm (ID3(ecp)) that uses a clean (without input errors) training set and an explicit measure of the input noise level and compare it to a traditional implicit algorithm, ID3(p) (the ID3 algorithm with the pessimistic pruning procedure). The novel feature of the explicit algorithm is that it injects noise in a controlled rather than random manner in order to reduce the performance variance due to noise. We show analytically that the implicit algorithm has the same expected partitioning behavior as the explicit algorithm. In contrast, however, the partitioning behavior of the explicit algorithm is shown to be more stable (i.e., lower variance) than the implicit algorithm. To extend the analysis to the predictive performance of the algorithms, a set of simulation experiments is described in which the average performance and coefficient of variation of performance of both algorithms are studied on real and artificial data sets. The experimental results confirm the analytical results and demonstrate substantial differences in stability of performance between the algorithms especially as the noise level increases.
|keyword = inductive expert systems,input data noise,performance stability,variance reduction,controlled scrambling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Using treemaps to visualize the Analytic Hierarchy Process'''
{{header}}
{{article
|author= T Asahi,D Turo,B Shneiderman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = Treemaps, a visualization method for large hierarchical data spaces, are used to augment the capabilities of the Analytic Hierarchy Process (AHP) for decision-making. Two direct manipulation tools, presented metaphorically as a ''pump'' and a ''hook,'' were developed and applied to the treemap to support AHP sensitivity analysis, Users can change the importance of criteria dynamically on the two-dimensional treemap and immediately see the impact on the outcome of the decision, This fluid process dramatically speeds up exploration and provides a better understanding of the relative impact of the component criteria, A usability study with six subjects using a prototype AHP application showed that treemap representation was acceptable from a visualization and data operation standpoint.
|keyword = visualization,treemap,Analytic Hierarchy Process, AHP,decision support,user interfaces,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The emergence of interpretivism in IS research'''
{{header}}
{{article
|author= G Walsham,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = This paper investigates aspects of the history and current state of interpretivism in IS research. The emergence of interpretivism is explored through the identification of a network of IS researchers working in the interpretive tradition, through an examination of the role of mainstream and alternative IS journals, and through an analysis of the rhetoric used to support interpretive claims. The paper contributes to analysis of the development of the IS field as a whole, and provides some conceptual ideas and a reference point for further work in this relatively neglected area of research.
|keyword = interpretivism,construction of IS field,IS journals,rhetorical analysis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Pulling the plug: Software project management and the problem of project escalation'''
{{header}}
{{article
|author= M Keil,
|source= MIS QUARTERLY
|year= 1995
|abstract = Information technology (IT) projects can fail for any number of reasons and in some cases can result in considerable financial losses for the organizations that undertake them. One pattern of failure that has been observed but seldom studied is the IT project that seems to take on a life of its own, continuing to absorb valuable resources without reaching its objective. A significant number of these projects will ultimately fail, potentially weakening a firm's competitive position while siphoning off resources that could be spent developing and implementing successful systems. The escalation literature provides a promising theoretical base for explaining this type of IT failure. Using a model of escalation based on the literature, a case study of IT project escalation is discussed and analyzed. The results suggest that escalation is promoted by a combination of project, psychological, social, and organizational factors. The managerial implications of these findings are discussed along with prescriptions for how to avoid the problem of escalation.
|keyword = software project management,IS failure,escalation,escalating commitment,implementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Added value and pricing with information technology'''
{{header}}
{{article
|author= BR Nault,AS Dexter,
|source= MIS QUARTERLY
|year= 1995
|abstract = This study evaluates the extent to which the added value to customers from a supplier's application of information technology (IT) is manifested through premium prices of a traded good. The study demonstrates that IT can add value to an otherwise undifferentiated good and shows how these benefits accrue to customers from the adoption of IT. Analyzing a case in which the traded good is a homogeneous commodity-commercial fueling-our study shows that the critical impacts of IT are convenience and control-that is, convenience that provides improved access to fuel and control that reduces problems of delegating purchasing authority for the customer. The value of this additional service is exhibited in premium prices customers are willing to pay for the IT-enhanced traded good, relative to the same good without IT. Compared to the price without IT, statistical analysis of the supplier's pricing history demonstrates that the application of IT to commercial fuel yielded price premiums of between five and 12 percent of the retail fuel price.
|keyword = economic impacts,interorganizational systems,pricing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic data interchange and small organizations: Adoption and impact of technology'''
{{header}}
{{article
|author= CL Iacovou,I Benbasat,AS Dexter,
|source= MIS QUARTERLY
|year= 1995
|abstract = Many EDI researchers and practitioners have recognized the importance of high penetration levels for the success of EDI. Unfortunately, such penetration is partly impeded by the resistance of small companies to become EDI capable. To investigate this issue, three major factors are identified that influence the EDI adoption practices of small firms. These factors are: organizational readiness (because of the low levels of IT sophistication and resource availability of small firms), external pressures to adopt (because of the weak market positions of small firms and the network nature of the technology), and perceived benefits (because of the limited impact that IT has on small firms due to under-utilization and lack of integration). By combining the anticipated effects of these factors, we developed a framework of EDI adoption by small businesses. The applicability of this framework is empirically demonstrated using the results of seven case studies. Finally, recommendations are made for the development of successful EDI partner expansion plans. These include the development of a long-term EDI partner expansion plan from the very beginning, the individual assessment of each partner's EDI preparedness level, and the selection of appropriate influence tactics to expedite adoption by small partners. Specifically, if is suggested that EDI initiators pursue promotional efforts to improve partners' perceptions of EDI benefits, provide financial and technological assistance to partners with low organizational readiness, and carefully select and enact influence strategies to reduce resistance.
|keyword = electronic data interchange,interorganizational systems,small business,electronic commerce,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information technology and sustained competitive advantage: A resource-based analysis'''
{{header}}
{{article
|author= FJ Mata,WL Fuerst,JB Barney,
|source= MIS QUARTERLY
|year= 1995
|abstract = The concept of IT as a powerful competitive weapon has been strongly emphasized in the literature, yet the sustainability of the competitive advantage provided by IT applications is not well-explained. This work discusses the resource-based theory as a means of analyzing sustainability and develops a model founded on this resource-based view of the firm. This model is then applied to four attributes of IT-capital requirements, proprietary technology, technical IT skills, and managerial IT skills-which might be sources of sustained competitive advantage. From this resource-based analysis, we conclude that managerial IT skills is the only one of these attributes that can provide sustainability.
|keyword = competitive advantage,resource-based theory,IT resources,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information systems design decisions in a global versus domestic context'''
{{header}}
{{article
|author= N Tractinsky,SL Jarvenpaa,
|source= MIS QUARTERLY
|year= 1995
|abstract = This study was motivated by the existence of two opposing schools of thought on managing information technology (IT) in a global context. One study proposes that managing IT in a global context is largely the same as managing IT in a domestic context. The other proposes that there is a difference. The results from interviews with 65 project managers, of whom 27 had international management experience, reflect a reality that lies somewhere between the two extremes. Using Q-methodology techniques, the project managers rated the relative importance of 33 items for decisions about the distribution of IT applications' hardware, software, and data. Although the most important factors influencing an application's IT distribution decision appear to hold across both domestic and global contexts, the global context contributes variability, unfamiliarity, and complexity that cannot be ignored. Compared with their domestic counterparts, project managers with global experience tended to be more cosmopolitan in their viewpoints, emphasized more local units' responsiveness, were more sensitive to power issues at headquarters as well as in local units, stressed the need for continuous, uninterrupted 24-hour services, and took into greater account the legal issues related to governmental regulations.
|keyword = IS management,global IS,distribution policy,IS planning,Q-methodology,IS project managers,international business,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The impact of computer alienation on information technology investment decisions: An exploratory cross-national analysis'''
{{header}}
{{article
|author= AH AbdulGader,KA Kozar,
|source= MIS QUARTERLY
|year= 1995
|abstract = Organizations in both developed and developing countries use information technology to support their operational, tactical, and strategic processes (cf., Bogod, 1979; Cooper and Zmud, 1990). Any strategic competitive advantage of information technology, however, is contingent on acquisition and assimilation of information technology products and applications into organizational processes. Using a value expectancy approach, this study proposes an expanded model to examine the variables that correlate with information technology investment decisions. The theory of alienation from social psychology is used as a basis to systematically define and measure decision makers' attitudes and internal beliefs toward information technology in an investment context. Detailed discussion of the development of a computer alienation measurement scale is presented. The scale was used to collect data from 97 decision makers in the United States, a developed country, and Saudi Arabia, a developing country. Results provide empirical evidence on the appropriateness of applying the computer alienation construct to computer purchase decisions. Computer-alienated decision makers were found to be more inclined to resist information technology adoption by refraining from buying computers. This resistance was evident in both the U.S. and the Saudi samples. The study findings also indicate that decision-maker computer knowledge, computer experience, and education level are closely associated with alienated beliefs and attitudes toward information technology. Alienated decision makers reported paying less attention to information technology information sources. Assuming technologies can provide advantages, these findings point to the need for change agents to minimize alienating beliefs and attitudes.
|keyword = information technology assimilation,alienation,computer alienation,value expectancy theory,cross-culture studies,globalization of information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECT OF COORDINATION AND UNCERTAINTY ON SOFTWARE PROJECT PERFORMANCE - RESIDUAL PERFORMANCE RISK AS AN INTERVENING VARIABLE'''
{{header}}
{{article
|author= S NIDUMOLU,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = In this research, a study of the effects of coordination mechanisms and risk drivers such as project uncertainty on the performance of software development projects was conducted. Two types of coordination mechanisms were considered: vertical and horizontal. The former refers to the extent to which coordination between users and IS staff is undertaken by authorized entities such as project managers or steering committees. The latter refers to the extent to which coordination is undertaken through mutual adjustments and communications between users and IS staff. A new research model was developed by synthesizing research using the structural contingency perspective from Organization Theory a;nd the risk-based perspective in Software Engineering. The model suggests that residual performance risk, i.e., the difficulty in estimating performance-related outcomes during the later stages of the project, can clarify the relationship between project uncertainty, coordination mechanisms and performance. Eight hypotheses were derived from the model for empirical testing. Data were collected from 64 software development projects in the banking and other industries. The results provide considerable support for a revised research model. As expected, project uncertainty increases residual performance risk. Both in turn have a direct negative effect on performance. Vertical coordination significantly reduces both project uncertainty and residual performance risk. However, horizontal coordination does not have any significant effect on residual performance risk. Instead, it has a direct positive effect on project performance. Moreover, higher levels of both vertical and horizontal coordination lead to higher levels of overall performance. Their differential impacts on residual performance risk are interesting areas of future research.
|keyword = PERFORMANCE RISK,PROJECT UNCERTAINTY,VERTICAL COORDINATION,HORIZONTAL COORDINATION,PROJECT PERFORMANCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODELING COORDINATION IN SOFTWARE CONSTRUCTION - AN ANALYTICAL APPROACH'''
{{header}}
{{article
|author= MV KOUSHIK,VS MOOKERJEE,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = Software development projects are typically team efforts, wherein groups of specialists work toward the common goal of building a software system. The individual efforts of team members need to be coordinated to ensure product quality and effectiveness of the team. In this paper we model the process of coordination in the construction phase of incrementally developed, modular software systems. The analytical model proposed here supports macro-level decisions regarding the development team size and the coordination policy, based upon micro-level interactions between the modules in a system. The objective in this model is to minimize the effort spent on coordination activities subject to the requirement that the system must be completed within a specified period. Results from the model are used to examine coordination related trade-offs. We show that: (1) more complex systems need a higher level of coordination than simpler ones, (2) if the time available for construction reduces, it is optimal to reduce the level of coordination, and (3) marginal productive output is a diminishing function of team size. The sensitivity of the analytical model with respect to its assumptions is studied by constructing a set of simulation experiments where these assumptions are relaxed. The results of these experiments provide support in establishing the robustness of the analytical model.
|keyword = SOFTWARE CONSTRUCTION,ECONOMIC MODEL,TRADEOFFS,COORDINATION POLICY,TEAM SIZE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODELING TEAM PROCESSES - ISSUES AND A SPECIFIC EXAMPLE'''
{{header}}
{{article
|author= HR RAO,A CHAUDHURY,M CHAKKA,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = This paper develops a perspective to modeling team processes by drawing on concepts from team theory, and the informational processing and organizational paradigms. In such a perspective, humans and their interactions in a team are modeled as objects in a computerized environment. The behavior of the objects are specified in terms of the executable programs. A simulation testbed is described. Various information structures for team decision making in an example financial domain are examined. Questions regarding the relationship between information structure (who (knows) what, when, and how (the information is used)) and team performance are studied for the example. Thus this study can be seen as a step in the translation of behavioral and normative viewpoints of team decision making into a computational framework. The results indicate that there are complex relationships between information structure and team performance. The conventional wisdom relating improved performance to more information is not always true. The experiments demonstrate several situations of team interaction where more information can lead to dysfunctional effects.
|keyword = TEAM PROCESSES,COMPUTATIONAL MODELING,INFORMATION STRUCTURE,TEAM PERFORMANCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE RELEVANCE OF APPLICATION DOMAIN KNOWLEDGE - THE CASE OF COMPUTER-PROGRAM COMPREHENSION'''
{{header}}
{{article
|author= TM SHAFT,I VESSEY,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = The held of software, has, to date, focused almost exclusively on application-independent approaches. In this research, we demonstrate the role of application domain knowledge in the processes used to comprehend computer programs. Our research sought to reconcile two apparently conflicting theories of computer program comprehension by proposing a key role for knowledge of the application domain under examination. We argue that programmers use more top-down comprehension processes when they are familiar with the application domain. When the application domain is unfamiliar, programmers use processes that are more bottom-up in nature. We conducted a protocol analysis study of 24 professional programmers comprehending programs in familiar and unfamiliar application domains. Our findings confirm our thesis.
|keyword = COMPUTER PROGRAM COMPREHENSION,APPLICATION DOMAIN,APPLICATION DEPENDENCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE USE OF INFORMATION TECHNOLOGY TO ENHANCE MANAGEMENT SCHOOL EDUCATION - A THEORETICAL VIEW'''
{{header}}
{{article
|author= DE LEIDNER,SL JARVENPAA,
|source= MIS QUARTERLY
|year= 1995
|abstract = To use information technology to improve learning processes, the pedagogical assumptions underlying the design of information technology for educational purposes must be understood. This paper reviews different models of learning, surfaces assumptions of electronic teaching technology, and relates those assumptions to the differing models of learning. Our analysis suggests that initial attempts to bring information technology to management education follow a classic story of automating rather than transforming. IT is primarily used to automate the information delivery function in classrooms. In the absence of fundamental changes to the teaching and learning process, such classrooms may do little but speed up ineffective processes and methods of teaching. Our mapping of technologies to learning models identifies sets of technologies in which management schools should invest in order to informate up and down and ultimately transform the educational environment and processes. For researchers interested in the use of information technology to improve learning processes, the paper provides a theoretical foundation for future work.
|keyword = EDUCATIONAL TECHNOLOGY,CLASSROOM TECHNOLOGY,ELECTRONIC CLASSROOMS,LEARNING,INSTRUCTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USING IT TO REENGINEER BUSINESS EDUCATION - AN EXPLORATORY INVESTIGATION OF COLLABORATIVE TELELEARNING'''
{{header}}
{{article
|author= M ALAVI,BC WHEELER,JS VALACICH,
|source= MIS QUARTERLY
|year= 1995
|abstract = This longitudinal field study (three work sessions plus an initial training session) investigates the efficacy of a new technology-desktop videoconferencing (DVC)-in support of collaborative telelearning (i.e., collaborative learning among non-proximate team members). Two types of collaborative telelearning environments are considered: One involves local groups (i.e., students on the same campus), and the other involves non-proximate distant groups (i.e., students on two separate campuses). The collaborative telelearning environments are compared to each other and to a traditional face-to-face collaborative learning environment. The study found that the three environments are equally effective in terms of student knowledge acquisition, however, higher critical-thinking skills were found in the distant DVC environment The subjects in the three learning environments were equally satisfied with their learning process and outcomes. At the conclusion of the longitudinal assessment, the distant students using DVC were more committed and attracted to their groups compared to local students who worked face-to-face or through DVC.
|keyword = COLLABORATIVE TELELEARNING,TECHNOLOGY MEDIATED LEARNING,DESK-TOP VIDEOCONFERENCING,COLLABORATIVE WORK SYSTEMS,IT IN BUSINESS EDUCATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CRITICAL SKILLS AND KNOWLEDGE REQUIREMENTS OF IS PROFESSIONALS - A JOINT ACADEMIC-INDUSTRY INVESTIGATION'''
{{header}}
{{article
|author= DMS LEE,EM TRAUTH,D FARWELL,
|source= MIS QUARTERLY
|year= 1995
|abstract = This study was initiated in response to concerns expressed by the membership of the Boston Chapter of the Society for Information Management (Boston SIM) to investigate anticipated changes in the information systems (IS) profession, to study the impact of these changes on the skills and knowledge requirements, and to relate these requirements to the academic preparation of future IS professionals. To provide as broad a perspective as possible, the study was conducted by a joint industry/academic group of investigators. A series of focus group meetings was conducted first with representatives of the profession's different stake-holder groups (i.e., IS managers, user managers, and IS consultants) for issue generation. A survey instrument was then designed for data collection on computing trends and changing knowledge and skills requirements. Overall, our study suggests that industry will demand a cadre of IS professionals with knowledge and skills in technology, business operations, management, and interpersonal skills to effectively lead organizational integration and process reengineering activities. The lower-level IS jobs are rapidly disappearing, and the requirements for IS professionals are becoming more demanding in multiple dimensions, particularly in the areas of business functional knowledge and interpersonal/management skills. Our results also found some clear patterns in IS staffing and activity trends that point to the shift in emphasis from a traditional, central IS organization toward a more decentralized, end-user-focused business orientation. Aligning IS solutions with business goals and needs as well as building the infrastructure for technological integration are becoming the top priorities for IS activities. Our results indicate these changes will likely lead to different career tracks with differing emphasis on the multi-dimensional knowledge/skills for IS professionals. The realignment of IS activities in organizations will require corresponding re-structuring of IS curricula at universities. Our findings suggest that current IS curricula are often ill-matched with business needs. Many subjects emphasized in the typical IS curricula are assigned low priorities by practitioners, while them is pressing need to add both breadth and depth to the education of IS professionals. We argue further that the concept of a generic curriculum to meet the educational needs of all future IS professionals is obsolete, and different IS curricula must be tailored to meet the needs of different IS careers. These career-driven IS programs will require the adoption of multi-disciplinary approaches and educational innovations for adding breadth, depth, and relevance to the curriculum in accordance with the focused mission of each specific program.
|keyword = HUMAN RESOURCE MANAGEMENT,IS CAREER PATH,IS CURRICULUM,IS EDUCATION AND RESEARCH,IS SKILL REQUIREMENTS,IS TRAINING AND DEVELOPMENT,IS STAFFING ISSUES,SIM,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IS95 - GUIDELINE FOR UNDERGRADUATE IS CURRICULUM'''
{{header}}
{{article
|author= JD COUGER,GB DAVIS,DG DOLOGITE,DL FEINSTEIN,JT GORGONE,AM JENKINS,GM KASPER,JC LITTLE,HE LONGENECKER,JS VALACICH,
|source= MIS QUARTERLY
|year= 1995
|abstract = This paper provides an overview report of the first joint curriculum development effort for undergraduate programs in information systems. The curriculum recommendations are a collaborative effort of the following organizations. ACM, AIS, DPMA, and ICIS. After a summary of the objectives and rationale for the curriculum, the curriculum model is described. Input and output attributes of graduates are delineated Resource requirements for effective IS programs are then identified. Lastly, there is a proposal for maintaining currency of the curriculum through electronic media.
|keyword = EDUCATION,UNDERGRADUATE CURRICULUM,INFORMATION SYSTEMS,SYSTEM ANALYSIS,SYSTEM DESIGN,FACULTY RESOURCES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE INFORMATION TECHNOLOGY INTERACTION-MODEL - A FOUNDATION FOR THE MBA CORE COURSE'''
{{header}}
{{article
|author= MS SILVER,ML MARKUS,CM BEATH,
|source= MIS QUARTERLY
|year= 1995
|abstract = This paper presents a teaching model that has been used successfully in the MBA core course in information systems at several universities. The model is referred to as the ''information Technology interaction Model'' because it maintains that the consequences of information systems in organizations follow largely from the interaction of the technology with the organization and its environment. The model serves a number of pedagogical purposes: to integrate the various course components, to provide a formal foundation for the course content to foster practical analytical skills, and to provide a framework for case discussions and student projects. Moreover, the model is intended to acquaint students with the dynamics of information systems in organizations and to help them recognize the benefits, dangers, and limitations of these systems. The paper includes a discussion and examples of how the model can be used for proactive and reactive analyses, and if concludes with observations on the model's effectiveness in the core course.
|keyword = MBA CORE COURSE,INTERACTION MODEL,INFORMATION SYSTEM EFFECTS,IMPLEMENTATION PROCESS,ORGANIZATIONAL CONTEXT,INFORMATION SYSTEM FEATURES,TRANSFORMATION,BUSINESS CASES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INSIDE AN INTEGRATED MBA - AN INFORMATION-SYSTEMS VIEW'''
{{header}}
{{article
|author= JS SLATER,DJ MCCUBBREY,RA SCUDDER,
|source= MIS QUARTERLY
|year= 1995
|abstract = This paper chronicles the evolution and content of an integrated MBA (IMBA) at the University of Denver and relates an IS-oriented view of the curriculum integration and teaching experience. The study emphasizes IS faculty efforts to integrate curricula from the traditional MIS course into three of the seven team-taught, interdisciplinary IMBA courses. Their teaching experiences illustrate pedagogy, and ensuing sections summarize student feedback and provide insights on what appears to work and what does not, as well as what remains to be done. A process is recommended to MIS faculty for infusing IS topics into an integrated curriculum. Gaining respect from other faculty for teaching the use and management of IT beyond its use for personal productivity, and incorporating IS into multiple class sections with scarce IS faculty resources are major problems. What appears to work best for MIS faculty is to attend curriculum discussions with (I) an appreciation of what IS/IT knowledge is essential for an MBA to (a) understand IS contributions to the solution of business problems and (b) participate in systems development activities to obtain (and later manage) IT applications; and (2) a willingness to look for opportunities to position IS/IT topics to complement other curricula.
|keyword = IS EDUCATION,IS CURRICULUM,INTEGRATED MBA EDUCATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''APPLICATION OF SOCIAL COGNITIVE THEORY TO TRAINING FOR COMPUTER SKILLS'''
{{header}}
{{article
|author= DR COMPEAU,CA HIGGINS,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = While computer training is widely recognized as an essential contributor to the productive use of computers in organizations, very little research has focused on identifying the processes through which training operates, and the relative effectiveness of different methods for such training. This research examined the training process, and compared a behavior modeling training program, based on Social Cognitive Theory (Bandura 1977, 1978, 1982, 1986), to a more traditional, lecture-based program. According to Social Cognitive Theory, watching others performing a behavior, in this case interacting with a computer system, influences the observers' perceptions of their own ability to perform the behavior, or self-efficacy, and the expected outcomes that they perceive, as well as providing strategies for effective performance. The findings provide only partial support for the research model. Self-efficacy exerted a strong influence on performance in both models. In addition, behavior modeling was found to be more effective than the traditional method for training in Lotus 1-2-3, resulting in higher self-efficacy and higher performance. For WordPerfect, however, modeling did not significantly influence performance. This finding was unexpected, and several possible explanations are explored in the discussion. Of particular surprise were the negative relationships found between outcome expectations and performance. Outcome expectations were expected to positively influence performance, but the results indicated a strong negative effect. Measurement limitations are presented as the most plausible explanation for this result, but further research is necessary to provide conclusive explanations.
|keyword = END USER TRAINING,SELF EFFICACY,CAUSAL MODELS,PARTIAL LEAST SQUARES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''UNDERSTANDING INFORMATION TECHNOLOGY USAGE - A TEST OF COMPETING MODELS'''
{{header}}
{{article
|author= S TAYLOR,PA TODD,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = The Technology Acceptance Model and two variations of the Theory of Planned Behavior were compared to assess which model best helps to understand usage of information technology. The models were compared using student data collected from 786 potential users of a computer resource center, Behavior data was based on monitoring 3,780 visits to the resource center over a 12-week period. Weighted least squares estimation revealed that all three models performed well in terms of fit and were roughly equivalent in terms of their ability to explain behavior. Decomposing the belief structures in the Theory of Planned Behavior provided a moderate increase in the explanation of behavioral intention. Overall, the results indicate that the decomposed Theory of Planned Behavior provides a fuller understanding of behavioral intention by focusing on the factors that are likely to influence systems use through the application of both design and implementation strategies.
|keyword = INFORMATION TECHNOLOGY USAGE,TECHNOLOGY ACCEPTANCE MODEL,THEORY OF PLANNED BEHAVIOR,INNOVATION CHARACTERISTICS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A CONFIRMATORY FACTOR-ANALYSIS OF THE USER INFORMATION SATISFACTION INSTRUMENT'''
{{header}}
{{article
|author= WJ DOLL,TS RAGHUNATHAN,JS LIM,YP GUPTA,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = The structure and dimensionality of the user information satisfaction (UIS) construct is an important theoretical issue that has received considerable attention. Building upon the work of Bailey and Pearson (1983), Ives et al. (1983) conduct an exploratory factor analysis and recommend a 13-item instrument (two indicators per item) for measuring user information satisfaction. Ives et al. also contend that UIS is comprised of three component measures (information product, EDP staff and services, and user knowledge or involvement). In a replication using exploratory techniques, Baroudi and Orlikowski (1988) confirm the three factor structure and support the diagnostic utility of the three factor model. Other researchers have suggested a need for caution in using the UIS instrument as a single measure of user satisfaction; they contend that the instrument's three components measure quite different dimensions whose antecedents and consequences should be studied separately.
|keyword = CONFIRMATORY FACTOR ANALYSIS,USER INFORMATION SATISFACTION,USER SATISFACTION,VALIDITY,RELIABILITY,HIGHER-ORDER FACTOR MODELS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUSINESS VALUE OF INFORMATION TECHNOLOGY - A STUDY OF ELECTRONIC DATA INTERCHANGE'''
{{header}}
{{article
|author= T MUKHOPADHYAY,S KEKRE,S KALATHUR,
|source= MIS QUARTERLY
|year= 1995
|abstract = A great deal of controversy exists about the impact of information technology on firm performance. While some authors have reported positive impacts, others have found negative or no impacts. This study focuses on Electronic Data Interchange (EDI) technology. Many of the problems in this line of research are overcome in this study by conducting a careful analysis of the performance data of the past decade gathered from the assembly centers of Chrysler Corporation. This study estimates the dollar benefits of improved information exchanges between Chrysler and its suppliers that result from using EDI. After controlling for variations in operational complexity arising from mix, volume, parts complexity, model, and engineering changes, the savings per vehicle that result from improved information exchanges are estimated to be about $60. Including the additional savings from electronic document preparation and transmission, the total benefits of EDI per vehicle amount to over $100. System wide, this translates to annual savings of $220 million for the company.
|keyword = BUSINESS VALUE,ELECTRONIC DATA INTERCHANGE,INFORMATION TECHNOLOGY,INVENTORY COSTS,TRANSPORTATION COSTS,INFORMATION HANDLING COSTS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF EXPLANATION FACILITIES ON USER ACCEPTANCE OF EXPERT-SYSTEMS ADVICE'''
{{header}}
{{article
|author= LR YE,PE JOHNSON,
|source= MIS QUARTERLY
|year= 1995
|abstract = Providing explanations for recommended actions is deemed one of the most important capabilities of expert systems (ES). There is little empirical evidence, however, that explanation facilities indeed influence user confidence in, and acceptance of, ES-based decisions and recommendations. This paper investigates the impact of ES explanations on changes in user beliefs toward ES-generated conclusions. Grounded on a theoretical model of argument, three alternative types of ES explanations-trace, justification, and strategy-were provided in a simulated diagnostic expert system performing auditing tasks. Twenty practicing auditors evaluated the outputs of the system in a laboratory setting. The results indicate that explanation facilities can make ES-generated advice more acceptable to users and that justification is the most effective type of explanation to bring about changes in user attitudes toward the system. These findings are expected to be generalizable to application domains that exhibit similar characteristics to those of auditing: domains in which decision making tends to be judgmental and yet highly consequential, and the correctness or validity of such decisions cannot be readily verified.
|keyword = AUDITING,EXPERT SYSTEMS,EXPLANATION FACILITIES,JUSTIFICATION,USER ACCEPTANCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SERVICE QUALITY - A MEASURE OF INFORMATION-SYSTEMS EFFECTIVENESS'''
{{header}}
{{article
|author= LF PITT,RT WATSON,CB KAVAN,
|source= MIS QUARTERLY
|year= 1995
|abstract = The IS function now includes a significant service component. However, commonly used measures of IS effectiveness focus on the products, rather than the services, of the IS function. Thus, there is the danger that IS researchers will mismeasure IS effectiveness if they do not include in their assessment package a measure of IS service quality. SERVQUAL, an instrument developed by the marketing area, is offered as a possible measure of IS service quality. SERVQUAL measures service dimensions of tangibles, reliability, responsiveness, assurance, and empathy. The suitability of SERVQUAL was assessed in three different types of organizations in three countries. After examination of content validity, reliability, convergent validity, nomological validity, and discriminant validity, the study concludes that SERVQUAL is an appropriate instrument for researchers seeking a measure of IS service quality.
|keyword = IS MANAGEMENT,SERVICE QUALITY,MEASUREMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPUTER SELF-EFFICACY - DEVELOPMENT OF A MEASURE AND INITIAL TEST'''
{{header}}
{{article
|author= DR COMPEAU,CA HIGGINS,
|source= MIS QUARTERLY
|year= 1995
|abstract = This paper discusses the role of individuals' beliefs about their abilities to competently use computers (computer self-efficacy) in the determination of computer use. A survey of Canadian managers and professionals was conducted to develop and validate a measure of computer self-efficacy and to assess both its impacts and antecedents. Computer self-efficacy was found to exert a significant influence on individuals' expectations of the outcomes of using computers, their emotional reactions to computers (affect and anxiety), as well as their actual computer use. An individual's self-efficacy and outcome expectations were found to be positively influenced by the encouragement of others in their work group, as well as others' use of computers. Thus, self-efficacy represents an important individual trait, which moderates organizational influences (such as encouragement and support) on an individual's decision to use computers. Understanding self-efficacy, then, is important to the successful implementation of systems in organizations. The existence of a reliable and valid measure of self-efficacy makes assessment possible and should have implications for organizational support, training, and implementation.
|keyword = USER BEHAVIOR,PSYCHOLOGY,MEASUREMENT,CAUSAL MODELS,PARTIAL LEAST SQUARES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TASK-TECHNOLOGY FIT AND INDIVIDUAL-PERFORMANCE'''
{{header}}
{{article
|author= DL GOODHUE,RL THOMPSON,
|source= MIS QUARTERLY
|year= 1995
|abstract = A key concern in Information Systems (IS) research has been to better understand the linkage between information systems and individual performance. The research reported in this study has two primary objectives: (1) to propose a comprehensive theoretical model that incorporates valuable insights from two comple mentary streams of research, and (2) to empirically test the core of the model. At the heart of the new model is the assertion that for an information technology to have a positive impact on individual performance, the technology: (1) must be utilized and (2) must be a good fit with the tasks it supports. This new model is moderately supported by an analysis of data from over 600 individuals in two companies. This research highlights the importance of the fit between technologies and users' tasks in achieving individual performance impacts from information technology. It also suggests that task-technology fit, when decomposed into its more detailed components, could be the basis for a strong diagnostic tool to evaluate whether information systems and services in a given organization are meeting user needs.
|keyword = TASK-TECHNOLOGY FIT,INDIVIDUAL PERFORMANCE,IMPACT OF INFORMATION TECHNOLOGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION TECHNOLOGIES AND BUSINESS VALUE - AN ANALYTIC AND EMPIRICAL-INVESTIGATION'''
{{header}}
{{article
|author= A BARUA,CH KRIEBEL,T MUKHOPADHYAY,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = An important management question today is whether the anticipated economic benefits of Information Technology (IT) are being realized. In this paper, we consider this problem to be measurement related, and propose and test a new process-oriented methodology for ex post measurement to audit IT impacts on a strategic business unit (SBU) or profit center's performance. The IT impacts on a given SBU are measured relative to a group of SBUs in the industry. The methodology involves a two-stage analysis of intermediate and higher level output variables that also accounts for industry and economy wide exogenous variables for tracing and measuring IT contributions. The data for testing the proposed model were obtained from SBUs in the manufacturing sector. Our results show significant positive impacts of IT at the intermediate level. The theoretical contribution of the study is a methodology that attempts to circumvent some of the measurement problems in this domain. It also provides a practical management tool to address the question of why (or why not) certain IT impacts occur. Additionally, through its process orientation, the suggested approach highlights key variables that may require managerial attention and subsequent action.
|keyword = INFORMATION TECHNOLOGY INVESTMENTS,BUSINESS VALUE,MANUFACTURING SECTOR,PROCESS ORIENTED MODELS,STRATEGIC BUSINESS UNITS,CONTRIBUTION MEASUREMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONTROLLING INFORMATION-SYSTEM DEPARTMENTS IN THE PRESENCE OF COST INFORMATION ASYMMETRY'''
{{header}}
{{article
|author= ETG WANG,T BARRON,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = The control of an information systems (IS) department is studied when its manager has private information about the department's cost and has objectives which may differ from those of the organization. The computing resource is represented by a queueing model, and it is assumed there is no access to external information processing markets by either users or the IS department. A mechanism design approach is used. We derive conditions that the optimal mechanism must satisfy; the first-order conditions of the full-information problem generalize in a clear way with the virtual marginal cost replacing the full information marginal capacity cost. The consequences of the information asymmetry include reduced capacity, arrival rate and utilization rate, and higher prices and mean waiting time compared to the full-information solution. Thus the organization suffers losses due not only to the IS manager's informational rent, but also to the opportunity cost of jobs not served. The revelation principle guarantees that the resulting mechanism is at least as good as a profit center, as well as outperforming any other centralized method of control. The mechanism design approach is also shown to be robust with respect to uncertainty on the part of the central management about the degree of incentive conflict with the IS manager. An example and numerical results give some feeling for the magnitudes of the effects, and managerial implications are also discussed. The paper also serves to illustrate the application of mechanism design to an IS problem; we briefly discuss other promising IS applications of this important methodology.
|keyword = MANAGEMENT OF INFORMATION SYSTEMS,INFORMATION ASYMMETRY,MECHANISM DESIGN,CAPACITY,COST CENTER,PROFIT CENTER,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DESIGNING INFORMATION-SYSTEMS TO OPTIMIZE THE ACCURACY-TIMELINESS TRADEOFF'''
{{header}}
{{article
|author= DP BALLOU,HL PAZER,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = It is well known, of course, that the assessment of this month's economic activity will improve with the passage of time. The same situation exists for many of the inputs to managerial and strategic decision processes. Information regarding some situation or activity at a fixed point in time becomes better with the passage of time. However, as a consequence of the dynamic nature of many environments, the information also becomes less relevant over time. This balance between using current but inaccurate information or accurate but outdated information we call the accuracy-timeliness tradeoff. Through analysis of a generic family of environments, procedures are suggested for reducing the negative consequences of this tradeoff. In many of these situations, rather general knowledge concerning relative weights and shapes of functions is sufficient to determine optimizing strategies.
|keyword = DATA QUALITY,INFORMATION QUALITY,ACCURACY,TIMELINESS,DECISION MAKING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPORTANCE OF SPECIFICATION IN CAUSAL-MODELING - THE CASE OF END-USER COMPUTING SATISFACTION'''
{{header}}
{{article
|author= WW CHIN,PR NEWSTED,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1995
|abstract = In a survey of IS instruments spanning the years 1973 to 1988 (Zmud and Boynton 1991), Doll and Torkzadeh's(l988) 12-item End-User Computing Satisfaction instrument was reported as one of three IS instruments that met conditions to qualify as ''well developed.'' Recently, Etezadi-Amoli and Farhoomand (1991) questioned the validity of these measures. Part of their critique centered on the poor model fit obtained in a re-analysis using LISREL. While other potentially valid points were raised by Etezadi-Amoli and Farhoomand's critique, this report focuses only on their use of confirmatory factor analysis. In our re-analyses of Doll and Torkzadeh's original covariance measures, we show how model fit is extremely dependent on model specification. While still maintaining the same number of constructs and respective measures, we demonstrate how two alternatives to the original model analyzed by Etezadi-Amoli and Farhoomand can result in models with acceptable fits.
|keyword = STRUCTURE EQUATION MODELING,CAUSAL MODELING,END-USER COMPUTING SATISFACTION,MODEL SPECIFICATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EVOLUTION OF IS JOB SKILLS - A CONTENT-ANALYSIS OF IS JOB ADVERTISEMENTS FROM 1970 TO 1990'''
{{header}}
{{article
|author= PA TODD,JD MCKEEN,RB GALLUPE,
|source= MIS QUARTERLY
|year= 1995
|abstract = Changes in the knowledge and skill requirements of information systems (IS) positions were examined by analyzing the content of advertisements for IS professionals placed in four major newspapers over the 20-year period 1970-1990. Three types of jobs were examined: programmers, systems analysts, and IS managers. The analysis of the frequency of phrases in these advertisements suggests that job ads for programmers have changed very little-technical requirements remain high, and business and systems knowledge requirements remain relatively low (although the frequency of mention of business requirements has increased somewhat). IS management positions are also relatively stable (as reflected in the makeup of job ads) from the standpoint that business knowledge requirements have remained high, with technical and systems requirements specified less frequently. The greatest transition in specified job requirements over this 20-year period has occurred for systems analysts. Although this is perhaps not surprising, the nature of this transition is. Contrary to expectations, the relative frequency and proportion of stated technical knowledge requirements in ads have increased dramatically, while the relative frequency of business and systems knowledge requirements has actually decreased slightly. These results raise questions concerning the implicit understanding by academics and practitioners alike of the need for business knowledge on the part of systems analysts and other IS professionals. Various interpretations of these findings are provided, and the implications for both education and recruitment are discussed.
|keyword = INFORMATION SYSTEMS JOBS,JOB SKILLS,JOB ADVERTISEMENTS,CONTENT ANALYSIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ELECTRONIC DOCUMENT MANAGEMENT - CHALLENGES AND OPPORTUNITIES FOR INFORMATION-SYSTEMS MANAGERS'''
{{header}}
{{article
|author= RH SPRAGUE,
|source= MIS QUARTERLY
|year= 1995
|abstract = Harnessing information technology to manage documents is one of the most important challenges facing IS managers in this decade. It is important because most of the valuable information in organizations is in the form of documents such as business forms, reports, letters, memos, policy statements, contracts, agreements, etc. Moreover, most of the important business processes in organizations are based on, or driven by, document flows. Electronic Document Management (EDM) promises major productivity and performance increases by applying new technology to documents and document processing. The purposes of this paper are to show the value of new technology for managing documents, to illustrate the variety of ways this value can be realized, to develop some structure for understanding this rapidly evolving field, and to suggest some actions IS managers can take now to prepare for this revolution in information management. The paper argues that the IS Department, as the developers and managers of the technical infrastructure for EDM, will be in a position to lead this evolution as major change agents like they did in the EDP and MIS eras; but some specific actions will be needed to assume this leadership role. This paper explores the scope and importance of EDM in more detail and illustrates how it expands our view of information management. It is designed to help structure the field by approaching it from three perspectives: technologies that are making EDM possible, the application areas in which business value is being realized, and the roles and responsibilities of several departments that will be involved in developing EDM. The paper suggests what IS managers can do now to begin preparing for this major advancement in information management.
|keyword = DOCUMENT MANAGEMENT,DOCUMENT PROCESSING,IS MANAGEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EARLY EXPERT-SYSTEMS - WHERE ARE THEY NOW'''
{{header}}
{{article
|author= TG GILL,
|source= MIS QUARTERLY
|year= 1995
|abstract = Expert systems (ES) were among the earliest branches of artificial intelligence (AI) to be commercialized. But how successful have they actually been? Many well-publicized applications have proven to be pure hype, numerous AI vendors have failed or been completely reorganized, major companies have reduced or eliminated their commitment to expert systems, and even Wall Street has become disillusioned-a predicted $4 billion market proving to be smaller by an order of magnitude. Yet, in spite of these setbacks, there are many companies who remain enthusiastic proponents of the technology and continue to develop important ES applications. This paper explores how the first wave of commercial expert systems, built during the early and mid-1980s, fared over time. An important subset of these systems, identified in a catalog of commercial applications compiled in 1987, was located through a telephone survey, and detailed information on each system was gathered. The data collected show that most of these systems fell into disuse or were abandoned during a five-year period from 1987 to 1992, while about a third continued to thrive. Quantitative and qualitative analysis of the data further suggests that the short-lived nature of many systems was not attributable to failure to meet technical performance or economic objectives. Instead, managerial issues such as lack of system acceptance by users, inability to retain developers, problems in transitioning from development to maintenance, and shifts in organizational priorities appeared to be the most significant factors resulting in long-term expert system disuse.
|keyword = ARTIFICIAL INTELLIGENCE,EXPERT SYSTEMS,IMPLEMENTATION,SYSTEMS DEVELOPMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLORING THE FACTORS ASSOCIATED WITH EXPERT-SYSTEMS SUCCESS'''
{{header}}
{{article
|author= YH YOON,T GUIMARAES,Q ONEAL,
|source= MIS QUARTERLY
|year= 1995
|abstract = As the widespread use and company dependency on expert systems (ES) increase, so does the need to assess their value and to ensure implementation success. This study identifies and emprically tests eight major variables proposed in the literature as determinants of ES success, in this case measured in terms of users satisfaction. IBM's Corporate Manufacturing Expert Systems Project Center collected information from 69 project managers to support the study. The results clearly support the hypothesized relationships and suggest the need for ES project managers to pay special attention to these determinants of ES implementation success. ES success is directly related to the quality of developers and the ES shells used, end-user characteristics, and degree of user involvement in ES development, as each has been defined in this study. For exploratory purposes, the component items for each of these major variables were correlated with the components of user satisfaction. Based on the results, several recommendations are proposed for ES project managers to enhance the likelihood of project success, including: adding problem difficulty as a criterion for ES application selection; increasing ES developer training to improve people skills, having the ability to model and use a systems approach in solving business problems; shaping end-user attitudes and expectations regarding ES; improving the selection of domain experts; more thoroughly understanding the ES impact on end-user jobs; restricting the acquisition of ES shells based on a proposed set of criteria; and ensuring a proper match of ES development techniques and tools to the business problem at hand.
|keyword = EXPERT SYSTEMS,EXPERT SYSTEMS SUCCESS,USER SATISFACTION,DETERMINANTS OF SUCCESS,ES DEVELOPMENT,ES IMPLEMENTATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF JOB-PERFORMANCE EVALUATIONS ON CAREER ADVANCEMENT PROSPECTS - AN EXAMINATION OF GENDER DIFFERENCES IN THE IS WORKPLACE'''
{{header}}
{{article
|author= M IGBARIA,JJ BAROUDI,
|source= MIS QUARTERLY
|year= 1995
|abstract = Despite the significant demographic changes in the work force projected by the year 2000 and beyond, little empirical research has been made on the obstacles faced by women in the field of computing. Since career advancement prospects are especially salient for IS employees, and IS workers are considered a distinct occupational group, it is important to understand the career advancement prospects of IS employees. This study examines the impact of gender on job performance evaluations, job performance attributions, and career advancement prospects. The results show that there are no significant gender differences in job performance ratings; however, women are perceived to have less favorable chances for promotion than men. We found that job performance ratings play an important role in influencing an individual's chances for advancement. We also found that the effect of job performance on attributions is stronger among males than females. Additionally, we found that while the effect of job performance ratings on career advancement prospects is stronger among males, the effect of attributions on career advancement prospects is stronger among females. Suggestions regarding areas for future research are offered, and implications for human resource management are identified.
|keyword = GENDER DIFFERENCES,IS CAREER,CAREER ADVANCEMENT,JOB PERFORMANCE,ATTRIBUTIONS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE CONTRADICTORY STRUCTURE OF SYSTEMS-DEVELOPMENT METHODOLOGIES - DECONSTRUCTING THE IS-USER RELATIONSHIP IN INFORMATION ENGINEERING'''
{{header}}
{{article
|author= CM BEATH,WJ ORLIKOWSKI,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = In this paper we show that systems development methodologies may contain incompatible assumptions about the role of users and information systems (IS) personnel during systems development. Using deconstruction, we analyze and interpret a systems development methodology currently receiving considerable attention--Information Engineering. We find that this methodology's characterization of IS-user relations and, in particular, its recommended partitioning of responsibility between IS and users is inconsistent and contradictory. Despite a heavy emphasis on user involvement, users are given a relatively passive role to play during development. At the same time, users are expected to sign off on projects and take responsibility for project outcomes. We suggest that such prescriptions, when put into action during systems development, make the relationship between users and IS personnel problematic. Further, we argue that the contradictions we surface in the methodology reflect contradictions and ideologies in the context within which systems development occurs. Our analysis raises important questions about the relationship between the production and consumption of information technology in organizations.
|keyword = INFORMATION SYSTEMS IMPLEMENTATION,SYSTEMS DEVELOPMENT METHODOLOGIES,INFORMATION ENGINEERING,IS-USER RELATIONSHIP,USER INVOLVEMENT,DECONSTRUCTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPROVING END-USER PROFICIENCY - EFFECTS OF CONCEPTUAL TRAINING AND NATURE OF INTERACTION'''
{{header}}
{{article
|author= R SANTHANAM,MK SEIN,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Users of information technology form mental models that reflect their understanding and knowledge of an information system. These models affect the proficiency with which they use these systems. In this paper, we draw upon assimilation theory of learning to propose and test a two-stage model of mental model development. We examined the effects of two types of training method, namely conceptual model and procedural, and two levels of nature of interaction, namely novel and simple tasks, on end-users' proficiency in proficiency in forming accurate mental models of an electronic mail system. Our results indicate that the actual mental models of the system formed by the users predict learning success instead of the type of training provided. Subjects who formed mental models that were conceptual in nature performed significantly better than those who formed mental models that were procedural. Main effects for nature of interaction were not significant. However, we observed a significant interaction effect between the models formed by the users and the nature of their interaction with the system. Our findings suggest that end-user performance is enhanced through training methods that provide good conceptual models but only if users form conceptual mental models and retain them.
|keyword = USER PROFICIENCY,USER TRAINING,MENTAL MODELS,ASSIMILATION THEORY,CONCEPTUAL MODELS,HUMAN-COMPUTER INTERACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHATS MINE IS OURS, OR IS IT - A STUDY OF ATTITUDES ABOUT INFORMATION SHARING'''
{{header}}
{{article
|author= D CONSTANT,S KIESLER,L SPROULL,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = As technology for information access improves, people have more opportunities to share information. A theory of information sharing is advanced and we report the results of three experiments on attitudes about sharing technical work and expertise in organizations. Based on research on sensitive topics difficult to study in the field, we derived vignette-based measures of attitudes. Subjects read a description of an employee's encounter with a previously unhelpful coworker who subsequently requested help-in the form of a computer program or computer advice. The influence of prosocial attitudes and organizational norms is inferred from subjects' support of sharing despite the previous unhelpful behavior of the coworker. Experiments 1 and 3 demonstrated that greater self interest reduces support of sharing, but that a belief in organizational ownership of work encourages and mediates attitudes favoring sharing. Work experience and business schooling contribute to these attitudes. The theory asserts that information as expertise belongs to a special category of information that is part of people's identity and is self-expressive. Experiments 2 and 3 demonstrated that subjects felt computer expertise belonged more to its possessor than the computer program did but would share it more than the program. Hence, attitudes about information sharing depend on the form of the information. Sharing tangible information work may depend on prosocial attitudes and norms of organizational ownership; sharing expertise may depend on people's own self-expressive needs.
|keyword = PROPRIETARY INFORMATION,INFORMATION SHARING,INFORMATION EXCHANGE,ATTITUDES,NORMS,ORGANIZATIONAL CITIZENSHIP,OWNERSHIP,SHARING DATA,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USER PARTICIPATION, CONFLICT, AND CONFLICT-RESOLUTION - THE MEDIATING ROLES OF INFLUENCE'''
{{header}}
{{article
|author= H BARKI,J HARTWICK,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = User participation has long been considered a key ingredient in information system development (ISD). However, research has generally failed to clearly demonstrate the benefits of user participation. A better description of interpersonal processes which occur during system development could be used to help explain the weak results. The present study builds upon the work of Robey and his colleagues (Robey and Farrow 1982, Robey et al. 1989, Robey et al. 1993), who examined user participation, influence, conflict, and conflict resolution during ISD. Results obtained in a field study of 74 IS projects suggest the following four conclusions: (i) conflict is best represented and measured as a multidimensional construct; (ii) the relationship between user participation and conflict is more complex than previously believed; (iii) influence has a dual role in the emergence of conflict; and (iv) influence plays a key role in the satisfactory resolution of conflict.
|keyword = USER PARTICIPATION,CONFLICT,INFLUENCE,SYSTEM DEVELOPMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODELING INTERPERSONAL PROCESSES DURING SYSTEM-DEVELOPMENT - FURTHER THOUGHTS AND SUGGESTIONS'''
{{header}}
{{article
|author= D ROBEY,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = This article offers commentary on the paper by Barki and Hartwick (1994), to replicate and extend the model of conflict during systems development reported in previous research by Robey and his colleagues (Robey and Farrow 1982, Robey et al. 1989, Robey et al. 1993). Because of differences in the approach to measurement and data analysis, Barki and Hartwick's contribution is more properly considered as an extension of the model rather than a replication. Barki and Hartwick's strategy of model fitting is appropriate for such an extension, but it is not clear what role their ''hypothesized model'' plays in this exploratory work. A more careful distinction between hypothesis testing and data exploration is suggested. Finally, all of the studies using the original model or its variants are limited in their ability to support theoretical reasoning about the process of system development. The direct use of process research strategies is encouraged as a means of overcoming this limitation.
|keyword = INFORMATION SYSTEM DEVELOPMENT,CONFLICT MANAGEMENT,RESEARCH METHODS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPETING THROUGH EDI AT BRUN-PASSOT - ACHIEVEMENTS IN FRANCE AND AMBITIONS FOR THE SINGLE EUROPEAN MARKET'''
{{header}}
{{article
|author= T JELASSI,O FIGON,
|source= MIS QUARTERLY
|year= 1994
|abstract = To differentiate its customer service, Brun Passot, a small French company specializing in the distribution of office supplies, developed a set of telepurchasing applications. In 1982, it launched Bureautel, a videotex-based service that allows customers to electronically place their orders. In 1986, at the request of its large customers, it developed a PC-based service, then in 1989 an advanced electronic data interchange (EDI) application linking customers to its supply information system. These services allow data on product availability, price lists, orders, acknowledgement receipts, delivery notices, invoices, and related bank payments to be electronically transmitted. Using ISDN, they also make it possible to look up the photos of the 12,000 products that Brun Passot markets. This article illustrates how a small-sized company has used IT to improve the quality of its customer service, shorten lead time and reduce management costs, as well as create new business opportunities in France. It also raises some issues related to the adoption and diffusion of EDI and presents Brun Passot's ambitions to use this technology as an essential enabler to expand its geographical coverage. The 1993 fall of mobility barriers within the European Community, leading to the formation of the single European market, presented for Brun Passot a unique business opportunity to further leverage its IT infrastructure and gain new markets.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PERCEPTIONS OF THE BENEFITS FROM THE INTRODUCTION OF CASE - AN EMPIRICAL-STUDY'''
{{header}}
{{article
|author= PN FINLAY,AC MITCHELL,
|source= MIS QUARTERLY
|year= 1994
|abstract = Computer-aided software engineering (CASE) tools have generated much interest as potential means for easing the software development and maintenance bottleneck. To date, the picture regarding their contribution is incomplete and confused, particularly concerning the realization of productivity and quality gains. An in-depth study of one company's experiences with the introduction of CASE is described. Quantitative data is available to allow objective comparison of changes in productivity and IS quality consequent upon the CASE introduction. Questionnaires were used to determine the perceptions of both developers and their customers to the new methodology and tools. The importance to the successful introduction of CASE of the human resource, technical, and managerial infrastructural factors are also investigated.
|keyword = CASE,ICASE,PRODUCTIVITY MEASUREMENT,SOFTWARE DEVELOPMENT,SOFTWARE ENGINEERING,INFORMATION ENGINEERING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ALIGNMENT OF THE IS FUNCTIONS WITH THE ENTERPRISE - TOWARD A MODEL OF ANTECEDENTS'''
{{header}}
{{article
|author= CV BROWN,SL MAGILL,
|source= MIS QUARTERLY
|year= 1994
|abstract = Identifying the best way to organize the IS functions within an interprise has been a critical IS management issue since the mid-1980s. Yet to date, MIS researchers have offered little empirical evidence on which to base guidelines for the practitioner. This study seeks to explain a firm's IS organization design decision for a decentralized, centralized, or ''hybrid'' locus of responsibility from an expanded set of environmental, overall organizational, and IS-specific antecedents as well as a larger concept of organizational alignment. Potential antecedents (drivers or enablers) are selected from prior contingency research and the IS literature; other variables emerge from the data collection. Data collected via on-site interviews from IS and general managers in six multi-divisional firms, paired by industry, confirm that centralized, decentralized, and hybrid IS structures exist-but often not in ''pure'' form-and that industry type is not a strong predictor. Data was also collected via survey form to capture ratings of importance for drivers (for enablers) for a recent IS design change in each firm. Based on both qualitative and quantitative data, four configurations are discussed: patterns of antecedents that are associated with (1) highly centralized or (2) highly decentralized IS structures; and patterns of antecedents that explain a firm's choice to (3) decentralize or (4) recentralize systems development and application planning functions in particular. A model based on these configurations is then proposed. The article concludes with implications for researchers and practitioners.
|keyword = IS CENTRALIZATION DECENTRALIZATION,STRUCTURE OF THE IS FUNCTION,ISSUES IN ORGANIZING IS,IS MANAGEMENT,ORGANIZATIONAL DESIGN,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CONCEPTUAL VERSUS PROCEDURAL SOFTWARE TRAINING FOR GRAPHICAL USER INTERFACES - A LONGITUDINAL-FIELD EXPERIMENT'''
{{header}}
{{article
|author= L OLFMAN,M MANDVIWALLA,
|source= MIS QUARTERLY
|year= 1994
|abstract = Graphical user interfaces (GUIs) are rapidly becoming ubiquitous in organizations. Most of what we know about software training comes from studies of command-line interfaces. This paper compares concept-based versus procedure-based content of training materials. Concept-based materials define the nature and associations of the objects in the interface, while procedure-based materials define how specific tasks are carried out. This comparison was done using a field experiment. Eighty-two volunteers participated in a three-week Windows training program and completed a follow-up questionnaire seven months later. The results show that the amount learned in such sessions is a function of neither concept-based nor procedure-based training. GUI training should provide both kinds of information because trainees need to learn both. In addition, trainers should be aware of an apparent early plateau in learning the Windows GUI.
|keyword = USER TRAINING,USER BEHAVIOR,END USERS,DIRECT MANIPULATION,GRAPHICAL USER INTERFACE,COMPUTER LITERACY,LONGITUDINAL STUDY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE RELATIONSHIP BETWEEN USER PARTICIPATION AND USER SATISFACTION - AN INVESTIGATION OF 4 CONTINGENCY FACTORS'''
{{header}}
{{article
|author= JD MCKEEN,T GUIMARAES,JC WETHERBE,
|source= MIS QUARTERLY
|year= 1994
|abstract = User participation has been widely touted by the MIS community as a means to improve user satisfaction within systems development. This claim, however, has not been consistently substantiated in the empirical literature. In seeking to explain such equivocal results, the effects of four contingency factors-task complexity, system complexity, user influence, and user-developer communication-on the relationship between user participation and user satisfaction were investigated. As suggested in the literature, this research tests hypotheses that these specific contingency factors should aid in identifying situations where user participation would have a strong relationship with satisfaction. Analysis of 151 independent systems development projects in eight different organizations indicated that user participation has a direct relationship with user satisfaction. In addition, the four contingency factors were found to play key roles on this relationship. Task complexity and system complexity proved to be pure moderators. That is, the strength of the participation-satisfaction relationship depended on the level of these factors. In projects where there was a high level of task complexity or system complexity, the relationship between user participation and user satisfaction was significantly stronger than in projects where task complexity or system complexity was low. User influence and user-developer communication were shown to be independent predictors of user satisfaction. That is, user influence, or user-developer communication, was positively related to user satisfaction regardless of the level of participation. The results help explain the relationship between user participation and user satisfaction by suggesting the nature of the relationship under different sets of conditions. The implications are relevant to systems developers and to academicians seeking to explain how, when, why, and where user participation is needed.
|keyword = USER PARTICIPATION,CONTINGENCY THEORY,USER SATISFACTION,USER DEVELOPER COMMUNICATION,SYSTEM COMPLEXITY,TASK COMPLEXITY,USER INFLUENCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODEL INTEGRATION USING METAGRAPHS'''
{{header}}
{{article
|author= A BASU,RW BLANNING,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = The availability of a large and diverse collection of stored modules such as data relations and decision models is a desirable feature in a decision support system (DSS). However, it is usually infeasible to design a DSS in which every problem instance can be solved using a single module. Instead, it may be necessary to combine several stored modules into an integrated model that is sufficient to solve the given problem. We show that modules such as data files and decision models in a DSS can be usefully represented by a metagraph, a graph-theoretic construct that captures relationships between pairs of sets of elements. In addition to the visualization benefits that graphical representation offers, we show that many useful questions faced by the designers and users of DSS can be addressed by exploiting analytical properties of metagraphs. In particular, we show that the process of model integration can be significantly facilitated by exploiting certain connectivity properties in metagraphs.
|keyword = GRAPH THEORY,MODEL MANAGEMENT,MODELING,DECISION SUPPORT SYSTEMS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ROLE OF CONTROL IN THE MODEL FORMULATION PROCESS'''
{{header}}
{{article
|author= A SEN,A VINZE,SFT LIOU,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Control is being increasingly recognized as having a critical role in the automation of the model formulation process. This paper describes an approach to understanding this role of control by observing experts' behavior and studying their verbalizations during the process of formulating models. Control concerns were noted at two levels: strategic and tactical. At the strategic level, control behavior was found to be opportunistic, i.e., the modelers did not follow a prespecified approach. This paper focuses on the tactical level, where the emphasis was on scheduling the tasks used to construct the model (referred to as formulation tasks). The tactical controls were demonstrated by our AEROBA system implementation.
|keyword = MODEL MANAGEMENT,FORMULATION CONTROL,MODEL FORMULATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FORMAL MODELING OF COMPLEX COMMANDS IN INDUSTRIAL SOFTWARE SPECIFICATIONS'''
{{header}}
{{article
|author= MV MANNINO,S RATHNAM,IJ CHOI,V TSENG,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = We present a formal approach for modeling complex commands characterized by heavy overloading of function, large numbers of parameters, dependencies among parameters, subtle side effects, and lack of abstraction. Complex commands arise in a variety of business settings such as requesting a brokerage order, enrolling in a course, and specifying a product order. In addition, complex commands are also prevalent where specification of commands is strictly separated from multiple, independent implementations as in open software standards. Our approach is based on an inheritance structure known as a command lattice. Like other forms of inheritance, command lattices support incremental definition and abbreviation of specifications. Because a complete command lattice can have a large number of specifications, we develop another structure known as a minimal command tree in which a command lattice is derived from a much smaller number of independent specifications. To map from a minimal command tree to a command lattice, we present algorithms that materialize an arbitrary node of a command lattice and compactly generate the behavior of a command lattice. To demonstrate the potential of command lattices, we have implemented a set of tools that provide convenient specification and powerful reasoning capabilities. Our tool collection includes the Command Specification Language that supports a precise and rich specification of the structural and behavioral properties of commands, the incremental definition tool that ensures consistency of command lattices, the browsing tool that displays a command's inheritance structure, the type checker that ensures structural consistency of commands in expressions, and the target system tracer that simulates a sequence of command executions. We discuss our experiences applying the tools to IBM's Distributed Data Management, a large scale specification of data access on remote and heterogeneous IBM systems.
|keyword = ASSERTIONS,INHERITANCE,FORMAL METHODS,ABSTRACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BENCHMARKING DECISION-MODELS FOR DATABASE - MANAGEMENT-SYSTEMS'''
{{header}}
{{article
|author= D DEY,A SEIDMANN,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Benchmarking is the quantitative method most commonly used when managers contemplate procuring a large business information system. It consists of running a group of representative applications on the systems offered by vendors to validate their claims. The implementation of benchmarking can be very costly, as users need to convert, run, and test applications on several partially compatible computer systems. Benchmarking works well in modern database management systems (DBMS)-oriented applications because the system performance is more a function of the database structure and activities than of the complexity of the application code. Earlier research focused primarily on designing various benchmarks for database systems; the decision problem associated with finding an optimal mix of benchmarks has largely been overlooked. In this paper, we examine the problem of defining the most economical process for generating and evaluating the appropriate mix of benchmarks to be used across the contending information systems. Our analytical approach considers information-gathering priorities, acquisition and execution costs, resource consumption, and overall time requirements. We present a multiobjective decision-making approach for deriving the optimal mix of benchmarks; this approach reflects the major organizational objectives in more than simple one-dimensional numerical terms. A practical example illustrates the utility of this approach for evaluating a client-server relational database system.
|keyword = MULTICRITERIA DECISION MAKING (MCDM),GOAL PROGRAMMING,DATABASE BENCHMARKING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CLAIMS, ARGUMENTS, AND DECISIONS - FORMALISMS FOR REPRESENTATION, GAMING, AND COORDINATION'''
{{header}}
{{article
|author= R RAMESH,AB WHINSTON,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Decisions in large corporations continually evolve from several group processes, shaping the focus of business activities over time. These decisions arise out of a combination of formal analyses and less formal interactions among decision makers. We address the pragmatics of group decision processes from the perspective of argumentation and analysis. We develop formalisms for the representation of argumentative knowledge, gaming the argumentation process and the coordination of the games. The representation formalism provides a framework for organizing the logic underlying the claims and arguments in a group. The gaming formalism provides a framework for conducting and regulating the group interactions. The coordination formalism addresses the issues of scheduling the games and the resolution process. The three formalisms together constitute the basis for designing computer-assisted systems that support argumentation processes in groups. We introduce the term Argumentative Reasoning Facilitation Systems (ARFS), and develop a framework for their design. These systems would serve to record, organize, regulate and coordinate argumentative decision processes in organizations. The formalisms provide new windows for research on novel applications of decision support systems in organizations. Some of the systemic, organizational and behavioral research issues identified from this work are also presented.
|keyword = ORGANIZATION,DECISION-MAKING,GROUPS,ARGUMENT THEORY,KNOWLEDGE REPRESENTATION,NONMONOTONIC LOGIC,STRUCTURED MODELING,SYSTEMS INTEGRATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ADAPTATION OF A PLANNING SYSTEM SUCCESS MODEL TO INFORMATION-SYSTEMS PLANNING'''
{{header}}
{{article
|author= B RAGHUNATHAN,TS RAGHUNATHAN,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = The research reported here is an adaptation of a model developed to measure Planning Systems Success, to the information systems (IS) context. This study is motivated by the increasing importance given to IS planning in recent years, the lack of an empirical model in the IS literature to measure a construct as important as IS planning system success, and the usefulness of developing such a model to help guide future research efforts addressing the issue of IS planning effectiveness. An operational model for measuring IS planning system success is developed in terms of two interrelated dimensions: IS planning system capabilities and extent of fulfillment of key IS planning objectives. Due regard is paid to the call for greater attention to methodological issues in developing such measures. The model meets the measurement criteria suggested in the literature.
|keyword = PLANNING SUCCESS,IS PLANNING,INFORMATION SYSTEMS,PLANNING MODEL,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CREATING AND SUSTAINING OF GLOBAL COMMUNITY OF SCHOLARS'''
{{header}}
{{article
|author= R WATSON,
|source= MIS QUARTERLY
|year= 1994
|abstract = Against the backdrop of a brief history of communities of scholars, the shortcomings of the current intellectual infrastructure are discussed in this paper. This system is deeply rooted in printed matter, the postal system, and physical knowledge stores. As a result of the deficiencies of this infrastructure, many scholars, practitioners, and students have limited opportunities to participate in creating and sharing information. The Internet is put forward as the foundation of a new intellectual infrastructure that will overcome many of the problems of the old system. The significant benefits to be gained from redesigning the intellectual infrastructure are discussed. Some of the possible implications for universities and scholarly careers are considered. We have an opportunity to transform the basic infrastructure of the MIS community. By changing the way we store, process, and distribute information, we can create a broader, more collaborative, and more productive community of MIS scholars, students, and practitioners. This paper first briefly traces the history of scholarly communities and examines their development of an infrastructure for knowledge creation and distribution. After the shortcomings of our present infrastructure are addressed, some of the possibilities and the consequences of creating an MIS electronic community are discussed. In particular, the redesign opportunities made possible by the Internet (Kehoe, 1993) are investigated.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUSINESS REENGINEERING AT CIGNA-CORPORATION - EXPERIENCES AND LESSONS LEARNED FROM THE 1ST 5 YEARS'''
{{header}}
{{article
|author= JR CARON,SL JARVENPAA,DB STODDARD,
|source= MIS QUARTERLY
|year= 1994
|abstract = Considerable uncertainty and confusion exists about what business reengineering is and when it succeeds. This paper provides a longitudinal view of CIGNA Corporation's experiences in business reengineering since 1989. CIGNA is a leading provider of insurance and related financial services throughout the United States and the world. Between 1989 and 1993, CIGNA completed over 20 reengineering initiatives, saving more than $100 million. Each $1 invested in reengineering has ultimately brought $2-3 in returned benefits. This article describes projects with major payoffs: operating expenses reduced by 42 percent, cycle times improved by 100 percent, customer satisfaction up by 50 percent, quality improvements of 75 percent. It also highlights how CIGNA's reengineering started small and how learning was used to escalate from this quick hit to reengineering larger and more complex parts of the organization. CIGNA's reengineering successes have also required a willingness to allow failure and learn from failures. Only about 50 percent of the reengineering efforts bring the type of benefits expected initially. Repeated trials are often necessary. CIGNA's lessons can help other firms anticipate what they will experience as they ascend the learning curve of business reengineering.
|keyword = BUSINESS REENGINEERING,BUSINESS PROCESS REDESIGN,RADICAL CHANGE,LONGITUDINAL CASE STUDY,INSURANCE INDUSTRY,STRATEGIC ALIGNMENT,ORGANIZATIONAL LEARNING,AND KNOWLEDGE TRANSFER,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPUTERIZED LOAN ORIGINATION SYSTEMS - AN INDUSTRY CASE-STUDY OF THE ELECTRONIC MARKETS HYPOTHESIS'''
{{header}}
{{article
|author= CM HESS,CF KEMERER,
|source= MIS QUARTERLY
|year= 1994
|abstract = Much has been written in recent years about the changes in corporate strategies and industry structures associated with electronic coordination of market activities. This paper considers the advent of electronic market coordination in the home mortgage industry, focusing on Computerized Loan Origination (CLO) systems. Case studies of five CLOs (First Boston's Shelternet, PRC's LoanExpress, American Financial Network's Rennie Mae, Prudential's CLOS, and Citicorp's Mortgage Power Plus) reveal a range of system functionalities. Predictions from the Electronic Markets Hypothesis (EMH) are tested against the empirical results of the five case studies. As suggested by the EMH, financial intermediaries have been threatened by the introduction of CLOS, and in some cases opposition has been mounted against the systems. On the other hand, despite the availability of the technology and mortgages' seemingly favorable characteristics as an electronically mediated market product, the industry has not been fundamentally changed by the introduction of these systems, despite more than a decade of experience with them. Of the two case studies that could be characterized as electronic markets, neither continues to exist in that form today. And the system with the largest dollar volume of mortgages of the five is best characterized as an electronic hierarchy. These results suggest that either the full results predicted by the EMH require a longer gestation period or that the underlying hypothesis will require augmentation in order to fully explain the results in the home mortgage market.
|keyword = COMPUTERIZED LOAN ORIGINATION SYSTEMS,ELECTRONIC MARKETS,ELECTRONIC HIERARCHIES,INCOMPLETE CONTRACTS,SHELTERNET,MORTGAGES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN EXPLORATORY-STUDY OF ROLES IN COMPUTER-SUPPORTED GROUPS'''
{{header}}
{{article
|author= I ZIGURS,KA KOZAR,
|source= MIS QUARTERLY
|year= 1994
|abstract = The emerging technology of group support systems has the potential to enhance the effectiveness of team work in organizations. One critical factor that has received little attention in technology-supported environments is that of the roles participants fill in meetings. This paper develops a theoretical model of roles in computer-supported meetings and examines the impact of a group support system on roles. An exploratory field study of 10 work teams was conducted to investigate the perceptions of participants about their own roles and the roles that the group support system technology might fill. The study found a gap between the role expectations of meeting initiators and meeting participants, as well as between participants' role expectations and actual roles filled. The group support system technology was perceived to fill an unexpectedly large variety of roles. The study also showed that the group support system assumed some of the roles that participants expected to fill, resulting in fewer roles filled by participants.
|keyword = GROUP SUPPORT SYSTEMS,ELECTRONIC MEETING SYSTEMS,ROLES,GROUP INTERACTION PROCESS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE INFLUENCE OF IT MANAGEMENT PRACTICE ON IT USE IN LARGE ORGANIZATIONS'''
{{header}}
{{article
|author= AC BOYNTON,RW ZMUD,GC JACOBS,
|source= MIS QUARTERLY
|year= 1994
|abstract = This paper draws upon the absorptive capacity as the theoretical basis for a pragmatic explanation of key factors affecting information technology (IT) use in large, complex organizations. IT use is defined as the extent to which an organization deploys IT to support operational and strategic tasks. The study uses results from a survey of senior IT managers from 132 firms to examine hypothesized relationships among the following constructs: IT management climate, managerial IT knowledge, IT-management-process effectiveness, and IT use. A structural-equation model is developed using LISREL to assess the relative effects of and interrelationships among these constructs. The study's findings indicate that managerial IT knowledge is a dominant factor in explaining high levels of IT use and that both managerial IT knowledge and IT-management-process effectiveness are influenced by IT management climate.
|keyword = IT USE,IT MANAGEMENT,IT MANAGEMENT PROCESSES,ABSORPTIVE CAPACITY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ORGANIZATIONAL CHARACTERISTICS AND INFORMATION-SYSTEMS PLANNING - AN EMPIRICAL-STUDY'''
{{header}}
{{article
|author= G PREMKUMAR,WR KING,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Information Systems (IS) planning has gained considerable interest among researchers and practitioners in recent years because of the large investments that firms have made in IS and the increasingly strategic nature of the impact of information systems on organizational performance. Since IS planning is performed in an organizational context, characteristics of the organization may have a significant influence on the quality and effectiveness of the IS planning process. A research model is developed that links two major dimensions of IS planning-the quality of the planning process and planning effectiveness-with a set of eight organizational factors derived from contingency research in IS planning, strategic business planning, organizational studies, and technology innovation. The model is validated using data collected from a field survey of 249 senior IS executives. Canonical correlation analysis is used to test the research hypotheses and validate the model. The results of the study indicate that the two planning dimensions respectively reflecting the ''means'' and ''ends'' of IS planning are equally important. The results also indicate that planning resources, the intended strategic impact of IS on future business operations, the quality of facilitation mechanisms, the quality of implementation mechanisms, and the quality of strategic business planning are significantly associated with the quality and effectiveness of IS planning. The implications of the results to further research and to managerial practice are highlighted.
|keyword = STRATEGIC IS PLANNING,PLANNING CHARACTERISTICS,ORGANIZATIONAL CHARACTERISTICS,ORGANIZATION AND IS PLANNING,ROLE OF IS,IS MANAGEMENT,IS PLANNING,PLANNING DIMENSIONS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPUTER-MEDIATED COMMUNICATION FOR INTELLECTUAL TEAMWORK - AN EXPERIMENT IN GROUP WRITING'''
{{header}}
{{article
|author= J GALEGHER,RE KRAUT,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Contingency theory predicts that using computer-mediated communication to accomplish complex collaborative work will be difficult, especially for tasks that require interactive, expressive communication. This proposition was examined in an experiment in which 67 three-person groups of MBA students completed two collaborative writing projects under either Computer Only, Computer + Phone or Face-to-Face communication conditions. The effects of these manipulations on group processes and performance were examined using data obtained from questionnaires and scores on the projects themselves. Although communication modality did not affect project performance, being restricted to computer-mediated communication made completing the work more difficult and diminished the participants' satisfaction with their work and with the other members of their work groups. The results also provide partial support for the idea that tasks that require more intensive communication, such as project planning, were more difficult than those that can be completed more independently, but this premise was not consistently confirmed. Taken together, these findings tend to confirm the contingency hypothesis regarding the difficulty of accomplishing work that involves ambiguous goals, multiple perspectives, and information that is susceptible to multiple interpretations without an interactive multiperson communication medium, such as face-to-face meetings. However, the results also suggest that modifications of contingency theory are required to incorporate the evidence that individuals can, to some extent, adapt to restricted communication channels. Further research designed to examine patterns of adaptation under various task/technology combinations is recommended.
|keyword = COLLABORATIVE WRITING,COMPUTER-MEDIATED COMMUNICATION,CONTINGENCY THEORY,CSCW,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INSTITUTIONAL FACTORS IN INFORMATION TECHNOLOGY INNOVATION'''
{{header}}
{{article
|author= JL KING,V GURBAXANI,KL KRAEMER,FW MCFARLAN,KS RAMAN,CS YAP,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Innovation in information technology is well established in developed nations; newly industrializing and developing nations have been creating governmental interventions to accelerate IT innovation within their borders. The lack of coherent policy advice for creating government policy for IT innovation signals a shortfall in research understanding of the role of government institutions, and institutions more broadly, in IT innovation. This paper makes three points. First, long-established intellectual perspectives on innovation from neoclassical economics and organization theory are inadequate to explain the dynamics of actual innovative change in the IT domain. A broader view adopted from economic history and the new institutionalism in sociology provides a stronger base for understanding the role of institutions in IT innovation. Second, institutional intervention in IT innovation can be constructed at the intersection of the influence and regulatory powers of institutions and the ideologies of supply-push and demand-pull models of innovation. Examples of such analysis are provided. Third, institutional policy formation regarding IT innovation is facilitated by an understanding of the multifaceted role of institutions in the innovative process, and on the contingencies governing any given institution/innovation mix.
|keyword = INSTITUTIONS,INFORMATION TECHNOLOGY,INNOVATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PRINCIPLES FOR EXAMINING PREDICTIVE-VALIDITY - THE CASE OF INFORMATION-SYSTEMS SPENDING FORECASTS'''
{{header}}
{{article
|author= F COLLOPY,M ADYA,JS ARMSTRONG,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Research over two decades has advanced the knowledge of how to assess predictive validity. We believe this has value to information systems (IS) researchers. To demonstrate, we used a widely cited study of IS spending. In that study, price-adjusted diffusion models were proposed to explain and to forecast aggregate U.S. information systems spending. That study concluded that such models would produce more accurate forecasts than would simple linear trend extrapolation. However, one can argue that the validation procedure provided an advantage to the diffusion models. We reexamined the results using an alternative validation procedure based on three principles extracted from forecasting research: (1) use ex ante (out-of-sample) performance rather than the fit to the historical data, (2) use well-accepted models as a basis for comparison, and (3) use an adequate sample of forecasts. Validation using this alternative procedure did confirm the importance of the price-adjustment, but simple trend extrapolations were found to be more accurate than the price-adjusted diffusion models.
|keyword = BROWN LINEAR EXPONENTIAL SMOOTHING,COMBINED FORECASTS,DAMPED TREND EXPONENTIAL SMOOTHING,DIFFUSION,EXTRAPOLATION,FORECAST ACCURACY,INFORMATION SYSTEMS SPENDING,PRICE-ADJUSTED DIFFUSION MODELS,VALIDATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODELING VS FORECASTING - THE CASE OF INFORMATION-SYSTEMS SPENDING'''
{{header}}
{{article
|author= V GURBAXANI,H MENDELSON,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Collopy, Adya and Armstrong (1994) (CAA) advocate the use of atheoretical ''black box'' extrapolation techniques to forecast information systems spending. In this paper, we contrast this approach with the positive modeling approach of Gurbaxani and Mendelson (1990), where the primary focus is on explanation based on economics and innovation diffusion theory. We argue that the objectives and premises of extrapolation techniques are so fundamentally different from those of positive modeling that the evaluation of positive models using the criteria of ''black box'' forecasting approaches is inadequate. We further show that even if one were to accept CAA's premises, their results are still inferior. Our results refute CAA's claim that linear trend extrapolations are appropriate for forecasting future IS spending and demonstrate the risks of ignoring the guidance of theory.
|keyword = INFORMATION SYSTEMS SPENDING,POSITIVE MODELS,FORECASTING,EXPONENTIAL SMOOTHING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REENGINEERING - BUSINESS CHANGE OF MYTHIC PROPORTIONS'''
{{header}}
{{article
|author= TH DAVENPORT,DB STODDARD,
|source= MIS QUARTERLY
|year= 1994
|abstract = Reengineering is a powerful change approach that can bring about radical improvements in business processes. However, the popular management literature has created more myth than practical methodology regarding reengineering. It has relied more heavily on hype than on research, common sense, or lessons of the past. In this paper, we attempt to ''demythologize'' some key aspects of reengineering by describing what we have observed in our research and Practice. Seven reengineering myths are identified, discussed, and dispelled. By separating rhetoric from reality, we hope to help others to have reasonable expectations for success with their reengineering initiatives.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''GENDER DIFFERENCES IN THE INFORMATION-SYSTEMS MANAGERIAL RANKS - AN ASSESSMENT OF POTENTIAL DISCRIMINATORY PRACTICES'''
{{header}}
{{article
|author= GE TRUMAN,JJ BAROUDI,
|source= MIS QUARTERLY
|year= 1994
|abstract = This paper examines the extent to which gender discrimination is a force affecting the senior managerial ranks of the information systems (IS) occupation. While the employment trends of women in the IS occupation is encouraging, data are presented that suggest that IS may not be immune to the problems of gender discrimination. Analyzing data gathered by the Society for Information Management (SIM), a problem suggestive of discriminatory practices was found. Women receive lower salaries than men even when job level, age, education, and work experience are controlled.
|keyword = IS MANAGEMENT,PERSONNEL MANAGEMENT,GENDER DISCRIMINATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ELECTRONIC MAIL AS A MEDIUM FOR RICH COMMUNICATION - AN EMPIRICAL-INVESTIGATION USING HERMENEUTIC INTERPRETATION'''
{{header}}
{{article
|author= AS LEE,
|source= MIS QUARTERLY
|year= 1994
|abstract = This study provides an account of how richness Occurs in communication that uses electronic mail. In examining actual e-mail exchanged among managers in a corporation, the study interprets the managerial use of the communication medium of electronic mail as the users themselves understand and experience it. Employing the research approach of interpretivism in general and hermeneutics in particular, the study finds that richness or leanness is not an inherent property of the electronic-mail medium, but an emergent property of the interaction of the electronic-mail medium with its organizational context, where the interaction is described in terms of distanciation, autonomization, social construction, appropriation, and enactment. Conclusions and recommendations are that managers who receive e-mail are not passive recipients of data, but active producers of meaning; that the best or just an appropriate communication medium is not determined through an individual manager's exercise of rational decision making, but emerges as best or appropriate over time, over the course of the medium's interactions with many users, that systems professionals need to treat the managerial user of an e-mail system not merely as a client of information services, but also as a processor or co-processor to be integrated into the system design; and that information systems researchers need to dedicate attention to the actual processes by which the users of a communication medium come to understand themselves, their own use of the medium, and their organizational context.
|keyword = INFORMATION RICHNESS,CASE STUDY,HERMENEUTICS,POSITIVIST PERSPECTIVE,INTERPRETIVIST PERSPECTIVE,ORGANIZATIONAL COMMUNICATION,ELECTRONIC MAIL,IS RESEARCH METHODOLOGIES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COMPUTER-MEDIATED COLLABORATIVE LEARNING - AN EMPIRICAL-EVALUATION'''
{{header}}
{{article
|author= M ALAVI,
|source= MIS QUARTERLY
|year= 1994
|abstract = National Commissions and Scholarly reports on the status Of contemporary higher education have frequently been critical of the college experience; the emphasis on transmitting fixed bodies of information and a failure to develop problem solving and critical thinking skills have been cited as serious weaknesses in higher education systems. Colleges and universities have additional reasons to redevelop central pedagogies for students. Individuals need to learn at higher rates of effectiveness and efficiency than ever before because of rapidly growing bodies of relevant information and the escalation of knowledge and skill requirements for most jobs. Recent developments in computer hardware, software, and communication technologies create exciting new opportunities for the educational use of these technologies. The objective of this study is to go beyond the traditional classroom instructional modes (e.g., lectures and class discussions) to develop and evaluate computer-supported pedagogical approaches. More specifically, this study investigates whether the use of a group decision support system (GDSS) in a collaborative learning process enhances student learning and evaluation of classroom experiences. The findings of a study involving 127 MBA students indicate that GDSS-supported collaborative learning leads to higher levels of Perceived skill development, Self-reported learning, and evaluation of classroom experience in comparison with non-GDSS supported collaborative learning. Furthermore, the final test grades of the group of students who were exposed to GDSS-supported collaborative learning were significantly higher than those of the other group of students who participated in the experiment.
|keyword = COMPUTER-MEDIATED LEARNING,COOPERATIVE LEARNING,COMPUTER-SUPPORTED TEAM LEARNING,EDUCATIONAL TECHNOLOGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FREE-ACCESS POLICY FOR INTERNAL NETWORKS'''
{{header}}
{{article
|author= PS GIRIDHARAN,H MENDELSON,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = This paper evaluates the free-access policy as a control mechanism for internal networks. We derive the optimal message pricing scheme, compare it to the free-access policy, and study the associated net-value loss. We derive uniform upper bounds on this value loss, and apply our results to the polar implementations of ethernet and token ring networks. The results show that the free-access policy is often attractive.
|keyword = PRICING,COMPUTER NETWORKS,FREE-ACCESS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECT OF CULTURE ON IT DIFFUSION - E-MAIL AND FAX IN JAPAN AND THE UNITED-STATES'''
{{header}}
{{article
|author= DW STRAUB,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Few cross-cultural studies have investigated how firms diffuse new information technologies (IT). Still fewer have advanced a theoretical perspective on possible cultural effects. In a world moving rapidly toward corporate multinationalism, this oversight seems notable. As foreign managers locate plants and offices in the U.S. and as American managers establish foreign subsidiaries and offices abroad, it is important for these managers to know in advance as much as possible about the impact of culture on technological innovation. Japan and the U.S. are cases in point. Both have subsidiaries and actively market goods and services in the other country, far flung enterprises for which IT seems to be a natural coordinating mechanism. Yet while U.S. companies exploit the advantages of IT such as E-mail, Japanese firms do not. The Japanese, however, do utilize FAX extensively. Culture is one fruitful explanation for these differences. To examine these two markedly different cultures and the effect of these differences on technological innovation, a large Japanese airline and financial institution were chosen as representative Asian sites. The IT experiences of 209 Japanese knowledge workers are contrasted with those of 71 1 knowledge workers in comparable firms in the United States on certain dimensions. Using Hofstede's work on culture and social presence/information richness theory as grounding, it was hypothesized that high uncertainty avoidance in Japan and structural features of the Japanese written language could explain Japanese perceptions about new work technologies such as E-Mail and FAX. Furthermore, the theoretical conceptualization in the paper attempts to account for Japanese departures from the U.S. experience. Results from empirical tests verified many, but not all of the predicted differences between Japanese and American knowledge workers. In general, cultural effects seem to play an important role in the predisposition toward and selection of electronic communications media. Surprisingly, responses to traditional media such as face-to-face and telephone were remarkably similar between cultures.
|keyword = IT CROSS-CULTURAL STUDIES,IT DIFFUSION,UNCERTAINTY AVOIDANCE,PERCEIVED USEFULNESS,SYSTEM USE,SOCIAL PRESENCE,INFORMATION RICHNESS,JAPANESE BUSINESS,PRODUCTIVITY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USER RESPONSE DATA - THE POTENTIAL FOR ERRORS AND BIASES'''
{{header}}
{{article
|author= EM HUFNAGEL,C CONCA,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1994
|abstract = Surveys that require users to evaluate or make judgments about information systems and their effect on specific work activities can produce misleading results if respondents do not interpret or answer questions in the ways intended by the researcher. This paper provides a framework for understanding both the cognitive activities and the errors and biases in judgment that can result when users are asked to categorize a system, explain its effects, or predict their own future actions and preferences with respect to use of a system. Specific suggestions are offered for wording survey questions and response categories so as to elicit more precise and reliable responses. In addition, possible sources of systematic bias are discussed, using examples drawn from published IS research. Recommendations are made for further research aimed at better understanding how and to what extent judgment biases could affect the results of IS surveys.
|keyword = IS EVALUATION,USER SATISFACTION,SURVEY RESEARCH,RESPONSE BIAS,RESEARCH METHODS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE FRENCH VIDEOTEX SYSTEM MINITEL - A SUCCESSFUL IMPLEMENTATION OF A NATIONAL INFORMATION TECHNOLOGY INFRASTRUCTURE'''
{{header}}
{{article
|author= WL CATSBARIL,T JELASSI,
|source= MIS QUARTERLY
|year= 1994
|abstract = Building an advanced national information technology infrastructure can provide a competitive advantage for the countries that develop it as well as for the companies that operate in those countries. This article describes the development of the French national videotex system Teletel, also known as Minitel. The factors -technical and political-that made Teletel successful are explained and contrasted against other national videotex systems that became commercial failures. Political intrigue, technical capability, creative choices, and the deep pockets of a government-owned utility are all part of the Teletel story.
|keyword = INFORMATION TECHNOLOGY,COMPUTERIZATION OF SOCIETY,DIFFUSION OF INNOVATION,IS TECHNOLOGY TRANSFER,PUBLIC POLICY,VIDEOTEX,CASE STUDY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MULTIMEDIA SOLUTION TO PRODUCTIVITY GRIDLOCK - A RE-ENGINEERED JEWELRY APPRAISAL SYSTEM AT ZALE-CORPORATION'''
{{header}}
{{article
|author= J NEWMAN,KA KOZAR,
|source= MIS QUARTERLY
|year= 1994
|abstract = Zale Corporation once melted down most of its damaged, returned, or repossessed jewelry, resulting in substantial lost revenues. It was determined that additional revenue could be produced from salvageable jewelry if the value of the items could be accurately determined. This meant the jewelry had to be appraised by experienced gemologists to determine the most profitable disposition. The gemologists' productivity suffered because the appraisal was extremely labor intensive. To address this problem, an automated multimedia system utilizing electronically linked measuring instruments, voice recognition, and interconnected LAN databases was developed, Although the unique voice recognition feature of the system was later abandoned, the use of the system enhanced productivity. This paper describes the systems development, its subsequent evolution, and the lessons learned from the process.
|keyword = HUMAN-MACHINE SYSTEMS,MULTIMEDIA,VOICE RECOGNITION,BUSINESS PROCESS REENGINEERING,USER MACHINE DIALOG DESIGN,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MEASURING USER PARTICIPATION, USER INVOLVEMENT, AND USER ATTITUDE'''
{{header}}
{{article
|author= H BARKI,J HARTWICK,
|source= MIS QUARTERLY
|year= 1994
|abstract = Defining user participation as the activities performed by users during systems development, user involvement as the importance and personal relevance of a system to its user, and user attitude as the affective evaluation of a system by the user, this study aims to: (1) develop separate measures of user participation, user involvement, and user attitude, (2) identify key dimensions of each construct, and (3) investigate the relationships among them. Responses from users in organizations developing new information systems were used to create an overall scale measuring user participation (along with three subscales reflecting the dimensions of responsibility, user-IS relationship, and hands-on activities), an overall scale measuring user involvement (along with two subscales reflecting the dimensions of importance and personal relevance), and a scale measuring user attitude. Analysis of the data provides evidence for the reliability and validity of the three constructs and their dimensions. User participation has long been considered a key variable in the successful development of information systems. However, Past research has failed to clearly demonstrate its benefits. The measures developed in this study provide a useful starting point for deciphering the precise nature of the relationship among user participation, involvement, and attitude during systems implementation.
|keyword = IS IMPLEMENTATION,USER INVOLVEMENT,USER PARTICIPATION,MEASUREMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Systems Research Thematics: Submissions to a New Journal, 1987-1992'''
{{header}}
{{article
|author= E. Burton Swanson,Neil C. Ramiller,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = The flow of manuscripts through the editorial offices of an academic journal can provide valuable information both about the performance of the journal as an instrument of its field and about the structure and evolution of the field itself. We undertook an analysis of the manuscripts submitted to the journal Information Systems Research (JSR) during its start-up years, 1987 through 1992, in an effort to provide a foundation for examining the performance of the journal, and to open a window on to the information systems (IS) field during that period. We identified the primary research question for each of 397 submissions to ISR, and then categorized the research questions using an iterative classification procedure. Ambiguities in classification were exploited to identify relationships among the categories, and some overarching themes were exposed in order to reveal levels of structure in the journal's submissions stream. We also examined the distribution of submissions across categories and over the years of the study period, and compared the structures of the submissions stream and the publication stream. We present the results with the goal of broadening the perspectives which individual members of the IS research community have of JSR and to help fuel community discourse about the nature and proper direction of the field. We provide some guidelines to assist readers in this interpretive task, and offer some observations and speculations to help launch the discussion.
|keyword = Information systems research,Research questions,Research themes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MODFORM: A Knowledge-Based Tool to Support the Modeling Process'''
{{header}}
{{article
|author= Srinivasan Raghunathan,Ramayya Krishnan,Jerrold H. May,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = The value of mathematical modeling and analysis in the decision support context is well recognized. However, the complex and evolutionary nature of the modeling process has limited its widespread use. In this paper, we describe our work on knowledge-based tools which support the formulation and revision of mathematical programming models. In contrast to previous work on this topic, we base our work on an indepth empirical investigation of experienced modelers and present three results: (a) a model of the modeling process of experienced modelers derived using concurrent verbal protocol analysis. Our analysis indicates that modeling is a synthetic process that relates specific features found in the problem to its mathematical model. These relationships, which are seldom articulated by modelers, are also used to revise models. (b) an implementation of a modeling support system called MODFORM based on this observationally derived model, and (c) the results of a preliminary experiment which indicates that users of MODFORM build models comparable to those formulated by experts. We use the formulation of mathematical programming models of production planning problems illustratively throughout the paper
|keyword = Model management,Knowledge-based systems,Process analysis,Object-based methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Comparative Analysis of the Empirical Validity of Two Rule-Based Belief Languages'''
{{header}}
{{article
|author= Shimon Schocken,Yu-Ming Wang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Rule-based expert systems deal with inexact reasoning through a variety of quasi-probabilistic methods, including the widely used subjective Bayesian (SB) and certainty factors (CF) models, versions of which are implemented in many commercial expert system shells. Previous research established that under certain independence assumptions, SB and CF are ordinally compatible: when used to compute the beliefs in several hypotheses of interest under the same set of circumstances, the hypothesis that will attain the highest posterior probability will also attain the highest certainty factor, etc. This monotonicity is important in the context of expert systems, where most inference-engines and explanation facilities are designed to utilize relative scales of posterior beliefs, making little or no use of their absolute magnitudes. This research extends the comparative analysis of SB and CF to the field, where subjective degrees of belief and different elicitation procedures are likely to complicate their analytic similarity and impact their actual validity. In particular, we describe an experiment in which CF was shown to dominate SB in terms of several validity criteria, a finding which we attribute to parsimony and robustness considerations. The paper is relevant to (i) practitioners who use belief languages in rule-based systems, and (ii) researchers who seek a methodology to investigate the validity of other belief languages in controlled experiments.
|keyword = Belief revision,Inexact reasoning,Certainty factors,Uncertainty in artificial intelligence,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''(UN)STRUCTURED CREATIVITY IN INFORMATION-SYSTEMS ORGANIZATIONS'''
{{header}}
{{article
|author= JD COUGER,LF HIGGINS,SC MCINTYRE,
|source= MIS QUARTERLY
|year= 1993
|abstract = The subject of creativity is a neglected area in the literature of the information systems field. Yet, according to a Delphi survey of chief information officers, the field needs to be developing more creative and innovative solutions to its problems. Organizations must first be sure that certain preconditions and organizational components be in place to help individuals and teams become more creative. They can then use numerous creativity improvements techniques that have proved successful in other disciplines. In this article, six case studies show how analytical techniques (progressive abstraction, interrogatories, and force field analysis) and intuitive techniques (associations/images, wishful thinking, and analogy/metaphor) have been used in several industries to solve a variety of IS-related problems and/or opportunities. All told, some 20 creativity techniques prove especially appropriate for the IS field. Once managers understand when and where to use creativity techniques, they can move forward with implementing formal creativity improvement programs in their organizations.
|keyword = CREATIVITY,CREATIVITY IN INFORMATION SYSTEMS,CREATIVE CLIMATE,CREATIVITY TECHNIQUES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DISTRIBUTED GROUP SUPPORT SYSTEMS'''
{{header}}
{{article
|author= M TUROFF,SR HILTZ,ANF BAHGAT,AR RANA,
|source= MIS QUARTERLY
|year= 1993
|abstract = Distributed group support systems are likely to be widely used in the future as a means for dispersed groups of people to work together through computer networks. They combine the characteristics of computer-mediated communication systems with the specialized tools and processes developed in the context of group decision support systems, to provide communications, a group memory, and tools and structures to coordinate the group process and analyze data. These tools and structures can take a wide variety of forms in order to best support computer-mediated interaction for different types of tasks and groups. This article summarizes five case studies of different distributed group support systems developed by the authors and their colleagues over the last decade to support different types of tasks and to accommodate fairly large numbers of participants (tens to hundreds). The case studies are placed within conceptual frameworks that aid in classifying and comparing such systems. The results of the case studies demonstrate that design requirements and the associated research issues for group support systems can be very different in the distributed environment as compared to the decision room approach.
|keyword = GROUP DECISION SUPPORT SYSTEMS,COMPUTER-MEDIATED COMMUNICATIONS,COMPUTERIZED CONFERENCING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TAILORING DATABASE TRAINING FOR END-USERS'''
{{header}}
{{article
|author= JD AHRENS,CS SANKAR,
|source= MIS QUARTERLY
|year= 1993
|abstract = Lack of familiarity with database design methods could prevent many end users from effectively implementing their database management system packages. An inexpensive solution would be for end users to learn required database design skills from software tutors tailored to their needs. This research describes two tutors developed to teach these skills to end users. The tutors were based on a modified Entity-Relationship database design method. They improved an end user's natural learning process by incorporating design principles and facilitators. Empirical comparison of the tutors tested the teaching effectiveness of the facilitators. The results lead to recommendations for closing the gap between skills required and skills learned by end users in database design. Development of tutors that teach specific database design skills irrespective of the software package used in implementation has important implications for practitioners and researchers.
|keyword = END-USER TRAINING,SOFTWARE TUTORS,COGNITIVE SKILL ACQUISITION,DATABASE DESIGN,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USER-DATABASE INTERFACE - THE EFFECT OF ABSTRACTION LEVELS ON QUERY PERFORMANCE'''
{{header}}
{{article
|author= HC CHAN,KK WEI,KL SIAU,
|source= MIS QUARTERLY
|year= 1993
|abstract = A common classification of data models is based on their abstraction levels: physical, logical and conceptual. The user-database interaction can be similarly classified. For the conceptual-level interaction, the user and the database exchange information on the user's world, e.g., information of entities, relationships, and attributes. For the logical-level interaction, the user and the database communicate based on concepts in the database system, e.g., relations and join operations. We expect users to be familiar with concepts in their world but not the concepts in the database system. This is especially so for infrequent or naive database users. The conceptual level should therefore be easier because it is semantically closer to the user. This deduction was tested in an experiment using the entity-relationship (ER) model for the conceptual-level model and the relational model for the logical-level model. The results were affirmative. The users at the conceptual level had 38 percent higher accuracy and 16 percent higher confidence than users at the logical level. The conceptual-level users took 65 percent less time than the logical-level users, and it took 33 percent less time to train them. The differences were statistically significant with p<0.003. The huge differences indicate that noticeable improvements can be made by switching from the relational model to the ER model. The experiment also provided valuable data on errors commonly made by users.
|keyword = DATA RESOURCE UTILIZATION,USER-DATABASE INTERFACE,ABSTRACTION LEVELS,CONCEPTUAL LEVEL,LOGICAL LEVEL,RELATIONAL MODEL,ENTITY-RELATIONSHIP MODEL,QUERY LANGUAGES,SQL,EXPERIMENTAL STUDY,USER PERFORMANCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACT OF COMMUNICATION MEDIUM AND COMPUTER SUPPORT ON GROUP PERCEPTIONS AND PERFORMANCE - A COMPARISON OF FACE-TO-FACE AND DISPERSED MEETINGS'''
{{header}}
{{article
|author= L CHIDAMBARAM,B JONES,
|source= MIS QUARTERLY
|year= 1993
|abstract = Economic, social, and political institutions worldwide are relying increasingly on communication technology to perform a variety of functions: holding electronic town meetings where hundreds of people in numerous cities participate simultaneously; forging strategic links with business partners, thereby forming ''virtual corporations'' that can be instantly disbanded; redefining the conventional notion of a college campus by offering classes via interactive media to non-traditional students; and enabling consumers with personal digital assistants to remain connected with their children and families at all times. In this environment, where geographic and temporal boundaries are shrinking rapidly, electronic meeting systems (EMS) are playing an important role. This study examines the impact on teams of using EMS in dispersed and face-to-face settings. The results suggest that EMS can be effective in augmenting traditional audio-conferencing by strengthening the medium and allowing additional communication cues to be exchanged among participants. They also indicate that EMS can improve decision-making performance, given proper task-technology fit and adequate facilitation. As businesses expand globally, such systems will provide instant communication capabilities and help coordinate dispersed decision-making activities.
|keyword = ELECTRONIC MEETING SYSTEMS,GROUP DECISION SUPPORT SYSTEMS,COMMUNICATION MEDIA,AUDIO-CONFERENCING,DISPERSED/REMOTE MEETING SUPPORT,GROUP PERFORMANCE,GROUP PERCEPTIONS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EFFECTS OF INFORMATION-SYSTEM USER EXPECTATIONS ON THEIR PERFORMANCE AND PERCEPTIONS'''
{{header}}
{{article
|author= B SZAJNA,RW SCAMELL,
|source= MIS QUARTERLY
|year= 1993
|abstract = The consequences of information system failure become more acute as organizations continue to invest in information technology and application development. Being able to better predict IS failure before implementation of a system could facilitate changes in the information system that can lead to implementation success. The realism of user expectations has been suggested as one possible means of assessing the eventual success or failure of an IS. Cognitive dissonance theory was used to hypothesize the behavior and attitudes of end users having certain expectations of a system. This experiment investigates the association between unrealistic expectations with both users' perceptions (i.e., user satisfaction) and their performance with the IS (i.e., decision performance). A longitudinal experiment was performed in which the expectations of the subjects were manipulated to be unrealistically high, realistically moderate, or unrealistically low. The results suggest an association between realism of users' expectations and their perceptions but not their actual performance. Future research should be directed toward the development of an instrument to measure user expectations, as well as toward understanding the causes of unrealistic user expectations.
|keyword = INFORMATION SYSTEMS DEVELOPMENT,REALISM OF USER EXPECTATIONS,COGNITIVE DISSONANCE THEORY,IMPLEMENTATION EXPECTATIONS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Interactivity and Communication Mode Choice in Ongoing Management Groups'''
{{header}}
{{article
|author= Michael H. Zack,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Management is communication intensive and, therefore, managers may derive benefits from computer-based alternatives to the traditional communication modes of face-to-face (FTF), telephone, and written memo. This research examined the use of electronic messaging (EM) by ongoing management groups performing a cooperative task. By means of an in-depth multimethod case study of the editorial group of two daily newspapers, it examined the fit between the interactivity of the chosen communication mode (FTF vs. EM) and the mode of discourse it was used for (alternation vs. interaction/discussion). Two propositions were derived from this exploratory study. The first proposes that FTF, being highly interactive, is appropriate for building a shared interpretive context among group members, while CMC, being less interactive, is more appropriate for communicating within an established context. Groups exhibiting effective communication will use FTF primarily for interactive discourse and EM for discourse consisting primarily of alternating adjacency pairs. The second proposes that to the extent that the appropriate communication modes are chosen, communication will be more effective.
|keyword = Computer-mediated communication,Media choice,Work group effectiveness,Social communication,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Information Technology and the Perceived Mood of the Feedback Giver on Feedback Seeking'''
{{header}}
{{article
|author= Soon Ang,Larry L. Cummings,Detmar W. Straub,P. Christopher Earley,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = A major tenet in organizational behavior literature is that feedback improves performance. If feedback is thought to improve performance, then individuals should actively seek feedback in their work. Yet, surprisingly, individuals seldom seek feedback perhaps because of face-loss costs of obtaining feedback face-to-face. Furthermore, in cases where the giver is perceived to be in a bad mood, individuals may be even more reluctant to seek feedback if they believe seeking feedback risks the giver's wrath and a negative evaluation. In this paper, we explain how information technology can be designed to mediate feedback communication and deliver feedback that promotes feedback seeking. In a laboratory experiment, the effects of information technology and the perceived mood of the feedback giver on the behavior of feedback seekers are examined. The results showed that individuals in both the computer-mediated feedback environment and the computer-generated feedback environment sought feedback more frequently than individuals in the face-to-face feedback environment. In addition, individuals sought feedback more frequently from a giver who was perceived to be in a good mood than from a giver who was perceived to be in a bad mood.
|keyword = Feedback seeking,Mood,Electronic communication,Feedback systems,Information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Heuristics for Reconciling Independent Knowledge Bases'''
{{header}}
{{article
|author= Andrew Trice,Randall Davis,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = One of the major unsolved problems in knowledge acquisition is reconciling knowledge originating from different sources. This paper proposes a technique for reconciling knowledge in two independent knowledge bases, describes a working program built to implement that technique, and discusses an exploratory study for validating the technique. The technique is based on the use of heuristics for identifying and resolving discrepancies between the knowledge bases. Each heuristic developed provides detection and resolution procedures for a distinct variety of discrepancy in the knowledge bases. Sample discrepancies include using synonyms for the same term, conflicting rules, and extra reasoning steps. Discrepancies are detected and resolved through the use of circumstantial evidence available from the knowledge bases themselves and by asking sharply focussed questions to the experts responsible for the knowledge bases. The technique was tested on two independently developed knowledge bases designed to aid novice statisticians in diagnosing problems in linear regression models. The heuristics located a significant number of the discrepancies between the knowledge bases and assisted the experts in creating a consensus knowledge base for diagnosing multicollinearity problems. We argue that the task of identifying discrepancies between independent bodies of knowledge is an inevitable part of any large knowledge acquisition effort. Hence the heuristics developed in this work are applicable even when knowledge acquisition is not done by reconciling two complete knowledge bases. We also suggest that our approach can be extended to other knowledge representations such as frames and database schemas, and speculate about its potential application to other domains involving the reconciliation of knowledge, such as requirements determination, negotiation, and design. Knowledge, such as requirements determination, negotiation, and design.
|keyword = Knowledge acquisition,Consensus formation,Expert systems,Conflict resolution,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ASSESSING THE VALUE OF CONOCOS EIS'''
{{header}}
{{article
|author= LW BELCHER,HJ WATSON,
|source= MIS QUARTERLY
|year= 1993
|abstract = Corporate-wide efforts to eliminate unnecessary expenses prompted Conoco's decision to assess the benefits and costs of its executive information system (EIS). Because there was no accepted methodology for doing this, one was developed and successfully used. First, system usage statistics were collected in order to identify which applications were being used in each department, who was using them, and how often. Key users were then interviewed to further support the assessment of the tangible and intangible benefits derived from the EIS. The benefits included improved productivity, improved decision making, information distribution cost savings, services replacement cost savings, and software replacement cost savings. The costs included the direct costs of maintaining the EIS and the indirect costs absorbed by the operating groups who provide personnel to perform EIS-related tasks. Benefits were found to exceed the system's costs; low value applications that should be eliminated were identified, as were applications that needed to be added or enhanced. A number of lessons were learned that are generalizable to other organizations. The successful assessment of the benefits of Conoco's EIS raises the hopes that other types of applications with ''soft'' benefits might be better quantified.
|keyword = EXECUTIVE INFORMATION SYSTEMS,EXECUTIVE SUPPORT SYSTEMS,SYSTEM EVALUATION,COST BENEFIT ANALYSIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DETERMINING INFORMATION REQUIREMENTS FOR AN EIS'''
{{header}}
{{article
|author= HJ WATSON,MN FROLICK,
|source= MIS QUARTERLY
|year= 1993
|abstract = Executive information systems (EISs) are challenging applications to develop, and many organizations are unsuccessful in their efforts, A major problem is determining the information requirements for the system. A multi-stage study was conducted to explore (1) methods used to determine the information requirements for the initial and ongoing versions of an EIS; (2) how frequently the methods are used; (3) how useful the methods are; and (4) in what situations the methods are useful or not useful. Telephone interviews identified 16 methods used to determine information requirements and provided insights into what makes the methods useful or not useful. Survey questionnaires revealed how frequently the methods are used and how useful they are. The use of the 16 methods for the initial and ongoing versions of an EIS are discussed as are suggestions for further research.
|keyword = EXECUTIVE INFORMATION SYSTEMS,EXECUTIVE SUPPORT SYSTEMS,INFORMATION REQUIREMENTS DETERMINATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF INFORMATION TECHNOLOGY ON MIDDLE MANAGERS'''
{{header}}
{{article
|author= A PINSONNEAULT,KL KRAEMER,
|source= MIS QUARTERLY
|year= 1993
|abstract = This article reviews studies that examine the impact of information technology (IT) on the number of middle managers in of organizations. Contradictory evidence is found to suggest, paradoxically, that IT both increases and decreases the number of the middle managers. This ''empirical paradox'' is resolved by looking at the effects of IT on middle managers as contingent upon the degree of centralization of computing decisions, and of organizational decisions more broadly. When both computing decisions and organizational decisions are centralized, top managers tend to use IT to reduce the number of middle managers. When these decisions are decentralized, middle managers use IT to increase their numbers. A recent case study provides preliminary support for this perspective by showing an interesting case of reduction in middle managers.
|keyword = INFORMATION TECHNOLOGY,IMPACT ON MIDDLE MANAGERS,INCREASE DECREASE IN MIDDLE MANAGERS,CAPITAL-LABOR SUBSTITUTION,TECHNOLOGICAL DETERMINISM,MANAGERIAL ACTIONALISM,STRUCTURATIONAL PERSPECTIVE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IS EXPECTATION GAP - INDUSTRY EXPECTATIONS VERSUS ACADEMIC PREPARATION'''
{{header}}
{{article
|author= EM TRAUTH,DW FARWELL,D LEE,
|source= MIS QUARTERLY
|year= 1993
|abstract = Recent changes in information systems technologies, applications, and personnel require us to reconsider the skills for tomorrow's IS professionals. This study uses data from four groups-IS managers, end-user managers, IS consultants, and IS professors-to identify the key skills and knowledge that will be required of future IS professionals. These requirements were then compared with current IS academic programs. The results reveal that despite a shared vision of the future IS professional, there is an ''expectation gap '' between industry needs and academic preparation. Industry and universities must work together to close this gap. Universities need to place more emphasis on the integration of technologies, applications, data, and business functions and less on traditional and formal system development. Firms need to send consistent messages to universities about their expectations while recognizing that the mission of university business programs is career education, not job training.
|keyword = INFORMATION SYSTEMS EDUCATION,INFORMATION SYSTEMS PROFESSION,INFORMATION SYSTEMS CURRICULUM, TRAINING,SKILLS,END-USER COMPUTING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CASE TOOLS AS ORGANIZATIONAL-CHANGE - INVESTIGATING INCREMENTAL AND RADICAL CHANGES IN SYSTEMS-DEVELOPMENT'''
{{header}}
{{article
|author= WJ ORLIKOWSKI,
|source= MIS QUARTERLY
|year= 1993
|abstract = This paper presents the findings of an empirical study into two organizations' experiences with the adoption and use of CASE tools over time. Using a grounded theory research approach, the study characterizes the organizations' experiences in terms of processes of incremental or radical organizational change. These findings are used to develop a theoretical framework for conceptualizing the organizational issues around the adoption and use of these tools-issues that have been largely missing from contemporary discussions of CASE tools. The paper thus has important implications for research and practice, Specifically, the framework and findings suggest that in order to account for the experiences and outcomes associated with CASE tools, researchers should consider the social context of systems development, the intentions and actions of key players, and the implementation process followed by the organization. Similarly, the paper suggests that practitioners will be better able to manage their organizations' experiences with CASE tools if they understand that such implementations involve a process of organizational change over time and not merely the installation of a new technology.
|keyword = CASE TOOLS,CHANGE MANAGEMENT ORGANIZATIONAL CHANGE,SYSTEMS DEVELOPMENT,SYSTEMS IMPLEMENTATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HOW DID THEY GET MY NAME - AN EXPLORATORY INVESTIGATION OF CONSUMER ATTITUDES TOWARD SECONDARY INFORMATION USE'''
{{header}}
{{article
|author= MJ CULNAN,
|source= MIS QUARTERLY
|year= 1993
|abstract = Strategic uses of information technology based on personal information may raise privacy concerns among consumers if these applications do not reflect a common set of values. This study addresses what differentiates consumers who object to certain uses of personal information from those who do not object. Data collected by questionnaire from young consumers are used to identify a research approach for investigating attitudes toward the secondary use of personal information for direct marketing. Secondary information use occurs when personal information collected for one purpose is subsequently used for a different purpose. While secondary information use is both widespread and legal, it may be viewed as an invasion of privacy when it occurs without the knowledge or consent of the consumer. The setting for the study is the use of point-of-sale data from a supermarket frequent shopper program to generate direct mail solicitations. Control emerges as a clear theme in differentiating individuals with positive overall attitudes toward secondary information use from those with negative attitudes. Study participants with positive attitudes are less concerned about privacy (measured as control over personal information), perceive shopping by mail as beneficial, and have coping strategies for dealing with unwanted mail. The results also suggest that theory related to categorization of strategic issues as positive-negative with outcomes that are controllable/uncontrollable provides a basis for understanding differences in the ways individuals perceive practices involving personal information. Future research should focus on the specific characteristics of secondary use practices, including the sensitivity of the information, it source), its perceived relevance to the original transaction, and whether disclosure reflects informed consent or results in a potentially harmful outcome to the individual.
|keyword = PRIVACY,DIRECT MARKETING,CONSUMER ATTITUDES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Inductive Expert System Design: Maximizing System Value'''
{{header}}
{{article
|author= Vijay S. Mookerjee,Brian L. Dos Santos,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = There is a growing interest in the use of induction to develop a special class of expert systems known as inductive expert systems. Existing approaches to develop inductive expert systems do not attempt to maximize system value and may therefore be of limited use to firms We present an induction algorithm that seeks to develop inductive expert systems that maximize value. The task of developing an inductive expert system is looked upon as one of developing an optimal sequential information acquisition strategy. Information is acquired to reduce uncertainty only if the benefits gained from acquiring the information exceed its cost. Existing approaches ignore the costs and benefits of acquiring information. We compare the systems developed by our algorithm with those developed by the popular ID3 algorithm. In addition, we present results from an extensive set of experiments that indicate that our algorithm will result in more valuable systems than the ID3 algorithm and the ID3 algorithm with pessimistic pruning.
|keyword = Inductive expert systems,Information costs and benefits,Economic expert system design,Economics of machine learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Formal Approach for Designing Distributed Expert Problem-solving Systems'''
{{header}}
{{article
|author= Prabuddha De,Varghese S. Jacob,Ramakrishnan Pakath,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = In this paper, we consider the problem of generating effective information gathering, communication, and decision-making (ICD) strategies for a distributed expert problem-solving (DEPS) system. We focus on the special case of a dual-processor DEPS system and present a decision-theoretic model that enables the characterization of feasible, efficient, and optimal ICD strategies. In view of the tremendous amount of computing needed to generate optimal strategies for problems of practical size, we develop useful heuristic procedures for constructing high-quality efficient ICD strategies. We illustrate the use of the model and the solution procedure through an example.
|keyword = Expert systems,Distributed problem solving,Economic decision theory,Information-gathering communication and decision-making strategies,Computational complexity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Classification of Information Systems: Analysis and Interpretation'''
{{header}}
{{article
|author= Phillip Ein-Dor,Eli Segev,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Seventeen major types of information systems are identified and defined by vectors of their attributes and functions. These systems are then classified by numerical methods. The quantitative analysis is interpreted in terms of the development history of information system types. Two major findings are that the numerical classification autonomously follows the chronological appearance of system types and that, along the time line, systems have followed two major paths of development; these have been termed the applied artificial intelligence path and the human interface path. The development of new types of systems is considered within the framework of a theory of technological evolution. It is shown that newer types of systems result from gradual accretion of new technologies on one hand, and loss of older ones on the other. Conclusions are drawn concerning the value of taxonomy in studying information systems, in suggesting possible research directions, and the desirability of rationalizing research efforts within the IS discipline.
|keyword = attributes,classification,functions,history,information systems,technology evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE USEFULNESS OF COMPUTER-BASED INFORMATION TO PUBLIC MANAGERS'''
{{header}}
{{article
|author= KL KRAEMER,JN DANZIGER,DE DUNKLE,JL KING,
|source= MIS QUARTERLY
|year= 1993
|abstract = This study uses data from 260 public managers information technology and computer-based information (CBI) to serve public managers: the ''knowledge executive'' and the ''CBI consumer.'' The data were collected as part of a longitudinal study of computer use in more than 40 U.S cities conducted in 1976 and again in 1988. The results show that computer-based information is important for most managers, and many report they are extremely dependent upon it. Also. the managers currently find the information more valuable for control of financial resources than for management of operations. Furthermore, among four sets of factors that might account for differences in the usefulness of computer-based information to managers, quality and accessibility of the information and the manager's style of use are particularly important, Finally, managers who are most satisfied with the usefulness of computer-based information are those who use support staff to mediate their computer-based information environment, rather than those who use the computer to access information directly. Such indirect use of computing might be the most appropriate mode for many contemporary managers. Therefore, the focus of design efforts for information systems for managers should be as much on these intermediaries as on the executives themselves.
|keyword = COMPUTER-BASED INFORMATION,USEFULNESS OF INFORMATION,PUBLIC MANAGERS,KNOWLEDGE EXECUTIVES,STYLE OF COMPUTING USE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''METAPHORS AND METHODOLOGIES - LIVING BEYOND THE SYSTEMS MACHINE'''
{{header}}
{{article
|author= JE KENDALL,KE KENDALL,
|source= MIS QUARTERLY
|year= 1993
|abstract = Metaphors are the cognitive lenses we use to make sense of all situations. Intimately interconnected with the way we think, metaphors are fundamental in shaping reality. Building on work about metaphor's in organizational life, this paper examines the language of information systems users in 16 different organizations. The results confirm the existence of six main metaphors (journey, war, game, organism, society, and machine) and adds three metaphors that also emerged from the language of IS users (family, zoo, and jungle). Dramatistic analysis was used to reveal that seven of these principal metaphors are found in commonly used systems development methodologies. For example, the systems development life cycle draws upon the ''game'' metaphor, and structured methodologies and CASE tools are akin to the ''machine'' metaphor. Analysts who are aware of the existence of these metaphors (both in the user organization and within the methodologies themselves) will begin to see the systems development process in an entirely different light. Caution must be undertaken, however, when using this approach. First, analysts should lead the systems development Process by selecting a methodology to match user metaphors, not the other way around. Second, analysts must see, rather than suppress, the paradoxical richness of metaphors. Third, analysts should not limit the number of metaphors because it limits the usefulness of this approach. Fourth, analysts should be adequately trained in a variety of systems development methodologies. Finally, analysts should use metaphorical analysis in conjunction with other approaches Using the recommendations and findings for guidance, analysts can begin to develop the power of metaphorical analysis to better understand and communicate with users during the development process.
|keyword = SYSTEMS ANALYSIS,SYSTEMS DESIGN,IS DEVELOPMENT STRATEGIES,PARTICIPATIVE DESIGN,METAPHORS,SOCIOTECHNICAL APPROACH,USER-ANALYST INTERACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLORING MODES OF FACILITATIVE SUPPORT FOR GDSS TECHNOLOGY'''
{{header}}
{{article
|author= GW DICKSON,JEL PARTRIDGE,LH ROBINSON,
|source= MIS QUARTERLY
|year= 1993
|abstract = The use of group decision support systems (GDSS) is rapidly growing. One key factor in the effectiveness of these systems may be the manner in which users are supported in their use of this technology. This paper explores two types of GDSS facilitative support: chauffeur-driven and facilitator-driven. In the former case, a person is used to reduce the mystique of the GDSS technology for users. In the latter case, a person assists the group with its group process in addition to reducing the mystique of the technology. The work unfolds a research story in which the original thinking of the research team to the effect that facilitator-driven GDSS facilitative support is superior is proven incorrect. The results of a pilot study caused the research team to reverse its thinking and hypothesize that, given the nature of the facilitation used and the task faced by the group, chauffeur-driven facilitation would have an advantage. The results of the experiment reported in this paper support this hypothesis. Arguments are presented to the effect that, to be effective in a judgment task environment, facilitation must be open and adaptive rather than restrictive.
|keyword = DECISION SUPPORT,ELECTRONIC MEETINGS,GROUP SUPPORT SYSTEMS,GROUP CONSENSUS,GROUP FACILITATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPLORING INDIVIDUAL USER SATISFACTION WITHIN USER-LED DEVELOPMENT'''
{{header}}
{{article
|author= M LAWRENCE,G LOW,
|source= MIS QUARTERLY
|year= 1993
|abstract = User-led development is gaining popularity with organizations wishing to increase user involvement and control. Typically in this approach a small group of users is given the responsibility for managIng the project and representing the user community in determining requirements, testing, training, and system implementation. This paper explores the end user's perception of the quality of his or her representation and satisfaction with the application system within an organization employing user-led development. The end-user communities for two systems developed in a large Australian government corporation were surveyed The results indicate that the user perception of representation is the most significant influence on user satisfaction-the correlation scores for the two systems studied were in excess of 0.6. Also of importance is the user's perception of management support. Both systems recorded a low average score for user representativeness (2.5 and 2.7), which is attributed in part to the large number and geographic spread of the users and to the approach adopted by the user representatives in the user-led development team.
|keyword = USER INVOLVEMENT,USER-LED DEVELOPMENT,USER REPRESENTATION,USER SATISFACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Information Technology Investment Announcements on the Market Value of the Firm'''
{{header}}
{{article
|author= Brian L. Dos Santos,Ken Peffers,David C. Mauer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Determining whether investments in information technology (IT) have an impact on firm performance has been and continues to be a major problem for information systems researchers and practitioners. Financial theory suggests that managers should make investment decisions that maximize the value of the firm. Using event-study methodology, we provide empirical evidence on the effect of announcements of IT investments on the market value of the firm for a sample of 97 IT investments from the finance and manufacturing industries from 1981 to 1988. Over the announcement period, we find no excess returns for either the full sample or for any one of the industry subsamples. However, cross-sectional analysis reveals that the market reacts differently to announcements of innovative IT investments than to followup, or noninnovative investments in IT. Innovative IT investments increase firm value, while noninnovative investments do not. Furthermore, the market's reaction to announcements of innovative and noninnovative IT investments is independent of industry classification. These results indicate that, on average, IT investments are zero net present value (NPV) investments; they are worth as much as they cost. Innovative IT investments, however, increase the value of the firm.
|keyword = Event study,Information technology evaloation,lavestment value,Information technology Investments,Market value,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Information Age Confronts Education: Case Studies on Electronic Classrooms'''
{{header}}
{{article
|author= Dorothy E. Leidner,Sirkka L. Jarvenpaa,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Information technology is slowly becoming a part of educational classrooms and corporate training facilities. The current study examines the use and outcomes of computer-based instructional technology in the context of graduate business education. Case study data is gathered to explore how computer technology is used in the university classroom, and how computer-based teaching methods differ from traditional teaching methods in terms of class interaction and in-class learning. The study found that there are many potential computer-based teaching methods and that the methods can have different outcomes. The use of computer-based teaching methods requiring hands-on student use appear to offer an advantage over traditional methods and over computer-based methods not requiring hands-on student use in providing a forum for exploratory analysis during class and for acquiring technical procedural knowledge. A model of in-class learning is developed for future research.
|keyword = Educational technology,Electronic classrooms,Computer-assisted Instruction,Information Technology,Educational methods,Case studies,Inclass learning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reframing Decision Problems: A Graph-grammar Approach'''
{{header}}
{{article
|author= Shimon Schocken,Christopher Jones,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = One fundamental requirement in the expected utility model is that the preferences of rational persons should be independent of problem description. Yet an extensive body of research in descriptive decision theory indicates precisely the opposite: when the same problem is cast in two different but normatively equivalent "frames," people tend to change their preferences in a systematic and predictable way. In particular, alternative frames of the same decision-tree are likely to invoke different sets of heuristics, biases and risk-attitudes in the user's mind. The paper presents a modeling environment in which decision-trees are cast as attributed-graphs, and reframing operations on trees are implemented as graph-grammar productions. In addition to the basic functions of creating and analyzing decision-trees, the environment offers a natural way to define a host of "debiasing mechanisms" using graphical programming techniques. Some of these mechanisms have appeared in the decision theory literature, whereas others were directly inspired by the novel use of graph-grammars in modeling decision problems. The modeling environment was constructed using NETWORKS, a new model management system based on a graph-grammar formalism. Thus, a second objective of the paper is to illustrate how a general-purpose modeling environment can be used to produce, with relatively little effort, a specialized decision support system for problems that have a strong graphical orientation.
|keyword = Descriptive decision theory,Decision-trees,Decision-making under uncertainty,Graph-grammars,Model management,Model management systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Adoption of Microcomputer Technology: The Role of Sector'''
{{header}}
{{article
|author= Stuart Bretschneider,Dennis Wittmer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1993
|abstract = Microcomputer and work-station technology is the latest wave in computing technology to influence day-to-day operations in business and government organization. Does sector affect adoption of this new information technology? If so, how? Utilizing the data from a large comparative national survey of data processing organizations, this proposition was examined. The results confirm that after controlling for other factors such as organizational size, experience with computer technology, current investment in computer technology, procurement practices, and the task environment of the organization, the sector an organization operates within has a major differential effect on adoption of microcomputer technology. Public organizations have more microcomputers per employee, a result that is potentially due to a more information intensive task environment and the potential use of microcomputer technology as a side payment in lieu of salary. The latter factor derives from lower wage rates faced by public employees.
|keyword = Microcomputers,Public sector,Adoption decisions,Diffusion,Public-Private companies,Organizational environment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXPERIENCES IN STRATEGIC INFORMATION-SYSTEMS PLANNING'''
{{header}}
{{article
|author= MJ EARL,
|source= MIS QUARTERLY
|year= 1993
|abstract = Strategic information systems planning (SISP) remains a top concern of many organizations. Accordingly, researchers have investigated SISP practice and proposed both formal methods and principles of good practice. SISP cannot be understood by considering formal methods alone. The processes of planning and the implementation of plans are equally important. However, there have been very few field investigations of these phenomena. This study examines SISP experience in 27 companies and, unusually, relies on interviews not only with IS managers but also with general managers and line managers. By adopting this broader perspective, the investigation reveals companies were using five different SISP approaches: Business-Led, Method-Driven, Administrative, Technological, and Organizational. Each approach has different characteristics and, therefore, a different likelihood of success. The results show that the Organizational Approach appears to be most effective. The taxonomy of the five approaches potentially provides a diagnostic tool for analyzing and evaluating an organization's experience with SISP.
|keyword = STRATEGIC PLANNING,IS STRATEGIC PLANNING,IS MANAGEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''KNOWLEDGE-BASED APPROACHES TO DATABASE DESIGN'''
{{header}}
{{article
|author= VC STOREY,RC GOLDSTEIN,
|source= MIS QUARTERLY
|year= 1993
|abstract = Database design is often described as an intuitive, even artistic, process. Many researchers, however, are currently working on applying techniques from artificial intelligence to provide effective automated assistance for this task. This article presents a summary of the current state of the art for the benefit of future researchers and users of this technology. Thirteen examples of knowledge-based tools for database design are briefly described and then compared in terms of the source, content, and structure of their knowledge bases; the amount of support they provide to the human designer; the data models and phases of the design process they support, and the capabilities they expect of their users. The findings show that there has apparently been very little empirical verification of the effectiveness of these systems. In addition, most rely exclusively on knowledge provided by the developers themselves and have little ability to expand their knowledge based on experience. Although such systems ideally would be used by application specialists rather than database professionals, most of these systems expect the user to have some knowledge of database technology.
|keyword = KNOWLEDGE-BASED SOFTWARE,DATABASE DESIGN,EXPERT SYSTEMS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SMALL-FIRM COMPUTING - MOTIVATORS AND INHIBITORS'''
{{header}}
{{article
|author= PB CRAGG,M KING,
|source= MIS QUARTERLY
|year= 1993
|abstract = This paper examines information system evolution in small firms. It focuses on applications growth, and uses the experiences of six small manufacturing firms to identify motivators and inhibitors of growth. Many factors were identified. Motivators of growth included: improved enthusiasm for the technology. Inadequate resources and limited education about information systems were among the factors that inhibited applications growth.
|keyword = SMALL-FIRM COMPUTING,GROWTH MOTIVATORS,GROWTH INHIBITORS,GROWTH PROCESS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TRAINING END USERS - AN EXPERIMENTAL INVESTIGATION OF THE ROLES OF THE COMPUTER-INTERFACE AND TRAINING METHODS'''
{{header}}
{{article
|author= SA DAVIS,RP BOSTROM,
|source= MIS QUARTERLY
|year= 1993
|abstract = End-user computing has become an area of major importance to organizations over the past several years. As non-professional computer users come to rely on computer systems to perform more and more of their basic tasks, MIS managers need to ensure that those individuals learn to use software packages effectively. Two common, and often complementary, approaches for achieving this goal have been to develop in-house training programs and to utilize computer systems that are ''user friendly. '' Using Assimilation Theory as a basis for this study, we investigate the impacts of two types of training methods and two computer interfaces on users' learning performance and attitudes toward a computer system. A laboratory experiment was conducted to compare a commonly used direct manipulation interface-DMI (the Apple Macintosh) with a commonly used command-based interface-Disk Operating System (DOS). Each interface was presented to subjects by using either an exploration training approach or an instruction-based training approach. Results of the study indicate that individuals using the DMI performed substantially better than those using the command-based interface. However, there was no difference between these two groups in terms of perceived ease of system use. Also, in contrast to previous studies, there were no differences in outcomes related to the two types of training methods. Assimilation Theory is used to explain these results, and recommendations are made for future research.
|keyword = END-USER TRAINING,COMPUTER INTERFACE,LEARNING,NOVICE USERS,ATTITUDES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''COGNITIVE FEEDBACK IN GDSS - IMPROVING CONTROL AND CONVERGENCE'''
{{header}}
{{article
|author= K SENGUPTA,D TEENI,
|source= MIS QUARTERLY
|year= 1993
|abstract = Cognitive feedback in group decision making is information that provides decision makers with a better understanding of their own decision processes and that of the other group members. It appears to be an effective aid in group decision making. Although it has been suggested as a potential feature of group decision support systems (GDSS), little research has examined its use and impact. This article investigates the effect of computer generated cognitive feedback in computer-supported group decision processes. It views group decision making as a combination of individual and collective activity. The article tests whether cognitive feedback can enhance control over the individual and collective decision making processes and can facilitate the process of convergence among group members. In a laboratory experiment with groups of three decision makers, 15 groups received online cognitive feedback and 15 groups did not. Users receiving cognitive feedback maintained a higher level of control over the decision-making process as their decision strategies converged. This research indicates that (1) developers should include cognitive feedback as an integral part of the GDSS at every level, and (2) they should design the human-computer interaction so there is an intuitive and effective transition across the components of feedback at all levels. Researchers should extend the concepts explored here to other models of conflict that deal with ill-structured decisions, as well as study the impact of cognitive feedback over time. Finally, researchers trying to enhance the capabilities of GDSS should continue examining how to take advantage of the differences between individual, interpersonal, and collective decision making.
|keyword = GROUP DECISION MAKING,GROUP DECISION SUPPORT SYSTEMS,GROUP PROCESSES,COGNITIVE FEEDBACK,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Relationship Between Investment in Information Technology and Firm Performance: A Study of the Valve Manufacturing Sector'''
{{header}}
{{article
|author= Peter Weill,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = Large amounts of resources have been and continue to be invested in information technology (IT). Much of this investment is made on the basis of faith that returns will occur. This study presents the results of an empirical test of the performance effects of IT investment in the manufacturing sector. Six years of historical data on IT investment and performance were collected for 33 valve manufacturing firms from the CEO, the controller and the production manager in each firm. Investment was perceptually categorized by management objective (i.e., strategic, informational and transactional) and tested against four measures of performance (sales growth, return on assets, and two measures of labor productivity). Heavy use of transactional IT investment was found to be significantly and consistently associated with strong firm performance over the six years studied. Heavy use of strategic IT was found to be neutral in the long term and associated only with relatively poorly performing firms in the short term. This study suggests that early adopters of strategic IT could have spectacular success but once the technology becomes common, the competitive advantage is lost. In addition, the context of the firm was included in the analysis. Conversion effectiveness, which measures the quality of the firm-wide management and commitment to IT, was found to be a significant moderator between strategic IT investment and firm performance.
|keyword = Information technology (IT),Strategic IT,Management of IT,Investment,Firm performance,Computers,Manufacturing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Diffusion of Information Technology Outsourcing: Influence Sources and the Kodak Effect'''
{{header}}
{{article
|author= Lawrence Loh,N. Venkatraman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = The governance of an organizational information technology (IT) infrastructure is steadily shifting away from pure hierarchical and market mechanisms toward hybrid and partnership modes that involve external vendors. In particular, IT outsourcing has recently emerged as a significant administrative innovation in an organization's IT strategy. This paper seeks to explore the sources of influence in the adoption of this innovation. For this purpose, we generated a comprehensive sample of outsourcing contracts in the US using an electronic bibliometric search process. Using diffusion modeling, our empirical analysis shows that the adoption of IT outsourcing is motivated more by internal influence (or imitative behavior) than by external influence amongst the user organizations. Subsequently, we considered the widely publicized Eastman Kodak's outsourcing decision as a critical event to assess whether this internal influence is more pronounced in the post-Kodak regime than in the pre-Kodak regime. Our results show that internal influence is dominant in the post-Kodak regime but not in the pre-Kodak regime. Implications and directions for future research are discussed.
|keyword = Diffusion models,Information technology governance,Information technology outsourcing,Information technology strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Integrating Mathematical and Symbolic Models Through AESOP: An Expert for Stock Options Pricing'''
{{header}}
{{article
|author= James Clifford,Jr. Henry C. Lucas,Rajan Srikanth,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = This paper reports on an effort to integrate symbolic and mathematical models to tailor the output of a mathematical model to the particular domain of a decision maker. AESOP combines the Black-Scholes model of stock options pricing with an expert system; the integrated model is designed for use by an options specialist on the American Stock Exchange. The specialist makes a number of adjustments to the output of the mathematical model; the purpose of the symbolic model is to make as many of these modifications as possible automatically. The paper reports on the development and structure of AESOP and presents data on its use.
|keyword = Expert systems,Systems design,Symbolic models,Mathematical models,Black-Scholes model,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Performance Evaluation Metrics for Information Systems Development: A Principal-Agent Model'''
{{header}}
{{article
|author= Rajiv D. Banker,Chris F. Kemerer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = The information systems (IS) development activity in large organizations is a source of increasing cost and concern to management. IS development projects are often over-budget, late, costly to maintain, and not done to the satisfaction of the requesting user. These problems exist, in part, due to the organization of the IS development process, where information systems development is typically assigned by the user (principal) to a systems developer (agent). These two parties do not have perfectly congruent goals, and therefore a contract is developed to specify their relationship. An inability to directly monitor the agent requires the use of performance measures, or metrics, to represent the agent's actions to the principal. The use of multiple measures is necessary given the multi-dimensional nature of successful systems development. In practice such contracts are difficult to develop satisfactorily, due in part to an inability to specify appropriate metrics. This paper develops a principal-agent model that provides a set of decision criteria for the principal to use to develop an incentive compatible contract for the agent. These criteria include the precision and the sensitivity of the performance metric. After presenting the formal model, some current software development metrics are discussed to illustrate how the model can be used to provide a theoretical foundation and a formal vocabulary for performance metric analysis. The model is also used in a positive (descriptive) manner to explain why current practice emphasizes metrics that possess relatively high levels of sensitivity and precision. Finally, some suggestions are made for the improvement of current metrics based upon these criteria.
|keyword = Software engineering,Management of computing and information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ETHICAL STANDARDS FOR INFORMATION-SYSTEMS PROFESSIONALS - A CASE FOR A UNIFIED CODE'''
{{header}}
{{article
|author= E OZ,
|source= MIS QUARTERLY
|year= 1992
|abstract = Professions have adopted ethical codes and codes of conduct. Physicians, lawyers, and engineers have moral responsibilities and know to whom they are responsible. Professionals in the information systems field need similar guidance. Unfortunately, multiple professional standards in the field are offered by individual organizations. Many of the precepts in these codes are similar, but some are not. This article Presents the principles of five codes: three U.S. codes, the Canadian code, and the British code. After an examination of their similarities and differences, it is proposed that we (1) resolve the differences and (2) adopt a single, coherent, international code of ethics for the information systems community.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''UNDERSTANDING THE CEO CIO RELATIONSHIP'''
{{header}}
{{article
|author= DF FEENY,BR EDWARDS,KM SIMPSON,
|source= MIS QUARTERLY
|year= 1992
|abstract = The need for top management involvement in the exploitation of information technology (17) is a recurring theme of information management. Previous research has suggested that this involvement is linked, with a two-way relationship between CEO and CIO. This paper reports on an exploratory research study that set out to identify the determinants of a successful two-way relationship. CEOs and CIOs were interviewed in-depth in 14 large organizations based in the United Kingdom. This paper describes an explanatory framework that links the quality of the CEO/CIO relationship to identified attributes of each of the parties, and of their host organization. Successful relationships seem to be linked to a shared vision of the role of IT as an agent of transformation. The CIOs in these successful relationships may have extensive IT backgrounds, but they are accepted into the top management team and are seen to contribute beyond their functional responsibilities.
|keyword = CEOS,CIOS,PARTNERSHIP,RELATIONSHIPS,INFORMATION MANAGEMENT,EXECUTIVE INVOLVEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXECUTIVE OR FUNCTIONAL MANAGER - THE NATURE OF THE CIOS JOB'''
{{header}}
{{article
|author= CS STEPHENS,WN LEDBETTER,A MITRA,FN FORD,
|source= MIS QUARTERLY
|year= 1992
|abstract = The role of the chief information officer (CIO) continues to be the subject of much discussion and speculation. Is this just a new name for the MIS manager, or is there truly a new and significantly different function? How has the role of the information systems manager evolved with the changing needs of business? How does the CIO bridge the gap between the organization's strategy and its use of information technology? According to much of the prescriptive literature, bridging this gap is the CIO's definitive function. This article addresses these questions by studying five successful CIOs in five divergent industries. The CIOs were studied using the structured observation methodology employed by Mintzberg in his study of CEOs and by Ives and Olson in their study of MIS managers. The findings suggest that the CIO operates as an executive rather than a functional manager. He or she is an active participant in strategy planning and acts as a bridge between the information technology group, the functional areas, and external entities. This study provides a view of how these difficult tasks are accomplished on a day-to-day basis: through scheduled meetings, interaction outside the information technology unit, a skilled reading of situations, and a strategic focus. Factors affecting the CIO's participation in strategy planning meetings include whether he or she has formal resource allocation authority and their level of peer acceptance. Lessons learned from this research pertain to the delegation of day-to-day tasks, expenditure authority, avoiding adversarial relationships, liaison activities, careful use of language, being perceived as a user of information technology, and the need for quiet time.
|keyword = CHIEF INFORMATION OFFICERS,IS MANAGEMENT,CORPORATE ROLES AND RELATIONSHIPS,STRUCTURED OBSERVATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''NEW INFORMATION-SYSTEMS LEADERS - A CHANGING-ROLE IN A CHANGING WORLD'''
{{header}}
{{article
|author= LM APPLEGATE,JJ ELAM,
|source= MIS QUARTERLY
|year= 1992
|abstract = It is widely argued that the information systems (IS) leadership function has undergone fundamental changes over the past decade. To better understand the changes, this study compares the backgrounds, responsibilities, reporting relationships, and power of newly appointed IS executives (who had been in their position for two years or less) with established IS executives (who had been in their position for five years or more). The study found that approximately half of the new IS executives were external hires, whereas almost all of the established IS executives were promoted from within the company. More than two-thirds of the new IS executives had more than five years' experience managing a non-IS function within the past 15 years. Established IS executives had spent the majority of their career within the IS function. The activities receiving the most attention from new IS executives were information technology (IT) strategic planning and control, IT architecture management and standards development, and human resource management. For established IS executives, the activities receiving the most attention were IT architecture management and standards development, human resource management, and operations. An increasing number of new IS executives reported directly to the CEO, and almost half were members of the senior management/strategic policy committee. These findings have several important implications. First, the senior IS executive must be able to bring a broad business perspective to the position. Current senior IS executives who have not broadened their own knowledge, skills, and experiences in business strategy, management, and operations should immediately develop a personal career development program to gain these valuable perspectives. Second, senior IS executives should implement career development strategies within their own organizations that ensure that IS professionals have the opportunity to acquire the business management experience necessary to advance to higher IS management levels. Third, graduate and executive programs designed to prepare future IS managers and leaders must provide both a business and IT perspective throughout the curriculum.
|keyword = IS LEADERSHIP,IS AND BUSINESS ALIGNMENT,IS MANAGERS,CIO,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EMPOWERMENT - KEY TO IS WORLD-CLASS QUALITY'''
{{header}}
{{article
|author= HR SHREDNICK,RJ SHUTT,M WEISS,
|source= MIS QUARTERLY
|year= 1992
|abstract = Total customer satisfaction is the pre-eminent goal in Corning Incorporated's Information Services Division (ISD). ISD's total quality management approach, characterized by a work system that embodies significant empowerment through self-managing teams, is proving to be a powerful means of delivering outstanding service that customers value, especially in a competitive services-for-sale business environment. Teams influence how ISD does business, fundamentally changing the way work is organized and how service is delivered. Teams have substantially improved customer satisfaction, service, and productivity, while staff skills have been enhanced and costs reduced. This article describes these results and then discusses the approach for developing and sustaining the new work system as well as the key factors that led to its success.
|keyword = TOTAL QUALITY MANAGEMENT,EMPLOYEE EMPOWERMENT,SELF-MANAGING TEAMS,HIGH-PERFORMANCE WORK SYSTEMS,WORLD-CLASS SERVICE,CUSTOMER SATISFACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ORGANIZATIONAL EXPERIENCES AND CAREER SUCCESS OF MIS PROFESSIONALS AND MANAGERS - AN EXAMINATION OF RACE DIFFERENCES'''
{{header}}
{{article
|author= M IGBARIA,WM WORMLEY,
|source= MIS QUARTERLY
|year= 1992
|abstract = The role that race plays in influencing the work experiences and career outcomes of MIS personnel has been neglected in the MIS human resource management literature to date, despite the significant demographic changes in the work force projected by the year 2000 and beyond. Thus, the need to understand and eliminate any barriers to advancement for various subgroups is essential if employers are to continue attracting the most qualified employees. This study examines relationships among race, organizational experiences, job performance evaluation, and career outcomes of black and white MIS employees in a large, multi-national telecommunications company. Direct and indirect effects of race on job performance evaluations and career outcomes (i.e., advancement prospects, career satisfaction, and organizational commitment) are examined. Compared to white MIS employees, black MIS employees reported less job discretion, less career support, and lower levels of met expectations. In addition, blacks received lower job performance ratings and were less satisfied with their careers than whites. Among other things, it is recommended that organizations be sensitive to and work to prevent the disparate treatment of black and white employees in all areas of the company, especially by supervisors, which negatively affects black employees' opportunities for promotion and advancement and the quality of their experiences on the job. Employers cannot assume automatically that black and white employees, or other subgroups, have the same experiences on the job or that their perceptions of their experiences are the same. Suggestions for additional research are offered and human resouce management implications are identified.
|keyword = MIS MANAGEMENT,MIS PERSONNEL,CAREER MANAGEMENT,RACE DIFFERENCES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD A BETTER UNDERSTANDING OF INFORMATION TECHNOLOGY ORGANIZATION - A COMPARATIVE CASE-STUDY'''
{{header}}
{{article
|author= JE BLANTON,HJ WATSON,J MOODY,
|source= MIS QUARTERLY
|year= 1992
|abstract = The role of information technology (IT) has changed from being merely a tool for processing transactions to a weapon that can affect an organization's competitive position. Because of this change, previous organizational structures for IT groups may no longer be adequate. Organizational theorists have shown that the appropriate structure is influenced by the organization's external environment and strategy. This study contributes toward a better understanding of IT organization by exploring the relationship between the organizational structure of IT groups and the effectiveness of IT support in two companies with similar IT environments. Specifically, the study uses: (1) qualitative analysis to examine IT organizational structure in two very similar companies; (2) quantitative analysis to determine which company has more effective IT support; and (3) an expert panel to identify those differences in IT organizational structure that appear to facilitate effective IT support. Several propositions from these findings are presented and discussed.
|keyword = ORGANIZATIONAL STRUCTURE,DIFFERENTIATION,INTEGRATION,INFORMATION TECHNOLOGY,MANAGEMENT OF INFORMATION SYSTEMS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic Meeting Systems: Evidence from a Low Structure Environment'''
{{header}}
{{article
|author= Poppy Lauretta McLeod,Jeffrey K. Liker,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = The research literature to date on electronic meeting systems (EMS) has been dominated by studies of high structure systems. High and low structure are defined here in terms of the preponderance of influence that technology and groups exert on each other. High structure EMS attempt to directly influence group structure and processes through explicit rules and procedures embedded in the software. Low structure EMS do not explicitly build in rules or procedures that govern group interaction. Low structure EMS are hypothesized to increase task focus through the use of shared flexible software tools and shared views of joint work. Results of two experiments, that compare low structure EMS-supported groups to manually-supported groups on equality of participation, degree of task focus, task performance and member satisfaction, for two different tasks, are reported. The experiments found that this low structure EMS had no effect on participation equality or member satisfaction. Contrary to the hypotheses, the EMS was found to decrease task focus. The EMS led to marginally better task performance on a simple evaluative task, and to worse performance on a complex generative task. A revision of the study's theoretical model is developed which takes into account the nature of the task.
|keyword = Electronic meeting systems,GDSS,Structure,Participation equality,Task focus,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects of Variations in Capabilities of GDSS Designs on Management of Cognitive Conflict in Groups'''
{{header}}
{{article
|author= V. Sambamurthy,Marshall Scott Poole,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = Group decision support systems have been advocated as mechanisms for facilitating conflict management in groups. Two noted shortcomings of research on GDSS effects motivate this study: (i) Most researchers have compared the effects of computer-based and manual delivery of structures for supporting group decision making. By treating the GDSS as a "black box," researchers have neglected attention toward examining the effects of specific capabilities delivered by a GDSS. (ii) Despite the volume of accumulated research, scant attention has been paid to examining GDSS impacts on the group interaction process itself. This research proposes a conceptual view of a GDSS as providing communication and consensus capabilities for supporting the cognitive conflict management process in group decision making. Through a manipulation of the delivery of communication and consensus structures to groups working on a strategic planning task, several exploratory research questions were examined. The GDSS used in this study was the SAMM system. Key results obtained were: (a) despite using the same GDSS structures, groups exhibited a variety of patterns of conflict management processes; (b) the delivery of communication and consensus structures together, as opposed to primarily communication structures, however, did result in higher confrontiveness, or the ability of groups to confront their conflicts and resolve them in positive ways; (c) higher confrontiveness resulted in higher levels of post-meeting consensus; and, (d) computerization of structures enabled groups to confront their conflict and resolve it more positively than when groups were provided with equivalent manual structures. The results demonstrate the value of process-oriented methodological approaches to investigating effects of GDSS designs.
|keyword = Conflict management,Group decision support systems,Process research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Comparison of Linear Keyword and Restricted Natural Language Data Base Interfaces for Novice Users'''
{{header}}
{{article
|author= Kil Soo Suh,A. Milton Jenkins,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = This study compares a linear keyword language interface and a restricted natural language interface for data retrieval by a novice user. The comparison focuses on the effect of different data base interfaces on user performance (as measured by query correctness and query writing time) in a query writing task across varying query types and training levels. To accomplish this objective, a laboratory experiment was conducted using a split-plot factorial design using two between-subjects factors and one within-subjects factor. The results indicate that the restricted natural language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time.
|keyword = Query language,Data base,Restricted natural language,Linear keyword language,User interface,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Determinants of Turnover Intentions: Comparing IC and IS Personnel'''
{{header}}
{{article
|author= Tor Guimaraes,Magid Igbaria,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = Personnel Is one of the most important resources for the performance of Information Systems (IS) and Information Center (IC) organizations. The scarcity of new employees, the difficulty of training and a high turnover make personnel management in these areas a difficult problem. For IS employees, the relationships between job satisfaction, organizational commitment and intention to leave the organization have been established. Because the size of the investment and the number of organizations establishing IC organizations are growing dramatically, it has become important to understand the determinants of turnover intentions for IC as well as IS employees. Are IC employees similar to their IS counterparts? Or, is their nature basically different, as some studies have suggested? This study examines the differences between IS and IC employees in terms of demographic characteristics, participation on boundary spanning activities, role stressors, overall job satisfaction, organizational commitment and turnover intentions. The differences are found to be significant and call for special attention from IC managers to manage mire properly their personnel resources. IC employees were found to participate more extensively in boundary spanning activities, experienced more role stressors (role ambiguity and role conflict), were less satisfied with their jobs and less committed to their organization. The findings also demonstrate the importance of organizational commitment as an intervening variable in models of turnover. While overall job satisfaction had both direct and indirect effects on turnover intentions among IC employees, for IS personnel it had only indirect effects through organizational commitment. The effects of role stressors and boundary spanning activities were found to be indirect via overall job satisfaction and organizational commitment for both IC and IS employees. The implications of these findings for practicing managers and for future research are discussed.
|keyword = Information Center,IC Personnel,IS Personnel,Turnover intentions,Job satisfaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE PRISM SYSTEM - A KEY TO ORGANIZATIONAL-EFFECTIVENESS AT FEDERAL EXPRESS CORPORATION'''
{{header}}
{{article
|author= PC PALVIA,JA PERKINS,SM ZELTMANN,
|source= MIS QUARTERLY
|year= 1992
|abstract = This paper examines a new type of information system rarely described in the MIS literature, probably because very few such systems exist today. The system, which we have labelled as an organizational effectiveness system (OES), is designed to ensure the health and well-being of an organization in an increasingly complex and turbulent world. Its purpose and net effect are to allow the organization to be flexible and responsive. PRISM, a system developed at Federal Express Corporation, is an imitable example of such a system. As an advanced multi-technology system, it includes core personnel functions, expanded personnel and organizational functions, and extensive external interface features. The PRISM system permits significant and constant interactions with all managers and all employees. This paper describes PRISM in considerable detail, as well as its impact and benefits. By extrapolating from the Federal Express experience, critical success factors are derived for the development and implementation of organizational effectiveness systems in other organizations.
|keyword = ORGANIZATIONAL EFFECTIVENESS,ORGANIZATIONAL FLEXIBILITY,INFORMATION SYSTEM,MANAGEMENT OF INFORMATION SYSTEMS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF DATA INTEGRATION ON THE COSTS AND BENEFITS OF INFORMATION-SYSTEMS'''
{{header}}
{{article
|author= DL GOODHUE,MD WYBO,LJ KIRSCH,
|source= MIS QUARTERLY
|year= 1992
|abstract = For many organizations, the ability to make coordinated, organization-wide responses to today's business problems is thwarted by the lack of data integration or commonly defined data elements and codes across different information systems. Though many researchers and practitioners have implicitly assumed that data integration always results in net benefits to an organization, this article questions that view. Based on theories of organizational information processing, a model of the impact of data integration is developed that includes gains in organization-wide coordination and organization-wide decision making, as well as losses in local autonomy and flexibility, and changes in system design and implementation costs. The importance of each of these impacts is defended by theoretical arguments and illustrated by case examples. This model suggests that the benefits of data integration will outweigh costs only under certain situations, and probably not for all the data the organization uses. Therefore, MIS researchers and practitioners should consider the need for better conceptualization and methods for implementing ''partial integration'' in organizations.
|keyword = ORGANIZATION-WIDE INFORMATION SYSTEMS,DATA INTEGRATION,INTERDEPENDENCE,FLEXIBILITY,IMPLEMENTATION COSTS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE APPLICATION OF ELECTRONIC MEETING TECHNOLOGY TO SUPPORT STRATEGIC MANAGEMENT'''
{{header}}
{{article
|author= CK TYRAN,AR DENNIS,DR VOGEL,JF NUNAMAKER,
|source= MIS QUARTERLY
|year= 1992
|abstract = Strategic management is often performed by groups of managers. By improving the communication process of such groups, strategic management might be enhanced. This paper investigates the application of electronic meeting systems (EMS) technology to support strategic management. Eight cases involving five organizations using an EMS facility are examined to: (1) determine how organizational groups use EMS for strategic management, (2) assess the overall effectiveness and efficiency of the EMS approach to strategic management, and (3) assess the capability of an EMS to address a variety of group process and communication issues in an organizational context. The findings indicate that EMS technology can address a number of the theoretical and practical concerns associated with strategic management meetings involving large heterogeneous groups of managers. Implications for the design of EMS to support strategic management are discussed, and opportunities for future research are identified.
|keyword = ELECTRONIC MEETING SYSTEMS,DECISION SUPPORT,GROUP DECISION SUPPORT SYSTEMS,GROUP SUPPORT SYSTEMS,STRATEGIC DECISION MAKING,STRATEGIC MANAGEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE USE OF DECISION CRITERIA IN SELECTING INFORMATION-SYSTEMS TECHNOLOGY INVESTMENTS'''
{{header}}
{{article
|author= CJ BACON,
|source= MIS QUARTERLY
|year= 1992
|abstract = In a competitive environment, selecting and effectively pursuing the right information systems/technology (IST) investments can be a key factor in sustaining corporate viability and prosperity. This study examines the criteria used by 80 organizations in allocating strategic IST resources. Senior executives were asked to indicate which of 15 criteria they use in deciding among competing projects. They also identified how frequently the criteria are used and ranked them by importance. The results indicate that criteria such as the support of explicit business objectives and response to competitive systems are now important in selecting IST investments. Although financial criteria are used by most organizations, the extent of analysis and application appears to leave room for improvement.
|keyword = PROJECT SELECTION,INFORMATION TECHNOLOGY INVESTMENTS,SYSTEMS SPENDING,PROJECT EVALUATION AND APPROVAL,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE INFLUENCE OF THE INFORMATION-SYSTEMS DEVELOPMENT APPROACH ON MAINTENANCE'''
{{header}}
{{article
|author= SM DEKLEVA,
|source= MIS QUARTERLY
|year= 1992
|abstract = This is an exploratory field study that examines the influence of selected system development methodologies on maintenance time. A number of factors related to the early stages of information systems development and to information systems maintenance were investigated: development methodology, maintenance time and its allocation, number of users, their understanding and involvement, system documentation, software quality, system characteristics, project controllability system size and age, organization of the maintenance function, use of tools, ability of personnel, stability of organization, and others. The survey findings do not support the proposition that the application of modern information systems development methodology decreases maintenance time. However, some benefits are identified. Time spent on emergency error correction, as well as the number of system failures, decreased significantly with the application of modem methodology Systems developed with modern methodologies seem to facilitate making greater changes in functionality as the systems age.
|keyword = INFORMATION SYSTEMS,SOFTWARE MAINTENANCE,SOFTWARE ENGINEERING,SYSTEMS DEVELOPMENT,SYSTEMS ANALYSIS,SYSTEMS MAINTENANCE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE USE OF INFORMATION IN DECISION-MAKING - AN EXPERIMENTAL INVESTIGATION OF THE IMPACT OF COMPUTER-BASED DECISION AIDS'''
{{header}}
{{article
|author= P TODD,I BENBASAT,
|source= MIS QUARTERLY
|year= 1992
|abstract = The traditional assumption in the decision support systems (DSS) literature is that if decision makers are provided with expanded processing capabilities they will use them to analyze problems in more depth and, as a result, make better decisions. Empirical studies investigating the relationship between DSS and decision quality have not borne this out. The explanation for such outcomes could be found in behavioral decision-making theories. The literature on behavioral decision making indicates that the conservation of effort may be more important than increased decision quality in some cases. If this is so, then the use of a decision aid may result in effort savings but not improved decision performance. The two experiments reported here use verbal protocol analysis to compare the extent of information use by unaided decision makers and users of a decision aid designed to support preferential choice problems. The results of the two studies indicate that subjects with a decision aid did not use more information than those without one. Overall, subjects behaved as if effort minimization was an important consideration. For DSS researchers these studies indicate that to understand the DSS-decision quality relationship, it is necessary to consider the decision maker's tradeoff between improving decision quality and conserving effort. For DSS designers these results imply a need to focus on the moderating role that effort will play in determining DSS effectiveness.
|keyword = DECISION SUPPORT SYSTEMS,INFORMATION PROCESSING,COGNITIVE COST BENEFIT THEORY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''FACTORS AFFECTING SOFTWARE DEVELOPERS PERFORMANCE - AN INTEGRATED APPROACH'''
{{header}}
{{article
|author= RH RASCH,HL TOSI,
|source= MIS QUARTERLY
|year= 1992
|abstract = Software developers' performance has a direct impact on software development productivity. An understanding of the factors that affect this performance could help determine where to concentrate management efforts (and related financial resources) from a practical standpoint, and where to focus research efforts from an academic perspective. To gain further insight into these factors, this study extends prior research by integrating elements from expectancy theory, goal-setting theory, and organizational behavior specific to the software development process. The research results provide new insights regarding the relative importance of how expectancy theory, goal-setting theory, and individual characteristics affect the perceived performance of software development professionals. These preliminary findings indicate that goal-setting theory may have complex implications for software development performance. Goal difficulty has a negative relationship to performance but a positive relationship to effort. Because of this off-setting effect, the degree of goal difficulty has a relatively small overall effect on performance. Goal clarity also has a relatively small effect on performance. Individual ability has the strongest direct effect on perceived performance, more than twice as strong as the effects of work effort, personality dimensions, and perceived characteristics of the task High achievement needs were directly related to both effort and perceived performance, whereas self-esteem and locus of control have a direct relationship to perceived performance.
|keyword = MANAGEMENT,PRODUCTIVITY,SOFTWARE DEVELOPMENT PERFORMANCE,EXPECTANCY THEORY,GOAL-SETTING THEORY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Unravelling Is-a Structures'''
{{header}}
{{article
|author= Robert C. Goldstein,Veda C. Storey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = Is-a relationships are widely recognized as conveying important information for database design. Although these relationships are implicitly hierarchical in nature, it is not uncommon to find collections of is-a relationships that form nonhierarchical structures. This paper formally defines structures and classifies and interprets them. It illustrates how is-a structures can be used to identify possible database design errors or inefficiencies and to produce a database design that incorporates more of the semantics of an application.
|keyword = Database design,Is-a relationships,Lattices,Integrity constraints,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Conceptual Model of Adaptive Knowledge-Based Systems'''
{{header}}
{{article
|author= Pi-Sheng Deng,Abhijit Chaudhury,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = The ability to learn or adapt is widely recognized as one of the most prominent abilities of any animate or inanimate intelligent system. While considerable progress has been made in the science and technology of machine learning, little of that has been incorporated in traditional knowledge-based systems such as diagnostic or expert systems operating in a managerial environment. In this paper a conceptual model of an adaptive expert system is proposed as an attempt to lay a foundation for building knowledge-based systems that can learn by interacting with the environment. In contrast to existing models for learning (such as for knowledge acquisition and skill refinement) where the issue of noise and uncertainty, is usually neglected. our model incorporates a stochastic environment and a learning response behavior which too is stochastic in nature.
|keyword = Adaptive expert systems,Admissible plans,learning,learning automata,Operational schema of expert systems,Opportunity cost,Recognization cycles,Relative loss,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Comparative Study of How Experts and Novices Use a Decision Aid to Solve Problems in Complex Knowledge Domains'''
{{header}}
{{article
|author= Jane M. Mackay,Joyce J. Elam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = This paper reports the results of a study that examined the way that professionals used a particular decision aid, spreadsheet software, to address a business -oriented task. The professionals were divided into four groups, depending on whether they were experts or novices in the functional area of business represented by the task and on whether they were experts or novices in the use of spreadsheet software. The physical interactions as well as the thought processes underlying these interactions were captured. Results of the study indicated that a lack of expertise in spreadsheet software usage inhibited the application of functional area knowledge. The behavior and outcomes of the functional area expert/spreadsheet novice group did not match those of the functional area expert/spreadsheet expert group. In fact, they paralleled more closely those of the other two groups that did not possess functional area knowledge. Results of the study also indicated that individuals need to obtain a level of expertise in using a decision aid before they are able to apply their functional area knowledge to the problem at hand.
|keyword = Decision aids,DSS,Problem solving,Decision making,Human-computer interaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''After-Hours Telecommuting and Work-Family Conflict: A Comparative Analysis'''
{{header}}
{{article
|author= Linda Elizabeth Duxbury,Christopher Alan Higgins,Shirley Mills,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = After-hours telecommuting (AHT) is a work arrangement where job -relevant work is done at home on a computer outside of regular office hours. This study examined how after-hours telecom muting affects an individual's ability to balance work and family demands (measured as role overload, spillover of interference from work to family and spillover of interference from family to work). It also examined the impact of gender and maternal career employment on these relationships. The analysis showed that men and women who performed after-hours telecommuting worked significantly more hours per week and a greater number of hours of overtime at home than did individuals without computers at home. After controlling for total work hours of both spouses, significant gender differences and differences due to performing after-hours telecommuting were found. These differences were associated with role overload and spillover of interference from work to family.
|keyword = Telecommuting,Computer-based homework,Supplemental work-at-home,Balancing work family,Work-family conflict,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXAMINING THE FEASIBILITY OF A CASE-BASED REASONING MODEL FOR SOFTWARE EFFORT ESTIMATION'''
{{header}}
{{article
|author= T MUKHOPADHYAY,SS VICINANZA,MJ PRIETULA,
|source= MIS QUARTERLY
|year= 1992
|abstract = Existing algorithmic models fail to produce accurate software development effort estimates. To address this problem, a case-based reasoning model, called Estor, was developed based on the verbal protocols of a human expert solving a set of estimation problems. Estor was then presented with 15 software effort estimation tasks. The estimates of Estor were compared to those of the expert as well as those of the function point and COCOMO estimations of the projects. The estimates generated by the human expert and Estor were more accurate and consistent than those of the function point and COCOMO methods. In fact, Estor was nearly as accurate and consistent as the expert. These results suggest that a case-based reasoning approach for software effort estimation holds promise and merits additional research.
|keyword = SOFTWARE EFFORT ESTIMATION,CASE-BASED REASONING,CONSTRUCTIVE COST MODEL,FUNCTION POINTS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THREATS TO INFORMATION-SYSTEMS - TODAYS REALITY, YESTERDAYS UNDERSTANDING'''
{{header}}
{{article
|author= KD LOCH,HH CARR,ME WARKENTIN,
|source= MIS QUARTERLY
|year= 1992
|abstract = Information systems security remains high on the list of key issues facing information systems executives. Traditional concerns range from forced entry into computer and storage rooms to destruction by fire, earthquake, flood, and hurricane. Recent attention focuses on protecting information systems and data from accidental or intentional unauthorized access, disclosure, modification, or destruction. The consequences of these events can range from degraded or disrupted service to customers to corporate failure. This article reports on a study investigating MIS executives' concern about a variety of threats. A relatively new threat, computer viruses, was found to be a particular concern. The results highlight a gap between the use of modem technology and the understanding of the security implications inherent in its use. Many of responding information systems managers have migrated their organizations into the highly interconnected environment of modem technology but continue to view threats from a perspective of a pre-connectivity era. They expose their firms to unfamiliar risks of which they are unaware, refuse to acknowledge, or are often poorly equipped to manage.
|keyword = THREATS,INFORMATION SYSTEMS SECURITY,COMPUTER VIRUSES,COMPUTER SECURITY,COMPUTER LAWS,INFORMATION RESOURCES MANAGEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MANAGING TELECOMMUNICATIONS BY STEERING COMMITTEE'''
{{header}}
{{article
|author= G TORKZADEH,WD XIA,
|source= MIS QUARTERLY
|year= 1992
|abstract = The increasing need for integration and the rapid growth of online systems have made telecommunications a vital part of management information systems (MIS). In search of competitive advantage, organizations make significant investments in telecomunications. Telecommunications management is becoming a top priority of information systems executives. The MIS literature suggests that steering committees are effective means of managing information systems. However, there is no information on how steering committees impact the management of the telecommunications function. Drawing on organizational theory and MIS literature, a framework is presented that relates firm size and telecommunications steering committees to planning practices and organizational recognition and support. Using a survey of 137 organizations, this framework is examined. The results of this exploratory research suggest that use of a telecommunications steering committee is associated with firm size, planning practices, and top management recognition and support. As firms grow, they tend to more frequently use steering committees for interunit coordination, setting policies, allocating recourses, and monitoring progress. These steering committees can also promote organizational recognition and secure funding commitments for the telecommunications function.
|keyword = TELECOMMUNICATIONS,STEERING COMMITTEE,STRATEGIC PLANNING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MICROCOMPUTER PLAYFULNESS - DEVELOPMENT OF A MEASURE WITH WORKPLACE IMPLICATIONS'''
{{header}}
{{article
|author= J WEBSTER,JJ MARTOCCHIO,
|source= MIS QUARTERLY
|year= 1992
|abstract = Microcomputer playfulness represents the degree of cognitive spontaneity in microcomputer interactions. Research on the general characteristic of playfulness has demonstrated relationships with measures such as creativity and exploration. Thus, with the widespread diffusion of computers in organizations, research in microcomputer playfulness can have significant practical implications for organizations. Five independent studies involving more than 400 participants provided initial evidence for the construct validity of a microcomputer playfulness measure with respect to its factor structure, internal consistency reliability, concurrent validity, discriminant validity, predictive validity, predictive efficacy, and test-retest reliability. As hypothesized, the measure related positively to computer attitudes, anxiety, competence, and efficacy, and did not relate to gender or age. In addition, the measure related positively to training outcomes of learning, mood, involvement, and satisfaction. Further, the evidence suggests the predictive efficacy of microcomputer playfulness as compared to other variables, such as computer anxiety and attitudes. Consequently, the findings indicate that researchers should focus more attention on positive influences on human-computer interaction, such as microcomputer playfulness, rather than on negative influences, such as computer anxiety.
|keyword = INDIVIDUAL CHARACTERISTICS,TRAITS,COMPUTER ATTITUDES,COMPUTER ANXIETY,COMPUTER TRAINING,MOTIVATION,LEARNING,EXPLORATION,SPONTANEITY,CREATIVITY,HUMAN-COMPUTER INTERACTION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PERCEIVED USEFULNESS, EASE OF USE, AND USAGE OF INFORMATION TECHNOLOGY - A REPLICATION'''
{{header}}
{{article
|author= DA ADAMS,RR NELSON,PA TODD,
|source= MIS QUARTERLY
|year= 1992
|abstract = This paper presents the findings of two studies that replicate previous work by Fred Davis on the subject of perceived usefulness, ease of use, and usage of information technology. The two studies focus on evaluating the psychometric properties of the ease of use and usefulness scales, while examining the relationship between ease of use, usefulness, and system usage. Study 1 provides a strong assessment of the convergent validity of the two scales by examining heterogeneous user groups dealing with heterogeneous implementations of messaging technology. In addition, because one might expect users to share similar perspectives about voice and electronic mail, the study also represents a strong test of discriminant validity. In this study a total of 1 18 respondents from 10 different organizations were surveyed for their attitudes toward two messaging technologies: voice and electronic mail. Study 2 complements the approach taken in Study 1 by focusing on the ability to demonstrate discriminant validity. Three popular software applications (WordPerfect, Lotus 1-2-3, and Harvard Graphics) were examined based on the expectation that they would all be rated highly on both scales. In this study a total of 73 users rated the three packages in terms of ease of use and usefulness. The results of the studies demonstrate reliable and valid scales for measurement of perceived ease of use and usefulness. In addition, the paper tests the relationships between ease of use, usefulness, and usage using structural equation modelling. The results of this model are consistent with previous research for Study 1, suggesting that usefulness is an important determinant of system use. For Study 2 the results are somewhat mixed, but indicate the importance of both ease of use and usefulness. Differences in conditions of usage are explored to explain these findings.
|keyword = USER ACCEPTANCE,END-USER COMPUTING,USER MEASUREMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A SOCIAL-PROCESS MODEL OF USER-ANALYST RELATIONSHIPS'''
{{header}}
{{article
|author= M NEWMAN,D ROBEY,
|source= MIS QUARTERLY
|year= 1992
|abstract = The development of an information system is a social process involving users and systems analysts, carried out in an organizational setting. This paper presents a process model of user-analyst relationships to guide research into the social dynamics of system development. The model identifies antecedent conditions, encounters, episodes, and outcomes over the course of a project. The model asserts that established relationships between analysts and users will persist unless critical encounters change the trajectory of the project. By conceiving of systems development as a series of encounters and episodes, researchers may identify critical encounters and study the connections between preceding events and their consequences. Practitioners may use the model to diagnose problems and to enact critical encounters that move a project in a different direction. The descriptive and predictive capacities of the process model are illustrated with two case studies.
|keyword = INFORMATION SYSTEMS DEVELOPMENT,USER PARTICIPATION,SOCIAL PROCESS MODELS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Model Integration and Modeling Languages: A Process Perspective'''
{{header}}
{{article
|author= Jeffrey E. Kottemann,Daniel R. Dolk,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = Development of large-scale models often involves or, certainly could benefit from linking existing models. This process is termed model integration and involves two related aspects: (1) the coupling of model representations, and (2) the coupling of the processes for evaluating, or executing, instances of these representations. Given this distinction, we overview model integration capabilities in existing executable modeling languages, discuss current theoretical approaches to model integration, and identify the limiting assumptions implicitly made in both cases. In particular. current approaches assume away issues of dynamic variable correspondence and synchronization in composite model execution. We then propose a process-oriented conceptualization and associated constructs that overcome these limiting assumptions. The constructs allow model components to be used as building blocks for more elaborate composite models in ways unforeseen when the components were originally developed. While we do not prove the sufficiency of the constructs over the set of all model types and integration configurations, we present several examples of model integration from various domains to demonstrate the utility of the approach.
|keyword = Model management,Model integration,Modeling systemc,Modeling languages,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effects and Limitations of Automated Text Condensing on Reading Comprehension Performance'''
{{header}}
{{article
|author= Andrew H. Morris,George M. Kasper,Dennis A. Adams,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = The optimal amount of information needed in a given decision-making situation lies somewhere along a continuum from "not enough" to "too much". Ackoff proposed that information systems often hinder the decision-making process by creating information overload. To deal with this problem, he called for systems that could filter and condense data so that only relevant information reached the decision maker. The potential for information overload is especially critical in text-based information. The purpose of this research is to investigate the effects and theoretical limitations of extract condensing as a text processing tool in terms of recipient performance. In the experiment described here, an environment is created in which the effects of text condensing are isolated from the effects of message and individual recipient differences. The data show no difference in reading comprehension performance between the condensed forms and the original document. This indicates that condensed forms can be produced that are equally as informative as the original document. These results suggest that it is possible to apply a relatively simple computer algorithm to text and produce extracts that capture enough of the information contained in the original document so that the recipient can perform as if he or she had read the original. These results also identify a methodology for assessing the effectiveness of text condensing schemes. The research presented here contributes to a small but growing body of work on text-based information systems and, specifically, text condensing.
|keyword = Text-based information systems,Text condensing,Abstracting/extracting,Performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Building an Information System Design Theory for Vigilant EIS'''
{{header}}
{{article
|author= Joseph G. Walls,George R. Widmeyer,Omar A. El Sawy,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = This paper defines an information system design theory (ISDT) to be a prescriptive theory which integrates normative and descriptive theories into design paths intended to produce more effective information systems. The nature of ISDTs is articulated using Dubin's concept of theory building and Simon's idea of a science of the artificial. An example of an ISDT is presented in the context of Executive Information Systems (EIS). Despite the increasing awareness of the potential of EIS for enhancing executive strategic decision making effectiveness, there exists little theoretical work which directly guides EIS design. We contend that the underlying theoretical basis of EIS can be addressed through a design theory of vigilant information systems. Vigilance denotes the ability of an information system to help an executive remain alertly watchful for weak signals and discontinuities in the organizational environment relevant to emerging strategic threats and opportunities. Research on managerial information scanning and emerging issue tracking as well as theories of open loop control are synthesized to generate vigilant information system design theory propositions. Transformation of the propositions into testable empirical hypotheses is discussed.
|keyword = Executive information systems,Design theory,Information system design,Vigilance,Open loop control,Issue management,Executive scanning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Systems Success: The Quest for the Dependent Variable'''
{{header}}
{{article
|author= William H. DeLone,Ephraim R. McLean,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1992
|abstract = A large number of studies have been conducted during the last decade and a half attempting to identify those factors that contribute to information systems success. However, the dependent variable in these studies I/S success has been an elusive one to define. Different researchers have addressed different aspects of success, making comparisons difficult and the prospect of building a cumulative tradition for I/S research similarly elusive. To organize this diverse research, as well as to present a more integrated view of the concept of I/S success, a comprehensive taxonomy is introduced. This taxonomy posits six major dimensions or categories of I/S success SYSTEM QUALITY, INFORMATION QUALITY, USE, USER SATISFACTION, INDIVIDUAL IMPACT, and ORGANIZATIONAL IMPACT. Using these dimensions, both conceptual and empirical studies are then reviewed (a total of 180 articles are cited) and organized according to the dimensions of the taxonomy. Finally, the many aspects of I/S success are drawn together into a descriptive model and its implications for future I/S research are discussed.
|keyword = Information systems success,Information systems assessment,Measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STRATEGIC DATA PLANNING - LESSONS FROM THE FIELD'''
{{header}}
{{article
|author= DL GOODHUE,LJ KIRSCH,JA QUILLARD,MD WYBO,
|source= MIS QUARTERLY
|year= 1992
|abstract = In spite of strong conceptual arguments for the value of strategic data planning as a means to increase data integration in large organizations, empirical research has found more evidence of problems than of success. In this paper, four detailed case studies of SDP efforts, along with summaries of five previously reported efforts, are analyzed. Fifteen specific propositions are offered, with two overall conclusions. The first conclusion is that SDP, though conceived of as a generally appropriate method, may not be the best planning approach in all situations. The second conclusion is that the SDP method of analyzing business functions and their data requirements may not be the best way to develop a "data architecture," given the required level of commitment of talented individuals, the cost, the potential errors, and the high level of abstraction of the result. These lessons can aid practioners in deciding when to use SDP and guide them as they begin the process of rethinking and modifying the SDP to be more effective.
|keyword = STRATEGIC DATA PLANNING,DATA INTEGRATION,DATA MANAGEMENT,DATA ADMINISTRATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE ORGANIZATIONAL INTERFACE - A METHOD FOR SUPPORTING END USERS OF PACKAGED SOFTWARE'''
{{header}}
{{article
|author= EM TRAUTH,E COLE,
|source= MIS QUARTERLY
|year= 1992
|abstract = The growth of end-user computing and the trend toward the use of packaged software have focused greater attention on issues related to user support. A varied user population is requiring more tailored support, yet vendor-supplied software with a generic interface is increasingly being used. Four case studies illustrate ways in which organizational forms of support can enhance or overcome the limitations of the software interface. A framework called the organizational interface incorporates these organizational mechanisms by integrating the computer-human interaction (CHI), management information systems (MIS), and end-user computing (EUC) approaches to user support. This framework can be used in the design of end-user systems developed with packaged software and for retrofitting outdated user interfaces for an end-user computing environment. The organizational interface provides IS and end-user managers with a means to consider alternative methods of providing support for systems based on purchased software with fixed user interfaces.
|keyword = END-USER COMPUTING,COMPUTER-HUMAN INTERACTION,HUMAN FACTORS,INFORMATION CENTERS,SOFTWARE INTERFACE,ERGONOMICS,USER INTERFACE,USER SUPPORT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PROBLEMS AND ISSUES IN THE MANAGEMENT OF INTERNATIONAL DATA COMMUNICATIONS NETWORKS - THE EXPERIENCES OF AMERICAN COMPANIES'''
{{header}}
{{article
|author= PJ STEINBART,R NATH,
|source= MIS QUARTERLY
|year= 1992
|abstract = The trend toward global competition is increasing the importance of international data communications networks. Indeed, many executives believe such networks are critical to their company's future success. The challenge for these companies is to successfully establish and manage a global network to facilitate the transfer of data across international borders. Prior to this study, however, little was known about the problems involved with developing a global network or about the factors contributing to their success. A survey of American companies indicates that the major problems encountered with global networks are technical in nature, related primarily to the quality of foreign telephone network services. These technical problems, however, are exacerbated by politically imposed constraints that restrict firms' options for managing their global network. Respondents also express concern about the lack of adequate top management support to deal with the problems involved in running a global network. The study also finds that both management policies and firm characteristics affect the successful operation of a global network. Firms that established a help center to deal with the issues involved in international data transfer are more satisfied with their networks than are firms that do not have such a help center. In addition, firms that use their networks to exchange data with outside parties (e.g., customers or suppliers) are also more satisfied with their networks than are firms that only exchange data internally.
|keyword = GLOBAL NETWORKS,INTERNATIONAL DATA COMMUNICATIONS,TRANSBORDER DATA FLOW,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ON GENERALIZING THE CONCEPT OF HYPERTEXT'''
{{header}}
{{article
|author= MP BIEBER,SO KIMBROUGH,
|source= MIS QUARTERLY
|year= 1992
|abstract = Hypertext has quickly become an established paradigm in the design of information systems. The success of products in the software market, evident benefits as reported by users, and the flowering of related research activity all attest to the significance and staying power of hypertext-rich information systems. Although standard hypertext has a number of unquestioned benefits, the concept also has a number of well-known problems and limitations. This article reviews the main problems and limitations of basic (standard) hypertext that constrain the use of hypertext in practical applications. Further, this article presents and discusses our "generalization" of the basic hypertext concept, which we call generalized hypertext. These generalizations encompass, among other things, automatic creation of hypertext elements. Generalized hypertext promises to be more powerful than standard hypertext as well as less expensive to implement and maintain. To illustrate these concepts, we describe the implementation of a decision support system currently in use by the U.S. Coast Guard.
|keyword = HYPERTEXT,GENERALIZED HYPERTEXT,HYPERTEXT COMPUTATION,VIRTUAL LINKING,DYNAMIC LINKING,DECISION SUPPORT SYSTEMS,INFORMATION PRESENTATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REVISITING DSS IMPLEMENTATION RESEARCH - A METAANALYSIS OF THE LITERATURE AND SUGGESTIONS FOR RESEARCHERS'''
{{header}}
{{article
|author= M ALAVI,EA JOACHIMSTHALER,
|source= MIS QUARTERLY
|year= 1992
|abstract = Information systems are becoming increasingly critical to the daily operations and success of many firms. This, combined with the rising investments in design and development of these systems, make implementation a high priority research topic. Although information systems implementation has been a topic of interest to researchers over the past two decades, the extent to which the existing body of research reflects substantial and cumulative development is not entirely clear. The objective of this study is to conduct a rigorous and quantitative review of the empirical DSS implementation literature as a basis for providing guidelines for implementation management and conduct of future research. Meta-analysis of 144 findings from 33 studies indicates that user-situational variables (involvement, training and experience) are more important than psychological factors to DSS implementation success and that user-situational variables can improve the implementation success by as much as 30 percent. Furthermore, the meta-analytic findings regarding the methodological characteristics of studies provide useful insights for the design of future research studies of implementation. The findings also allow us to put into perspective the incremental contribution of additional substantive and empirical studies in this area. Additionally, several specific domains (e.g., construct validation research on user involvement and casual modeling) might profit most from future research efforts.
|keyword = DECISION SUPPORT SYSTEMS,IMPLEMENTATION,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A SYNTHESIS OF RESEARCH ON REQUIREMENTS ANALYSIS AND KNOWLEDGE ACQUISITION TECHNIQUES'''
{{header}}
{{article
|author= TA BYRD,KL COSSICK,RW ZMUD,
|source= MIS QUARTERLY
|year= 1992
|abstract = Requirements analysis (RA) involves end users and systems analysts interacting in an effort to recognize and specify the data and information needed to develop an information system. In the design of expert systems, a similar process of eliciting information, in this case human knowledge, has been studied under the banner of knowledge acquisition (KA). When examined closely, many entities and processes involved in RA and KA are almost identical. However, researchers in each area are seemingly unaware of the developments in the other area. In order to facilitate a merged awareness of both research streams, this article compares representative RA and KA techniques, which are grouped, according to elicitation mode, on three dimensions: communication obstacles, a technique's locus of control, and the nature of the understanding gained from using the technique. This comparison demonstrates that these two research streams have many things in common and that researchers in one area can benefit from developments in the other area. Additionally, this analysis leads to several suggested research areas: (1) rigorous examinations of these techniques as they are used to overcome communication obstacles and enrich understanding; (2) investigations into the seeming match between certain elicitation types and problem domain categories; (3) examinations into synergetic effects of elicitation techniques; (4) development of more techniques for eliciting information requirements to serve emerging needs; and (5) comparisons of the relative advantage of generalized versus specialized elicitation techniques.
|keyword = INFORMATION REQUIREMENTS DETERMINATION,KNOWLEDGE ACQUISITION,INFORMATION SYSTEMS DESIGN,EXPERT SYSTEMS DESIGN,COMMUNICATION TOOLS,SYSTEM DEVELOPMENT TECHNIQUES AND TOOLS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Software-Effort Estimation: An Exploratory Study of Expert Performance'''
{{header}}
{{article
|author= Steven S. Vicinanza,Tridas Mukhopadhyay,Michael J. Prietula,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = An exploratory study was conducted (a) to examine whether experienced software managers could generate accurate estimates of effort required for proposed software projects and (b) to document the strategies they bring to bear in their estimations. Five experienced software project managers served as expert subjects for the study. Each manager was first asked to sort a set of 37 commonly-used estimation parameters according to the importance of their effect on effort estimation. Once this task was completed, the manager was then presented with data from ten actual software projects, one at a time, and asked to estimate the effort (in worker-months) required to complete the projects. The project sizes ranged from 39,000 to 450,000 lines of code and varied from 23 to 1,107 worker-months to complete. All managers were tested individually. The results were compared to those of two popular analytical models Function Points and COCOMO. Results show that the managers made more accurate estimates than the uncalibrated analytical models. Additionally, a process-tracing analysis revealed that the managers used two dissimilar types of strategies to solve the estimation problems algorithmic and analogical. Four managers invoked algorithmic strategies, which relied on the selection of a base productivity rate as an anchor that was further adjusted to compensate for productivity factors impacting the project. The fifth manager invoked analogical strategies, which did not rely on a base productivity rate as an anchor, but centered around the analysis of the Function Point data to assist in retrieving information regarding a similar, previously -managed project. The manager using the latter, analogical reasoning approach produced the most accurate estimates.
|keyword = Software-effort estimation,Expert reasoning,Analogical reasoning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Issues in Model Specification'''
{{header}}
{{article
|author= Anantaram Balakrishnan,Andrew B. Whinston,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = This paper explores the tradeoffs and information issues in model specification, i.e., choosing the appropriate level of detail and precision of decision support models. For many real-time and distributed decision problems, decision -makers face a wide spectrum of information gathering choices that vary in the amount and quality of information as well as their associated costs and delays. Modeling these problems raises several operational and strategic questions including: how much and what type of information to acquire before making a decision. what protocol to use for exchanging information among multiple decision -makers. what organization and information structure (e.g., centralized, coordinated, distributed) to use for effective information sharing and decision making. We highlight the modeling issues and tradeoffs using examples from production planning, distributed processing, and network routing, and draw upon research in diverse fields, including information economics and game theory, knowledge logic in computer science, and distributed computation, to obtain model specification insights. Our discussions emphasize and illustrate two main model specification themes. First, acquiring additional information often has diminishing returns (in terms of the quality of decisions suggested by the model); therefore, an imprecise model based on partial information might be more appropriate than using a detailed and accurate model that identifies 'optimal' decisions. Concepts from information economics and team theory provide a framework for analyzing this tradeoff. We also briefly discuss some heuristic methods to identify effective information gathering strategies. Our second theme applies to systems consisting of several decision makers who make interdependent decisions. In this context, the decision model for each agent must be based on an understanding of what information to exchange, how frequently, and what protocol to use for exchanging information. We illustrate how complete information sharing among distributed decision-makers might even be impossible because of a possible deadlock in decision-making. This game-theoretic phenomenon has implications for designing the organization structure, information systems, and communication protocols to support multi-agent decision-making.
|keyword = Model selection,Information economics,Distributed decision-making,Coordination,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Form and Substance in Physical Database Design: An Empirical Study'''
{{header}}
{{article
|author= Michael J. Prietula,Salvatore T. March,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = As with many complex design problems, physical database design is difficult, ill-structured, and highly human intensive. In order to effectively construct support systems or improve the practice of database design, it is important to understand how human designers reason about the task. We report an empirical study of physical database design problem solving. Thirteen subjects each solved two physical database design problems: five subjects were experienced designers; eight were graduate students with little or no actual design experience, though they were exposed to the principles of design through course work. For each problem, subjects were presented with a list of available problem information (hardware, content, and activity data) and were directed to generate a physical design (record structures and access paths) that would minimize retrieval time and storage space. All sessions were audiotaped. Three types of data were incorporated for the analysis: information acquisition patterns, solution generation patterns, and verbal protocol. It was hypothesized that database design reasoning embodies forms of deliberation to reduce problem-solving complexity and that these forms resemble those found in other design problem-solving studies commonality of task environmental demands will result in commonality in problem-solving methods in response to those demands. In particular, we expected to find specific control strategies, the use of hierarchical abstraction, the use of problem-specific heuristics, and the use of qualtitative reasoning with mental models of dynamic components of the task. Our results indicate that these forms are indeed present and of significant value in physical database design problem solving. Experience played a significant role in determining both the form and substance of reasoning used in physical database design. Both experienced and inexperienced database designers exhibited at least some of these forms of reasoning. Experienced designers, however, effectively applied these forms, demonstrating a substance of reasoning, although their methods of application varied considerably. The least experienced designers did not effectively apply these forms and, lost in the detail of the design problems, were unable to generate reasonable designs. It is concluded that recognition of appropriate reasoning forms and the effective application of these forms are critical to developing efficient physical database designs. The implications of the findings are discussed.
|keyword = Database design,Problem solving,Expertise,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION ARCHITECTURE - IN SEARCH OF EFFICIENT FLEXIBILITY'''
{{header}}
{{article
|author= BR ALLEN,AC BOYNTON,
|source= MIS QUARTERLY
|year= 1991
|abstract = This article addresses how information systems architecture can be used to support organizations in the 1990s-organizations that face the dual challenge of "speed and flexibility" and "low cost and efficiency." At the heart of this challenge is the basic notion that information systems have been anything but flexible in the past and that, for many firms, information systems are more disablers of flexibility than enablers. The article discusses two architectural solutions to this problem: "the high road and the low road," and the benefits and pitfalls of each. We conclude that neither solution will succeed on its own and that firms need to combine elements of both to meet the challenges of the 1990s. This article is based on some of the things we have learned through research, case writing, and consulting while working with a variety of organizations over the past three years. These experiences have illustrated the importance of and the struggle with IS architecture for today's global competitors. The content is intended to help guide, provoke, stimulate, and entertain others who believe that the integration of information technology with organizational strategy and structure is of paramount concern to senior managers.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ACADEMIC ISSUES IN MIS - JOURNALS AND BOOKS'''
{{header}}
{{article
|author= ML GILLENSON,JD STUTZ,
|source= MIS QUARTERLY
|year= 1991
|abstract = The range of journals in which to publish scholarly papers in the management information systems (MIS) field has expanded over the years. The perceived ranking of the journals by those in the field is an important issue to academics. Another question of importance to academic personnel in the MIS field is whether the writing of books counts toward tenure and promotion. This article presents the results of a survey relative to these two questions. It was conducted in late 1989 and early 1990 among MIS professors in AACSB accredited business schools, with about half of all such schools participating. The survey found that the most highly regarded journals for IS research include an assortment of IS-specific, computer science, and management science journals. This continues a long-standing tradition, despite the introduction of several new IS journals. It also showed that books are generally counted toward tenure and promotion.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN APPLIED FRAMEWORK FOR CLASSIFYING THE COMPLEXITY OF KNOWLEDGE-BASED SYSTEMS'''
{{header}}
{{article
|author= MH MEYER,KF CURLEY,
|source= MIS QUARTERLY
|year= 1991
|abstract = The development and use of knowledge-based (expert) systems has grown dramatically across a broad range of industries. Yet despite its growing importance, the study of expert systems lacks a cohesive framework for differentiating and comparing expert systems initiatives across different applications and in different industrial settings. The problem for IS managers is that a system that works in one situation may not be appropriate for another. This article presents a classification methodology for the systematic evaluation of a broad range of expert systems. Of primary concern in this study is the measurement of the complexity of such systems. Complexity in the area of expert systems consists of two basic dimensions. The first dimension is the complexity of the underlying knowledge residing with the key experts. The second dimension of the framework focuses on the complexity of the technology incorporated into a given system. This framework is then applied to a sample of 50 successfully developed knowledge-based systems. The results can be used as a foundation for generating research hypotheses for development time, budget, staffing, organizational control, and organizational participation.
|keyword = EXPERT SYSTEMS,MIS MANAGEMENT,TECHNICAL STAFFING,PROJECT PLANNING,CORPORATE STRATEGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION-SYSTEMS MANAGEMENT ISSUES FOR THE 1990S'''
{{header}}
{{article
|author= F NIEDERMAN,JC BRANCHEAU,JC WETHERBE,
|source= MIS QUARTERLY
|year= 1991
|abstract = This three-round delphi survey of senior IS executives is the third in a series designed to determine the most critical issues in IS management. Analysis focuses on respondents' assessments of specific issues as well as emerging trends. Key findings include: (1) Continued concern for traditional issues such as strategic planning and organizational alignment; (2) only six of the top issues from 1986 remained in the top 10; (3) one new issue, technology infrastructure, made the top 10; (4) three issues from previous studies rejoined the top 10-IS human resources, software development, and telecommunication systems; and (5) data-related issues now occupy the top two slots. This study reveals two important trends as the field enters the 1990s. First is the rising importance of technology infrastructure issues. Technology infrastructure issues now occupy three of the top 10 slots including the highest position. Second, it appears that internal effectiveness issues have made a strong comeback after being virtually ignored in 1986. IS human resources, software development, and the applications portfolio-issues that make up the core of the IS function-all increased in importance.
|keyword = INFORMATION SYSTEMS MANAGEMENT,IS EXECUTIVES,KEY ISSUES,MANAGEMENT PRIORITIES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EDUCATIONAL-NEEDS AS PERCEIVED BY IS AND END-USER PERSONNEL - A SURVEY OF KNOWLEDGE AND SKILL REQUIREMENTS'''
{{header}}
{{article
|author= RR NELSON,
|source= MIS QUARTERLY
|year= 1991
|abstract = One of the fundamental issues inherent to both practice and academia emanates from the need to know what knowledge and skills personnel must possess to successfully perform their jobs. This article addresses this issue by performing an educational needs assessment, one that considers the deficiencies of both IS and end-user personnel. To this end, an instrument has been developed, tested, and completed by 275 employees within eight different organizations. The findings of this study suggest that among six different knowledge and skill areas, both IS and end-user personnel are most deficient in the area of "general IS knowledge" (e.g., IS policies and plans, fit between IS and organization, etc.). In addition, IS personnel appear to be in need of more "organizational knowledge" (e.g., organizational goals and objectives, critical success factors, etc.). End users, on the other hand, seem to require more IS-related skills (e.g., data access, use of software packages, etc.). The implications of these findings for practitioners and academicians focus on finding ways to improve the education and training programs currently in place.
|keyword = ORGANIZATIONAL LEARNING, EDUCATION,TRAINING, KNOWLEDGE,SKILLS,INTEGRATION,END-USER COMPUTING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''USAA-IBM PARTNERSHIPS IN INFORMATION TECHNOLOGY - MANAGING THE IMAGE PROJECT'''
{{header}}
{{article
|author= DR LASHER,B IVES,SL JARVENPAA,
|source= MIS QUARTERLY
|year= 1991
|abstract = The introduction of a large-scale image processing system at United Services Automobile Association (USAA) required both external and internal partnerships. The USAA-IBM external partnership demonstrates how the traditional arms-length relationship between a vendor and a customer evolved into a close relationship of mutual benefit with blurred boundaries between buyer and seller. Two internal partnerships, one within USAA and the other within IBM, illustrate the importance of internal partnerships in making an external partnership successful. This paper discusses the "mosaic" of relationships and the "squiggly lines" of responsibility that characterized the internal and external partnerships from the perspective of senior information systems management. The article also provides conceptual frameworks that help in generalizing from the partnership arrangements within USAA's environments to those of other organizations.
|keyword = PARTNERSHIP,STRATEGIC ALLIANCES,IMAGE PROCESSING,USER INVOLVEMENT,EXECUTIVE SUPPORT,LARGE-SCALE PROJECT MANAGEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE APPLICATION SOFTWARE FACTORY - APPLYING TOTAL QUALITY TECHNIQUES TO SYSTEMS-DEVELOPMENT'''
{{header}}
{{article
|author= K SWANSON,D MCCOMB,J SMITH,D MCCUBBREY,
|source= MIS QUARTERLY
|year= 1991
|abstract = This paper describes an approach to application software development (the Application Software Factory) that enables over 90 percent reuse of code, produces application code where quality is measured in defects per million lines of code, and generates productivity exceeding that of interpretative 4GL environments. The environment is built on top of a commercial CASE tool and does not rely on exotic technology. The delivered systems are COBOL transaction processing systems using relational database technology and are predominantly online. Two applications of approximately 200,000 lines of code have each been developed by teams at two different sites. A third application of over 2 million lines of code is nearing completion. The paper depicts the important relationships between technology, management, methods, and design approaches that comprise the Application Software Factory.
|keyword = 
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Predicting User Intentions: Comparing the Technology Acceptance Model with the Theory of Planned Behavior'''
{{header}}
{{article
|author= Kieran Mathieson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = Information systems (IS) cannot be effective unless they are used. However, people sometimes do not use systems that could potentially increase their performance. This study compares two models that predict an individual's intention to use an IS: the technology acceptance model (TAM) and the theory of planned behavior (TPB). The comparison was designed to be as fair as possible, not favoring one model over the other. Both TAM and TPB predicted intention to use an IS quite well, with TAM having a slight empirical advantage. TAM is easier to apply, but only supplies very general information on users' opinions about a system. TPB provides more specific information that can better guide development.
|keyword = User behavior,User acceptance,User attitudes,I aboratory experiment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Development of an Instrument to Measure the Perceptions of Adopting an Information Technology Innovation'''
{{header}}
{{article
|author= Gary C. Moore,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = This paper reports on the development of an instrument designed to measure the various perceptions that an individual may have of adopting an information technology (IT) innovation. This instrument is intended to be a tool for the study of the initial adoption and eventual diffusion of IT innovations within organizations. While the adoption of information technologies by individuals and organizations has been an area of substantial research interest since the early days of computerization, research efforts to date have led to mixed and inconclusive outcomes. The lack of a theoretical foundation for such research and inadequate definition and measurement of constructs have been identified as major causes for such outcomes. In a recent study examining the diffusion of new end-user IT, we decided to focus on measuring the potential adopters' perceptions of the technology. Measuring such perceptions has been termed a "classic issue" in the innovation diffusion literature, and a key to integrating the various findings of diffusion research. The perceptions of adopting were initially based on the five characteristics of innovations derived by Rogers (1983) from the diffusion of innovations literature, plus two developed specifically within this study. Of the existing scales for measuring these characteristics, very few had the requisite levels of validity and reliability. For this study, both newly created and existing items were placed in a common pool and subjected to four rounds of sorting by judges to establish which items should be in the various scales. The objective was to verify the convergent and discriminant validity of the scales by examining how the items were sorted into various construct categories. Analysis of inter judge agreement about item placement identified both bad items as well as weaknesses in some of the constructs' original definitions. These were subsequently redefined. Scales for the resulting constructs were subjected to three separate field tests. Following the final test, the scales all demonstrated acceptable levels of reliability. Their validity was further checked using factor analysis, as well as conducting discriminant analysis comparing responses between adopters and nonadopters of the innovation. The result is a parsimonious, 38-item instrument comprising eight scales which provides a useful tool for the study of the initial adoption and diffusion of innovations. A short, 25 item, version of the instrument is also suggested.
|keyword = Instrument development,Innovation diffusion,Information technology adoptio,Research methodology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MCDM Approach for Generating and Evaluating Alternatives in Requirement Analysis'''
{{header}}
{{article
|author= Hemant K. Jain,Mohan R. Tanniru,Bijan Fazlollahi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = Determining user requirements and generating alternative system solutions to meet these requirements are two critical steps in the requirement analysis phase of the system development life cycle. Much of the MIS research in the requirement analysis phase has been devoted to the topic of requirement determination and its verification. Alternative generation and evaluation is left, to a significant degree, to the judgment and expertise of an analyst. This paper proposes a multiple criteria decision making (MCDM) approach for generating and evaluating alternatives when the user requirements are expressed in terms of certain operational criteria such as time, cost, risk, etc. These alternatives form the basis for the user to make the necessary trade-offs.
|keyword = Requirement analysis,Alternative generation,Goal programming,Information system design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Experimental Investigation of the Impact of Computer Based Decision Aids on Decision Making Strategies'''
{{header}}
{{article
|author= Peter Todd,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = Although Decision Support Systems (DSSs) have been in use since the early seventies, there is as yet no strong theoretical base for predicting how a DSS will influence decision making. Furthermore, the findings of various empirical studies on the outcomes of DSS use are often contradictory. Consequently, there is a need in the Decision Support Systems field for theories or explanatory models to formulate hypotheses, to conduct research in a directed, parsimonious manner and to interpret findings in a coherent way. This will assist both academics and practitioners interested in the use of information systems to support managerial workers. This paper proposes the use of a cognitive effort model of decision making to explain decision maker behavior when assisted by a DSS. The central proposition is that specific features can be incorporated within a DSS that will alter the effort required to implement a particular strategy, and thus influence strategy selection by the decision maker. This was investigated in a series of three experimental studies which examined the influence of computer based decision aids on decision making strategies. In the three experiments, subjects were given different degrees of support to deal with various components of cognitive effort (processing effort, memory effort and information tracking effort) associated with the strategies applicable to preferential choice problems. The results show that decision makers tend to adapt their strategy selection to the type of decision aids available in such a way as to reduce effort. These results suggest that the assumption that decision makers use a DSS exclusively to maximize decision quality is open to question. DSS studies which consider the joint effects of effort and quality, or control one while manipulating the other, are more likely to provide consistent and interpretable results.
|keyword = Decision support systems,Decision processes,Cognitive cost-benefit theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Computerized Performance Monitoring on Service Work: Testing a Causal Model'''
{{header}}
{{article
|author= Rebecca A. Grant,Chris A. Higgins,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = This research examined the impact of Computerized Performance Monitoring and Control Systems (CPMCSs) on service workers and their perceptions of work. Drawing on a thermostat model of control systems, the work built a causal model of CPMCS impact. The model demonstrated how four monitor design dimensions (tasks measured, frequency of measurement, object of monitoring, and recipient of the monitor data) affected the importance employees placed on production and service. Other constructs in the model included employees' acceptance of quantitative measures, computer appropriateness, computer accuracy, and employer's production and service messages. Using a holdback sample, an initial and revised model were tested on responses from 1,498 workers in 51 Canadian service sector organizations. Both versions of the model exhibited good explanatory power. The research led to three important conclusions. First, monitoring may not increase production. Even if it does, it need not reduce the importance of service. Second, studying monitors as multidimensional systems demonstrates that various monitor features can be altered to change the impact. Third, the credibility of the computer is a factor in the monitor's impact.
|keyword = Computerized performance monitoring,Worker surveillance,IS impact service productivity,IS research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology and the Structuring of Organizations'''
{{header}}
{{article
|author= Wanda J. Orlikowski,Daniel Robey,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = Recent work in social theory departs from prior traditions in proposing that social phenomena can be understood as comprising both subjective and objective elements. We apply this premise of duality to understanding the relationship between information technology and organizations. We construct a theoretical framework in which the development and deployment of information technology in organizations is a social phenomenon, and in which the organizational consequences of technology are products of both material and social dimensions. The framework is based on Giddens' theory of structuration, and it allows us to progress beyond several of the false dichotomies (subjective vs objective, socially constructed vs material, macro vs micro, and qualitative vs quantitative) that persist in investigations of the interaction between organizations and information technology. The framework can be used to guide studies in two main areas of nformation systems research- systems development and the organizational consequences of using information technology.
|keyword = Information systems,Organizational change,Organizational structure,Social theory,Structuration,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CAREER ORIENTATIONS OF MIS EMPLOYEES - AN EMPIRICAL-ANALYSIS'''
{{header}}
{{article
|author= M IGBARIA,JH GREENHAUS,S PARASURAMAN,
|source= MIS QUARTERLY
|year= 1991
|abstract = The career orientations of employees can have important implications for their job satisfaction, commitment, and retention within organizations. However, there is little empirical research on the correlates of career orientations held by managers and professionals in the MIS field. This study sought to address this gap in the literature and assessed the career orientations of 464 MIS employees, as well as their relationship with selected demographic characteristics, job type, and career outcomes. The most prevalent career orientations of MIS employees were found to be technical and managerial. Autonomy and lifestyle orientations were also found to be moderately represented in the sample. Women were more lifestyle oriented and less technically oriented than men. In addition, systems programmers, applications programmers, and software engineers tended to be technically oriented, whereas systems analysts, project leaders, and computer managers tended to be managerially oriented. The most significant finding was that employees whose career orientations were compatible with their job setting reported high job satisfaction, high career satisfaction, strong commitment to their organization, and low intentions to leave their organization. Firms need to recognize the diversity of career orientations so that appropriate reward systems and career paths can be developed. Research on this topic should continue to examine characteristics unique to MIS employees, as well as how these interrelationships change over time at different career stages.
|keyword = MIS PERSONNEL,CAREER MANAGEMENT,CAREER ORIENTATIONS ANCHORS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''KEY INFORMATION-SYSTEMS MANAGEMENT ISSUES FOR THE PUBLIC-SECTOR'''
{{header}}
{{article
|author= SL CAUDLE,WL GORR,KE NEWCOMER,
|source= MIS QUARTERLY
|year= 1991
|abstract = The public sector has multiple, conflicting, and often intangible goals. It produces "public goods" for problems that should be solved (like crime and poverty), even though these problems may have no known feasible solutions; and it is heavily impacted by politics and bureaucratic red tape. These and other features of the public sector make it potentially a much different setting for IS management than the private sector. This article reports on the first national survey of public sector managers identifying their most important IS issues. The survey, covering respondents from federal, state, and local governments, drew upon prior survey research in the private sector and the literature on public/private sector differences. While most of the top public sector issues also appear on the top private sector issue lists, the rankings show a lag in public IS development as compared to the private sector. Perhaps the most interesting results of the survey, however, are from a deeper analysis. At the main effects-level, we have preliminary evidence that (1) middle-level (instead of top-level) public managers are critical for IS technology development; (2) small government agencies are more interested in IS technology transfer than large ones; (3) governments with a lot of red tape tend to have flexible IS; and (4) local government IS issues are driven by transaction processing while state and federal governments have IS more suitable for their oversight mission.
|keyword = MIS MANAGERS,OPINION SURVEY,INFORMATION SYSTEMS MANAGEMENT,KEY ISSUES,MANAGEMENT PRIORITIES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IS OFFICE PRODUCTIVITY STAGNANT'''
{{header}}
{{article
|author= RR PANKO,
|source= MIS QUARTERLY
|year= 1991
|abstract = Has office productivity been stagnant in recent years, despite massive spending on information technology (IT)? This concern has been raised in both academic publications and the business trade press. This article examines the basis for that concern, which, if true, would have profound implications for both IT researchers and practitioners.
|keyword = OFFICE PRODUCTIVITY,PRODUCTIVITY,OUTPUT PER HOUR,INVESTMENT,PAYOFFS,IMPACTS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXECUTIVE INVOLVEMENT AND PARTICIPATION IN THE MANAGEMENT OF INFORMATION TECHNOLOGY'''
{{header}}
{{article
|author= SL JARVENPAA,B IVES,
|source= MIS QUARTERLY
|year= 1991
|abstract = Executive support is often prescribed as critical for fully tapping the benefits of information technology (IT). However, few investigations have attempted to determine what type of executive support is likely or organizationally appropriate. This article puts forward alternative models of executive support. The models are tested by examining chief executive officers' behaviors in and preceptions of IT activities. CEOs and information systems executives are surveyed and further data collected from industry handbooks and from chairmen's annual letters to shareholders. The results suggest that executive involvement (a psychological state) is more strongly associated with the firm's progressive use of IT than executive participation (actual behaviors) in IT activities. Executive involvement is influenced by a CEO's participation, prevailing organizational conditions, and the executive's functional background. CEO's perceptions about the importance of IT in their firms were generally positive, although they participated in IT activities rather infrequently.
|keyword = MANAGEMENT OF INFORMATION SYSTEMS,EXECUTIVE SUPPORT,INFORMATION SYSTEMS SUCCESS,CEOS,INFORMATION SYSTEMS MANAGERS,SURVEY RESEARCH,ANNUAL REPORT METHODOLOGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MODEL OF USERS PERSPECTIVE ON CHANGE - THE CASE OF INFORMATION-SYSTEMS TECHNOLOGY IMPLEMENTATION'''
{{header}}
{{article
|author= K JOSHI,
|source= MIS QUARTERLY
|year= 1991
|abstract = Change is a fundamental theme in human life. Yet some organizational behavior and MIS researchers have noted that individuals generally resist changes. MIS researchers have also attributed many implementation problems to users' resistance to change. However, there is no fundamental resistance to every change. Individuals readily adopt changes such as a pay raise or promotion. This article proposes that individuals attempt to evaluate most changes. Changes that are considered favorable are not resisted and may even be sought after and welcomed, while changes considered unfavorable are likely to be resisted. The equity-implementation (E-I) model provides a theory-based understanding of information systems users' resistance to change. It describes the processes employed by users in assessing the change associated with the implementation of an information system or technology in an organization. The model is based upon equity theory, which is a well-established and widely used theory in social sciences. Users employ three levels of analysis in evaluating the change introduced by an implementation. At the first level of analysis, a user is viewed as assessing a change in terms of the gain or loss in his or her equity status. At the second level of analysis, the user is viewed as comparing his or her relative outcomes with that of the organization. Finally, at the third level of analysis, the user is viewed as comparing his or her relative outcomes with that of other users in the reference group. Users who evaluate the change to be unfavorable in terms of inequity or loss of equity are likely to be distressed by the change and resist it. The E-I model provides a useful framework of analysis, for improving our understanding of users' assessment of a change. Managers will find the model useful for overcoming resistance to change during implementation.
|keyword = RESISTANCE TO CHANGE,SYSTEMS IMPLEMENTATION,EQUITY THEORY,POWER,USER BEHAVIOR,USER ACCEPTANCE,BEHAVIORAL FACTORS,ORGANIZATIONAL CHANGE,USER INVOLVEMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A MODEL FOR MEASURING INFORMATION-SYSTEM SIZE'''
{{header}}
{{article
|author= CD WRIGLEY,AS DEXTER,
|source= MIS QUARTERLY
|year= 1991
|abstract = Management of the software development process requires a thorough understanding of the environment in which development takes place. Ability to estimate, plan, and manage resource consumption is limited by the central problem of determining the size of system specifications. To address this issue, a general strategy for measurement and evaluation of system development environments needs to be established. This article presents a research model that will help managers and researchers understand and establish the linkages between units of systems requirements specification, design, and source code. Initial validation of the model was performed by reverse engineering systems written in a fourth generation language from source code to design metrics. Results indicate that the model may provide reliable measures of system size in terms of both design metrics and lines of code.
|keyword = MODELS AND PRINCIPLES,TOOLS AND TECHNIQUES,SOFTWARE METRICS,REVERSE ENGINEERING,4TH GENERATION LANGUAGES,FOCUS,SOFTWARE MANAGEMENT,FIELD STUDY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Studying Information Technology in Organizations: Research Approaches and Assumptions'''
{{header}}
{{article
|author= Wanda J. Orlikowski,Jack J. Baroudi,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = We examined 155 information systems research articles published from 1983 to 1988 and found that although this research is not rooted in a single over-arching theoretical perspective, it does exhibit a single set of philosophical assumptions regarding the nature of the phenomena studied by information systems researchers, and what constitutes valid knowledge about those phenomena. We believe that a single research perspective for studying information systems phenomena is unnecessarily restrictive, and argue that there exist other philosophical assumptions that can inform studies of the relationships between information technology, people, and organizations. In this paper, we present two additional research philosophies for consideration the interpretive and the critical and for each we provide empirical examples to illustrate how they are used. We conclude by suggesting that much can be gained if a plurality of research perspectives is effectively employed to investigate information systems phenomena.
|keyword = Philosophical assumptions,Research approaches,Positivist research,Interpretivist research,Critical research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Symbolism and Information Systems Development: Myth, Metaphor and Magic'''
{{header}}
{{article
|author= Rudy Hirschheim,Mike Newman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = It is our intention to challenge the commonly held assumption that information systems development (ISD) can be conceived of as a normative process reflecting conventional economic rationality. We ask: is systems development the rational process so eloquently described in the "classic" works of DeMarco (1978), Gane and Sarson (1979), Weinberg (1980), Yourdon (1982), Jackson (1983), and Martin (1985)? Or does this orthodox view fail to explain the actual practice of systems development? It is our view that even the basic assumptions about the rationality of the actors and the social processes they engage in need to be critically appraised. We suggest that if the assumptions about economic rationality are closely analyzed, it can be seen that they do not reflect the reality of systems development. ISD tends to defy rational explanations. As an alternative to basing our understanding of systems development on economic rationality, we contend that symbolism holds more promise. Instead of focusing on data flow diagrams, structured walkthroughs, requirements specifications, and the like, we concentrate on the role of myth, metaphor, and magic. These concepts offer considerable scope in interpreting the social actions that are embodied within ISD. We feel they facilitate a much richer understanding of systems development.
|keyword = Myth,Magic,Symbolism,Metaphor,Social aspects,IS development,Quafitative research methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cognitive Fit: An Empirical Study of Information Acquisition'''
{{header}}
{{article
|author= Iris Vessey,Dennis Galletta,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1991
|abstract = From a broad perspective, our research can be viewed as investigating the fit of technology to task, the user's view of the fit between technology and task, and the relative importance of each to problem-solving or decision-making performance. The technology investigated in this research is the mode of information presentation. Although there has been a considerable amount of research into problem solving using graphs and tables, until recently the circumstances in which each is more effective have been largely unresolved. Recent research has suggested that performance benefits accrue when cognitive fit occurs, i.e., when factors such as the problem representation and problem solving tools match the characteristics of the task. In this paper, we investigate the effects of the basic paradigm of cognitive fit and extensions to the paradigm in a laboratory experiment that examined the nature of subjects' mental representations as well as problem-solving performance. The experiment, using 128 MBA students in two identical, repeated measures designs, produced the following results: center dot Performance improved markedly for symbolic tasks when the problem representation matched the task. center dot Performance effects also resulted from matching specific problem-solving skills to the problem representation and the task, and to a lesser extent when the skills matched the task alone. center dot The incremental effects of matching skills to the problem representation and/or the task were small compared with the primary effects of cognitive fit that of matching problem representation to task. center dot A large proportion of problem solvers have insight into the concept of supporting tasks with certain types of problem representation and vice versa. center dot Participants preferred to use tables rather than graphs; they also preferred to solve symbolic rather than spatial problems. center dot Finally, the problem representation more significantly influenced the mental representation than did task conceptualization. This research suggests that providing decision support systems to satisfy individual managers' desires will not have a large effect on either the efficiency or the effectiveness of problem' solving. Designers should, instead, concentrate on determining the characteristics of the tasks that problem solvers must address, and on supporting those tasks with the appropriate problem representations and support tools. Sufficient evidence now exists to suggest that the notion of cognitive fit may be one aspect of a general theory of problem solving. Suggestions are made for extending the notion of fit to more complex problem-solving environments.
|keyword = Cognitive fit,Information acquisition,Spatial tasks,Symbolic tasks,Spatial skills,Numeric skills,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXECUTIVE INFORMATION-SYSTEMS - A FRAMEWORK FOR DEVELOPMENT AND A SURVEY OF CURRENT PRACTICES'''
{{header}}
{{article
|author= HJ WATSON,RK RAINER,CE KOH,
|source= MIS QUARTERLY
|year= 1991
|abstract = Executive information systems (EIS) are now successfully providing computer support for senior executives in a growing number of organizations. Previous attempts to support senior executives are discussed with a focus on why these attempts failed and what was learned that should be incorporated in future efforts. An EIS development framework is presented that includes a structural perspective of the elements and their interaction, the development process, and the dialog between the user and the system. Survey data from 50 firms having an EIS are presented and discussed in the context of the development framework. While most of the findings confirm conventional EIS wisdom, others are somewhat surprising, such as the significant role that information systems management often plays in initiating the development of an EIS or serving as its operational sponsor. The findings lead to additional suggestions for EIS research opportunities, as well as predictions about the future nature of EIS.
|keyword = EXECUTIVE INFORMATION SYSTEMS,EXECUTIVE SUPPORT SYSTEMS,DECISION SUPPORT,SYSTEMS DEVELOPMENT,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''APPLICATIONS OF GLOBAL INFORMATION TECHNOLOGY - KEY ISSUES FOR MANAGEMENT'''
{{header}}
{{article
|author= B IVES,SL JARVENPAA,
|source= MIS QUARTERLY
|year= 1991
|abstract = Carefully crafted investments in global information technology offer firms an opportunity to increase control and enhance coordination, while opening access to new global markets and businesses. But engineering such global systems presents numerous challenges to management. In this article, we relate these challenges as they were described to us by 25 senior managers from Fortune 500 firms responsible for implementing and managing global applications of information technology. Among the findings of the interviews are four common approaches for managing global information technology.
|keyword = IS MANAGEMENT,WORLDWIDE MIS,STRATEGIC INFORMATION SYSTEMS,GLOBAL INFORMATION SYSTEMS,KEY MIS ISSUES,INTERNATIONAL BUSINESS,MULTINATIONAL COMPANY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EXECUTIVE INFORMATION REQUIREMENTS - GETTING IT RIGHT'''
{{header}}
{{article
|author= JC WETHERBE,
|source= MIS QUARTERLY
|year= 1991
|abstract = Most managers spend half their time trying to get the information they need, whether it be informally through meetings, phone conversations, or reading, or formally through organizational computer-based information. During this process they have to sift through a great deal of useless information, a situation commonly referred to as "information overload." With the proliferating capabilities and plummeting cost of computers, it seems relief should be in sight for weary executives. Unfortunately, most information systems-formal or informal-do not meet executive needs. Indeed, most new systems require extensive revision (after they are supposedly completed) to even partially fulfill needs. This is a terrible loss. Most systems are expensive enough to develop. They are even more expensive to revise. As the pace of business accelerates, decisions that could wait for weeks must now be made in days, hours, or even minutes. Failure to get executives the information they need in a timely manner can result in lost opportunities or in a problem not being solved in time. Increasingly, executives have little reaction time to make decisions on pricing, product introduction, resource allocation, media inquiries, response to competition, and mergers. They need access to information without waiting several weeks or months for a computer project. Why can't executives and system designers work together to more correctly anticipate and determine information requirements? In this article, four reasons information requirements are not met are discussed, and four straightforward solutions executives can use to solve this problem are offered.
|keyword = INFORMATION REQUIREMENTS DETERMINATION,PROTOTYPING,JOINT APPLICATION DESIGN,CROSS FUNCTIONAL DESIGN,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''ON INFORMATION-SYSTEMS PROJECT ABANDONMENT - AN EXPLORATORY-STUDY OF ORGANIZATIONAL PRACTICES'''
{{header}}
{{article
|author= K EWUSIMENSAH,ZH PRZASNYSKI,
|source= MIS QUARTERLY
|year= 1991
|abstract = Information systems failure is a widely recognized problem in the IS community. However, abandonment of IS projects is an aspect of IS failure that has not gained much attention in either IS practice or research. This article examines the organizational practices resulting in the underlying characteristics of IS project abandonment. The results of a survey show IS project abandonment to be a complex multidimensional issue defying easy explanations. IS projects may be abandoned for any combination of factors including cost overruns and/or schedule slippages, technological inadequacies, and behavioral, political, or organizational issues. The last set of factors emerged as being the most dominant in most companies' decisions.
|keyword = INFORMATION SYSTEMS FAILURE OR ABANDONMENT,INFORMATION SYSTEMS DEVELOPMENT,TECHNOLOGY AND ORGANIZATIONAL BEHAVIORAL POLITICAL ISSUES,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IDENTIFICATION OF STRATEGIC INFORMATION-SYSTEMS OPPORTUNITIES - APPLYING AND COMPARING 2 METHODOLOGIES'''
{{header}}
{{article
|author= F BERGERON,C BUTEAU,L RAYMOND,
|source= MIS QUARTERLY
|year= 1991
|abstract = Much has been said about opportunities for the strategic use of information technology by organizations aiming to gain a competitive advantage. However, not much is known about the actual process by which opportunities for the use of strategic information systems are identified. While various planning methodologies have been proposed, there is at present a paucity of information on empirical results obtained from applying them, and on their effectiveness, efficiency, and specificity. This article presents the results of a field experiment aimed at applying and comparing two well-known methodologies for identifying information systems opportunties from a competitive advantage perspective-Porter's value chain and Wiseman's strategic thrusts methodology. An instrument was prepared to operationalize each methodology, which was then applied in two matched sets of 10 medium-sized enterprises. Both methodologies were found to be effective in generating a significant number of ideas for information systems worthy of implementation. Similarities and differences are analyzed and discussed in terms of the number, estimated implementation costs and duration, managerial level, and decision to implement the applications identified by the two methodologies. These applications are also classified from the perspective of both Porter's and Wiseman's framework. The results seem to indicate that while there is an overall similarity between the two methodologies, there are certain differences that show the more outward orientation of the strategic thrusts framework and its greater attractiveness for organizations in unstable environments.
|keyword = PLANNING METHODOLOGIES,STRATEGIC INFORMATION SYSTEMS,COMPETITIVE ADVANTAGE,INTERORGANIZATIONAL SYSTEMS,ELECTRONIC DATA INTERCHANGE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DECISIONAL GUIDANCE FOR COMPUTER-BASED DECISION SUPPORT'''
{{header}}
{{article
|author= MS SILVER,
|source= MIS QUARTERLY
|year= 1991
|abstract = In the course of interacting with a decision support system (DSS), decision makers may have numerous opportunities for exercising judgment. Some judgments pertain to what to do next; others require predictions or evaluations. Either deliberately or inadvertently, a DSS may guide its users in performing these judgments. This article lays a foundation and proposes an agenda for researching such "decisional guidance." Studying decisional guidance matters for two reasons. First, deliberately incorporating guidance in a system offers the potential of more supportive systems while raising a number of design questions. Second, understanding the consequences of guidance-deliberate or not-contributes to comprehending how DSSs affect decision-making behavior. This article examines three aspects of decisional guidance: (1) when and why system designers should provide decisional guidance, considering the opportunities, motives, and means for guiding; (2) how designers can provide guidance, introducing a three-dimensional typology for deliberate guidance; and (3) the consequences of decisional guidance-that is, its effects and effectiveness. This article provides a coherent approach to a set of behavioral questions just now beginning to be addressed by researchers in a fragmented, technologically oriented manner.
|keyword = DECISION SUPPORT SYSTEMS,DECISION-MAKING PROCESS,DECISION MAKING,DECISIONAL GUIDANCE,HUMAN JUDGMENT,META-SUPPORT,META-CHOICE,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PERSONAL COMPUTING - TOWARD A CONCEPTUAL-MODEL OF UTILIZATION'''
{{header}}
{{article
|author= RL THOMPSON,CA HIGGINS,JM HOWELL,
|source= MIS QUARTERLY
|year= 1991
|abstract = Organizations continue to invest heavily in personal computers for their knowledge workers. When use is optional, however, having access to the technology by no means ensures it will be used or used effectively. To help us gain a better understanding of factors that influence the use of personal computers, researchers have recently adapted the theory of reasoned action proposed by Fishbein and Azjen (1975). This study uses a competing theory of behavior proposed by Triandis (1980). Responses were collected from 212 knowledge workers in nine divisions of a multi-national firm, and the measures and research hypotheses were analyzed using partial least squares (PLS). The results show that social norms and three components of expected consequences (complexity of use, fit between the job and PC capabilities, and long-term consequences) have a strong influence on utilization. These findings confirm the importance of the expected consequences of using PC technology, suggesting that training programs and organizational policies could be instituted to enhance or modify these expectations.
|keyword = PERSONAL COMPUTING,INFORMATION TECHNOLOGY UTILIZATION,ATTITUDES,BEHAVIOR,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology and Corporate Strategy: A View from the Top'''
{{header}}
{{article
|author= Sirkka L. Jarvenpaa,Blake Ives,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Letters to shareholders in 649 annual reports published between 1972 and 1987 were analyzed for CEOs' views about information technology. Significant differences were found across industries banking, publishing, petroleum, and retailing in the number of times information technology was mentioned, the types of applications discussed, and the content of the discussion. The results of the industry analysis were in keeping with expectations based on the relative information intensity oft he various industries. An analysis of the letters over time suggests that the position of IT in the firm, at least as seen by the CEO, was not much different n 1987 than it had been in 1982, but has expanded considerably from its position in 1972 and 1973. Reassuringly, we also found that the number of IT related phrases in the CEOs' letters to the shareholders was positively correlated with the firm's yearly net profits as a percentage of sales. A lagged analysis on profitability data could not, however, resolve the competing explanations for the correlation between profits and the number of IT-related phrases. These findings contribute new insights concerning strategic information systems and support the use of annual report data in analyzing organizational information technology phenomena.
|keyword = Information technology management,CFOs,Senior management,Annual reports,Strategic information systems,Longitudinal data,Industry comparisons,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Electronic Integration and Strategic Advantage: A Quasi-Experimental Study in the Insurance Industry'''
{{header}}
{{article
|author= N. Venkatraman,Akbar Zaheer,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Strategic advantage through information technology is a popular and an important theme, but the extent of research support is minimal, anecdotal, and sporadic. This paper reports the results of a quasi-experimental study on the impact of dedicated electronic integration [between a focal insurance carrier and its independent agents in the property and casualty (P&C) market] for the focal carrier. The results indicate that the agents that are electronically interfaced with the carrier report improvements in a set of four performance factors in the expected direction (six months after system installation), but statistically different from a matched set of non-interfaced agents (based on size, state, and location category) only in terms of increases in new business policies, but not in terms of effectiveness-namely, neither increases in premiums and commissions nor operating efficiency. Some explanations, extensions and research implications are outlined.
|keyword = Electronic integration,Strategic advantage,Insurance industry,Electronic interfacing,Quasi-experimental study,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Study of Collaborative Group Work With and Without Computer-Based Support'''
{{header}}
{{article
|author= Joey F. George,George K. Easton,Jr. J. F. Nunamaker,Gregory B. Northcraft,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = As organizational environments become more turbulent and as managers spend more time in meetings in an effort to deal with that turbulence, using information technology to support meetings has become more important. This paper reports on an experiment that compared meetings supported by information technology to meetings with conventional manual support only. The experiment differs from most previous group decision support system (GDSS) experiments in that solutions to the task it used could be objectively scored, it introduced assigned leadership as an independent variable, and it is the first GDSS experiment to compare use of a subset of the University of Arizona GroupSystems GDSS tools to manual group methods. In addition to a communication condition (GDSS or manual) and assigned leadership, the experiment also investigated the effects of anonymity on group process and outcomes. The experiment found that GDSS groups were less likely to reach consensus, took more time to reach a decision, and had more equal levels of member participation than manual groups. No main effects were found for assigned leadership or anonymity.
|keyword = Group decision support systems,Electronic meeting systems,Decision support,Anonymity,Leadership,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Attribute Space for Organizational Communication Channels'''
{{header}}
{{article
|author= Robert W. Zmud,Mary R. Lind,Forrest W. Young,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = The primary objective of this study was to identify the perceptual dimensions used by 158 managers and their professional staff at a single large manufacturing firm in differentiating fourteen distinct communication channels available in the firm. Six candidate criteria for differentiating these channels were examined (channel accessibility, information quality, immediate feedback, cue variety, personalization, and receiver accessibility) using multidimensional scaling. A secondary objective involved assessing whether communication direction influenced perceptions. Responses were obtained for two intraorganizational communication directions: lateral and downward. Results indicated that these individuals applied a perceptual framework involving three dimensions: information feedback, accessibility, and quality. Further, a perspective shift from the "message sender" to the "message receiver" was observed in moving from lateral to downward communication. The observations of directional differences demonstrate the inappropriateness of either ignoring communication direction in research designs and of directly transferring research models and instruments that pertain to one communication direction to another direction. Taken together, these results may prove helpful in developing a richer theoretical basis for exploring task/media relationships, which in turn may lead to future research findings providing recommendations for improving individual and organizational performance.
|keyword = Electronic media,Organizational communication,Information richness,Multidimensional scaling,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TELCOT - AN APPLICATION OF INFORMATION TECHNOLOGY FOR COMPETITIVE ADVANTAGE IN THE COTTON INDUSTRY'''
{{header}}
{{article
|author= D LINDSEY,PH CHENEY,GM KASPER,B IVES,
|source= MIS QUARTERLY
|year= 1990
|abstract = This paper describes the strategic use of information technology by the Plains Cotton Cooperative Association (PCCA). TELCOT, a computer-based system developed by PCCA, provides cotton traders with functions much like those available to NYSE or AMEX traders. TELCOT transformed PCCA from a small cotton merchant to a major cotton broker. Handling 115,000 to 240,000 computer transactions per day, TELCOT provides over 20,000 cotton producers, 40 buyers, and 200 gin operators with an electronic marketing service that has helped PCCA grow from a $50 million to a $500 million enterprise in just 15 years.
|keyword = COMPUTER-BASED TRADING SYSTEMS,ELECTRONIC COMMODITY EXCHANGE,STRATEGIC ADVANTAGE,SMALL BUSINESS,MANAGEMENT OF INFORMATION SYSTEMS,APPLICATION OF INFORMATION TECHNOLOGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE MARRIAGE OF RETAIL MARKETING AND INFORMATION-SYSTEMS TECHNOLOGY - THE ZELLERS CLUB-Z EXPERIENCE'''
{{header}}
{{article
|author= KR WIGHTMAN,
|source= MIS QUARTERLY
|year= 1990
|abstract = This paper examines the major influences and events that led to the creation and implementation of Club Z, a very successful frequent buyer program, by Zellers Inc., a Canadian mass merchandiser and a division of the Hudson's Bay Company. While the paper describes this major retailer's innovation in the adaptation of computer technology to store-level marketing, it also analyzes some of the significant aspects of the Club Z implementation from an IS management perspective: how a champion emerged to advocate and protect the project at the highest executive level; how an outside systems consulting firm was brought in to manage the Zellers IS function during the project life; and how the cultural problem of anticipated store resistance to the required new technology was overcome.
|keyword = COMPUTER APPLICATIONS,ADMINISTRATIVE DATA PROCESSING MARKETING,MANAGEMENT OF COMPUTING AND INFORMATION SYSTEMS,PROJECT AND PEOPLE MANAGEMENT SYSTEMS DEVELOPMENT,TRAINING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPLEMENTING ELECTRONIC MEETING SYSTEMS AT IBM - LESSONS LEARNED AND SUCCESS FACTORS'''
{{header}}
{{article
|author= R GROHOWSKI,C MCGOFF,D VOGEL,B MARTZ,J NUNAMAKER,
|source= MIS QUARTERLY
|year= 1990
|abstract = Businesses of the future will rely more than ever on the work of teams. Making better use of the time teams spend in meetings will be a high priority, as will being able to adapt rapidly to change. Electronic meeting systems (EMS), which apply information technology to support the meeting process, can help accomplish these goals. This paper describes and discusses the implementation of EMS at IBM in an alliance with the University of Arizona. During the past three years, the project has grown from initial support for a single site to 33 IBM sites, with more on the way. Over 15,000 people have used the ever-expanding and evolving EMS tool kit. Use of EMS has improved group performance by an average of 55 percent, with even more dramatic reductions in project calendar time. The lessons learned and success factors at IBM can assist managers in effectively introducing EMS to their organizations.
|keyword = ELECTRONIC MEETING SYSTEMS,GROUP DECISION SUPPORT,IMPLEMENTATION,MEETING PRODUCTIVITY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A PRINCIPLES-BASED ENTERPRISE ARCHITECTURE - LESSONS FROM TEXACO AND STAR ENTERPRISE'''
{{header}}
{{article
|author= GL RICHARDSON,BM JACKSON,GW DICKSON,
|source= MIS QUARTERLY
|year= 1990
|abstract = Star Enterprise is a joint venture partnership between Texaco Inc. and the Saudi Arabian Oil Company. Created in 1988, it became fully operational on January 1, 1989. This new organization inherited staff, facilities, and information resources existing within Texaco Inc. at the time of formation. A significant opportunity for the new organization was to create a new Enterprise Information Technology Architecture to support business functions and management decision making. While this venture was an opportunity, it was also a challenge because of the existing information technology that was comprised of incompatible hardware and nonintegrated systems. This paper describes the architecture that emerged and reviews its current status. Two major contributions of the paper are to identify the principles upon which the architecture is being created and to review what has been learned to date in the process of implementing the architecture.
|keyword = INFORMATION ARCHITECTURE,PRINCIPLES,LAN,NETWORKING,ENTERPRISE,CASE STUDY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A NETWORK INFRASTRUCTURE TO CONTAIN COSTS AND ENABLE FAST RESPONSE - THE TRW PROCESS'''
{{header}}
{{article
|author= L RAILING,T HOUSEL,
|source= MIS QUARTERLY
|year= 1990
|abstract = A growing problem for many organizations is spiraling telecommunications costs coupled with the need to respond faster to changing market conditions. To handle such a situation, TRW's Space and Defense Sector management authorized a new integrated voice and data digital network for 20,000 employees located in 65 buildings. TRW's top management saw the network as an opportunity to integrate the company's islands of information, which were isolated within separate data networks. The project took four and one half years from design to implementation at a cost of $45 million. The management techniques employed resulted in critical milestones being met not only on time but also 10 percent under the original budget. The new network provided a single access mechanism that resulted in some unanticipated advantages that went beyond the original goals of fast response and cost containment. The network successfully addressed such problems as developing effective data networking policies, network sizing in a multi-vendor environment, sharing excess information system capacity, centralizing the network while keeping the information systems decentralized, and overcoming the "not controlled here" syndrome. TRW's experiences should prove valuable for large information-intensive companies attempting to meet similar challenges.
|keyword = NETWORKING,TELECOMMUNICATIONS,DATA NETWORK,NETWORK MATURITY CYCLE,FAST RESPONSE NETWORKS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''REUSABILITY-BASED STRATEGY FOR DEVELOPMENT OF INFORMATION-SYSTEMS - IMPLEMENTATION EXPERIENCE OF A BANK'''
{{header}}
{{article
|author= U APTE,CS SANKAR,M THAKUR,JE TURNER,
|source= MIS QUARTERLY
|year= 1990
|abstract = This paper describes the experience of a large bank in designing and implementing an information systems strategy that is based on the concept of resuability. The design and implementation was performed in two stages: (1) building a prototype to investigate the feasibility and attractiveness of reusability concept for the bank; and (2) its subsequent implementation using a library of reusable entities and a programmer's workbench. The implementation experience confirmed that applying the reusability concept to all stages of the system's life cycle results in both strategic (e.g., improving programmer productivity and increasing the bank's capacity for timely response to market opportunities) and operational (e.g., reducing and controlling system development and maintenance costs) benefits. It is estimated that the library of reusable entities embedded within the programmer workbench saved the bank over $1.5 million in development costs in 1989 alone. Two of the most important lessons learned in implementing the reusability-based strategy are: (1) reusability comes in many flavors and should be applied to all stages of systems life cycle; and (2) major challenges in implementing the reusability-based strategy are managerial, not technical.
|keyword = SOFTWARE REUSABILITY,PROGRAMMER PRODUCTIVITY,PROGRAMMER WORKBENCH,BANKING SYSTEMS,INFORMATION SYSTEMS DEVELOPMENT,INFORMATION SYSTEMS STRATEGY,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''EIS - IT CAN WORK IN THE PUBLIC-SECTOR'''
{{header}}
{{article
|author= L MOHAN,WK HOLSTEIN,RB ADAMS,
|source= MIS QUARTERLY
|year= 1990
|abstract = Advances in the development of executive information systems (EIS) have predominantly occurred in the private sector, with far less progress taking place in the public sector. Surely, the need for EIS in the public sector exists. Despite problems of EIS development in the public sector, successful systems can be built. This paper explores the differences between public and private systems and describes an EIS developed for a large agency of the New York state government. The system is being used in different and creative ways, leading to a change in the organization's culture, with implicit and explicit impact on the focus of the organization and its measurement systems. A key feature of the system is its very low development cost. Sensitivity to cost and risk inhibits development in public agencies, and in the private sector as well. The approach described includes the use of standard, easily available programming and software tools for development and prototyping with live data. An iterative process was employed to develop new data where none previously existed. The experience reported here highlights how commitment from top management is a primary factor for EIS success in the public sector, even more so than in the private sector.
|keyword = EXECUTIVE INFORMATION SYSTEMS,GOVERNMENT INFORMATION SYSTEMS,PRIVATE SECTOR VS PUBLIC SECTOR SYSTEMS,DATA PROBLEMS,IMPLEMENTATION ISSUES,PROTOTYPING,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Dimensions of I/S Planning and Design Aids: A Functional Model of CASE Technology'''
{{header}}
{{article
|author= John C. Henderson,Jay G. Cooprider,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Information technology is playing an increasingly integral role in the competitive strategies of many organizations. As this trend continues, it is not surprising that there is growing emphasis on the ability of organizations to plan, design and implement critical information systems. A major strategy to improve the effectiveness of these processes is the use of computer-based planning and design aids. However, there is little empirical evidence that using this technology provides a significant performance impact. One factor limiting research on the impact of technology on planning and design is the manner in which this technology has been conceptualized for measuring usage behavior. This research develops a functional model of I/S planning and design support technology that distinguishes three general functional dimensions: Production Technology, Coordination Technology and Organizational Technology. An empirical analysis is used to test the robustness of the proposed model and its ability to discriminate among current design aids in a meaningful way. Implications for the use of this model in the study of I/S planning and design processes are discussed.
|keyword = Computer-aided software engineering (CASE),Planning and design,Automated software development tools,Management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effective IS Security: An Empirical Study'''
{{header}}
{{article
|author= Jr. Detmar W. Straub,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Information security has not been a high priority for most managers. Many permit their installations to be either lightly protected or wholly unprotected, apparently willing to risk major losses from computer abuse. This study, based on the criminological theory of general deterrence, investigates whether a management decision to invest in IS security results in more effective control of computer abuse. Data gathered through a survey of 1,211 randomly selected organizations indicates that security countermeasures that include deterrent administrative procedures and preventive security software will result in significantly lower computer abuse. Knowledge about these relationships is useful for making key decisions about the security function.
|keyword = Computer security,Information security,General Deterrence Theory,Computer crime,Computer ethics,Protection of the information resource,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Refresh Scheme for Remote Snapshots'''
{{header}}
{{article
|author= Aditya N. Saharia,George Diehr,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = This article presents a scheme called "Difference Table" for maintaining database snapshots stored at sites remote from a central database which are refreshed only upon user request. Database snapshots are currently in widespread use where a subset of the central database is extracted and transmitted to a local workstation and utilized for decision support. The Difference Table method checks each update to a central database table against the definition of the snapshot. If the update is relevant, its effect is stored in a difference table. On receiving the refresh request. the contents of the difference table are transmitted to the remote site where they update the snapshot. The Difference Table scheme allows a selective refresh of the snapshot. in the sense that only the changes to a snapshot since the last refresh are transmitted. We discuss the additional database tables and processes required to support the Difference Table scheme. Performance measures are developed, and both quantitative and qualitative comparisons are made to alternative methods such as full regeneration and the approach used by System R*. By most criteria and in many environments, the Difference Table scheme is preferable to these alternatives. It also has several attractive side benefits which are not available in alternative methods.
|keyword = Database snapshot,Materialized views,Snapshot management,Distributed database,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Assessing the Economic Benefits of Information Systems Auditing'''
{{header}}
{{article
|author= J. Christopher Westland,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = As corporate management attempts to extract more end user benefit from information systems department expenditures, interest has grown toward the use of information systems auditing to assure software quality. This research shows that information systems departments are motivated by end users to provide lower quality systems than they would if allowed to pursue their own objectives. The research continues by demonstrating that auditing cannot a priori be assumed to raise the quality of corporate information systems. In fact, auditing tends to establish objectives that lower software quality. It demonstrates that audits are most beneficial in managing unsophisticated information systems departments in which end users are currently dissatisfied with their level of support. Augmenting the systems development process via technologies such as computer aided software engineering and prototyping may more consistently and effectively improve quality than does auditing. Recent developments among the large audit firms indicate that they recognize the importance of new software development technology, and are restructuring their businesses accordingly.
|keyword = Economics of information systems management,EDP auditing,Information strategy and policy,Organitational information processing,System performance assessment,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Investigation of Factors Influencing the Success of Customer-Oriented Strategic Systems'''
{{header}}
{{article
|author= Blaize Horner Reich,Izak Benbasat,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = A case research strategy was utilized to study first-mover strategic systems which companies had built and offered to their customers in support of a primary product or service. The study investigated eleven systems to discover the factors which enabled or inhibited the following outcomes: developing a first-mover customer-oriented strategic system (COSS); achieving a high level of adoption of the COSS by customers; obtaining competitive advantage from the COSS. In general, the findings supported previous research in the IS implementation and strategic systems literature. Factors that are related to the successful implementation of information systems and the competitive environment of the firm were associated with systems that were developed and introduced to the market first. Factors that are related to the adoption of innovations and information systems and to successful product marketing were associated with high adoption. There were several findings which had not been previously reported in the literature. Early adoption of the system was inhibited by poor support for the sales force and poor quality pilot tests. Long-term penetration was inhibited in case, where the champion lost direct control over the COSS. Competitive advantage was not achieved by any system which had spent less than three years in the market or by those which did not achieve high long-term adoption.
|keyword = Strategic information systems (SIS),Information for competitive advantage,Inter organizational systems (IOS),Information technology adoption,Information technology implementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Adoption of Spreadsheet Software: Testing Innovation Diffusion Theory in the Context of End-User Computing'''
{{header}}
{{article
|author= James C. Brancheau,James C. Wetherbe,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Judging by the wealth of problems reported in the literature, information systems (IS) and general managers are not sure how to manage the introduction of new information technology. One step toward providing sound management guidelines is to improve understanding of the social forces which affect the introduction and diffusion process within organizations. This research takes a step toward that goal by examining the validity of innovation diffusion theory within the context of end-user computing. The research involved a field study and historical analysis of the diffusion of spreadsheet software in organizations. To assist in controlling exogenous factors, only finance and accounting departments were studied. Over 500 professionals in 24 business units from 18 large businesses in manufacturing and services participated in the research. Findings supported hypotheses that earlier adopters of spreadsheet software were younger, more highly educated, more attuned to mass media, more involved in interpersonal communication, and more likely to be opinion leaders. Also supported was the hypothesized sigmoidal distribution of adoption over time. Application of the theory was not supported in all areas, however, suggesting that information technology diffusion is different from other diffusion phenomena. Contrary to theory, interpersonal channels of communication were dominant in all phases of adoption decision making. And contrary to their hypothesized role as change agent, IS departments played a minor role in the diffusion process. This was consistent with the observed user-led nature of the phenomenon. Implications for research and practice are discussed.
|keyword = End-user computing,Innovation diffusion,Adopter characteristics,Emerging information technology,Spreadsheet software,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Automated Construction of Knowledge-Bases from Examples'''
{{header}}
{{article
|author= Kar Yan Tam,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = The process of knowledge acquisition has long been regarded as the bottleneck in expert systems development. In this paper, a concept induction methodology for automated construction of knowledge-bases is presented and its use for knowledge acquisition is discussed. The applicability of such a tool to build expert systems is demonstrated using CONIS in some selected experiments. CONIS is a concept induction system that infers concept description from sample instances of the concept. We have also compared CONIS with conventional statistical techniques in solving classification problems. The results suggest that concept induction could become be a viable tool to automate the process of knowledge acquisition. By shortening the development cycle, domains that were once too volatile for expert systems application would become feasible using such an automated aid.
|keyword = Expert systems,Knowledge acquisition,Machine learning,Knowledge-bases,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Role Theory Perspective on End-User Development'''
{{header}}
{{article
|author= Dennis F. Galletta,Jr. R. L. Heckman,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = Recent advances in computer technology have been accompanied by significant changes in the roles of both users and developers. One well-known example is end-user development, which here provides a context for the application of role theory to IS issues. A framework is proposed for classifying IS roles in a way that can be used for role behavior analysis. The framework takes the form of a matrix, with activities relating to the construction and use of information systems on one axis, and activities relating to generic organizational levels on the other. Role theory permits analysis of organizational phenomena from either a structural or a process-oriented perspective. Propositions from both perspectives are presented which can be used to direct empirical studies.
|keyword = Developers,End-User computing,End-User development,Role theory,Users,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Optimal Information Structures for the Seller of a Search Good'''
{{header}}
{{article
|author= Terence Barron,A. N. Saharia,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = This paper examines an information system design problem faced by the seller of a search good who sells his product in a competitive market to well-informed consumers. The formulation results in a nonlinear optimization problem having a special structure which can be exploited in solving the first-order conditions. Closed-form solutions and comparative statics results are given in the case of a uniformly-distributed attribute, and we provide a numerical example in the case of a normally-distributed attribute.
|keyword = economics of information systems,design of information systems,design of information structures,information economics,optimal measurement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of the Environment on the Management of Information Systems'''
{{header}}
{{article
|author= Albert L. Lederer,Aubrey L. Mendelow,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = The dynamic environment of an organization makes management difficult. This is especially true in information systems (IS) departments. A new theoretical model, based on structured interviews with IS executives, proposes ways that some dimensions of the environment create problems for IS management. IS managers develop coping mechanisms to attenuate or prevent these problems. They use these coping mechanisms either to resolve the problems directly or to influence the environment to prevent the problems. The new theoretical model facilitates an organized study of the relationship between the IS department and its dynamic environment.
|keyword = Information management,Information systems,Information technology,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Can Software Influence Creativity?'''
{{header}}
{{article
|author= Joyce J. Elam,Melissa Mead,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = The concept of decision support systems that emerged in the seventies offered the promise of computer-based tools that would enable decision makers to develop better and more creative solutions to the problems they face. The realization of this promise depends on the validity of two assumptions: (1) individuals can be helped to be more creative and (2) software, appropriately designed, can provide this help. Evidence from creativity research supports the first assumption. This paper explores the second assumption. A set of guidelines for designing DSS is developed from the creativity literature and two hypotheses involving the use of a DSS developed in accordance with these guidelines are proposed. To test these hypotheses, an experiment involving three groups of auditors from a 'big eight' accounting firm was conducted. Each group was asked to respond to two different decision situations. Two groups were given separate versions of a DSS specifically selected because it contained features that were thought to enhance creativity. Another group was given no software. The results of the experiment indicate significant differences in the creativity of the responses generated by the three groups. Possible explanations for these differences are explored.
|keyword = Decision support systems,User-computer interaction,Creativity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Integrative Model of Information Systems Spending Growth'''
{{header}}
{{article
|author= Vijay Gurbaxani,Haim Mendelson,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = This paper develops a model of the growth of information systems expenditures in the United States. The model incorporates two major factors that influence the rate and pattern of spending growth-the diffusion of technological innovation and the effect of price on the demand for computing. Traditional studies have focused on the role of innovation while ignoring the effects of price on the growth process. We show that while information systems expenses initially grew following an S-curve, more recent growth has converged to an exponential pattern. These patterns are consistent with our integrative price-adjusted S-curve growth model.
|keyword = Information systems expenditures,Budget,Diffusion of innovation,Demand for computing,Computing costs,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decision Support Systems: Directed and Nondirected Change'''
{{header}}
{{article
|author= Mark S. Silver,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = The Decision Support Systems (DSS) literature is in general agreement that use of DSS leads to individual and organizational change, but there is no consensus as to whether DSS and their designers serve as agents for directed or nondirected change. Researchers have proceeded from two different sets of premises, drawing different conclusions about the nature of DSS. This paper considers both views, examining how differences in designers' attitudes toward change agency ought to be manifest in the features of the DSS they implement. Two attributes of DSS, "system restrictiveness" and "decisional guidance." are discussed as the basis for understanding differences in DSS following from differences in designer attitudes toward change. Using these two attributes, four DSS strategies for directed change and five strategies for nondirected change are presented.
|keyword = Decision support systems,Change agency,System restrictiveness,Decisional guidance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alternative Mechanisms of Allocating Computer Resources Under Queueing Delays'''
{{header}}
{{article
|author= Seungjin Whang,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = A theoretical framework is developed in which alternative mechanisms of allocating congestion-prone computer resources are studied and compared. Two-discrete and continuum-models of economies are presented to depict a small and a large economy respectively. Alternative allocation mechanisms are discussed in these two models: (1) the private bargaining approach, (2) allocations attainable through Nash equilibria and (3) the Clarke-Groves tax mechanism in the discrete economy model, and (4) Mendelson's (1985) job-by-job pricing and (5) the exchange-market-based allocation in the continuum economy model. We find equivalence among the bribes and prices associated with these mechanisms. The theory is related to practical implications pertaining to the design of computer chargeback systems and the role of the system manager.
|keyword = Control of computer resources,Queueing delays,Allocation methods,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User Involvement as an Interaction Process: A Case Study'''
{{header}}
{{article
|author= Michael Newman,Faith Noble,
|source= INFORMATION SYSTEMS RESEARCH
|year= 1990
|abstract = User involvement is recommended to analysts as a technique of successful system development, but as a process it is little understood. This case study compares four process models of user involvement learning, conflict, political and garbage-can with each other and with an empirical example of system development. Different models are seen as appropriate to explaining the nature of user involvement in different stages of development and contexts. Structural conditions and issues of power are shown to be decisive in the development of conflict and conflict resolution. A two-stage model of user involvement based on Robey and Farrow's work (1982) is proposed which distinguishes conflict development from conflict resolution.
|keyword = Information systems/information systems development,Resistance,User involvement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Application Approach Worksheet: An Evaluative Tool for Matching New Development Methods with Appropriate Applications'''
{{header}}
{{article
|author= Jack Shomenta,Gary Kamp,Bob Hanson,Brandon Simpson,
|source= MIS QUARTERLY
|year= 1983
|abstract = This article presents a method used by the Hennepin County Information Services development staff to help determine how best to use new development methods. The tool described and the method for using it were designed to help both data processing personnel and user personnel make the transition from using one development approach only, to a variety of approaches using new technologies and methods. In brief, the "Application Approach Worksheet" is a document used by mid-level and senior user management to identify, in data processing terms, the characteristics of a proposed application. Once the application's characteristics are defined, the user utilizes the worksheet's matrix to determine which development method might be most appropriate for the application. The completed worksheet and a work request then becomes the basis to propose development of the new application. At that point, an Information Services Department (ISD) project manager discusses the application with the user. Together, they agree on the new project's priority relative to other projects, and work out how the project will be handled in general. This article first describes the environment in which the "Application Approach Worksheet" was developed and the circumstances that led to its development. The article goes on to discuss how the worksheet was created and describes the criteria that form the heart of the document. Finally, the article describes how the worksheet is used and discusses the results of its use since it became a part of the work request process in January of 1983.
|keyword = Life cycle,selection criteria,application development,systems analysis and design,traditional,user developed systems personal computer,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Reshaping the IS Organization'''
{{header}}
{{article
|author= Edwin B. Shore,
|source= MIS QUARTERLY
|year= 1983
|abstract = A variety of forces have been serving to alter the traditional triangular shaped IS organization. This article describes an approach to dealing with those forces that maximizes career path opportunities and helps to develop and retain a staff of highly capable professionals. The result is an evolution from a triangular to a pentagonal-shaped organization.
|keyword = Human factors,staffing,systems development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Developing a Corporate Private Network'''
{{header}}
{{article
|author= Herbert N. McCauley,
|source= MIS QUARTERLY
|year= 1983
|abstract = The integration of telecommunication resources has been fundamental to Harris' information management systems strategy since the inception of a corporate-wide MIS program in 1977. From 1977 to 1979, communication requirements expanded rapidly. Traffic volume was evaluated periodically to determine the appropriate time to integrate voice, data, facsimile, text and, ultimately, video transmission for teleconferencing. In 1979, corporate MIS started a major program to optimize transmission of digitized voice and data over a private, satellite/microwave network, with switch/access networks at the three satellite earth station nodes to concentrate local traffic for wide area satellite transmission. The primary objective was improved cost-effectiveness by minimizing distance-tariff-usage sensitivity. The network has been in operation since December, 1980 and is achieving all cost and performance goals. Due to the high visibility of the network, Harris divisions are more sensitive to the cost and management of communications. The network positively impacted the overall MIS program, particularly the consolidation of telecommunications/office/data processing systems management, and its success influenced Harris' plans for future markets.
|keyword = Telecommunications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Field Study of End User Computing: Findings and Issues'''
{{header}}
{{article
|author= David H. Benson,
|source= MIS QUARTERLY
|year= 1983
|abstract = This article reports on a series of interviews on end user computing carried out in twenty locations in St. Louis. The interviews took place between December 1982 and March 1983. Sixty-seven end users from all levels of management and nineteen IS professionals were interviewed in their work locations regarding the practice of interactive computing by non-DP professionals. Though end user computing is still in its early stages, signs of rapid growth are present. In response to this growth, IS departments are attempting to develop policies for its control and support. The picture is complicated by the recent introduction of microcomputers to the business office. The study explores this new phenomenon and examines some of the differences between those who use the mainframe environment and those who use micro-computers. Software used, the varieties of applications developed, and the training background of end users are examined, as well as some of the problems encountered and some of the early results. A prime objective of the study is accomplished in identifying end users' educational goals. Finally, five critical issues are identified which those interviewed saw as needing resolution in the near future.
|keyword = End User,Microcomputer,personal computer,education,training,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Natural Language for Database Queries: A Laboratory Study'''
{{header}}
{{article
|author= Yannis Vassiliou,Matthias Jarke,Edward A. Stohr,Jon A. Turner,Norman H. White,
|source= MIS QUARTERLY
|year= 1983
|abstract = Are natural language systems for database queries meeting their goals?, and, are these goals appropriate? The recently completed Advanced Language Project at New York University combined a field experiment with two laboratory studies to examine these issues by comparing performance between subjects using the formal database language SQL and subjects using the prototype natural language system, USL. This article describes the design and results of the larger laboratory experiment. The results presented offer some promise for the usability of natural language under certain conditions.
|keyword = Query language,natural language,controlled laboratory experiments,exploratory study,databases,subject performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Needs of Top MIS Managers'''
{{header}}
{{article
|author= E. W. Martin,
|source= MIS QUARTERLY
|year= 1983
|abstract = Despite the importance of the role of the Chief MIS Executive, little is known about the "Information Needs of Top MIS Managers." This article uses a Critical Success Factors (CSF) approach to explore the information needs of these managers. It has been widely speculated that top MIS Managers are remiss in their use of the technology they provide to others. This article also reports on a study of the formal information systems used personally by chief MIS executives and contrasts these results with their needs as revealed by the CSF analysis.
|keyword = Critical success factors,management of MIS,information requirements,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Impact of Information Management on the Organization: Two Scenarios'''
{{header}}
{{article
|author= Daniel J. Power,
|source= MIS QUARTERLY
|year= 1983
|abstract = A concept called information management has been discussed for many years by computer and management scientists. Implementing this concept may revolutionize organizations and have a profound effect on organizational decision making. Since the technology needed to implement sophisticated information systems is now available, managers need to address the potential impact of this innovation on their organizations. This article presents two scenarios that may help managers to anticipate the effect of information management on organizational decision making.
|keyword = information management,organization design,decision making,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Diagnosing and Treating the Credibility Syndrome'''
{{header}}
{{article
|author= William J. Doll,Mesbah U. Ahmed,
|source= MIS QUARTERLY
|year= 1983
|abstract = The credibility syndrome is a special case of MIS development failure with a distinctive set of symptoms and causes. In credibility problems, the directors of data processing - their management approaches and systems development philosophies - play an important role.
|keyword = MIS,MIS management,credibility syndrome,executive steering committee,systems development process,system planning objectives,application selection criteria,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer Assisted Planning (CAP) at Dinero International Bancorporation'''
{{header}}
{{article
|author= James R. Doyle,Jack D. Becker,
|source= MIS QUARTERLY
|year= 1983
|abstract = This article describes three inter-related computer based systems that were designed and developed to assist with the implementation of a major long-term strategic planning effort at a multi-billion dollar bank holding company, Dinero International Bancorporation, (DIB). New strategies at DIB included: (1) a major redirection of its marketing efforts to a multi-state mid-western region in the U.S., and (2) to correspond with this new marketing direction, a major change in the bank's image, including a name change. The first system, the Regional Banking Information System (REBIS) was designed to extract key information from a 1G (gigabyte) bank database. With REBIS, DIB was able to monitor the level of banking activity in a designated region and to measure the performance and level of competition of over 1,000 selected banks with operations in this region. A second system, the SAS/Dunn and Bradstreet (D & B) system, was designed to analyze and plot key financial characteristics (sales, debt, etc.) of the more than 20,000 selected companies also in the region. Information pertaining to business activity by industry was plotted on maps of the region in order to identify the most desirable marketing opportunities. Finally, the third system, AUTO TRAC, an automated project tracking system, was used to monitor and control the progress of the DB name change plan.
|keyword = Financial information systems,banking information systems,strategic planning,strategic information,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Successful Development Strategies for Business Application Systems'''
{{header}}
{{article
|author= James D. McKeen,
|source= MIS QUARTERLY
|year= 1983
|abstract = In a field study of thirty-two business application systems, the relationship between the time spent in various phases of the development life cycle and the outcome of the development was examined: Results indicate that systems which spent more time in the analysis phase required less time to code, resulted in greater user satisfaction, and were developed in agreement with established budgets and deadlines. These results suggest preferred strategies for the development of application systems and have implications for their successful management and control.
|keyword = Application system development,user satisfaction,development life cycle,implementation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Linking the MIS Plan with Corporate Strategy: An Exploratory Study'''
{{header}}
{{article
|author= Philip J. Pyburn,
|source= MIS QUARTERLY
|year= 1983
|abstract = In recent years there has been an increasing amount of attention paid to effective strategic planning of the information systems resource in many organizations. The motivation for this attention derives from a number of sources, especially the rapid rate of technological progress in computer and telecommunications facilities and the increasing application of these facilities to problem domains that are critical to organizational success. As a wide range of new opportunities become apparent, it has become more difficult to match the operational, managerial, and strategic needs of the business with appropriate systems activities. Based upon an exploratory study of eight organizations which involved extensive interviews with IS and senior managers, this article describes the planning practices observed and identifies several factors which seemed particularly important to their planning success, or lack of it. These factors include such things as the style of senior management decision making, the volatility of the business (and by extension, the applications development portfolio), the complexity of the IS organization and management task, and the status and physical location of the IS manager.
|keyword = Planning,strategy,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Management's Role in the Approval and Administration of Decision Support Systems'''
{{header}}
{{article
|author= Jack T. Hogue,Hugh J. Watson,
|source= MIS QUARTERLY
|year= 1983
|abstract = Decision support systems (DSS) have become an increasingly important type of computer-based information system. However, there has been limited research on management's role, in DSS approval and administration. Because of this, the authors recently investigated eighteen decision support systems and report the findings here. The specific areas explored include motivations for developing a DSS, methods for evaluating the desirability of creating a DSS, planning and organizing for building. a DSS, techniques for reviewing and controlling DSS projects, and managing a DSS as an organizational entity.
|keyword = Decision support systems (DSS),system planning and management,administrative policies,operating procedures,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''User Managers' Systems Needs'''
{{header}}
{{article
|author= Robert M. Alloway,Judith A. Quillard,
|source= MIS QUARTERLY
|year= 1983
|abstract = Based on responses from over five hundred user managers, this article investigates managers' demand for new application systems. To begin, the current situation is assessed from two aspects. First, how many systems by type do user managers now have and how appropriate are the systems. Second, for important managerial tasks, what support (by systems type) do users have and how appropriate are those systems. Then the two components of user managers' demand for new systems, the number of systems and the types of systems, are examined. The results reveal an overwhelming level of managerial demand for new systems and major shifts in demand mix by systems type. The implications of this current and future demand for IS management are presented.
|keyword = MIS management,new systems development,users' needs,backlog,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Effectiveness of External Information Channels in Facilitating Innovation Within Software Development Groups'''
{{header}}
{{article
|author= Robert W. Zmud,
|source= MIS QUARTERLY
|year= 1983
|abstract = This study of forty-nine software development groups investigated the effectiveness of ten information channels, linking the software groups to potential information sources about new developments in software methodologies, as a means of facilitating software group innovativeness. While the findings suggest that software group innovativeness can be improved by providing appropriate external information channels, this relationship is contingent on a software group's internal environment. The channels most commonly provided by those organizations participating in the study tended to be those least effective in promoting innovation.
|keyword = Software development,software management,information channels,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Empirical Assessment of the Stages of DP Growth'''
{{header}}
{{article
|author= D. H. Drury,
|source= MIS QUARTERLY
|year= 1983
|abstract = This study attempts to validate the stages of growth hypothesis by analyzing data concerning the benchmarks that were prescribed by Nolan [15]. Although the hypothesis was not validated using the entire set of benchmarks for each stage, individual benchmarks were related to various DP management issues. The joint relationship of user awareness and technology to various management issues was also assessed. In addition, the planning and control techniques that have been used in the stages of growth literature were factor analyzed. The factor groupings were compared with those groupings that the stages of growth idea associate respectively with early and late states of DP evolution.
|keyword = Information systems,management,life cycle,systems development,planning and control,benchmarks,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Cost/Benefit Analysis of Computer Based Message Systems'''
{{header}}
{{article
|author= Ian Montgomery,Izak Benbasat,
|source= MIS QUARTERLY
|year= 1983
|abstract = This article provides a review and synthesis of the recent literature in computer based message systems (CBMS). It describes the problems associated with the current methods of communication and discusses how CBMS attempts to solve these problems. The article then presents some estimates of the costs of CBMS and potential problems that could slow down the implementation of these systems. The article concludes by outlining a methodology for developing CBMS.
|keyword = Message systems,office automation,electronic mail,telecommunications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Data Dictionary: An Evaluation from the EDP Audit Perspective'''
{{header}}
{{article
|author= Michael T. Vanecek,Ira Solomon,Michael V. Mannino,
|source= MIS QUARTERLY
|year= 1983
|abstract = The data dictionary system is a documentation source that is useful for management reviews of existing and proposed systems, EDP audits, and system development functions. Early data dictionary systems had limitations that reduced their effectiveness and contributed to their limited usage. Many of these limitations have been or are being resolved with the result that evolving data dictionary systems offer many benefits to management and EDP auditors. This article evaluates the features, potential benefits, and limitations of data dictionary systems from the perspective of the EDP auditor.
|keyword = Data dictionary,database administration,data management,database management systems,EDP auditing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Effects of Work Stress and Social Support on Information Systems Managers'''
{{header}}
{{article
|author= Madeline Weiss,
|source= MIS QUARTERLY
|year= 1983
|abstract = This study investigates the sources of organizational stress among information systems (IS) managers, the resulting symptoms of strain, and whether social support can reduce symptoms of strain. A field study comprised of a survey questionnaire was chosen as the most appropriate design for this investigation. The respondents were IS managers, ranging in the organizational hierarchy from vice president or director to project manager, in both governmental and private sector organizations of varying sizes. The study reveals that job stresses among IS managers are positively related to psychological and physiological strains. While all of the stressors included in this investigation are significantly related to strain symptoms, certain stressors emerge as having the greatest impact. Likewise, certain strains that result from these stressors are more prevalent than others. Concerning social support, the study reveals that the level of social support among IS managers is lower than among other managers. When social support exists, strain among these managers is significantly lower. The implications of the study's findings are considerable both for the health prognosis of IS managers and for their job performance.
|keyword = Information systems managers,organizational stress,social support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Influence of Computer Graphics on the Recall of Information'''
{{header}}
{{article
|author= Collin J. Watson,Russell W. Driver,
|source= MIS QUARTERLY
|year= 1983
|abstract = This article describes an experiment that examined the influence of computer plots of three dimensional graphics on the recall of information. Three dimensional graphics and a tabular mode of presentation were used to convey information to two experimental groups of subjects. The graphics were produced by using perspective projections. The outcome was that the three dimensional graphics did not result in greater recall of information than did a tabular presentation for the task that was assigned.
|keyword = Computer graphics,MIS management,information recall,three dimensional graphics,information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Chauffeured Versus End User Access to Commercial Databases: The Effects of Task and Individual Differences'''
{{header}}
{{article
|author= Mary J. Culnan,
|source= MIS QUARTERLY
|year= 1983
|abstract = This field study investigated the task and individual characteristics of 184 professionals who accessed commercial database services to acquire external information directly (as "end users") or through an intermediary ("chauffeur"). Chauffeured access appears to be most appropriate when the individual has a one-time need for new information while direct access appears to be most appropriate when a database is used on a regular basis by the same individual. The results of this study are consistent with prior research which suggests that multiple access arrangements are necessary in order for organizations to make effective use of these and other types of online database systems.
|keyword = Information storage and retrieval,information management,system use,organizational theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Assessment of the Concept of Decision Support Systems as Viewed by Senior-Level Executives'''
{{header}}
{{article
|author= Maryam Alavi,
|source= MIS QUARTERLY
|year= 1982
|abstract = This article describes a study performed to (1) investigate senior executives' perceptions of decision support systems concepts, and (2) identify executives' needs and desired benefits related to decision support systems. A series of indepth interviews was conducted with senior-level industrial executives. The findings suggest three guidelines for the design of effective decision support systems: (1) executive users should be directly involved in the development and evaluation of the initial design concept, (2) if possible, a prototype should be built before developing the full-scale system, and (3) the decision support system should provide capabilities for complexity coping, conflict resolution, and uncertainty reduction.
|keyword = Decision support systems,executives' requirements for decision support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Integration of Word Processing Into a Management Information System'''
{{header}}
{{article
|author= Raymond McLeod,Donald H. Bender,
|source= MIS QUARTERLY
|year= 1982
|abstract = Both the computing and word processing industries have grown to billion dollar size, but little real effort has been made to integrate them at the application level. The effort that has been expended to date has been directed at integrating word processing and data processing. What is needed is a serious look at the potential for integrating word processing into management information systems. One approach is to evaluate theoretical constructs of management information systems that indicate areas of potential word processing applications. Henry Mintzberg's concept of managerial roles and information flows is one such construct. The authors use Mintzberg's theory as a basis for a word processing and management information system integration that is being followed in a life insurance company. The company had recognized the logic of integrating word processing and data processing, but the potential for also involving information processing in the long range plan became clear only after an interpretation of the Mintzberg theory in light of word processing opportunities. The experiences of this company should be of value to others engaged in loosely-tied word processing, data processing, and management information system development programs.
|keyword = Word processing,information theory,management roles,hard information,soft information,Mintzberg,multifunctional work station,action oriented processing,multifunctional management information station,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Alternative Designs in Information System Development'''
{{header}}
{{article
|author= William R. King,
|source= MIS QUARTERLY
|year= 1982
|abstract = The concept of creating and considering alternative designs in the system development process is integral to design methodology in many fields. In MIS the idea is little implemented despite the existence of some techniques that facilitate the consideration of alternative designs. Some of these techniques are reviewed and evaluated in terms of their utility in MIS.
|keyword = MIS design process,alternative designs,MIS development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Personality Characteristics of MIS Project Teams: An Empirical Study and Action-Research Design'''
{{header}}
{{article
|author= Kate M. Kaiser,Robert P. Bostrom,
|source= MIS QUARTERLY
|year= 1982
|abstract = Information systems for large firms are typically designed by a team comprised of both users and systems personnel. The Management Information System (MIS) literature discusses a communication gap between the organization oriented users and the more technical systems staff. It is often hypothesized that systems personnel and users are different in terms of personality and behavior characteristics and that these differences are one of the primary reasons for the existence of a communication gap. This article summarizes a two-phased study. The first phase investigated personality characteristics of respondents from thirty-two large organizations who worked on design teams. The second phase examines, in detail, a system success and failure in one organization. Analysis was performed to see if there are significant differences on personality dimensions between users and systems personnel and to explore the relationship between these differences and system success, An operationalization of Jung's personality typology (Myers-Briggs Type Indicator) was employed. The results show that the users involved in the systems design are very similar to their systems counterparts. Even more surprising is that the characteristics of these users are closer to the popular descriptions of systems staff than the analysts are. They also suggest that these similarities in personality types may have an impact on system success. The general implications of these findings in terms of the management of project teams and the MIS designs they create are discussed.
|keyword = Information systems,cognitive style,socio-technical systems,communication gap,MIS design,user involvement,action-research,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Knowledge Utilization Among MIS Researchers'''
{{header}}
{{article
|author= Scott Hamilton,Blake Ives,
|source= MIS QUARTERLY
|year= 1982
|abstract = Knowledge utilization among MIS researchers is examined via an analysis of 9,911 references appearing in 532 MIS journal articles published in fifteen journals during the 1970-1979 period. Quantitative indicators of references per article, type of publication referenced, elapsed time between citing and cited publications, and cross disciplinary references are used to categorize knowledge utilization by researchers in the MIS discipline and to analyze changes in reference patterns over the ten year period and differences between academician and practitioner authored works.
|keyword = Management information systems,citation analysis,knowledge utilization,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Corporate Electronic Mail - A Communication-Intensive Application of Information Technology'''
{{header}}
{{article
|author= Jr. A. B. Crawford,
|source= MIS QUARTERLY
|year= 1982
|abstract = Extending the ARPANET technology of an asynchronous, packet-switched "electronic mailbox," the Corporate Information Systems department introduced a pilot mail service within Digital Equipment Corporation which has now grown into a full-fledged production system with some 6,000 users - and is still growing. The architecture for the Electronic Mail System (EMS) is based on a multinode network of dedicated minicomputers. Technical, administrative and human factors, and cost considerations were recorded throughout the pilot and production period. Lessons learned have highlighted the need for better network engineering, capacity planning, and operational policies/procedures. User surveys were used to capture demographic data and reaffirmed the highly favorable impact on personal productivity and each manager's effectiveness. Recommendations are offered on how to plan for a pilot and to assure a smooth transition to production service.
|keyword = EMS,electronic mail,office automation,office systems,telecommunications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organization Development Methods in the Management of the Information Systems Function'''
{{header}}
{{article
|author= R. D. Loftin,J. M. Moosbruker,
|source= MIS QUARTERLY
|year= 1982
|abstract = The work presented in this article relates directly to perhaps the most serious problem facing the Information Systems manager in a large, complex organization today, namely how to plan and manage in a rapidly changing, high-demand, resource-limited environment. The article describes an organizational change effort undertaken within a major data processing organization to seek improvements in four broad areas: data center production performance, responsiveness of the systems development activity, management control and decision making, and long range and operational planning processes. Much of the change effort involved activities and tasks which were defined and implemented using Organization Development (OD) methods. OD involves the application of behavioral science knowledge in a collaborative and participative process in response to some perceived need within the organization. It is a planned and systematic way to alter patterns of organizational behavior. The general change strategy utilized is described along with specific examples of particular OD techniques, those which worked well and those which did not. Finally the article presents some observations as to why OD methods are important to Information Systems (IS) managers. OD methods are powerful skills and tools for working change of the most important kind, namely change in organizational and group behavior. As the primary agent for change in the corporation, IS managers can employ these methods to make the IS organization more responsive to the needs of the business. More importantly they can bring these same skills to bear on the larger problem of helping their corporation adjust to change. facilitating and leveraging that change in ways which mere technology cannot accomplish.
|keyword = Organization development,change management,data center performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Prototyping: The New Paradigm for Systems Development'''
{{header}}
{{article
|author= Justus D. Naumann,A. Milton Jenkins,
|source= MIS QUARTERLY
|year= 1982
|abstract = Leading MIS executives and academicians have identified systems development as one of the most critical issues of the 1980s. Their concerns include providing user accessibility to stored information, reducing development cost and delay, increasing developer productivity, and increasing MIS's impact on organizational growth, productivity, and profitability. Among (the number of proposed alternative approaches to traditional systems development, prototyping is mentioned frequently. Prototyping is routine in hardware development but not software. The authors review published references to prototyping and related concepts, and synthesize a process model for information systems. In this model, resource requirements are enumerated and discussed. The article includes an analysis of the economics of prototyping, and a brief discussion of several examples. Prototyping for information systems development addresses today's critical issues; it will no doubt raise a new set of research questions for tomorrow.
|keyword = Information systems,systems design,systems analysis,methodology,economics,productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Marketing the MIS During Times of Resource Scarcity'''
{{header}}
{{article
|author= Jr. Raymond McLeod,William L. Fuerst,
|source= MIS QUARTERLY
|year= 1982
|abstract = MIS managers generally have not stressed the marketing aspects of their operations. A more widespread concern is with the shortage of systems analyst and programmer resources. In an effort to learn how one group of MIS managers views their marketing responsibilities and practices in a shortage era, a study was conducted and the results were used to describe both short term and long term MIS marketing plans. These plans are developed by identifying critical areas in the organization in light of corporate objectives, user needs, and MIS resources.
|keyword = Corporate computing facility,demarketing,marketing,marketing mix,marketing plan,resource shortage,marketing strategy,MIS manager,target market,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Context and MIS Structure: Some Empirical Evidence'''
{{header}}
{{article
|author= Phillip Ein-Dor,Eli Segev,
|source= MIS QUARTERLY
|year= 1982
|abstract = An empirical investigation of some relationships between organizational context and MIS structure is reported. The major findings are that MIS structure is significantly correlated with organizational structure which, in turn, is closely associated with organizational size. Additional relationships were found between organizational size and the rank of the MIS director, between implementor/user relationships and psychological climate toward MIS, and between the last two variables and degree of MIS integration.
|keyword = Centralization,implementor/user relationships,integration,MIS director,MIS structure,organizational size,organizational structure,propensity to pioneer,psychological climate,rank,time frame,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Critical Success Factors of Chief MIS/DP Executives'''
{{header}}
{{article
|author= E. W. Martin,
|source= MIS QUARTERLY
|year= 1982
|abstract = A critical success factor is a key area where things must go right if an organization is to be successful. In order to better understand the management of the MIS area, we explore the critical success factors of a group of chief MIS managers in successful organizations. Knowledge of these critical success factors is useful to top management in understanding how to cope with and evaluate the MIS organization. Critical success factors are also helpful to MIS managers in improving their own performance, in communicating with their boss and with those reporting to them, and in defining their personal information needs.
|keyword = Critical success factors,management of MIS,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Information Technology in the 1990s: A Long Range Planning Scenario'''
{{header}}
{{article
|author= Robert I. Benjamin,
|source= MIS QUARTERLY
|year= 1982
|abstract = This article describes a reasonable scenario for in technology, and its use within a major organization in the year 1990. The scenario is based upon a model used at Xerox to portray the use of Information Systems (IS) internally within the corporation in the year 1990. Assumptions are made about the technology and economics, and by coupling these with observed trend linos from historical data, predictions about the 1990 IS organization, technology, investment requirements, and support structure are drawn: Estimates are developed for the extent of distributed processing at four levels of the organizational hierarchy. Finally, a number of conclusions which deal with the changing nature of the IS technology and role. of IS management are described.
|keyword = Management information systems,information systems technology,information systems strategic planning,cost performance,technology trends,operational computing,end user computing,functional architecture,office systems,applications software,distributed processing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Approaches to Strategic Planning for Information Resource Management (IRM) in Multinational Corporations'''
{{header}}
{{article
|author= Gad J. Selig,
|source= MIS QUARTERLY
|year= 1982
|abstract = Multnational corporations (MNCs) face increasingly higher rewards and risks when choosing amongst alternative investments for their computer and communications based information systems. The multi-layered management and technical issues and decisions confronting multinational corporations are seemingly endless and require new insights. A rapidly growing number of MNCs are utilizing computer based information systems, office automation and administrative support systems, telecommunications systems, factory information systems, and home information systems as critical fools in managing and monitoring their global businesses. As the resources, scope, and criticality of the "information commodity" grow, more senior managers are being forced to concentrate on better ways of planning for rapid changes to capitalize on new opportunities and reduce their risks. The conclusions and recommendations presented in this article are based on an examination of the actual information resource management (IRM) planning and coordination practices of 25 large multinational corporations as well as an indepth review of two case studies.
|keyword = Strategic planning,Information Resource Management,office automation,telecommunications,multinational coordination practices,key requirements,key ideas,framework and components,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Chargeback Systems and User Involvement in Information Systems - An Empirical Investigation'''
{{header}}
{{article
|author= Margrethe H. Olson,Blake Ives,
|source= MIS QUARTERLY
|year= 1982
|abstract = Computer chargeback systems are installed to meet various data processing objectives. One objective is to increase user involvement in decisions regarding information systems development and use. Presumably, increasing user involvement will result in more effective information systems. In a field study the authors examine the relationship between various characteristics of a computer chargeback system, the quality of the chargeback system's user interface, user involvement, and user attitudes about information systems services. Suggestions are given both for the practicing information systems manager and for future information systems researchers.
|keyword = Chargeback,information satisfaction,user attitudes,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Human Information Processing in Information and Decision Support Systems'''
{{header}}
{{article
|author= Daniel Robey,William Taggart,
|source= MIS QUARTERLY
|year= 1982
|abstract = This article explores the relevance of human information processing to the development and use of computer based information and decision support systems. Human information processing is related to the biological specializations of the human brain, Basically, the left cerebral hemisphere performs rational, sequential, analytical functions, while the right hemisphere operates intuitively, simultaneously, and holistically. In contrast, the electronic computer performs only logical, sequential operations. The electronic computer is in this sense a model of the left brain and not the right. Three implications emerge from this understanding. First, research using cognitive style to predict decision behavior should include intuitive styles as well as heuristic and analytical styles, even though intuition cannot be modeled in the traditional sense. Second, this article sheds light on the appropriate division of labor between the electronic computer and the human "bio-computer" for various types of organizational decision mailing. Third, information systems should be designed to support the type of processing required by the task, including both right and left hemisphere processes. Information systems which engage both hemispheres of the decision maker are likely to be more useful in complex tasks than those which support only the activities of the logical, left hemisphere.
|keyword = Information systems,cognitive style,human information processing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Computer-Aided Analysis of Office Systems'''
{{header}}
{{article
|author= Bann R. Konsynski,Lynne C. Bracker,
|source= MIS QUARTERLY
|year= 1982
|abstract = Computer-aided support in the analysis and design of office information systems is discussed. The support system described includes a language for description of office practice, analysis and database maintenance systems, and analysis report generation software. The system components are consistent with a general model of office interactions. The objects and relations unique to he office environment are supported in the terminology for description of both manual and automated office activities. Analysis support providing consistency and completeness evaluations and alternative views of the system serve the analysis activity. A case study is briefly overviewed and conclusions are drawn concerning the utility of the tools. The major utility of the tools was perceived by management and analysts to be the application as a dynamic and structured documentation with consistency evaluation.
|keyword = Office systems,computer-aided analysis,requirements specifications,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''SMIS Members: A Membership Analysis'''
{{header}}
{{article
|author= Leslie Ball,Richard Harris,
|source= MIS QUARTERLY
|year= 1982
|abstract = This article discusses the results of a survey of SMIS members. Members were asked to respond to questions regarding demographics, their satisfaction with SMIS services, and the importance of eighteen management issues which MIS management might address. The article presents percentage results of individual questions and interprets the results of those questions. No attempt is made to use any bivariate statistics in this article. All questions asked on the survey are presented in this article.
|keyword = SMIS,issues,needs,SMIS services,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Selection Model for Systems Development Tools'''
{{header}}
{{article
|author= Justus D. Naumann,Shailendra Palvia,
|source= MIS QUARTERLY
|year= 1982
|abstract = Selecting from the many currently available systems development methodologies (SDMs) and development techniques is a difficult problem with economic, technical, and behavioral implications. A quantitative approach to the selection problem is presented. The selection model begins with a definition of a superset of functions expected of a systems development tool Functions are then weighted, using a Delphi approach to achieve acceptable valuations among system managers. Next, each approach under consideration is evaluated with respect to each function desired. After scores are computed for each methodology, economic and qualitative aspects such as training availability and cost can be used to differentiate the highest ranked alternatives. A four-person MBA project team from the Graduate School of Management at the University of Minnesota, with the guidance from the authors, applied the model to a methodology selection problem. In addition to producing) a quantitative ranking of competing methodologies, the approach described furthered understanding of the functions to be performed by the methodologies being considered. It also gained acceptance, admittedly reluctant, of the recommended methodology from managers who strongly advocated their own favorites.
|keyword = Management,systems development,standards,methodology,SDM,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Manager-Analyst Interface in Systems Development'''
{{header}}
{{article
|author= Kate M. Kaiser,William R. King,
|source= MIS QUARTERLY
|year= 1982
|abstract = The user-analyst liaison function in system development was studied in thirty-eight large firms using a structured interviewing process. The study revealed that the most prevalent form of liaison is still the traditional systems representative and that the liaison function is generally performed much less formally than the IS literature would suggest. However, a trend toward greater formalism was detected. Other aspects of the liaison function, such as perceptions of the role, relevant career paths, and attitudes toward the function were also studied.
|keyword = Information Analyst,liaison,systems development,project teams,user involvement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Multiple Criteria Approach for Evaluating Information Systems'''
{{header}}
{{article
|author= John S. Chandler,
|source= MIS QUARTERLY
|year= 1982
|abstract = An information system can be viewed as a symbiotic relationship between the users of the system and the system itself. Ideally, an information system should be evaluated with equal consideration given to both user constrains and to system constraints. The approach described in this article provides the analyst with a framework for gaining insight into information system performance from both user and system viewpoints by establishing a causal relationship between user goal attainment and system activity. This approach produces not only measures of current performance, but also predictive measures of future performance. The approach is based on a multiple goal programming formulation of the information system design evaluation problem. This article presents an overview of the formulation and its interpretation. The focus is on the analysis of an example system facilitated by this approach. A discussion of the applicability of the approach concludes this article.
|keyword = MIS evaluation,goal programming,multiple criteria evaluation,simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Graphical User Interfaces for Business Information Systems'''
{{header}}
{{article
|author= Blake Ives,
|source= MIS QUARTERLY
|year= 1982
|abstract = Proponents of computer graphics foresee them having a dramatic impact on decision maker productivity. The availability of inexpensive graphic computer technology permits organizations to rely extensively on graphics for standard information presentations. Despite the claims of the proponents, however, there is little substantial evidence linking the use of graphics with increased management productivity and/or higher quality decisions. Moreover, the use of powerful graphics capabilities by end users, unskilled in graphics design, presents the likelihood that information presentations will suffer rather than improve as computer generated graphics grow in popularity. This article discusses typical and atypical applications of computer graphics for presenting business information. Existing evidence relating the use of graphics with improvements in user productivity is discussed. Much of the article is focused on computer graphics design within the organization including: who should do design, the conceptual foundations of good graphics design, and a set of guidelines and cautions applicable to the design of quality graphics. The article concludes with a list of suggested research topics.
|keyword = User interface,interface,graphics,human information processing,spatial management,ICONS,color,graphic design,dual brain,MIS,graphics research,business charts,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''New Information Technology and Organizational Culture'''
{{header}}
{{article
|author= Margrethe H. Olson,
|source= MIS QUARTERLY
|year= 1982
|abstract = Predictions of the effects of office automation on organizations vary widely. This article focuses on changes in individual work patterns, management control, and organizational structure that may occur as a result of implementation of office technology. The most significant change predicted is that organizations will no longer be limited by a central office work environment operating between the traditional office work hours of nine and five. Computer and communications technology will facilitate the relaxing of these physical constraints as necessitated by societal and economic pressures. Relevant research to date regarding the effects of the new technology on organizational behavior is reviewed. Management guidelines for preparing for the coming changes are included.
|keyword = Office automation,telecommunications/transportation tradeoffs,remote work,telecommuting,electronic mail,professional work station,satellite work center,remote management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Inter-Organization Information Sharing Systems'''
{{header}}
{{article
|author= Stephanie Barrett,Benn Konsynski,
|source= MIS QUARTERLY
|year= 1982
|abstract = The authors discuss concepts underlying the growth of Inter-Organizational Information Sharing Systems (IS'). A classification scheme is presented and the issues of cost commitment, responsibility, and complexity of the operating environment are discussed. Organizational impacts associated with IS "participation are also examined.
|keyword = Distributed information systems,inter-organizational systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Word Processing in a Major Corporation: Policies, Plans, and Management'''
{{header}}
{{article
|author= Thomas W. West,Carolyn J. Mullins,
|source= MIS QUARTERLY
|year= 1981
|abstract = At Indiana University in the late 1970's, the demand for word processing, the burden on the University's text processors, and the variety of commercial equipment demanded coordination. In 1978 a task force studied and made recommendations on the future of word processing throughout the corporation. As a result of these efforts, the Office of Information and Computer Services (OICS) assumed leadership and developed a small consulting team to assist users in choosing and installing word processing systems. By 1980 the consultants' workload and the existence of perceived needs for $ 1.8 million in word processing products demanded a more systematic and encompassing strategy. Therefore, OICS formed a temporary organization, the Office Systems Group, to devise objectives, policies, and implementation strategies for the 1980's. This team needed to learn what questions to ask and how to find answers. Furthermore, because modern office technologies will not work unless people accept them, the group had to deal with the social dynamics of office units. The group did a technical study of available products and developed a self-study method to determine office needs. Today the University's policies and strategy foster compatible systems, enable adequate maintenance, and promote staff mobility. The products chosen meet the needs of all users - teachers, students, clerical staff, professional staff, and administrators.
|keyword = Data processing,future office,information systems,management information systems,management policies,MIS management,modern office technologies,office automation,office management,office systems management,photo-composition,techno-resistance,text management,word processing,user support,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''An Approach to Structured MIS Development'''
{{header}}
{{article
|author= Ashok Shenolikar,
|source= MIS QUARTERLY
|year= 1981
|abstract = Toward the beginning of 1980, PRO Electronics began developing a Management Information System (MIS) which would monitor the test activity at an Automatic Test Equipment (ATE) Station. This article describes the implementation of a novel MIS development strategy and explains the circumstances that influenced the decision to prepare a user's manual at the beginning of the system development cycle rather than at the end. The article further describes the management strategy of creating a Systems Engineering Department to exercise centralized project control and emphasizes how the generation of Activity Description Charts, and Hierarchy plus Input-Process-Output (HIPO) documentation during system development affects the productivity of project personnel by: (a) clearly understanding their individual responsibilities, and (b) knowing the relationship of their work to the overall project development goals.
|keyword = Automatic Test Equipment,Unit Under Test,Hybrid Test System,weapon system assemblies,Maintenance Management Information System,Hierarchy plus Input-Process-Output,programmer productivity,user's manual,system definition,user acceptance,user satisfaction,system development methodology,activity description chart,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Decision Support Systems: An MIS Manager's Perspective'''
{{header}}
{{article
|author= Robert K. Vierck,
|source= MIS QUARTERLY
|year= 1981
|abstract = Any discussion of Decision Support Systems must of necessity include the MIS manager. It is crucial that the MIS manager understand the need for and the role of DSS in the full context of Information Resource Management, if a workable DSS environment is to be created. First, to facilitate this understanding, the need for DSS is established by examining the effectiveness of data processing in meeting organizational goals. Business Problems and opportunities are considered in the light of typical EDP performance and a gap is identified. DSS is proposed to bridge this gap. Then decision support systems are contrasted with traditional data systems. This is accomplished by defining characteristics and objectives of classical data processing, MIS and DSS. A three-dimensional view portrays their different but complementary functions. Next, the issue of how to fit decision support systems into an organization's information resource architecture is addressed. A conceptual organization is drawn for DSS, including office, process, classical, and telecommunications support systems. Finally, a plan of action is presented.
|keyword = Decision support systems,MIS manager's view of DSS,information resource management,EDP performance,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Manager or Technician? The Nature of the Information Systems Manager's Job'''
{{header}}
{{article
|author= Blake Ives,Margrethe H. Olson,
|source= MIS QUARTERLY
|year= 1981
|abstract = The role of the information systems manager has evolved in twenty years from that of a technician managing a relatively unimportant service function into that of a vice presidential-level, general manager whose department can substantially impact the entire organization. In this article we trace, by example, the historical evolution of the job, and through an observational study of six information systems managers, examine the position today. The analysis includes the daily activities of the managers, the nature of the oral contacts that constitute 76% of their day, and other notable observations. The information systems manager's role is depicted as one of coordinator, motivator, and planner, with a cadre of experts, both internal and external, who provide technical expertise.
|keyword = Managing computing,observing managers,information resource management,administration of computer centers,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Firm Size and the Characteristics of Computer Use'''
{{header}}
{{article
|author= William H. DeLone,
|source= MIS QUARTERLY
|year= 1981
|abstract = This article studies the proposition that firms of different sizes manage their computer operations differently, and it seeks to determine whether small firms face special circumstances in their uses of computers. The author presents several hypotheses which associate computer related variables with differences in firm size. These hypotheses are tested using data collected from seventy-four Los Angeles manufacturing firms of various sizes. The results indicate that selected computer-use characteristics vary with firm size. These research findings and their implications are discussed.
|keyword = Firm Size,small vs. large,computer use,data processing management,small business computer systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating Information System Effectiveness - Part II: Comparing Evaluator Viewpoints'''
{{header}}
{{article
|author= Scott Hamilton,Norman L. Chervany,
|source= MIS QUARTERLY
|year= 1981
|abstract = Evaluations of Management Information System (MIS) tend to be subjective and are influenced by the perceptions of system objectives, as well as the experiences with system performance in accomplishing organizational objectives. Consequently, the assessments of MIS effectiveness are often controversial and can be sources of disagreement and conflict between different functional groups involved in MIS implementation - users, MIS development, internal audit, and top management personnel. This article, the second of two parts, describes and compares the evaluator viewpoints on system effectiveness for these various functional groups. The first part, which appeared in the previous issue of the MIS Quarterly (Volume 5, Number 3) presented a conceptual hierarchy of system objectives and compared various approaches for evaluating accomplishment of objectives.
|keyword = Management information systems,MIS,evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Managers, Computer Systems, and Productivity'''
{{header}}
{{article
|author= Franz Edelman,
|source= MIS QUARTERLY
|year= 1981
|abstract = The work reported in this article concerns the development and application of a radically new system architecture. This has been found, through controlled experimentation, to have substantial impact on managerial effectiveness and on the productivity of the firm's supporting white collar labor force. The article describes the motivation which first gave rise to these efforts and the solution developed to address the motivating problem. It describes a general model of the "cost of information" concept and its implications on one specific company. Finally, it reports an experiment performed to test the basic hypothesis, the results achieved, and the lessons learned from the total experience. The implications of these findings appear to be far-reaching. They affect dramatically the concept of "system efficiency" currently in vogue in most places, the design process of computer based systems in general, and the emphasis on applications software, especially with regard to the use of non-procedural languages. In the final analysis, it affects the outlook and activities of the information professional of the future.
|keyword = Decision support systems,cost/benefit analysis,productivity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Organizational Strategies For Personal Computing In Decision Support Systems'''
{{header}}
{{article
|author= Richard D. Hackathorn,Peter G. W. Keen,
|source= MIS QUARTERLY
|year= 1981
|abstract = This article suggests a strategy for using Personal Computing to provide decision support to individuals within an organizational context. The growing phenomenon of personal computing is recognized as a valid technological base for providing decision support to individuals. However, the challenge is the exploitation of this capability within an organizational context. In Particular, personal computing raises many management control issues. The article suggests a useful distinction between personal decision support and organizational decision support. This distinction is critical in formulating a strategy for personal support within the organizational context.
|keyword = Decision support systems,personal computing,organizational theory,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Golden Scope Syndrome, the Availability Effect, and MIS'''
{{header}}
{{article
|author= Lawrence F. Young,
|source= MIS QUARTERLY
|year= 1981
|abstract = Some underlying causes of increasing costs in the health cam field and some of their proposed remedies are examined for relevance to the practice of MIS. Conflicting values between specialists and administrators have been described and illustrated in health cam by behavior patterns called "The Golden Scope Syndrome" and The Availability Effect." These are seen by the author as working in similar ways in computer using organizations. Underlying organizational factors and ways to bring specialist goals and organizational objectives into alignment are discussed.
|keyword = MIS cost control,MIS management,professional values,MIS and organizational behavior,computer specialist's behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Structured Systems Planning'''
{{header}}
{{article
|author= Jim Highsmith,
|source= MIS QUARTERLY
|year= 1981
|abstract = Information systems planning is of growing interest to many MIS practitioners. This article postulates that one key element missing from most literature on systems planning is the transition from a strategic plan to an executable, operational plan. The article then describes the application of "structured" tools and methodology to assist in improving this transition task. The process and the examples explained in the article were taken from the author's experience in implementing them while at Oglethorpe Power Corporation in Atlanta, Georgia.
|keyword = Systems planning,structured techniques,strategic planning,operational planning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evaluating Information System Effectiveness - Part I: Comparing Evaluation Approaches'''
{{header}}
{{article
|author= Scott Hamilton,Norman L. Chervany,
|source= MIS QUARTERLY
|year= 1981
|abstract = While the use and costs of Management Information Systems (MIS) have become highly visible, little attention has been paid to assessing and communicating system effectiveness. Evaluation of system effectiveness is difficult due to its multidimensionality. its quantitative and qualitative aspects, and the multiple. and often conflicting, evaluator viewpoints. This article provides an overview of what system effectiveness means and how it should be measured. It is the first of two articles to appear in consecutive issues of the MIS. Quarterly. Starting with a definition of system effectiveness, this article examines evaluation of system effectiveness in terms of a conceptual hierarchy of system objectives. The hierarchy is used to discuss problems in, and recommendations for, evaluating system effectiveness, and to compare MIS evaluation approaches. The second article characterizes and compares the evaluator viewpoints on system effectiveness for decision makers in several functional groups involved in MIS implementation - user, MIS, internal audit, and management. The second article recommends several MIS evaluation approaches for incorporating multiple dimensions and multiple evaluator viewpoints into evaluations of information system effectiveness.
|keyword = Management information systems,MIS,evaluation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Nature of Organizational Decision Making and the Design of Decision Support Systems'''
{{header}}
{{article
|author= George P. Huber,
|source= MIS QUARTERLY
|year= 1981
|abstract = Today's Decision Support Systems (DSS) are almost invariably designed to function in rational, or rationalized, decision making environments. Many organizational environments, such as political environments or garbage can environments, are more accurately portrayed, however, with models other than the Rational Model. Can DSS be useful in such environments? What are the boundary conditions for the application of DSS? These are the questions addressed in this article.
|keyword = Decision support systems,information requirements,decision making,organizational behavior,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Data Processing Control: A State-of-the-Art Survey of Attitudes and Concerns of DP Executives'''
{{header}}
{{article
|author= Allan G. Merten,Dennis G. Severance,
|source= MIS QUARTERLY
|year= 1981
|abstract = Organizations continue to increase their use of computers in strategic, tactical, and operational information systems. During the last few years, corporate and data processing management have often expressed concern over the potential for computer system failure and abuse. This article presents the results of a survey of data processing executives which addressed both the characteristics and quality of data processing control systems. The survey was performed as a component of a comprehensive study of the state-of-the-art of internal control in U.S corporations.
|keyword = Data processing,control,internal control,Foreign Corrupt Practices Act,auditing,disaster plans,system life cycle,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Systems Development Dilemma - A Programming Perspective'''
{{header}}
{{article
|author= Jack Ewers,Iris Vessey,
|source= MIS QUARTERLY
|year= 1981
|abstract = Information Systems (IS) is facing a dilemma: software is absorbing an ever-increasing proportion of the total IS budget while maintenance is absorbing an ever-increasing proportion of the software budget. In the not too distant future, unless this trend is arrested, or reversed, nearly all software resources may be required for maintenance. There are certain projected developments which give hope for the future though not in the short term. In the meantime IS management must use other approaches, approaches that are available today, but that have been adopted very slowly. This article discusses one viable approach the use of automated programmer productivity tools. It presents a classification of programmer productivity tools presently on the market and outlines the ways in which each class addresses productivity. It also discusses the important considerations IS management must make when evaluating the adoption of such tools.
|keyword = MIS management,MIS software,programming productivity,software development tools,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Key Recurrent Issues in the MIS Implementation Process'''
{{header}}
{{article
|author= Michael J. Ginzberg,
|source= MIS QUARTERLY
|year= 1981
|abstract = Previous analyses of the MIS implementation process have focused on the conduct of individual process stages as explanatory of successful or unsuccessful implementation. Process stages, however, involve multiple issues, and it is unlikely that all issues are equally important for good performance at a stage. This article attempts to identify generic implementation issues those issues which arise at many process stages. An empirical analysis identified six such issues, and suggested that three of them were quite important for differentiating between successful and unsuccessful MIS implementations.
|keyword = MIS development,MIS implementation,organization change,user involvement,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Value Analysis: Justifying Decision Support Systems'''
{{header}}
{{article
|author= Peter G. W. Keen,
|source= MIS QUARTERLY
|year= 1981
|abstract = Managers face a dilemma in assessing DSS proposals. The issue of qualitative benefits is central, but they must find some way of deciding if the cost is justified. A general weakness of the cost-benefit approach is that it requires knowledge, accuracy, and confidence. about issues which for innovations are unknown, ill-defined, and uncertain. The benefit of a DSS is the incentive for going ahead. The complex calculations of cost-benefit analysis ale replaced in value analysis by rather simple questions about its usefulness.
|keyword = Decision support systems,value analysis,innovation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''The Management of Information Resources - A Challenge for American Business'''
{{header}}
{{article
|author= Franz Edelman,
|source= MIS QUARTERLY
|year= 1981
|abstract = Managing the resources - people, machines, technologies, and ideas - required to deal with business information has become a task so large and complex that a new perspective is needed for both management and professional communities. This article develops a framework for a better understanding of what needs to be done and why, based on a historical view of the information management arts and sciences. This framework is also based on a resulting organizational structure with the potential of avoiding many difficulties traditionally encountered in American industry. Several critical factors are identified and supported which must be addressed in order for the enterprise to derive adequate returns from its large and growing investment in the resources of information management. A profile of the information professional of the future is developed as the most critical prerequisite for meeting the information challenges of the 1980's, and some implementation strategies are recommended for the successful launching of the types of information systems which will have genuine and measurable impact on the firm's productivity.
|keyword = Information resource management,data orientation,future framework,future information professional,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Online Computer Auditing Through Continuous and Intermittent Simulation'''
{{header}}
{{article
|author= Harvey S. Koch,
|source= MIS QUARTERLY
|year= 1981
|abstract = A new computer auditing technique, called Continuous and Intermittent Simulation (CIS), is introduced. It has been specifically designed as a compliance auditing technique for timesharing systems that can be used to audit internal controls. CIS is an auditing technique that simulates the instruction execution of the application at the time the application is processing a transaction. All data and input to the application is accessible by and shared with the simulation. This means that the simulation is notified about each transaction that is entered to the application and accesses to the database by the DBMS. It is not necessary for all transactions to be audited in order to have the capability of performing online auditing. Before any updates are made to the database, or before any output is returned to the users, the simulation can verify the results by executing the appropriate instructions that evaluate the internal controls of the application. If an inconsistency is found, all pertinent information about the system's status can be put into the exception log. The simulation can then choose to use the results computed by the application or by the simulation, or choose not to use any of the results, as if there was no transaction.
|keyword = Computer auditing,internal auditing,online auditing,internal controls,parallel simulation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Observing Organizational Environments: A Systematic Approach for Information Analysts'''
{{header}}
{{article
|author= Kenneth E. Kendall,Julie E. Kendall,
|source= MIS QUARTERLY
|year= 1981
|abstract = Information Analysts observe the elements of an organization in order to gain information unavailable through interviewing and the investigation of hard data. In the past he process of observation has been intuitive at best. This article describes and develops a systematic methodology for analyzing the internal organizational environment. The approach is based on a framework used in film criticism called mise-en-scene analysis. Seven major concrete and abstract elements which influence organizational decisions are identified: office lighting and color; office design, space, and location; clothing of decision makers; individual and group decision making; abilities of decision makers; attention to multiple objectives; and cognitive maps of decision makers. The systematic framework for observation developed in this article is an alternative to the common sense approach to observation. The major advantage of the mis-en-scene approach is that it allows the Information Analyst to classify, document, and interpret important factors which usually remain at the subconscious level.
|keyword = Information analyst,information analysis,observation,organizational environment,mise-en-scene,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Behavioral Analysis for Planning MIS Implementation'''
{{header}}
{{article
|author= Mary Lippitt Nichols,
|source= MIS QUARTERLY
|year= 1981
|abstract = Planning for a successful MIS implementation requires understanding the effects of the introduction of a new system or; the work lives of potential adopters. The effects can be assessed by the MIS practitioner/change agent if the agent will first obtain a thorough and specific understanding of the task activities performed by potential adopters, the interactions they engage in as they perform their tasks, and their preferences for, or feelings about, those activities and interactions. An interview based approach to gathering this data is described, and a short description of an implementation in which the approach was used is provided.
|keyword = Implementation,potential adopters,behavioral data,task activities,interactions,role preferences,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Framework for the Development of Decision Support Systems'''
{{header}}
{{article
|author= Jr. Ralph H. Sprague,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article proposes a framework to explore the nature, scope, and content of the evolving topic of Decision Support Systems (DSS). The first part of the framework considers (a) three levels of technology which have been designated DSS, (b) the developmental approach that is evolving for the creation of a DSS, and (c) the roles of several key types of people in the building and use of a DSS. The second part develops a descriptive model to assess the performance objectives and the capabilities of a DSS as viewed by three of the major participants in their continued development and use. The final section outlines several issues in the future growth and development of a DSS as a potentially valuable type of information system in organizations.
|keyword = Decision Support Systems,development approach,performance objectives,capabilities,issues,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Planning, Critical Success Factors, and Management's Information Requirements'''
{{header}}
{{article
|author= Malcolm C. Munro,Basil R. Wheeler,
|source= MIS QUARTERLY
|year= 1980
|abstract = Focusing on a manager's goals and critical success factors has been advocated as an approach to defining senior and middle managers' information requirements. In this article a field study is described in which the planning processes in a corporation were used as a mechanism for identifying goals, critical success factors, and performance measures and standards, i.e., information requirements for managerial control. A general approach generated from the field study is described and the advantages and disadvantages of the approach are analyzed.
|keyword = Information requirements,information analysis,critical success factors,systems analysis,management control,planning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Auditability of Software: A Survey of Techniques and Costs'''
{{header}}
{{article
|author= Ira R. Weiss,
|source= MIS QUARTERLY
|year= 1980
|abstract = Numerous techniques have been identified to assist EDP auditors in accomplishing audit objectives. Within this realm a major issue of interest is identifying methodologies to monitor the reliability and integrity of production software. This article presents a number of available techniques to audit software, and evaluates each on the basis of development cost, operational cost, appropriate timing of usage, and effectiveness.
|keyword = EDP auditing,auditability of software,software integrity,software reliability,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MANAGING THE IMPLEMENTATION OF STANDARDIZED COMPUTER BASED SYSTEMS'''
{{header}}
{{article
|author= Lee L. Gremillion,
|source= MIS QUARTERLY
|year= 1980
|abstract = Systems development and implementation are traditionally approached as a process of designing and fitting a tool to its user's needs. Cases arise, however, due to economic constraints or the desire to standardize, in which an organization directs its subunits to implement a system "as is." In such a case, organizational change is mandated and must be managed as such. Certain units of the U. S. Forest Service have been successful in developing a process to manage the implementation of a standardized program planning system across multiple national forests. The process includes a systematic evaluation of the knowledge, skills, and/or attitude changes indicated for specific individuals in the implementing units. This in turn allows the selection of the most appropriate techniques to foster the changes. The results of these Forest Service efforts have been encouraging and offer some insights into the process of managing system implementation.
|keyword = Systems implementation,governmental systems,user behavior,implementation management,organizational change,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A Systematic Approach Toward Assessing the Value of an Information System'''
{{header}}
{{article
|author= Niv Ahituv,
|source= MIS QUARTERLY
|year= 1980
|abstract = A multiattribute utility approach is adopted to assess the value of an information system. Various economic analyses of the value of information are reviewed and the conceptual problems regarding the definition of this value and some measurement difficulties are discussed. A list of possible utility attributes is proposed for a reporting system value assessment, add for each attribute a measure and a utility function is suggested. Some techniques which constitute a joint utility function are presented, accompanied by two examples. A real case of minicomputer selection is given in order to illustrate the structured approach.
|keyword = Information evaluation,system selection,information economics,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INCORPORATING DATA DICTIONARY/DIRECTORY AND TEAM APPROACHES INTO THE SYSTEMS DEVELOPMENT PROCESS'''
{{header}}
{{article
|author= Phil Semprevivo,
|source= MIS QUARTERLY
|year= 1980
|abstract = Between late 1977 and April 1979 the State University of New York at Albany was able to successfully move from completely manual systems to a very contemporary and integrated online Financial and Accounting Management Information System (FAMIS). The development of FAMIS involved the application of innovative organizational and technological approaches which contributed significantly to the success of the project. For example, the University utilized a systems, user, and management team in its design of FAMIS. Also, the technological approach involved use of a Dictionary/Directory Facility (DDF) developed by the University's Administrative Systems group. The use of the DDF proved to reduce significantly the amount of conventional computer programming required to develop FAMIS. Furthermore, it proved to be a valuable aid in facilitating systems, and user and management interaction.
|keyword = Teams,dictionary/directory,data directories,data dictionaries,systems development techniques,financial and accounting systems,alternatives to programming,organizational strategies,design strategies,system development teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''DEVELOPING A COST JUSTIFIED NATION-WIDE COMMUNICATIONS NETWORK: A SYSTEMS APPROACH'''
{{header}}
{{article
|author= George M. Perry,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article traces the efforts of a development team seeking to define and install a timely cost-effective communications network in a situation that promised little in potential hard dollar savings. The "Information Systems" approach selected involved some newer technologies in combination with some older, almost prosaic, techniques. The article shows how value-added networks, non-simultaneous communications, voice/data sharing, ASCI protocols and the electronic PBX were combined to produce a data network at literally no incremental cost to the corporation.
|keyword = Telecommunications planning,value added networks,information systems management,voice/data sharing,electronic mail,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Evolving and Implementing a Worldwide Management Information System (IMS/MIS)'''
{{header}}
{{article
|author= Arnold M. Kneitel,
|source= MIS QUARTERLY
|year= 1980
|abstract = Du Pont's international business is becoming a more important part of its worldwide total. The Polymer Products Department (PPD), due to the merging of former independent departments, represents about 25% of Du Pont's worldwide revenues, and has the largest export sales section several hundred million dollars annually. As PPD's product lines increased, managers with geographic responsibilities must cope with an increasingly complex array of detailed product, end use, sales, and forecast information. These changes require a more efficient way to organize and access business information. This article describes the development of a management information system that could cope with an anticipated rise in meeting new information needs for both sales and order processing. This article explores the operational philosophy that permitted the rapid development of the IMS/MIS at a relatively low cost. It explores the techniques used to interact with users at all organizational levels and outlines a plan to implement the IMS/MIS at headquarters and subsidiary locations around the world.
|keyword = Implementation,system development,evolving systems,worldwide MIS,management interaction,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AMANDA: A Computerized Document Management System'''
{{header}}
{{article
|author= Ronald Schwartz,Joan Fortune,Julian Norwich,
|source= MIS QUARTERLY
|year= 1980
|abstract = AMANDA, an acronym for Automated Management of Document Access, is a completely online intraorganizational document based retrieval system. It is both a reference system, in that an indexed summary or surrogate of a document is created and stored for retrieval by computer, and a full text system, and in that the complete document can be entered, searched, and retrieved if needed. AMANDA is capable of handling a wide variety of document types (e.g., letter, memo, report, contract, reprint, book) by tailoring an organizational communication. A system which widens the channels of communication and helps to integrate disjoint functions provides large and complex organizations the means of over coming a basic operational deficiency. AMANDA is currently in active use within several departments at American Critical Care, a division of American Hospital Supply Corporation. Departments at other divisions access the system from remote sites.
|keyword = User orientation,applications development,decision support system,interactive software,online system,database systems,management information system,information systems design,information systems,document management,records management,filing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''TOWARD A COMPREHENSIVE FRAMEWORK FOR MIS RESEARCH'''
{{header}}
{{article
|author= Richard L. Nolan,James C. Wetherbe,
|source= MIS QUARTERLY
|year= 1980
|abstract = A comprehensive framework for Management Information Systems (MIS) that attempts to define the domain of MIS research is proposed. The framework draws extensively from existing MIS frameworks and organizational theory concepts and definitions. Validation of the framework is accomplished by categorizing a cross-section of MIS literature within the framework.
|keyword = Management information systems,MIS research,MIS frameworks,MIS literature,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''BUILDING AN EFFECTIVE INFORMATION SYSTEMS FUNCTION'''
{{header}}
{{article
|author= William B. Miller,
|source= MIS QUARTERLY
|year= 1980
|abstract = The small number of truly successful Information Systems departments is well known. Part of the problem clearly lies in the management and direction of the Information Systems department in an organization. The organizations with successful Information Systems departments appear to have some elements in common. These elements include defining the department's role and responsibilities, structuring its basic approach, establishing management direction, developing a plan, and development of good people. This article describes these common elements in detail, with emphasis on case studies. Alternative means of achieving objectives are described. In summary, a conceptual framework for building an effective Information Systems department is established.
|keyword = Information systems,MIS management,MIS planning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''PROGRAMMER AND ANALYST TIME/COST ESTIMATION'''
{{header}}
{{article
|author= Izak Benbasat,Iris Vessey,
|source= MIS QUARTERLY
|year= 1980
|abstract = The ability to estimate the personnel time and costs required for the completion of programming and systems projects is an important managerial tool for the information systems department. This article presents a survey of the estimation techniques found in the literature by describing each technique and discussing its strengths and weaknesses. Some empirical evidence on how the various program and programmer/analyst characteristics affect project time and cost are also reported.
|keyword = Systems development,time and cost estimation,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MANAGEMENT OF LARGE SOFTWARE DEVELOPMENT EFFORTS'''
{{header}}
{{article
|author= Robert W. Zmud,
|source= MIS QUARTERLY
|year= 1980
|abstract = The high development and maintenance costs, and the late delivery experienced by many organizations when developing large software systems is well documented. Modern software practices have evolved to overcome many of the technical difficulties associated with software development. To a large extent, however, the high costs and schedule slippages can be traced to management, not technical, deficiencies. This article develops an approach for managing the software development effort that exploits the benefits of modern software practices in staffing, planning, and controlling software development.
|keyword = Software development,software engineering,life cycle,project management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE RELATIONSHIP BETWEEN ORGANIZATIONAL CHARACTERISTICS AND THE STRUCTURE OF THE INFORMATION SERVICES FUNCTION'''
{{header}}
{{article
|author= Margrethe H. Olson,Norman L. Chervany,
|source= MIS QUARTERLY
|year= 1980
|abstract = Although much has been written about the centralization versus decentralization of information services, relatively little is known about how the structure of an organization influences the level of centralization or decentralization. This paper presents the results of a study which examined the organizational context of the information services function. The authors attempted to identify those organizational characteristics that are associated with the structure of the information services function. It was found that the organizational environment does have an influence upon the level of decentralization of information services. Few organizational characteristics, however, were found to influence the structure of information services consistently across all organizations. Further research relating organizational form to its impact on information services is suggested.
|keyword = Centralization/decentralization,information services organization,organization structure,organization contingencies,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INTEGRATION OF ALGORITHMIC AIDS INTO DECISION SUPPORT SYSTEMS'''
{{header}}
{{article
|author= L. C. Barbosa,R. G. Hirko,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article reports on an application for which a districting algorithm was integrated into an existing decision support system. From this experience a set of guidelines was derived which may be helpful to the system designer who has a similar task. These guidelines aim to improve the interface, feedback, flexibility, and control of the integrated system.
|keyword = Decision support systems,algorithmic aids,aids for decision making,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MIS DESIGN: A CONTINGENCY APPROACH'''
{{header}}
{{article
|author= Richard J. Schonberger,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article identifies six MIS design approaches, ranging from no user involvement to considerable user involvement. It also examines the justification for their use under different conditions. The six approaches are merged with Gorry and Scott Morton's [10] MIS framework and Simon's [23] classes of decision making to create a contingency model for MIS design. The contingency model provides for MIS design leadership to be dependent upon type of decision making. A broad view of MIS design is taken wherein executives or even various stakeholder groups may assume active leadership where warranted by the circumstances. With design leadership correctly placed, project purpose may be more carefully identified and design activities may be properly channeled to meet those objectives.
|keyword = MIS development,MIS design,contingency theory,systems analysis,information analysis,user involvement,information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''IMPACT OF ORGANIZATIONAL MATURITY ON INFORMATION SYSTEM SKILL NEEDS'''
{{header}}
{{article
|author= Izak Benbasat,Albert S. Dexter,Robert W. Mantha,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article presents the results of a study which analyzes skills perceived as useful by information systems (IS) managers and systems analysts in IS organizations of different levels of maturity. These IS skills were examined under two major subgroups of generalist, managerial and specialist/technical skills as well as under more detailed categories of organizations, people, society, systems, computers, and models skills. Generalist, i.e., organizational and people skills, were rated highest. Based on these rankings, which deviated little between managers and analysts and across the maturity spectrum, recommendations concerning graduate IS curriculum are suggested.
|keyword = Information systems curriculum,information systems maturity,information systems stage hypothesis,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''INFORMATION SYSTEMS SKILL REQUIREMENTS: A SURVEY'''
{{header}}
{{article
|author= Paul H. Cheney,Norman R. Lyons,
|source= MIS QUARTERLY
|year= 1980
|abstract = This study reports on some of the employment trends and skill requirements in the information systems area as seen by the information systems managers of 32 of the nation's largest organizations. The data was gathered via personal interviews and questionnaires. We believe the results to be quite interesting in terms of both work force projections and the perceived job skills that are required for programmers, systems analysts, and data processing managers.
|keyword = Information systems management,skills,programming,systems analyst,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPACT OF SOME MIS DESIGN VARIABLES ON MANAGERS' EVALUATIONS OF SUBORDINATES' PERFORMANCES'''
{{header}}
{{article
|author= Michael Firth,
|source= MIS QUARTERLY
|year= 1980
|abstract = This article reports the results of an experimental analysis of the impact of MIS design variables on managers evaluations of subordinates' performances. The experiment involved examining whether different ways of reporting a specific standard cost accounting situation resulted in differences in managers' evaluations. When actual costs differed significantly from standard, it was found that the form of the accounting report did have an impact on evaluations. The presentation and the frequency of the report were found to be important variables in MIS design which affected managers' evaluations.
|keyword = Management information systems,performance evaluation,systems design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STRATEGIC, TACTICAL, AND OPERATIONAL PLANNING AND BUDGETING: A STUDY OF DECISION SUPPORT SYSTEM EVOLUTION'''
{{header}}
{{article
|author= Richard K. Davis,
|source= MIS QUARTERLY
|year= 1979
|abstract = This paper describes the evolution of a planning and budgeting decision support system. The system supports farm program planning and budgeting within the Agricultural Stabilization and Conservation Service of the U. S. Department of Agriculture. Online interactive techniques are used through the Foresight planning and System 2000 database software. Numerous extensions of these techniques are formulated and comparisons made with industry. The study probes the nature of the system development process in a highly volatile environment, and the psychological motives of the professionals involved. The forces at work are generalized to fit other environments. The results shed light on how other successful systems might be developed employing similar methods and techniques. Future extensions of this system and new scenarios are postulated and generalized in order to provide a basic framework for analysis of key decision support functions.
|keyword = Decision support system,planning and budgeting,farm programs,Department of Agriculture,interactive software,on-line systems,database systems,management information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE EXECUTIVE INTELLIGENCE SYSTEM AS A DESIGN STRATEGY'''
{{header}}
{{article
|author= Jr. F. Parker Fowler,
|source= MIS QUARTERLY
|year= 1979
|abstract = The failure of the MIS idea to achieve its ultimate goal of informing managers rests in the single wrong assumption that the manager can define his information needs prior to the time for decision making. If the assumption were true, then management could be implemented by automata. Because it is not true, the conventional MIS maximum capability is to produce hackneyed status reports based upon technocrats' perceptions of what the manager needs; its minimum capability is coughing up answers to irrelevant, unasked questions. This spectrum from impotence to mediocrity cannot be usefully extended to meet managements' needs because the design strategy is wrong. A proper design strategy for providing managers with relevant information for choosing courses of action recognizes the principle of variety reduction, and that human beings have unique and peculiar ways of reducing variety in noisy information circumstances. Assuming that the distinguishing feature, beyond luck, of the successful executive vs. the controller, is creativity, including novel and non-structured uses of information, then any structured reporting system will fail to meet his needs. This article describes the design criteria for and the successful implementation of a system to support decision making in an information rich and volatile environment: the Independent Practice Association form of Health Maintenance Organization. Issues of cost and quality of medical care, the measurement of professional practice, the assumption of risk, etc., combine with an abundance of difficult data processing procedures to make this job a worthy challenge, and its successful implementation a model for complex organizations.
|keyword = Information systems design,health information systems,executive intelligence system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''STABILIZING AN MIS'''
{{header}}
{{article
|author= D. H. McNeil,
|source= MIS QUARTERLY
|year= 1979
|abstract = Technical managers of MIS appear to have only two problems: End user requirements continually evolve; Vendor product offerings are in constant flux. In other words, everything is subject to somewhat unpredictable change from all sides at once. This is the quintessence of unmanageability. The excitement and adventure of the situation are more than offset by the excessive costs of development and maintenance which accrue to those who are trying to provide an MIS of consistently high quality. One company which performs MIS and conventional data processing services for more than 400 hospitals was able to design, implement and maintain a rather complete set of functional interface standards for all software run on its central processing facility. Seven primary functional types were identified and implemented over a five year period with an expenditure of more than 50 person-years of effort. User profile management and user data distribution were the two best developed and most essential standard interfaces. The diversity of client characteristics and of output media which can be handled through the interfaces has made possible an extraordinary corporate growth rate in excess of 25% per year, with a corresponding expansion in client base and MIS services. The management ramifications and the technological outcomes of this MIS stabilizing project are the complementary subjects of this paper.
|keyword = MIS management,standards,information systems,planning,design,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''END USERS AS APPLICATION DEVELOPERS'''
{{header}}
{{article
|author= E. R. McLean,
|source= MIS QUARTERLY
|year= 1979
|abstract = The demand for new or expanded computer-based information systems far exceeds the capacity of present DP organizations to meet this demand. Assuming that a massive expansion of DP personnel is not feasible, one solution is to make existing computer professionals more productive; and efforts in this direction have met with a fair measure of success. Another approach is to allow end users to function as their own developers. In this way, the programmer "middle man" is eliminated and users can create and modify their own applications as the need arises. Not only does this help relieve the DP department's development workload, but it also helps lessen the maintenance load. However, to accomplish this transfer of application development from DP professional to DP user, certain key variables must be understood. This article discusses these variables, including the nature of the development process, a user classification scheme, the scope and orientation of systems, and the respective roles of design and maintenance in the applications development life cycle. In particular, the misunderstood nature of "maintenance" is explored, as well as the potential value of the use of application prototypes. Finally, there is a discussion of specific tools and techniques that may aid the end user in acting as application developer.
|keyword = Applications development,management information systems,end users,DP professionals,systems maintenance,distributed data processing,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''AN ANALYSIS OF THE IMPACT OF DISTRIBUTED DATA PROCESSING ON ORGANIZATIONS IN THE 1980's'''
{{header}}
{{article
|author= Charles K. Davis,James C. Wetherbe,
|source= MIS QUARTERLY
|year= 1979
|abstract = Distributed Data Processing (DDP) is examined from a technological, organizational, and economic perspective. DDP projections are made for the 1980's as are recommendations for capitalizing on and minimizing the risks of this emerging technology.
|keyword = Distributed data processing,database,centralized/decentralized computing,information infrastructure,future EDP,MIS planning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''WHAT IS THE VALUE OF INVESTMENT IN INFORMATION SYSTEMS?'''
{{header}}
{{article
|author= Gerald Matlin,
|source= MIS QUARTERLY
|year= 1979
|abstract = Managers frequently ask, "What is the value of our investment in information systems?" This Question must be answered if business decisions regarding expenditures for information systems are to be made rationally. This article presents a method for developing an answer. Although The Question appears to be simple, the answer is complex. In order to reduce the complexity of the answer, The Question is divided into two parts: "Did I get what I paid for?" and "Was it worth it?" The answers to these questions can be provided through the easy-to-understand management reports defined in this article. The primary issue, in the process recommended in this article, is placing dollar amounts on intangible benefits. Although recognized as "a perilous venture," the author believes that some evaluation process must be used if the primary question of value is ever to be answered adequately. This article discusses and describes evaluation techniques for intangible benefits. The application of the methods and techniques provided in this article can lead to a clearer understanding of the information system investment. This understanding will lead to greater effectiveness in information systems decisions.
|keyword = Information system investment,cost/benefit calculations,management reporting,performance evaluation,systems business objectives,management issues,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE FINANCIAL PLANNING SYSTEM AT LOUISIANA NATIONAL BANK'''
{{header}}
{{article
|author= Jr. Ralph H. Sprague,Ronald L. Olson,
|source= MIS QUARTERLY
|year= 1979
|abstract = In 1974 Louisiana National Bank designed and implemented a Financial Planning System. For the past five years, it has been the primary reporting, analysis, and decision support tool for the Budget Committee and the primary source of financial reports for the Board of Directors. Top management of the Bank believes that, as a direct result of the use of this System, Louisiana National Bank: 1. reversed a decreasing profit trend by increasing profits more than 92% in one year, 2. creatively managed the Bank's liquidity during severe market pressures, 3. restructured the Bank's asset/liability management policies to better align with newly perceived market and investment potential, and 4. sharply reduced the cost and management burden of reporting to regulators. In addition to the quantifiable benefits cited above, use of the System resulted in several other improvements, including 1. a reduction in the amount of time required to meet periodic management reporting requirements, 2. increased understanding of financial reports by the Board of Directors, 3. improved communications between Bank executives and regulatory examiners, and 4. a discipline and structure within which the Budget Committee could establish goals and action plans. The Financial Planning System supports financial decision making at both the tactical and strategic levels. It is an integrated system which includes reporting and analysis of historical data, long and short range forecasting, and modeling.
|keyword = Financial modeling,simulation,forecasting,banking MIS,financial planning,decision support system,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MOTIVATION LEVELS OF MIS MANAGERS VERSUS THOSE OF THEIR EMPLOYEES'''
{{header}}
{{article
|author= J. Daniel Couger,Robert A. Zawacki,Edward B. Oppermann,
|source= MIS QUARTERLY
|year= 1979
|abstract = A national study of key factors for motivation was conducted for three levels of MIS management. Analysis of the results show that MIS managers perceive their jobs to contain all the necessary ingredients for motivation. In two areas, however, problems exist. MIS managers at all three levels perceive supervisory feedback as needing improvement. So do their subordinates, as identified in a previous study. Also, the survey revealed both low social need and high growth need of MIS managers, compared to other managers. These characteristics may inhibit effective communication, both with subordinates and with other managers. Once recognized, the negative aspects of these characteristics may be counteracted, with techniques previously proven in behavioral science.
|keyword = motivation,employee behavior,job design,MIS management,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''MAKING OPERATIONAL LONG-RANGE PLANNING FOR INFORMATION SYSTEMS'''
{{header}}
{{article
|author= Marjorie A. Lyles,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article describes a methodology for making operational a formalized long-range planning process within an information systems (IS) group. The process seeks to develop a framework for identifying the types of activities and responsibilities of the different roles within an IS group.
|keyword = Long-range planning,strategic planning,MIS planning,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A "BALANCED" ORIENTATION FOR THE INFORMATION SYSTEMS MANAGER'''
{{header}}
{{article
|author= William M. Taggart,Valdur Silbey,
|source= MIS QUARTERLY
|year= 1979
|abstract = The orientation of an information systems department is influenced by the emphasis shown in the manager's activities. Each incident in the manager's daily work is another bead strung on a necklace. The choice of what type of bead it should be is up to the manager. Every incident permits a choice of emphasis: either a systems professional or a user community orientation. Recent analyses suggest that a new breed of information systems manager must emerge for advanced systems in organizations. Some of these analyses emphasize the user view without pointing to the continuing need for systems competence. Lest the pendulum swing too far in the opposite direction by ignoring technical aspects, this analysis suggests that the emerging information systems manager must bring a "balanced" systems and user orientation to the position. In so doing, the manager establishes the organizational climate in which the information systems activity proceeds.
|keyword = Information systems department,user orientation,systems professional orientation,stage of systems evolution,systems manager activity,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''THE IMPLEMENTATION PROCESS: A CHANGE APPROACH'''
{{header}}
{{article
|author= Robert W. Zmud,James F. Cox,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article develops an approach to MIS Implementation deemed necessary when a substantial amount of organizational change is intended or expected once a proposed MIS becomes operational. The approach focuses upon the roles adopted by and the interrelations among implementation participants, and the role of education in implementation.
|keyword = Implementation,system development,system life cycle,project management,personnel involvement,personnel training,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''Planning for MIS: A Decision-Oriented Approach'''
{{header}}
{{article
|author= John C. Henderson,Jr. John M. West,
|source= MIS QUARTERLY
|year= 1979
|abstract = The concept of using critical decisions as a basis for defining information needs has been suggested by several authors. This article reports on results of a decision-oriented approach to planning for MIS. A structured group process Is used to generate both critical decisions and information needs for a manufacturing firm. Results indicate that the decision approach Is effective, particularly for organizational units that are line oriented.
|keyword = MIS planning,Information requirements analysis,nominal group technique,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''HEURISTIC DEVELOPMENT: A REDESIGN OF SYSTEMS DESIGN'''
{{header}}
{{article
|author= Thomas Berrisford,James Wetherbe,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article proposes a major modification to the traditional systems development process. Traditional systems development approaches delay the delivery of tangible information system capabilities to users (i.e., query capabilities and decision support models, etc.) until the last stages of systems development. Consequently, it is not until after systems are developed that shortcomings in systems design surface. The authors advocate providing or at least simulating user capabilities early in the systems development process. Such an approach is made possible by the use of an online relational-type Database Management System. Introduction of such user capabilities allows users to interact with the system and heuristically determine information requirements. The authors present behavioral, technical, and operational arguments for a heuristic approach. Also, several case studies of heuristic development are discussed. The heuristic approach to information systems development is a useful concept for MIS managers, systems analysts, and users of computer-based information systems.
|keyword = Systems analysis,systems design,database management,online systems,information systems development,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''CRYPTOGRAPHIC PROTECTION OF COMPUTER-BASED DATA FILES'''
{{header}}
{{article
|author= Thomas J. Murray,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article explores the nature of cryptographic transformation and its application to the protection of data contained in files used by computer-based information systems. It presents some of the basic concepts of cryptography and structures of taxonomy of cryptographic transformations. The implications of computer oriented implementations of cryptographic protection are explored and the significance of such transformations for information systems is discussed.
|keyword = cryptography,data protection,management information systems,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A FRAMEWORK FOR MIS SOFTWARE DEVELOPMENT PROJECTS'''
{{header}}
{{article
|author= Jeffrey H. Moore,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article questions the tacit assumption that computer software development is a homogeneous activity across differing tasks. The author proposes that conventional statistics on the performance of MIS software development groups are too often based upon norms and procedures relevant to groups developing systems-oriented software. To evaluate the statement a simple descriptive framework is presented which distinguishes software developmental attributes for MIS from those of other computer software systems. The framework is validated by data from twenty-four organizations and the implications for management of MIS software development are discussed.
|keyword = Software,MIS,framework,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A USER'S BEHAVIOR TOWARD HIS MIS'''
{{header}}
{{article
|author= Alexander M. Maish,
|source= MIS QUARTERLY
|year= 1979
|abstract = This article describes a study of information system users in four Federal agencies in Washington, D.C., to determine associations between user behavior toward their systems and feelings toward the system staff, management support, and several other factors. The data was collected using a self-administered questionnaire.
|keyword = MIS,attitudes,motivation,impact studies,user behavior,user attitudes,user survey,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
 
---start---
'''A TEAM APPROACH TO MANAGING THE DEVELOPMENT OF A DECISION SUPPORT SYSTEM'''
{{header}}
{{article
|author= William B. Locander,H. Albert Napier,Richard W. Scamell,
|source= MIS QUARTERLY
|year= 1979
|abstract = Decision support systems (DSS) often fail when their development lacks a thorough problem definition and/or an adequate commitment of individuals from the associated functional areas. A team-oriented structure for developing a DSS, like the one described in this article, can reduce such problems by outlining the tasks that managers, operating personnel, and information systems personnel should complete during the DSS's definition, design, and implementation stages.
|keyword = decision support systems,systems develpment,interdisciplinary teams,
}}
{{topics}}
{{theories}}
{{methods}}
{{footer}}
[[Category:article]]
---end---
